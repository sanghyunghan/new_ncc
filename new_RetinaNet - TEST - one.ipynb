{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATE = \"1010_resnet50\"\n",
    "DATE = \"1010_resnet101\"\n",
    "#DATE = \"1010_resnet152\"\n",
    "DATA_TO_TEST = 'all' # 'abn1' or 'abn2' or 'normal'\n",
    "MODEL_EPOCH = '_100ep' ###### 아래 스냅샷 파일과 일치하는지 반드시 확인!!!!!!! ######\n",
    "\n",
    "SCORE_VALUE = 0.5\n",
    "PRINT_BENIGN_DETECTION = False\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from itertools import groupby\n",
    "from itertools import chain\n",
    "\n",
    "result_save_base_path = 'D:/Work/NCC/han_test/results/'\n",
    "#result_save_base_path = '/home/huray/workspace/han_work/han_test/results/'\n",
    "#result_save_path = os.path.join(result_save_base_path, DATE, DATA_TO_TEST + MODEL_EPOCH + '_' + str(SCORE_VALUE))\n",
    "result_save_path = os.path.join(result_save_base_path, DATE, DATA_TO_TEST + MODEL_EPOCH)\n",
    "if not os.path.exists(result_save_path):\n",
    "    os.makedirs(result_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import keras\n",
    "import keras.preprocessing.image\n",
    "from keras_retinanet.models.resnet import custom_objects\n",
    "# from keras_retinanet.preprocessing.coco import CocoGenerator\n",
    "from keras_retinanet.preprocessing.csv_generator import CSVGenerator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def get_session():\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    return tf.Session(config=config)\n",
    "\n",
    "########## 2D list에서 특정 값이 위치한 index return ####################\n",
    "def find(searchList, elem):\n",
    "    for ix, row in enumerate(searchList):\n",
    "        for iy, i in enumerate(row):\n",
    "            if i==elem:\n",
    "                #print('{},{}'.format(ix,iy))\n",
    "                return ix, iy\n",
    "    return -1,-1    \n",
    "        \n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "keras.backend.tensorflow_backend.set_session(get_session())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load RetinaNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Work\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:290: UserWarning: Output \"nms\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"nms\" during training.\n",
      "  sample_weight_mode=sample_weight_mode)\n"
     ]
    }
   ],
   "source": [
    "#resnet50\n",
    "#model = keras.models.load_model('snapshots/1001_resnet50_ep100_with_valid/resnet50_csv_90.h5', custom_objects=custom_objects)\n",
    "#model = keras.models.load_model('snapshots/1001_resnet50_ep100_with_valid/1017_snapshot_resnet50_csv_90_ep100_with_valid/resnet50_csv_19.h5', custom_objects=custom_objects)\n",
    "#model = keras.models.load_model('snapshots/1001_resnet50_ep100_with_valid/1017_snapshot_resnet50_csv_90_ep100_with_valid/resnet50_csv_46.h5', custom_objects=custom_objects)\n",
    "\n",
    "#resnet101\n",
    "#model = keras.models.load_model('snapshots/0928_resnet101_ep100_with_valid/resnet101_csv_59.h5', custom_objects=custom_objects)\n",
    "\n",
    "#resnet152\n",
    "#model = keras.models.load_model('snapshots/0927_resnet152_ep100_with_valid/resnet152_csv_07.h5', custom_objects=custom_objects)\n",
    "#model = keras.models.load_model('snapshots/0927_resnet152_ep100_with_valid/1012_snapshot_resnet152_csv_74_ep100_with_valid/resnet152_csv_57.h5', custom_objects=custom_objects)\n",
    "#model = keras.models.load_model('snapshots/0927_resnet152_ep100_with_valid/1012_snapshot_resnet152_csv_74_ep100_with_valid/resnet152_csv_84.h5', custom_objects=custom_objects)\n",
    "\n",
    "\n",
    "####################################### 이전모델 ###################################\n",
    "#model_list.append('snapshots/0326_more_aug/resnet101_csv_44.h5')   \n",
    "#model_list.append('snapshots/0620_stronger_aug/resnet101_csv_25.h5')\n",
    "model_list.append('snapshots/0525_add_ncc_mass_data_strong_aug/resnet101_csv_29.h5')\n",
    "#model_list.append('snapshots/0620_stronger_aug/resnet101_csv_29.h5')\n",
    "#model_list.append('snapshots/0620_stronger_aug/resnet101_csv_47.h5')\n",
    "#model_list.append('snapshots/0620_stronger_aug/resnet101_csv_48.h5')\n",
    "#model_list.append('snapshots/0620_stronger_aug/resnet101_csv_49.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize data generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "463\n"
     ]
    }
   ],
   "source": [
    "# create image data generator object\n",
    "val_image_data_generator = keras.preprocessing.image.ImageDataGenerator()\n",
    "\n",
    "if DATA_TO_TEST == 'abn1':\n",
    "    #csv_path = 'D:/Work/NCC/data/NCC/img_retinanet/data_abn1.csv'\n",
    "    csv_path = '/home/huray/data/NCC/img_retinanet/data_abn1.csv'\n",
    "elif DATA_TO_TEST == 'abn2':\n",
    "    #csv_path = 'D:/Work/NCC/data/NCC/img_retinanet/data_abn2.csv'\n",
    "    csv_path = '/home/huray/data/NCC/img_retinanet/data_abn2.csv'\n",
    "elif DATA_TO_TEST == 'normal':\n",
    "    #csv_path = 'D:/Work/NCC/data/NCC/img_retinanet/data_normal.csv'\n",
    "    csv_path = '/home/huray/data/NCC/img_retinanet/data_normal.csv'\n",
    "elif DATA_TO_TEST == 'all':\n",
    "    #csv_path = 'D:/Work/NCC/data/NCC/img_retinanet/0914_new_NCC_test_only_abn_anno_for_win.csv'\n",
    "    csv_path = 'D:/Work/NCC/data/NCC/img_retinanet/0914_new_NCC_test_only_abn_for_win.csv'\n",
    "    #csv_path = '/home/huray/data/NCC/img_retinanet/0914_new_NCC_test_only_abn.csv'\n",
    "else:\n",
    "    print('WRONG DATA CSV PATH!')\n",
    "    raise\n",
    "\n",
    "# create a generator for testing data\n",
    "val_generator = CSVGenerator(\n",
    "    csv_path,\n",
    "    'D:/Work/NCC/data/new_trainset/class_2_malig_or_norm.csv',\n",
    "    #'/home/huray/data/new_trainset/class_2_malig_or_norm.csv',\n",
    "    #'D:/Work/NCC/data/new_trainset/class_malig_and_norm.csv',\n",
    "    transform_generator=val_image_data_generator,\n",
    "    batch_size=1\n",
    ")\n",
    "\n",
    "test_file_df = pd.read_csv(csv_path, header=None)\n",
    "num_of_test_files = len(test_file_df)\n",
    "print(num_of_test_files)\n",
    "\n",
    "test_file_list = list(test_file_df[0]) #csv에서 읽어온 테스트 이미지파일 정보 목록\n",
    "test_file_list.sort(key=lambda x: '-'.join(x.split('-')[:2])) #테스트 파일 목록을 groupby 하기 위해서 정렬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_with_no_detection = []\n",
    "image_with_more_than_one_anno = []\n",
    "num_of_anno = 0\n",
    "correct_detection = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "463\n"
     ]
    }
   ],
   "source": [
    "img_index_list = list(range(0, num_of_test_files))\n",
    "img_index_list_temp = img_index_list.copy()\n",
    "print(len(img_index_list))\n",
    "\n",
    "############# for 테스트 파일목록 Groupping ############\n",
    "groupby_key_list = [] #그룹명(테스트 파일 단위 그룹 명) 목록\n",
    "groupby_file_list = [] #그룹화된 테스트파일 목록\n",
    "groupby_detected_number_for_files = [] #검출(detect)된 수에 대한 그룹화된 목록(그룹화된 테스트 목록 구조와 동일하게 매핑)\n",
    "#list_not_detected_group_index = [] #검출되지 않은 그룹의 index 목록 \n",
    "\n",
    "M_max_scores = [] #abnormal 테스트 파일에서 detect된 anchor score 중 가장 높은 score\n",
    "#N_max_scores = [] #normal 각 테스트 파일에서 detect된 anchor score 중 가장 높은 score\n",
    "\n",
    "#테스트 파일 목록을 group by하여 list에 저장\n",
    "#groupby key(그룹핑 기준(단위)는 파일명을 '-'로 split 한뒤 앞쪽 2번째 요소까지 추출함)\n",
    "for key, group in groupby(test_file_list, key=lambda x: '-'.join(x.split('-')[:2])):\n",
    "    #그룹 명(key) 목록\n",
    "    groupby_key_list.append(key) \n",
    "    temp = list(group)\n",
    "    #그룹화된 전체 테스트파일 목록 \n",
    "    groupby_file_list.append(temp) \n",
    "    #그룹화 된 테스트 파일별 검출정보 목록(그룹화된 전체 테스트파일 목록과 동일한 구조로 구성),0 으로 초기화\n",
    "    groupby_detected_number_for_files.append([0]*len(temp))\n",
    "\n",
    "    #그룹화 된 테스트 파일별 검출된 최대 score정보 목록(그룹화된 전체 테스트파일 목록과 동일한 구조로 구성),0 으로 초기화\n",
    "    M_max_scores.append([0]*len(temp))\n",
    "    \n",
    "    \n",
    "    \n",
    "############################# !!!!!!!!!!!!!!!!!!!!!!!!!!! #############################    \n",
    "def get_detection_info():\n",
    "    list_not_detected_group_index = [] #검출되지 않은 그룹의 index 목록 \n",
    "    total_not_detected_file = 0 #검출되지 않은 전체 테스트 파일의 수    \n",
    "    i_group_member = 0\n",
    "\n",
    "    print('------------------------------------------------ Result info --------------------------------------------------------')\n",
    "    print('\\n')    \n",
    "    \n",
    "    for idx_x, row in enumerate(groupby_detected_number_for_files):\n",
    "        i_detections = 0\n",
    "        for idx_y, elem in enumerate(row):\n",
    "            i_detections += elem\n",
    "            if elem == 0:\n",
    "                total_not_detected_file += 1\n",
    "                \n",
    "        if i_detections == 0:\n",
    "            list_not_detected_group_index.append(idx_x)\n",
    "            i_group_member += len(row)\n",
    "            print('({})set have no detection!.'.format(groupby_key_list[idx_x]))\n",
    "\n",
    "    print('\\n')\n",
    "    print('Total Number of abnormal case not detected :::::::::: {}'.format(total_not_detected_file))\n",
    "    print('Number of abnormal case set not detected::::::: {}({} files)'.format(len(list_not_detected_group_index), i_group_member))\n",
    "    print('\\n')\n",
    "    print('---------------------------------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start.\n",
      "463/463 detected...\n",
      "\n",
      "end.(4.78 minutes elapsed)\n",
      "------------------------------------------------ Result info --------------------------------------------------------\n",
      "\n",
      "\n",
      "(D:/Work/NCC/data/NCC/img_retinanet/abn_1/_AC_-00001)set have no detection!.\n",
      "(D:/Work/NCC/data/NCC/img_retinanet/abn_1/_A_-00000)set have no detection!.\n",
      "(D:/Work/NCC/data/NCC/img_retinanet/abn_1/_A_-00001)set have no detection!.\n",
      "(D:/Work/NCC/data/NCC/img_retinanet/abn_1/_CDM_-00000)set have no detection!.\n",
      "(D:/Work/NCC/data/NCC/img_retinanet/abn_1/_CD_-00001)set have no detection!.\n",
      "(D:/Work/NCC/data/NCC/img_retinanet/abn_1/_CM_-00000)set have no detection!.\n",
      "(D:/Work/NCC/data/NCC/img_retinanet/abn_1/_C_-00003)set have no detection!.\n",
      "(D:/Work/NCC/data/NCC/img_retinanet/abn_1/_C_-00006)set have no detection!.\n",
      "(D:/Work/NCC/data/NCC/img_retinanet/abn_1/_M_-00001)set have no detection!.\n",
      "(D:/Work/NCC/data/NCC/img_retinanet/abn_2/_ACX_-00000)set have no detection!.\n",
      "(D:/Work/NCC/data/NCC/img_retinanet/abn_2/_A_-00003)set have no detection!.\n",
      "(D:/Work/NCC/data/NCC/img_retinanet/abn_2/_A_-00004)set have no detection!.\n",
      "(D:/Work/NCC/data/NCC/img_retinanet/abn_2/_A_-00005)set have no detection!.\n",
      "(D:/Work/NCC/data/NCC/img_retinanet/abn_2/_CM_-00009)set have no detection!.\n",
      "(D:/Work/NCC/data/NCC/img_retinanet/abn_2/_DMX_-00000)set have no detection!.\n",
      "(D:/Work/NCC/data/NCC/img_retinanet/abn_2/_M_-00002)set have no detection!.\n",
      "(D:/Work/NCC/data/NCC/img_retinanet/abn_2/_M_-00004)set have no detection!.\n",
      "(D:/Work/NCC/data/NCC/img_retinanet/abn_2/_M_-00005)set have no detection!.\n",
      "(D:/Work/NCC/data/NCC/img_retinanet/abn_2/_M_-00011)set have no detection!.\n",
      "(D:/Work/NCC/data/NCC/img_retinanet/abn_2/_M_-00018)set have no detection!.\n",
      "\n",
      "\n",
      "Total Number of abnormal case not detected :::::::::: 289\n",
      "Number of abnormal case set not detected::::::: 20(82 files)\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "number_abnormal_detect_from_normal = 0 #normal 파일에서 abnormal로 검출된 경우의 수\n",
    "image_path_list = [] #테스트 파일 목록\n",
    "number_of_proper_detection = 0 #검출된 수\n",
    "\n",
    "print(\"start.\")\n",
    "start = time.time()\n",
    "print('0/{} detected...\\r'.format(num_of_test_files), end='')\n",
    "#for index in img_index_list:\n",
    "for index_count, index in enumerate(img_index_list):    \n",
    "    pass_this_file = False\n",
    "    \n",
    "    # load image\n",
    "    image, image_path = val_generator.load_image(index, get_image_path=True)\n",
    "    #print('image :::::::::: {}'.format(image_path))\n",
    "    image_name_split = image_path.split('/')[-1].split('-')\n",
    "    \n",
    "    # copy to draw on\n",
    "    draw = image.copy()\n",
    "    draw = cv2.cvtColor(draw, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # preprocess image for network\n",
    "    image = val_generator.preprocess_image(image)\n",
    "    image, scale = val_generator.resize_image(image)\n",
    "    #annotations = val_generator.load_annotations(index)\n",
    "    \n",
    "\n",
    "    # process image\n",
    "#     start = time.time()\n",
    "    _, _, detections = model.predict_on_batch(np.expand_dims(image, axis=0))\n",
    "#     print(\"processing time: \", time.time() - start)\n",
    "\n",
    "    # compute predicted labels and scores\n",
    "    predicted_labels = np.argmax(detections[0, :, 4:], axis=1)\n",
    "    scores = detections[0, np.arange(detections.shape[1]), 4 + predicted_labels]\n",
    "\n",
    "    # correct for image scale\n",
    "    detections[0, :, :4] /= scale\n",
    "    \n",
    "    '''anno_center = []\n",
    "    \n",
    "    if len(annotations) > 2:\n",
    "        print(\"MANY ANNO!!! len: \" +str(len(annotations)) + ' path: '+ image_path )\n",
    "        image_with_more_than_one_anno.append(image_path)'''\n",
    "    \n",
    "    # visualize annotations\n",
    "    '''for annotation in annotations:\n",
    "        num_of_anno += 1\n",
    "        \n",
    "        label = int(annotation[4])\n",
    "        b = annotation[:4].astype(int)\n",
    "        \n",
    "        anno_center.append((b[0]+b[2])/2)\n",
    "        anno_center.append((b[1]+b[3])/2)\n",
    "        \n",
    "        \n",
    "        cv2.rectangle(draw, (b[0], b[1]), (b[2], b[3]), (50, 50, 255), 2)\n",
    "        caption = \"{}\".format(val_generator.label_to_name(label))\n",
    "        cv2.putText(draw, caption, (b[0], b[1] - 10), cv2.FONT_HERSHEY_PLAIN, 3, (0, 0, 0), 5)\n",
    "        cv2.putText(draw, caption, (b[0], b[1] - 10), cv2.FONT_HERSHEY_PLAIN, 3, (255, 255, 255), 4)'''\n",
    "            \n",
    "\n",
    "    \n",
    "    number_of_proper_detection = 0 #현재 파일에서 detect된 수\n",
    "    image_path_list.append(image_path)\n",
    "    M_max_score = 0 \n",
    "    N_max_score = 0\n",
    "    \n",
    "    for idx, (label, score) in enumerate(zip(predicted_labels, scores)):\n",
    "        if label == 0:\n",
    "            if M_max_score < score:\n",
    "                M_max_score = score    \n",
    "        else:\n",
    "            if N_max_score < score:\n",
    "                N_max_score = score\n",
    "        \n",
    "        if score < SCORE_VALUE or label != 0:\n",
    "            continue\n",
    "\n",
    "        number_of_proper_detection += 1\n",
    "        \n",
    "        b = detections[0, idx, :4].astype(int)\n",
    "            \n",
    "        caption = \"{} {:.3f}\".format(val_generator.label_to_name(label), score)\n",
    "        \n",
    "        ######### BENIGN 출력 여부 설정 #########\n",
    "        #if PRINT_BENIGN_DETECTION is False:\n",
    "        #    if 'B' in caption:\n",
    "        #        continue\n",
    "                \n",
    "                \n",
    "        '''if pass_this_file is False and len(anno_center) == 2 and b[0]<anno_center[0]<b[2] and b[1]<anno_center[1]<b[3]:\n",
    "            correct_detection += 1\n",
    "            pass_this_file = True\n",
    "            \n",
    "            # remove the detected image from the list\n",
    "            img_index_list_temp.remove(index)'''\n",
    "       \n",
    "        cv2.putText(draw, caption, (b[0], b[1] - 10), cv2.FONT_HERSHEY_PLAIN, 3, (0, 0, 0), 5)\n",
    "        cv2.putText(draw, caption, (b[0], b[1] - 10), cv2.FONT_HERSHEY_PLAIN, 3, (255, 255, 255), 4)\n",
    "        cv2.rectangle(draw, (b[0], b[1]), (b[2], b[3]), (0, 255, 0), 5)\n",
    "\n",
    "\n",
    "    # 테스트 파일 그룹 목록에서 현재 파일의 위치를 구함    \n",
    "    img_indics = find(groupby_file_list, image_path)\n",
    "    if img_indics[0] >= 0:\n",
    "        # 검출된 정보 중 Max 스코어 값을 파일 위치와 매칭하여 저장\n",
    "        M_max_scores[img_indics[0]][img_indics[1]] = M_max_score \n",
    "        \n",
    "    #이미지에서 (조건에 부합되게)detect된 것이 있을 경우\n",
    "    if number_of_proper_detection > 0:\n",
    "        if 'normal' in image_path:\n",
    "            number_abnormal_detect_from_normal += 1            \n",
    "        \n",
    "        ##################### 그룹화된 테스트 파일 정보와 동일한 구조의 detection 정보 목록에 반영 ##############\n",
    "        #img_indics = find(groupby_file_list, image_path)\n",
    "        if img_indics[0] >= 0:\n",
    "            #groupby_detected_number_for_files[img_indics[0]][img_indics[1]] += number_of_proper_detection\n",
    "            groupby_detected_number_for_files[img_indics[0]][img_indics[1]] += 1\n",
    "    #cv2.imwrite(os.path.join(result_save_path, image_path.split('/')[-1].split('.')[0]+'_'+str(index)+'.jpg'), draw)\n",
    "    \n",
    "    print('{}/{} detected...\\r'.format(index_count+1,num_of_test_files), end='')\n",
    "\n",
    "print(\"\\n\")    \n",
    "print(\"end.({:.2f} minutes elapsed)\".format((time.time() - start)/60))\n",
    "\n",
    "get_detection_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_data_df = pd.DataFrame({'img_path':list(chain(*groupby_file_list)),'Detection_Num':list(chain(*groupby_detected_number_for_files)), 'abnormal_max_score':list(chain(*M_max_scores))})\n",
    "result_data_df = result_data_df[['img_path', 'Detection_Num', 'abnormal_max_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_data_df.to_csv('D:/Work/NCC/han_test/results/test_result.csv', header=False, index=False)\n",
    "#result_data_df.to_csv('/home/huray/workspace/han_work/han_test/results/test_result.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DONE!\" + DATA_TO_TEST + MODEL_EPOCH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check remained images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check remained images\n",
    "img_index_list = img_index_list_temp.copy()\n",
    "\n",
    "print(\"FAILED TO DETECT:\")\n",
    "\n",
    "detection_failed = []\n",
    "\n",
    "for img_index in img_index_list:\n",
    "    image, image_path = val_generator.load_image(img_index, get_image_path=True)\n",
    "    image = val_generator.preprocess_image(image)\n",
    "    image, scale = val_generator.resize_image(image)\n",
    "    annotations = val_generator.load_annotations(img_index)\n",
    "    \n",
    "    if len(annotations) != 0: # images that the model failed to detect\n",
    "        detection_failed.append(image_path)\n",
    "        \n",
    "detection_failed.sort()\n",
    "\n",
    "for path in detection_failed:\n",
    "    print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(correct_detection, num_of_anno)\n",
    "if len(image_with_more_than_one_anno) != 0:\n",
    "    print(\"문제있다!!\" + str(image_with_more_than_one_anno))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_with_no_detection = []\n",
    "\n",
    "test_file_path_list = test_file_df[0].tolist()\n",
    "\n",
    "image_with_no_detection.sort()\n",
    "for l in image_with_no_detection:        \n",
    "    splitted = l.split('-')[:2]\n",
    "    case_path = '-'.join(splitted)\n",
    "    \n",
    "    if case_path in case_with_no_detection:\n",
    "        continue\n",
    "    \n",
    "    same_case = [k for k in image_with_no_detection if case_path in k]\n",
    "    if len(same_case) == len([k for k in test_file_path_list if case_path in k]):\n",
    "        case_with_no_detection.append(case_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(case_with_no_detection))\n",
    "\n",
    "for l in case_with_no_detection:\n",
    "    print(l)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
