/home/huray/venvs/ml/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2018-09-06 11:04:11.144834: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-09-06 11:04:11.231126: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-09-06 11:04:11.231393: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.721
pciBusID: 0000:01:00.0
totalMemory: 10.91GiB freeMemory: 10.69GiB
2018-09-06 11:04:11.231407: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
Creating model, this may take a second...
keras_retinanet/bin/new_train.py:95: UserWarning: Output "nms" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to "nms" during training.
  optimizer=keras.optimizers.adam(lr=LEARNING_RATE, clipnorm=0.001)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, None, None, 3 0                                            
__________________________________________________________________________________________________
padding_conv1 (ZeroPadding2D)   (None, None, None, 3 0           input_1[0][0]                    
__________________________________________________________________________________________________
conv1 (Conv2D)                  (None, None, None, 6 9408        padding_conv1[0][0]              
__________________________________________________________________________________________________
bn_conv1 (BatchNormalization)   (None, None, None, 6 256         conv1[0][0]                      
__________________________________________________________________________________________________
conv1_relu (Activation)         (None, None, None, 6 0           bn_conv1[0][0]                   
__________________________________________________________________________________________________
pool1 (MaxPooling2D)            (None, None, None, 6 0           conv1_relu[0][0]                 
__________________________________________________________________________________________________
res2a_branch2a (Conv2D)         (None, None, None, 6 4096        pool1[0][0]                      
__________________________________________________________________________________________________
bn2a_branch2a (BatchNormalizati (None, None, None, 6 256         res2a_branch2a[0][0]             
__________________________________________________________________________________________________
res2a_branch2a_relu (Activation (None, None, None, 6 0           bn2a_branch2a[0][0]              
__________________________________________________________________________________________________
padding2a_branch2b (ZeroPadding (None, None, None, 6 0           res2a_branch2a_relu[0][0]        
__________________________________________________________________________________________________
res2a_branch2b (Conv2D)         (None, None, None, 6 36864       padding2a_branch2b[0][0]         
__________________________________________________________________________________________________
bn2a_branch2b (BatchNormalizati (None, None, None, 6 256         res2a_branch2b[0][0]             
__________________________________________________________________________________________________
res2a_branch2b_relu (Activation (None, None, None, 6 0           bn2a_branch2b[0][0]              
__________________________________________________________________________________________________
res2a_branch2c (Conv2D)         (None, None, None, 2 16384       res2a_branch2b_relu[0][0]        
__________________________________________________________________________________________________
res2a_branch1 (Conv2D)          (None, None, None, 2 16384       pool1[0][0]                      
__________________________________________________________________________________________________
bn2a_branch2c (BatchNormalizati (None, None, None, 2 1024        res2a_branch2c[0][0]             
__________________________________________________________________________________________________
bn2a_branch1 (BatchNormalizatio (None, None, None, 2 1024        res2a_branch1[0][0]              
__________________________________________________________________________________________________
res2a (Add)                     (None, None, None, 2 0           bn2a_branch2c[0][0]              
                                                                 bn2a_branch1[0][0]               
__________________________________________________________________________________________________
res2a_relu (Activation)         (None, None, None, 2 0           res2a[0][0]                      
__________________________________________________________________________________________________
res2b_branch2a (Conv2D)         (None, None, None, 6 16384       res2a_relu[0][0]                 
__________________________________________________________________________________________________
bn2b_branch2a (BatchNormalizati (None, None, None, 6 256         res2b_branch2a[0][0]             
__________________________________________________________________________________________________
res2b_branch2a_relu (Activation (None, None, None, 6 0           bn2b_branch2a[0][0]              
__________________________________________________________________________________________________
padding2b_branch2b (ZeroPadding (None, None, None, 6 0           res2b_branch2a_relu[0][0]        
__________________________________________________________________________________________________
res2b_branch2b (Conv2D)         (None, None, None, 6 36864       padding2b_branch2b[0][0]         
__________________________________________________________________________________________________
bn2b_branch2b (BatchNormalizati (None, None, None, 6 256         res2b_branch2b[0][0]             
__________________________________________________________________________________________________
res2b_branch2b_relu (Activation (None, None, None, 6 0           bn2b_branch2b[0][0]              
__________________________________________________________________________________________________
res2b_branch2c (Conv2D)         (None, None, None, 2 16384       res2b_branch2b_relu[0][0]        
__________________________________________________________________________________________________
bn2b_branch2c (BatchNormalizati (None, None, None, 2 1024        res2b_branch2c[0][0]             
__________________________________________________________________________________________________
res2b (Add)                     (None, None, None, 2 0           bn2b_branch2c[0][0]              
                                                                 res2a_relu[0][0]                 
__________________________________________________________________________________________________
res2b_relu (Activation)         (None, None, None, 2 0           res2b[0][0]                      
__________________________________________________________________________________________________
res2c_branch2a (Conv2D)         (None, None, None, 6 16384       res2b_relu[0][0]                 
__________________________________________________________________________________________________
bn2c_branch2a (BatchNormalizati (None, None, None, 6 256         res2c_branch2a[0][0]             
__________________________________________________________________________________________________
res2c_branch2a_relu (Activation (None, None, None, 6 0           bn2c_branch2a[0][0]              
__________________________________________________________________________________________________
padding2c_branch2b (ZeroPadding (None, None, None, 6 0           res2c_branch2a_relu[0][0]        
__________________________________________________________________________________________________
res2c_branch2b (Conv2D)         (None, None, None, 6 36864       padding2c_branch2b[0][0]         
__________________________________________________________________________________________________
bn2c_branch2b (BatchNormalizati (None, None, None, 6 256         res2c_branch2b[0][0]             
__________________________________________________________________________________________________
res2c_branch2b_relu (Activation (None, None, None, 6 0           bn2c_branch2b[0][0]              
__________________________________________________________________________________________________
res2c_branch2c (Conv2D)         (None, None, None, 2 16384       res2c_branch2b_relu[0][0]        
__________________________________________________________________________________________________
bn2c_branch2c (BatchNormalizati (None, None, None, 2 1024        res2c_branch2c[0][0]             
__________________________________________________________________________________________________
res2c (Add)                     (None, None, None, 2 0           bn2c_branch2c[0][0]              
                                                                 res2b_relu[0][0]                 
__________________________________________________________________________________________________
res2c_relu (Activation)         (None, None, None, 2 0           res2c[0][0]                      
__________________________________________________________________________________________________
res3a_branch2a (Conv2D)         (None, None, None, 1 32768       res2c_relu[0][0]                 
__________________________________________________________________________________________________
bn3a_branch2a (BatchNormalizati (None, None, None, 1 512         res3a_branch2a[0][0]             
__________________________________________________________________________________________________
res3a_branch2a_relu (Activation (None, None, None, 1 0           bn3a_branch2a[0][0]              
__________________________________________________________________________________________________
padding3a_branch2b (ZeroPadding (None, None, None, 1 0           res3a_branch2a_relu[0][0]        
__________________________________________________________________________________________________
res3a_branch2b (Conv2D)         (None, None, None, 1 147456      padding3a_branch2b[0][0]         
__________________________________________________________________________________________________
bn3a_branch2b (BatchNormalizati (None, None, None, 1 512         res3a_branch2b[0][0]             
__________________________________________________________________________________________________
res3a_branch2b_relu (Activation (None, None, None, 1 0           bn3a_branch2b[0][0]              
__________________________________________________________________________________________________
res3a_branch2c (Conv2D)         (None, None, None, 5 65536       res3a_branch2b_relu[0][0]        
__________________________________________________________________________________________________
res3a_branch1 (Conv2D)          (None, None, None, 5 131072      res2c_relu[0][0]                 
__________________________________________________________________________________________________
bn3a_branch2c (BatchNormalizati (None, None, None, 5 2048        res3a_branch2c[0][0]             
__________________________________________________________________________________________________
bn3a_branch1 (BatchNormalizatio (None, None, None, 5 2048        res3a_branch1[0][0]              
__________________________________________________________________________________________________
res3a (Add)                     (None, None, None, 5 0           bn3a_branch2c[0][0]              
                                                                 bn3a_branch1[0][0]               
__________________________________________________________________________________________________
res3a_relu (Activation)         (None, None, None, 5 0           res3a[0][0]                      
__________________________________________________________________________________________________
res3b_branch2a (Conv2D)         (None, None, None, 1 65536       res3a_relu[0][0]                 
__________________________________________________________________________________________________
bn3b_branch2a (BatchNormalizati (None, None, None, 1 512         res3b_branch2a[0][0]             
__________________________________________________________________________________________________
res3b_branch2a_relu (Activation (None, None, None, 1 0           bn3b_branch2a[0][0]              
__________________________________________________________________________________________________
padding3b_branch2b (ZeroPadding (None, None, None, 1 0           res3b_branch2a_relu[0][0]        
__________________________________________________________________________________________________
res3b_branch2b (Conv2D)         (None, None, None, 1 147456      padding3b_branch2b[0][0]         
__________________________________________________________________________________________________
bn3b_branch2b (BatchNormalizati (None, None, None, 1 512         res3b_branch2b[0][0]             
__________________________________________________________________________________________________
res3b_branch2b_relu (Activation (None, None, None, 1 0           bn3b_branch2b[0][0]              
__________________________________________________________________________________________________
res3b_branch2c (Conv2D)         (None, None, None, 5 65536       res3b_branch2b_relu[0][0]        
__________________________________________________________________________________________________
bn3b_branch2c (BatchNormalizati (None, None, None, 5 2048        res3b_branch2c[0][0]             
__________________________________________________________________________________________________
res3b (Add)                     (None, None, None, 5 0           bn3b_branch2c[0][0]              
                                                                 res3a_relu[0][0]                 
__________________________________________________________________________________________________
res3b_relu (Activation)         (None, None, None, 5 0           res3b[0][0]                      
__________________________________________________________________________________________________
res3c_branch2a (Conv2D)         (None, None, None, 1 65536       res3b_relu[0][0]                 
__________________________________________________________________________________________________
bn3c_branch2a (BatchNormalizati (None, None, None, 1 512         res3c_branch2a[0][0]             
__________________________________________________________________________________________________
res3c_branch2a_relu (Activation (None, None, None, 1 0           bn3c_branch2a[0][0]              
__________________________________________________________________________________________________
padding3c_branch2b (ZeroPadding (None, None, None, 1 0           res3c_branch2a_relu[0][0]        
__________________________________________________________________________________________________
res3c_branch2b (Conv2D)         (None, None, None, 1 147456      padding3c_branch2b[0][0]         
__________________________________________________________________________________________________
bn3c_branch2b (BatchNormalizati (None, None, None, 1 512         res3c_branch2b[0][0]             
__________________________________________________________________________________________________
res3c_branch2b_relu (Activation (None, None, None, 1 0           bn3c_branch2b[0][0]              
__________________________________________________________________________________________________
res3c_branch2c (Conv2D)         (None, None, None, 5 65536       res3c_branch2b_relu[0][0]        
__________________________________________________________________________________________________
bn3c_branch2c (BatchNormalizati (None, None, None, 5 2048        res3c_branch2c[0][0]             
__________________________________________________________________________________________________
res3c (Add)                     (None, None, None, 5 0           bn3c_branch2c[0][0]              
                                                                 res3b_relu[0][0]                 
__________________________________________________________________________________________________
res3c_relu (Activation)         (None, None, None, 5 0           res3c[0][0]                      
__________________________________________________________________________________________________
res3d_branch2a (Conv2D)         (None, None, None, 1 65536       res3c_relu[0][0]                 
__________________________________________________________________________________________________
bn3d_branch2a (BatchNormalizati (None, None, None, 1 512         res3d_branch2a[0][0]             
__________________________________________________________________________________________________
res3d_branch2a_relu (Activation (None, None, None, 1 0           bn3d_branch2a[0][0]              
__________________________________________________________________________________________________
padding3d_branch2b (ZeroPadding (None, None, None, 1 0           res3d_branch2a_relu[0][0]        
__________________________________________________________________________________________________
res3d_branch2b (Conv2D)         (None, None, None, 1 147456      padding3d_branch2b[0][0]         
__________________________________________________________________________________________________
bn3d_branch2b (BatchNormalizati (None, None, None, 1 512         res3d_branch2b[0][0]             
__________________________________________________________________________________________________
res3d_branch2b_relu (Activation (None, None, None, 1 0           bn3d_branch2b[0][0]              
__________________________________________________________________________________________________
res3d_branch2c (Conv2D)         (None, None, None, 5 65536       res3d_branch2b_relu[0][0]        
__________________________________________________________________________________________________
bn3d_branch2c (BatchNormalizati (None, None, None, 5 2048        res3d_branch2c[0][0]             
__________________________________________________________________________________________________
res3d (Add)                     (None, None, None, 5 0           bn3d_branch2c[0][0]              
                                                                 res3c_relu[0][0]                 
__________________________________________________________________________________________________
res3d_relu (Activation)         (None, None, None, 5 0           res3d[0][0]                      
__________________________________________________________________________________________________
res4a_branch2a (Conv2D)         (None, None, None, 2 131072      res3d_relu[0][0]                 
__________________________________________________________________________________________________
bn4a_branch2a (BatchNormalizati (None, None, None, 2 1024        res4a_branch2a[0][0]             
__________________________________________________________________________________________________
res4a_branch2a_relu (Activation (None, None, None, 2 0           bn4a_branch2a[0][0]              
__________________________________________________________________________________________________
padding4a_branch2b (ZeroPadding (None, None, None, 2 0           res4a_branch2a_relu[0][0]        
__________________________________________________________________________________________________
res4a_branch2b (Conv2D)         (None, None, None, 2 589824      padding4a_branch2b[0][0]         
__________________________________________________________________________________________________
bn4a_branch2b (BatchNormalizati (None, None, None, 2 1024        res4a_branch2b[0][0]             
__________________________________________________________________________________________________
res4a_branch2b_relu (Activation (None, None, None, 2 0           bn4a_branch2b[0][0]              
__________________________________________________________________________________________________
res4a_branch2c (Conv2D)         (None, None, None, 1 262144      res4a_branch2b_relu[0][0]        
__________________________________________________________________________________________________
res4a_branch1 (Conv2D)          (None, None, None, 1 524288      res3d_relu[0][0]                 
__________________________________________________________________________________________________
bn4a_branch2c (BatchNormalizati (None, None, None, 1 4096        res4a_branch2c[0][0]             
__________________________________________________________________________________________________
bn4a_branch1 (BatchNormalizatio (None, None, None, 1 4096        res4a_branch1[0][0]              
__________________________________________________________________________________________________
res4a (Add)                     (None, None, None, 1 0           bn4a_branch2c[0][0]              
                                                                 bn4a_branch1[0][0]               
__________________________________________________________________________________________________
res4a_relu (Activation)         (None, None, None, 1 0           res4a[0][0]                      
__________________________________________________________________________________________________
res4b_branch2a (Conv2D)         (None, None, None, 2 262144      res4a_relu[0][0]                 
__________________________________________________________________________________________________
bn4b_branch2a (BatchNormalizati (None, None, None, 2 1024        res4b_branch2a[0][0]             
__________________________________________________________________________________________________
res4b_branch2a_relu (Activation (None, None, None, 2 0           bn4b_branch2a[0][0]              
__________________________________________________________________________________________________
padding4b_branch2b (ZeroPadding (None, None, None, 2 0           res4b_branch2a_relu[0][0]        
__________________________________________________________________________________________________
res4b_branch2b (Conv2D)         (None, None, None, 2 589824      padding4b_branch2b[0][0]         
__________________________________________________________________________________________________
bn4b_branch2b (BatchNormalizati (None, None, None, 2 1024        res4b_branch2b[0][0]             
__________________________________________________________________________________________________
res4b_branch2b_relu (Activation (None, None, None, 2 0           bn4b_branch2b[0][0]              
__________________________________________________________________________________________________
res4b_branch2c (Conv2D)         (None, None, None, 1 262144      res4b_branch2b_relu[0][0]        
__________________________________________________________________________________________________
bn4b_branch2c (BatchNormalizati (None, None, None, 1 4096        res4b_branch2c[0][0]             
__________________________________________________________________________________________________
res4b (Add)                     (None, None, None, 1 0           bn4b_branch2c[0][0]              
                                                                 res4a_relu[0][0]                 
__________________________________________________________________________________________________
res4b_relu (Activation)         (None, None, None, 1 0           res4b[0][0]                      
__________________________________________________________________________________________________
res4c_branch2a (Conv2D)         (None, None, None, 2 262144      res4b_relu[0][0]                 
__________________________________________________________________________________________________
bn4c_branch2a (BatchNormalizati (None, None, None, 2 1024        res4c_branch2a[0][0]             
__________________________________________________________________________________________________
res4c_branch2a_relu (Activation (None, None, None, 2 0           bn4c_branch2a[0][0]              
__________________________________________________________________________________________________
padding4c_branch2b (ZeroPadding (None, None, None, 2 0           res4c_branch2a_relu[0][0]        
__________________________________________________________________________________________________
res4c_branch2b (Conv2D)         (None, None, None, 2 589824      padding4c_branch2b[0][0]         
__________________________________________________________________________________________________
bn4c_branch2b (BatchNormalizati (None, None, None, 2 1024        res4c_branch2b[0][0]             
__________________________________________________________________________________________________
res4c_branch2b_relu (Activation (None, None, None, 2 0           bn4c_branch2b[0][0]              
__________________________________________________________________________________________________
res4c_branch2c (Conv2D)         (None, None, None, 1 262144      res4c_branch2b_relu[0][0]        
__________________________________________________________________________________________________
bn4c_branch2c (BatchNormalizati (None, None, None, 1 4096        res4c_branch2c[0][0]             
__________________________________________________________________________________________________
res4c (Add)                     (None, None, None, 1 0           bn4c_branch2c[0][0]              
                                                                 res4b_relu[0][0]                 
__________________________________________________________________________________________________
res4c_relu (Activation)         (None, None, None, 1 0           res4c[0][0]                      
__________________________________________________________________________________________________
res4d_branch2a (Conv2D)         (None, None, None, 2 262144      res4c_relu[0][0]                 
__________________________________________________________________________________________________
bn4d_branch2a (BatchNormalizati (None, None, None, 2 1024        res4d_branch2a[0][0]             
__________________________________________________________________________________________________
res4d_branch2a_relu (Activation (None, None, None, 2 0           bn4d_branch2a[0][0]              
__________________________________________________________________________________________________
padding4d_branch2b (ZeroPadding (None, None, None, 2 0           res4d_branch2a_relu[0][0]        
__________________________________________________________________________________________________
res4d_branch2b (Conv2D)         (None, None, None, 2 589824      padding4d_branch2b[0][0]         
__________________________________________________________________________________________________
bn4d_branch2b (BatchNormalizati (None, None, None, 2 1024        res4d_branch2b[0][0]             
__________________________________________________________________________________________________
res4d_branch2b_relu (Activation (None, None, None, 2 0           bn4d_branch2b[0][0]              
__________________________________________________________________________________________________
res4d_branch2c (Conv2D)         (None, None, None, 1 262144      res4d_branch2b_relu[0][0]        
__________________________________________________________________________________________________
bn4d_branch2c (BatchNormalizati (None, None, None, 1 4096        res4d_branch2c[0][0]             
__________________________________________________________________________________________________
res4d (Add)                     (None, None, None, 1 0           bn4d_branch2c[0][0]              
                                                                 res4c_relu[0][0]                 
__________________________________________________________________________________________________
res4d_relu (Activation)         (None, None, None, 1 0           res4d[0][0]                      
__________________________________________________________________________________________________
res4e_branch2a (Conv2D)         (None, None, None, 2 262144      res4d_relu[0][0]                 
__________________________________________________________________________________________________
bn4e_branch2a (BatchNormalizati (None, None, None, 2 1024        res4e_branch2a[0][0]             
__________________________________________________________________________________________________
res4e_branch2a_relu (Activation (None, None, None, 2 0           bn4e_branch2a[0][0]              
__________________________________________________________________________________________________
padding4e_branch2b (ZeroPadding (None, None, None, 2 0           res4e_branch2a_relu[0][0]        
__________________________________________________________________________________________________
res4e_branch2b (Conv2D)         (None, None, None, 2 589824      padding4e_branch2b[0][0]         
__________________________________________________________________________________________________
bn4e_branch2b (BatchNormalizati (None, None, None, 2 1024        res4e_branch2b[0][0]             
__________________________________________________________________________________________________
res4e_branch2b_relu (Activation (None, None, None, 2 0           bn4e_branch2b[0][0]              
__________________________________________________________________________________________________
res4e_branch2c (Conv2D)         (None, None, None, 1 262144      res4e_branch2b_relu[0][0]        
__________________________________________________________________________________________________
bn4e_branch2c (BatchNormalizati (None, None, None, 1 4096        res4e_branch2c[0][0]             
__________________________________________________________________________________________________
res4e (Add)                     (None, None, None, 1 0           bn4e_branch2c[0][0]              
                                                                 res4d_relu[0][0]                 
__________________________________________________________________________________________________
res4e_relu (Activation)         (None, None, None, 1 0           res4e[0][0]                      
__________________________________________________________________________________________________
res4f_branch2a (Conv2D)         (None, None, None, 2 262144      res4e_relu[0][0]                 
__________________________________________________________________________________________________
bn4f_branch2a (BatchNormalizati (None, None, None, 2 1024        res4f_branch2a[0][0]             
__________________________________________________________________________________________________
res4f_branch2a_relu (Activation (None, None, None, 2 0           bn4f_branch2a[0][0]              
__________________________________________________________________________________________________
padding4f_branch2b (ZeroPadding (None, None, None, 2 0           res4f_branch2a_relu[0][0]        
__________________________________________________________________________________________________
res4f_branch2b (Conv2D)         (None, None, None, 2 589824      padding4f_branch2b[0][0]         
__________________________________________________________________________________________________
bn4f_branch2b (BatchNormalizati (None, None, None, 2 1024        res4f_branch2b[0][0]             
__________________________________________________________________________________________________
res4f_branch2b_relu (Activation (None, None, None, 2 0           bn4f_branch2b[0][0]              
__________________________________________________________________________________________________
res4f_branch2c (Conv2D)         (None, None, None, 1 262144      res4f_branch2b_relu[0][0]        
__________________________________________________________________________________________________
bn4f_branch2c (BatchNormalizati (None, None, None, 1 4096        res4f_branch2c[0][0]             
__________________________________________________________________________________________________
res4f (Add)                     (None, None, None, 1 0           bn4f_branch2c[0][0]              
                                                                 res4e_relu[0][0]                 
__________________________________________________________________________________________________
res4f_relu (Activation)         (None, None, None, 1 0           res4f[0][0]                      
__________________________________________________________________________________________________
res5a_branch2a (Conv2D)         (None, None, None, 5 524288      res4f_relu[0][0]                 
__________________________________________________________________________________________________
bn5a_branch2a (BatchNormalizati (None, None, None, 5 2048        res5a_branch2a[0][0]             
__________________________________________________________________________________________________
res5a_branch2a_relu (Activation (None, None, None, 5 0           bn5a_branch2a[0][0]              
__________________________________________________________________________________________________
padding5a_branch2b (ZeroPadding (None, None, None, 5 0           res5a_branch2a_relu[0][0]        
__________________________________________________________________________________________________
res5a_branch2b (Conv2D)         (None, None, None, 5 2359296     padding5a_branch2b[0][0]         
__________________________________________________________________________________________________
bn5a_branch2b (BatchNormalizati (None, None, None, 5 2048        res5a_branch2b[0][0]             
__________________________________________________________________________________________________
res5a_branch2b_relu (Activation (None, None, None, 5 0           bn5a_branch2b[0][0]              
__________________________________________________________________________________________________
res5a_branch2c (Conv2D)         (None, None, None, 2 1048576     res5a_branch2b_relu[0][0]        
__________________________________________________________________________________________________
res5a_branch1 (Conv2D)          (None, None, None, 2 2097152     res4f_relu[0][0]                 
__________________________________________________________________________________________________
bn5a_branch2c (BatchNormalizati (None, None, None, 2 8192        res5a_branch2c[0][0]             
__________________________________________________________________________________________________
bn5a_branch1 (BatchNormalizatio (None, None, None, 2 8192        res5a_branch1[0][0]              
__________________________________________________________________________________________________
res5a (Add)                     (None, None, None, 2 0           bn5a_branch2c[0][0]              
                                                                 bn5a_branch1[0][0]               
__________________________________________________________________________________________________
res5a_relu (Activation)         (None, None, None, 2 0           res5a[0][0]                      
__________________________________________________________________________________________________
res5b_branch2a (Conv2D)         (None, None, None, 5 1048576     res5a_relu[0][0]                 
__________________________________________________________________________________________________
bn5b_branch2a (BatchNormalizati (None, None, None, 5 2048        res5b_branch2a[0][0]             
__________________________________________________________________________________________________
res5b_branch2a_relu (Activation (None, None, None, 5 0           bn5b_branch2a[0][0]              
__________________________________________________________________________________________________
padding5b_branch2b (ZeroPadding (None, None, None, 5 0           res5b_branch2a_relu[0][0]        
__________________________________________________________________________________________________
res5b_branch2b (Conv2D)         (None, None, None, 5 2359296     padding5b_branch2b[0][0]         
__________________________________________________________________________________________________
bn5b_branch2b (BatchNormalizati (None, None, None, 5 2048        res5b_branch2b[0][0]             
__________________________________________________________________________________________________
res5b_branch2b_relu (Activation (None, None, None, 5 0           bn5b_branch2b[0][0]              
__________________________________________________________________________________________________
res5b_branch2c (Conv2D)         (None, None, None, 2 1048576     res5b_branch2b_relu[0][0]        
__________________________________________________________________________________________________
bn5b_branch2c (BatchNormalizati (None, None, None, 2 8192        res5b_branch2c[0][0]             
__________________________________________________________________________________________________
res5b (Add)                     (None, None, None, 2 0           bn5b_branch2c[0][0]              
                                                                 res5a_relu[0][0]                 
__________________________________________________________________________________________________
res5b_relu (Activation)         (None, None, None, 2 0           res5b[0][0]                      
__________________________________________________________________________________________________
res5c_branch2a (Conv2D)         (None, None, None, 5 1048576     res5b_relu[0][0]                 
__________________________________________________________________________________________________
bn5c_branch2a (BatchNormalizati (None, None, None, 5 2048        res5c_branch2a[0][0]             
__________________________________________________________________________________________________
res5c_branch2a_relu (Activation (None, None, None, 5 0           bn5c_branch2a[0][0]              
__________________________________________________________________________________________________
padding5c_branch2b (ZeroPadding (None, None, None, 5 0           res5c_branch2a_relu[0][0]        
__________________________________________________________________________________________________
res5c_branch2b (Conv2D)         (None, None, None, 5 2359296     padding5c_branch2b[0][0]         
__________________________________________________________________________________________________
bn5c_branch2b (BatchNormalizati (None, None, None, 5 2048        res5c_branch2b[0][0]             
__________________________________________________________________________________________________
res5c_branch2b_relu (Activation (None, None, None, 5 0           bn5c_branch2b[0][0]              
__________________________________________________________________________________________________
res5c_branch2c (Conv2D)         (None, None, None, 2 1048576     res5c_branch2b_relu[0][0]        
__________________________________________________________________________________________________
bn5c_branch2c (BatchNormalizati (None, None, None, 2 8192        res5c_branch2c[0][0]             
__________________________________________________________________________________________________
res5c (Add)                     (None, None, None, 2 0           bn5c_branch2c[0][0]              
                                                                 res5b_relu[0][0]                 
__________________________________________________________________________________________________
res5c_relu (Activation)         (None, None, None, 2 0           res5c[0][0]                      
__________________________________________________________________________________________________
P5 (Conv2D)                     (None, None, None, 2 524544      res5c_relu[0][0]                 
__________________________________________________________________________________________________
P5_upsampled (UpsampleLike)     (None, None, None, 2 0           P5[0][0]                         
                                                                 res4f_relu[0][0]                 
__________________________________________________________________________________________________
C4_reduced (Conv2D)             (None, None, None, 2 262400      res4f_relu[0][0]                 
__________________________________________________________________________________________________
P4_merged (Add)                 (None, None, None, 2 0           P5_upsampled[0][0]               
                                                                 C4_reduced[0][0]                 
__________________________________________________________________________________________________
P4 (Conv2D)                     (None, None, None, 2 590080      P4_merged[0][0]                  
__________________________________________________________________________________________________
P4_upsampled (UpsampleLike)     (None, None, None, 2 0           P4[0][0]                         
                                                                 res3d_relu[0][0]                 
__________________________________________________________________________________________________
C3_reduced (Conv2D)             (None, None, None, 2 131328      res3d_relu[0][0]                 
__________________________________________________________________________________________________
P6 (Conv2D)                     (None, None, None, 2 4718848     res5c_relu[0][0]                 
__________________________________________________________________________________________________
P3_merged (Add)                 (None, None, None, 2 0           P4_upsampled[0][0]               
                                                                 C3_reduced[0][0]                 
__________________________________________________________________________________________________
C6_relu (Activation)            (None, None, None, 2 0           P6[0][0]                         
__________________________________________________________________________________________________
P3 (Conv2D)                     (None, None, None, 2 590080      P3_merged[0][0]                  
__________________________________________________________________________________________________
P7 (Conv2D)                     (None, None, None, 2 590080      C6_relu[0][0]                    
__________________________________________________________________________________________________
regression_submodel (Model)     (None, None, 4)      2443300     P3[0][0]                         
                                                                 P4[0][0]                         
                                                                 P5[0][0]                         
                                                                 P6[0][0]                         
                                                                 P7[0][0]                         
__________________________________________________________________________________________________
anchors_0 (Anchors)             (None, None, 4)      0           P3[0][0]                         
__________________________________________________________________________________________________
anchors_1 (Anchors)             (None, None, 4)      0           P4[0][0]                         
__________________________________________________________________________________________________
anchors_2 (Anchors)             (None, None, 4)      0           P5[0][0]                         
__________________________________________________________________________________________________
anchors_3 (Anchors)             (None, None, 4)      0           P6[0][0]                         
__________________________________________________________________________________________________
anchors_4 (Anchors)             (None, None, 4)      0           P7[0][0]                         
__________________________________________________________________________________________________
regression (Concatenate)        (None, None, 4)      0           regression_submodel[1][0]        
                                                                 regression_submodel[2][0]        
                                                                 regression_submodel[3][0]        
                                                                 regression_submodel[4][0]        
                                                                 regression_submodel[5][0]        
__________________________________________________________________________________________________
classification_submodel (Model) (None, None, 2)      2401810     P3[0][0]                         
                                                                 P4[0][0]                         
                                                                 P5[0][0]                         
                                                                 P6[0][0]                         
                                                                 P7[0][0]                         
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, None, 4)      0           anchors_0[0][0]                  
                                                                 anchors_1[0][0]                  
                                                                 anchors_2[0][0]                  
                                                                 anchors_3[0][0]                  
                                                                 anchors_4[0][0]                  
__________________________________________________________________________________________________
classification (Concatenate)    (None, None, 2)      0           classification_submodel[1][0]    
                                                                 classification_submodel[2][0]    
                                                                 classification_submodel[3][0]    
                                                                 classification_submodel[4][0]    
                                                                 classification_submodel[5][0]    
__________________________________________________________________________________________________
boxes (RegressBoxes)            (None, None, 4)      0           concatenate_1[0][0]              
                                                                 regression[0][0]                 
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, None, 6)      0           boxes[0][0]                      
                                                                 classification[0][0]             
__________________________________________________________________________________________________
nms (NonMaximumSuppression)     (None, None, 6)      0           boxes[0][0]                      
                                                                 classification[0][0]             
                                                                 concatenate_2[0][0]              
==================================================================================================
Total params: 35,813,622
Trainable params: 35,707,382
Non-trainable params: 106,240
__________________________________________________________________________________________________
None
evaluation........................ : True
lr................................ : 1e-05
Epoch 1/30
keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 242 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.046e+03, 1.046e+03, 1.435e+03, 1.344e+03, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]
keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 582 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.156e+03, 1.100e+02, 1.546e+03, 5.130e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

   1/1000 [..............................] - ETA: 42:21 - loss: 5.3026 - regression_loss: 4.1526 - classification_loss: 1.1500
   2/1000 [..............................] - ETA: 24:49 - loss: 5.3569 - regression_loss: 4.2089 - classification_loss: 1.1481
   3/1000 [..............................] - ETA: 19:01 - loss: 5.3973 - regression_loss: 4.2484 - classification_loss: 1.1489keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 967 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([8.700e+02, 0.000e+00, 1.495e+03, 7.610e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

   4/1000 [..............................] - ETA: 16:08 - loss: 4.2209 - regression_loss: 3.1863 - classification_loss: 1.0346keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 164 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([8.590e+02, 2.830e+02, 1.438e+03, 1.203e+03, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

   5/1000 [..............................] - ETA: 14:23 - loss: 4.5053 - regression_loss: 3.4472 - classification_loss: 1.0582
   6/1000 [..............................] - ETA: 13:08 - loss: 4.6740 - regression_loss: 3.6013 - classification_loss: 1.0727
   7/1000 [..............................] - ETA: 12:19 - loss: 4.8011 - regression_loss: 3.7176 - classification_loss: 1.0835
   8/1000 [..............................] - ETA: 11:41 - loss: 4.8591 - regression_loss: 3.7675 - classification_loss: 1.0916
   9/1000 [..............................] - ETA: 11:12 - loss: 4.9269 - regression_loss: 3.8288 - classification_loss: 1.0982
  10/1000 [..............................] - ETA: 10:50 - loss: 4.5033 - regression_loss: 3.4459 - classification_loss: 1.0574
  11/1000 [..............................] - ETA: 10:31 - loss: 4.6421 - regression_loss: 3.5730 - classification_loss: 1.0691
  12/1000 [..............................] - ETA: 10:15 - loss: 4.7127 - regression_loss: 3.6366 - classification_loss: 1.0760
  13/1000 [..............................] - ETA: 10:03 - loss: 4.7665 - regression_loss: 3.6849 - classification_loss: 1.0816keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 253 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.046e+03, 1.054e+03, 1.511e+03, 1.393e+03, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

  14/1000 [..............................] - ETA: 9:51 - loss: 4.4753 - regression_loss: 3.4217 - classification_loss: 1.0536 
  15/1000 [..............................] - ETA: 9:41 - loss: 4.2229 - regression_loss: 3.1936 - classification_loss: 1.0294
  16/1000 [..............................] - ETA: 9:32 - loss: 4.2888 - regression_loss: 3.2518 - classification_loss: 1.0370
  17/1000 [..............................] - ETA: 9:23 - loss: 4.3469 - regression_loss: 3.3033 - classification_loss: 1.0437keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 670 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.116e+03, 1.250e+02, 1.683e+03, 6.580e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

  18/1000 [..............................] - ETA: 9:16 - loss: 4.4316 - regression_loss: 3.3823 - classification_loss: 1.0492
  19/1000 [..............................] - ETA: 9:09 - loss: 4.5008 - regression_loss: 3.4464 - classification_loss: 1.0545
  20/1000 [..............................] - ETA: 9:04 - loss: 4.5275 - regression_loss: 3.4682 - classification_loss: 1.0593keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 821 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.194e+03, 1.270e+02, 1.578e+03, 5.640e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

  21/1000 [..............................] - ETA: 8:59 - loss: 4.3446 - regression_loss: 3.3031 - classification_loss: 1.0415
  22/1000 [..............................] - ETA: 8:54 - loss: 4.3917 - regression_loss: 3.3452 - classification_loss: 1.0465
  23/1000 [..............................] - ETA: 8:49 - loss: 4.4203 - regression_loss: 3.3692 - classification_loss: 1.0510
  24/1000 [..............................] - ETA: 8:45 - loss: 4.2647 - regression_loss: 3.2288 - classification_loss: 1.0358keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 278 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.147e+03, 2.930e+02, 1.499e+03, 6.420e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

  25/1000 [..............................] - ETA: 8:41 - loss: 4.3229 - regression_loss: 3.2827 - classification_loss: 1.0402
  26/1000 [..............................] - ETA: 8:38 - loss: 4.3705 - regression_loss: 3.3260 - classification_loss: 1.0444
  27/1000 [..............................] - ETA: 8:35 - loss: 4.3990 - regression_loss: 3.3506 - classification_loss: 1.0484
  28/1000 [..............................] - ETA: 8:31 - loss: 4.2663 - regression_loss: 3.2309 - classification_loss: 1.0354
  29/1000 [..............................] - ETA: 8:28 - loss: 4.3107 - regression_loss: 3.2687 - classification_loss: 1.0420
  30/1000 [..............................] - ETA: 8:25 - loss: 4.3435 - regression_loss: 3.2979 - classification_loss: 1.0456
  31/1000 [..............................] - ETA: 8:22 - loss: 4.2254 - regression_loss: 3.1915 - classification_loss: 1.0339
  32/1000 [..............................] - ETA: 8:20 - loss: 4.2606 - regression_loss: 3.2230 - classification_loss: 1.0376
  33/1000 [..............................] - ETA: 8:17 - loss: 4.2922 - regression_loss: 3.2512 - classification_loss: 1.0410
  34/1000 [>.............................] - ETA: 8:15 - loss: 4.3255 - regression_loss: 3.2810 - classification_loss: 1.0444
  35/1000 [>.............................] - ETA: 8:13 - loss: 4.2214 - regression_loss: 3.1873 - classification_loss: 1.0341
  36/1000 [>.............................] - ETA: 8:10 - loss: 4.2637 - regression_loss: 3.2265 - classification_loss: 1.0372
  37/1000 [>.............................] - ETA: 8:08 - loss: 4.1667 - regression_loss: 3.1393 - classification_loss: 1.0274
  38/1000 [>.............................] - ETA: 8:06 - loss: 4.2080 - regression_loss: 3.1775 - classification_loss: 1.0305
  39/1000 [>.............................] - ETA: 8:04 - loss: 4.2365 - regression_loss: 3.2030 - classification_loss: 1.0336
  40/1000 [>.............................] - ETA: 8:02 - loss: 4.2671 - regression_loss: 3.2306 - classification_loss: 1.0365keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 186 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.050e+03, 0.000e+00, 1.458e+03, 3.470e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

  41/1000 [>.............................] - ETA: 8:00 - loss: 4.2767 - regression_loss: 3.2352 - classification_loss: 1.0415
  42/1000 [>.............................] - ETA: 7:59 - loss: 4.2942 - regression_loss: 3.2501 - classification_loss: 1.0441
  43/1000 [>.............................] - ETA: 7:57 - loss: 4.3112 - regression_loss: 3.2646 - classification_loss: 1.0467
  44/1000 [>.............................] - ETA: 7:56 - loss: 4.3349 - regression_loss: 3.2859 - classification_loss: 1.0490keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 136 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.129e+03, 9.200e+01, 1.515e+03, 3.830e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

  45/1000 [>.............................] - ETA: 7:54 - loss: 4.3516 - regression_loss: 3.3003 - classification_loss: 1.0512keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 624 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.203e+03, 0.000e+00, 1.579e+03, 3.590e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

  46/1000 [>.............................] - ETA: 7:53 - loss: 4.2715 - regression_loss: 3.2286 - classification_loss: 1.0429keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 389 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.221e+03, 0.000e+00, 1.596e+03, 3.240e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

  47/1000 [>.............................] - ETA: 7:51 - loss: 4.3049 - regression_loss: 3.2598 - classification_loss: 1.0451
  48/1000 [>.............................] - ETA: 7:50 - loss: 4.3266 - regression_loss: 3.2794 - classification_loss: 1.0472
  49/1000 [>.............................] - ETA: 7:48 - loss: 4.3551 - regression_loss: 3.3059 - classification_loss: 1.0492
  50/1000 [>.............................] - ETA: 7:47 - loss: 4.3779 - regression_loss: 3.3266 - classification_loss: 1.0513
  51/1000 [>.............................] - ETA: 7:46 - loss: 4.3051 - regression_loss: 3.2614 - classification_loss: 1.0438
  52/1000 [>.............................] - ETA: 7:45 - loss: 4.3253 - regression_loss: 3.2796 - classification_loss: 1.0458
  53/1000 [>.............................] - ETA: 7:43 - loss: 4.2562 - regression_loss: 3.2177 - classification_loss: 1.0385
  54/1000 [>.............................] - ETA: 7:42 - loss: 4.2687 - regression_loss: 3.2281 - classification_loss: 1.0406
  55/1000 [>.............................] - ETA: 7:41 - loss: 4.2032 - regression_loss: 3.1694 - classification_loss: 1.0338
  56/1000 [>.............................] - ETA: 7:39 - loss: 4.1398 - regression_loss: 3.1128 - classification_loss: 1.0270
  57/1000 [>.............................] - ETA: 7:38 - loss: 4.0784 - regression_loss: 3.0582 - classification_loss: 1.0202
  58/1000 [>.............................] - ETA: 7:37 - loss: 4.1061 - regression_loss: 3.0836 - classification_loss: 1.0225
  59/1000 [>.............................] - ETA: 7:36 - loss: 4.1250 - regression_loss: 3.1003 - classification_loss: 1.0247keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 173 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([8.07e+02, 9.26e+02, 1.42e+03, 1.48e+03, 1.00e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

  60/1000 [>.............................] - ETA: 7:35 - loss: 4.1507 - regression_loss: 3.1240 - classification_loss: 1.0267
  61/1000 [>.............................] - ETA: 7:34 - loss: 4.1684 - regression_loss: 3.1396 - classification_loss: 1.0288
  62/1000 [>.............................] - ETA: 7:33 - loss: 4.1855 - regression_loss: 3.1548 - classification_loss: 1.0307
  63/1000 [>.............................] - ETA: 7:32 - loss: 4.1970 - regression_loss: 3.1644 - classification_loss: 1.0326
  64/1000 [>.............................] - ETA: 7:31 - loss: 4.2196 - regression_loss: 3.1852 - classification_loss: 1.0344
  65/1000 [>.............................] - ETA: 7:30 - loss: 4.2343 - regression_loss: 3.1982 - classification_loss: 1.0362keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 35 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([8.090e+02, 6.810e+02, 1.623e+03, 1.524e+03, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

  66/1000 [>.............................] - ETA: 7:29 - loss: 4.2513 - regression_loss: 3.2135 - classification_loss: 1.0379keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 865 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.216e+03, 0.000e+00, 1.549e+03, 3.180e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

  67/1000 [=>............................] - ETA: 7:29 - loss: 4.2632 - regression_loss: 3.2236 - classification_loss: 1.0395
  68/1000 [=>............................] - ETA: 7:28 - loss: 4.2756 - regression_loss: 3.2344 - classification_loss: 1.0412
  69/1000 [=>............................] - ETA: 7:27 - loss: 4.2936 - regression_loss: 3.2508 - classification_loss: 1.0428
  70/1000 [=>............................] - ETA: 7:26 - loss: 4.2414 - regression_loss: 3.2044 - classification_loss: 1.0371
  71/1000 [=>............................] - ETA: 7:25 - loss: 4.2533 - regression_loss: 3.2146 - classification_loss: 1.0387
  72/1000 [=>............................] - ETA: 7:24 - loss: 4.2652 - regression_loss: 3.2249 - classification_loss: 1.0403
  73/1000 [=>............................] - ETA: 7:23 - loss: 4.2844 - regression_loss: 3.2427 - classification_loss: 1.0417
  74/1000 [=>............................] - ETA: 7:22 - loss: 4.2956 - regression_loss: 3.2524 - classification_loss: 1.0432
  75/1000 [=>............................] - ETA: 7:22 - loss: 4.3157 - regression_loss: 3.2710 - classification_loss: 1.0447
  76/1000 [=>............................] - ETA: 7:21 - loss: 4.2664 - regression_loss: 3.2280 - classification_loss: 1.0384keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 438 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.08e+03, 0.00e+00, 1.67e+03, 4.59e+02, 1.00e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

  77/1000 [=>............................] - ETA: 7:20 - loss: 4.2186 - regression_loss: 3.1861 - classification_loss: 1.0325
  78/1000 [=>............................] - ETA: 7:19 - loss: 4.1723 - regression_loss: 3.1452 - classification_loss: 1.0271
  79/1000 [=>............................] - ETA: 7:18 - loss: 4.1820 - regression_loss: 3.1531 - classification_loss: 1.0290keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 387 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([9.290e+02, 2.180e+02, 1.449e+03, 7.210e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

  80/1000 [=>............................] - ETA: 7:18 - loss: 4.1970 - regression_loss: 3.1666 - classification_loss: 1.0305
  81/1000 [=>............................] - ETA: 7:17 - loss: 4.2103 - regression_loss: 3.1783 - classification_loss: 1.0320
  82/1000 [=>............................] - ETA: 7:16 - loss: 4.2348 - regression_loss: 3.2013 - classification_loss: 1.0336
  83/1000 [=>............................] - ETA: 7:15 - loss: 4.2467 - regression_loss: 3.2117 - classification_loss: 1.0349
  84/1000 [=>............................] - ETA: 7:14 - loss: 4.2604 - regression_loss: 3.2241 - classification_loss: 1.0363
  85/1000 [=>............................] - ETA: 7:14 - loss: 4.2762 - regression_loss: 3.2385 - classification_loss: 1.0377
  86/1000 [=>............................] - ETA: 7:13 - loss: 4.2908 - regression_loss: 3.2519 - classification_loss: 1.0390
  87/1000 [=>............................] - ETA: 7:12 - loss: 4.2472 - regression_loss: 3.2145 - classification_loss: 1.0327
  88/1000 [=>............................] - ETA: 7:12 - loss: 4.2629 - regression_loss: 3.2289 - classification_loss: 1.0339
  89/1000 [=>............................] - ETA: 7:11 - loss: 4.2746 - regression_loss: 3.2393 - classification_loss: 1.0353
  90/1000 [=>............................] - ETA: 7:10 - loss: 4.2325 - regression_loss: 3.2033 - classification_loss: 1.0292
  91/1000 [=>............................] - ETA: 7:09 - loss: 4.2422 - regression_loss: 3.2117 - classification_loss: 1.0305keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 727 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.113e+03, 4.060e+02, 1.607e+03, 1.025e+03, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

  92/1000 [=>............................] - ETA: 7:09 - loss: 4.2526 - regression_loss: 3.2208 - classification_loss: 1.0318
  93/1000 [=>............................] - ETA: 7:08 - loss: 4.2676 - regression_loss: 3.2345 - classification_loss: 1.0331keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 362 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.261e+03, 1.003e+03, 1.536e+03, 1.238e+03, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

  94/1000 [=>............................] - ETA: 7:07 - loss: 4.2779 - regression_loss: 3.2435 - classification_loss: 1.0343
  95/1000 [=>............................] - ETA: 7:07 - loss: 4.2870 - regression_loss: 3.2514 - classification_loss: 1.0356keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 66 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([8.560e+02, 5.700e+02, 1.567e+03, 1.322e+03, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

  96/1000 [=>............................] - ETA: 7:06 - loss: 4.2964 - regression_loss: 3.2597 - classification_loss: 1.0368
  97/1000 [=>............................] - ETA: 7:05 - loss: 4.3062 - regression_loss: 3.2682 - classification_loss: 1.0380keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 337 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.236e+03, 1.700e+01, 1.453e+03, 2.160e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

  98/1000 [=>............................] - ETA: 7:05 - loss: 4.3190 - regression_loss: 3.2798 - classification_loss: 1.0392
  99/1000 [=>............................] - ETA: 7:04 - loss: 4.3312 - regression_loss: 3.2909 - classification_loss: 1.0403keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 155 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.011e+03, 4.200e+02, 1.675e+03, 1.124e+03, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 100/1000 [==>...........................] - ETA: 7:03 - loss: 4.3378 - regression_loss: 3.2964 - classification_loss: 1.0414
 101/1000 [==>...........................] - ETA: 7:02 - loss: 4.3461 - regression_loss: 3.3035 - classification_loss: 1.0425keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 117 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.232e+03, 5.150e+02, 1.547e+03, 8.790e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 102/1000 [==>...........................] - ETA: 7:02 - loss: 4.3061 - regression_loss: 3.2712 - classification_loss: 1.0349
 103/1000 [==>...........................] - ETA: 7:01 - loss: 4.3144 - regression_loss: 3.2784 - classification_loss: 1.0360
 104/1000 [==>...........................] - ETA: 7:00 - loss: 4.2766 - regression_loss: 3.2469 - classification_loss: 1.0297
 105/1000 [==>...........................] - ETA: 7:00 - loss: 4.2886 - regression_loss: 3.2578 - classification_loss: 1.0308
 106/1000 [==>...........................] - ETA: 6:59 - loss: 4.2516 - regression_loss: 3.2271 - classification_loss: 1.0245
 107/1000 [==>...........................] - ETA: 6:59 - loss: 4.2599 - regression_loss: 3.2341 - classification_loss: 1.0257
 108/1000 [==>...........................] - ETA: 6:58 - loss: 4.2225 - regression_loss: 3.2042 - classification_loss: 1.0183
 109/1000 [==>...........................] - ETA: 6:57 - loss: 4.2338 - regression_loss: 3.2141 - classification_loss: 1.0197
 110/1000 [==>...........................] - ETA: 6:57 - loss: 4.1973 - regression_loss: 3.1849 - classification_loss: 1.0124
 111/1000 [==>...........................] - ETA: 6:56 - loss: 4.2064 - regression_loss: 3.1928 - classification_loss: 1.0137
 112/1000 [==>...........................] - ETA: 6:55 - loss: 4.1710 - regression_loss: 3.1643 - classification_loss: 1.0067
 113/1000 [==>...........................] - ETA: 6:55 - loss: 4.1790 - regression_loss: 3.1709 - classification_loss: 1.0081
 114/1000 [==>...........................] - ETA: 6:54 - loss: 4.1436 - regression_loss: 3.1431 - classification_loss: 1.0004
 115/1000 [==>...........................] - ETA: 6:54 - loss: 4.1502 - regression_loss: 3.1482 - classification_loss: 1.0021
 116/1000 [==>...........................] - ETA: 6:53 - loss: 4.1591 - regression_loss: 3.1557 - classification_loss: 1.0034
 117/1000 [==>...........................] - ETA: 6:53 - loss: 4.1666 - regression_loss: 3.1616 - classification_loss: 1.0050
 118/1000 [==>...........................] - ETA: 6:52 - loss: 4.1713 - regression_loss: 3.1647 - classification_loss: 1.0066
 119/1000 [==>...........................] - ETA: 6:51 - loss: 4.1780 - regression_loss: 3.1700 - classification_loss: 1.0080
 120/1000 [==>...........................] - ETA: 6:50 - loss: 4.1821 - regression_loss: 3.1727 - classification_loss: 1.0094
 121/1000 [==>...........................] - ETA: 6:50 - loss: 4.1485 - regression_loss: 3.1465 - classification_loss: 1.0020
 122/1000 [==>...........................] - ETA: 6:49 - loss: 4.1585 - regression_loss: 3.1550 - classification_loss: 1.0035keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 206 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.219e+03, 0.000e+00, 1.435e+03, 2.390e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 123/1000 [==>...........................] - ETA: 6:49 - loss: 4.1668 - regression_loss: 3.1617 - classification_loss: 1.0051
 124/1000 [==>...........................] - ETA: 6:48 - loss: 4.1735 - regression_loss: 3.1671 - classification_loss: 1.0064
 125/1000 [==>...........................] - ETA: 6:48 - loss: 4.1406 - regression_loss: 3.1418 - classification_loss: 0.9988
 126/1000 [==>...........................] - ETA: 6:47 - loss: 4.1082 - regression_loss: 3.1168 - classification_loss: 0.9914
 127/1000 [==>...........................] - ETA: 6:47 - loss: 4.1141 - regression_loss: 3.1212 - classification_loss: 0.9929
 128/1000 [==>...........................] - ETA: 6:46 - loss: 4.1206 - regression_loss: 3.1262 - classification_loss: 0.9944
 129/1000 [==>...........................] - ETA: 6:46 - loss: 4.1263 - regression_loss: 3.1298 - classification_loss: 0.9965
 130/1000 [==>...........................] - ETA: 6:45 - loss: 4.1339 - regression_loss: 3.1358 - classification_loss: 0.9981
 131/1000 [==>...........................] - ETA: 6:45 - loss: 4.1490 - regression_loss: 3.1495 - classification_loss: 0.9995
 132/1000 [==>...........................] - ETA: 6:44 - loss: 4.1567 - regression_loss: 3.1550 - classification_loss: 1.0016
 133/1000 [==>...........................] - ETA: 6:43 - loss: 4.1257 - regression_loss: 3.1313 - classification_loss: 0.9944
 134/1000 [===>..........................] - ETA: 6:43 - loss: 4.1443 - regression_loss: 3.1471 - classification_loss: 0.9972
 135/1000 [===>..........................] - ETA: 6:42 - loss: 4.1586 - regression_loss: 3.1594 - classification_loss: 0.9991
 136/1000 [===>..........................] - ETA: 6:42 - loss: 4.1743 - regression_loss: 3.1738 - classification_loss: 1.0004keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 498 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.065e+03, 2.700e+01, 1.628e+03, 8.120e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 137/1000 [===>..........................] - ETA: 6:41 - loss: 4.1799 - regression_loss: 3.1780 - classification_loss: 1.0018
 138/1000 [===>..........................] - ETA: 6:41 - loss: 4.1847 - regression_loss: 3.1816 - classification_loss: 1.0031
 139/1000 [===>..........................] - ETA: 6:40 - loss: 4.1905 - regression_loss: 3.1857 - classification_loss: 1.0047
 140/1000 [===>..........................] - ETA: 6:40 - loss: 4.1967 - regression_loss: 3.1907 - classification_loss: 1.0060
 141/1000 [===>..........................] - ETA: 6:39 - loss: 4.2035 - regression_loss: 3.1962 - classification_loss: 1.0073
 142/1000 [===>..........................] - ETA: 6:39 - loss: 4.2168 - regression_loss: 3.2068 - classification_loss: 1.0100keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 397 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([8.970e+02, 0.000e+00, 1.646e+03, 8.170e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 143/1000 [===>..........................] - ETA: 6:38 - loss: 4.2203 - regression_loss: 3.2091 - classification_loss: 1.0113
 144/1000 [===>..........................] - ETA: 6:37 - loss: 4.2258 - regression_loss: 3.2132 - classification_loss: 1.0126
 145/1000 [===>..........................] - ETA: 6:37 - loss: 4.2327 - regression_loss: 3.2191 - classification_loss: 1.0136
 146/1000 [===>..........................] - ETA: 6:36 - loss: 4.2040 - regression_loss: 3.1970 - classification_loss: 1.0070
 147/1000 [===>..........................] - ETA: 6:36 - loss: 4.1755 - regression_loss: 3.1753 - classification_loss: 1.0002
 148/1000 [===>..........................] - ETA: 6:35 - loss: 4.1807 - regression_loss: 3.1790 - classification_loss: 1.0017
 149/1000 [===>..........................] - ETA: 6:35 - loss: 4.1899 - regression_loss: 3.1854 - classification_loss: 1.0045
 150/1000 [===>..........................] - ETA: 6:34 - loss: 4.1942 - regression_loss: 3.1883 - classification_loss: 1.0059
 151/1000 [===>..........................] - ETA: 6:34 - loss: 4.2015 - regression_loss: 3.1935 - classification_loss: 1.0080keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 65 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.089e+03, 0.000e+00, 1.514e+03, 4.520e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 152/1000 [===>..........................] - ETA: 6:33 - loss: 4.2080 - regression_loss: 3.1987 - classification_loss: 1.0094
 153/1000 [===>..........................] - ETA: 6:33 - loss: 4.1806 - regression_loss: 3.1777 - classification_loss: 1.0029
 154/1000 [===>..........................] - ETA: 6:32 - loss: 4.1842 - regression_loss: 3.1785 - classification_loss: 1.0058
 155/1000 [===>..........................] - ETA: 6:32 - loss: 4.1940 - regression_loss: 3.1842 - classification_loss: 1.0099
 156/1000 [===>..........................] - ETA: 6:31 - loss: 4.2015 - regression_loss: 3.1888 - classification_loss: 1.0127
 157/1000 [===>..........................] - ETA: 6:31 - loss: 4.2091 - regression_loss: 3.1933 - classification_loss: 1.0157
 158/1000 [===>..........................] - ETA: 6:30 - loss: 4.1825 - regression_loss: 3.1731 - classification_loss: 1.0094
 159/1000 [===>..........................] - ETA: 6:29 - loss: 4.1929 - regression_loss: 3.1786 - classification_loss: 1.0143
 160/1000 [===>..........................] - ETA: 6:29 - loss: 4.2011 - regression_loss: 3.1836 - classification_loss: 1.0175
 161/1000 [===>..........................] - ETA: 6:28 - loss: 4.2098 - regression_loss: 3.1861 - classification_loss: 1.0237
 162/1000 [===>..........................] - ETA: 6:28 - loss: 4.1840 - regression_loss: 3.1664 - classification_loss: 1.0175
 163/1000 [===>..........................] - ETA: 6:27 - loss: 4.1890 - regression_loss: 3.1703 - classification_loss: 1.0187
 164/1000 [===>..........................] - ETA: 6:27 - loss: 4.1960 - regression_loss: 3.1763 - classification_loss: 1.0197
 165/1000 [===>..........................] - ETA: 6:26 - loss: 4.2021 - regression_loss: 3.1812 - classification_loss: 1.0209
 166/1000 [===>..........................] - ETA: 6:26 - loss: 4.2097 - regression_loss: 3.1870 - classification_loss: 1.0227
 167/1000 [====>.........................] - ETA: 6:25 - loss: 4.2183 - regression_loss: 3.1937 - classification_loss: 1.0246keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 406 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.136e+03, 3.540e+02, 1.513e+03, 6.310e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 168/1000 [====>.........................] - ETA: 6:25 - loss: 4.2256 - regression_loss: 3.1997 - classification_loss: 1.0259
 169/1000 [====>.........................] - ETA: 6:24 - loss: 4.2295 - regression_loss: 3.2009 - classification_loss: 1.0285
 170/1000 [====>.........................] - ETA: 6:24 - loss: 4.2346 - regression_loss: 3.2046 - classification_loss: 1.0300
 171/1000 [====>.........................] - ETA: 6:23 - loss: 4.2453 - regression_loss: 3.2130 - classification_loss: 1.0322
 172/1000 [====>.........................] - ETA: 6:23 - loss: 4.2496 - regression_loss: 3.2162 - classification_loss: 1.0334
 173/1000 [====>.........................] - ETA: 6:22 - loss: 4.2504 - regression_loss: 3.2145 - classification_loss: 1.0359
 174/1000 [====>.........................] - ETA: 6:22 - loss: 4.2617 - regression_loss: 3.2249 - classification_loss: 1.0368keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 521 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.153e+03, 9.400e+02, 1.473e+03, 1.150e+03, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 175/1000 [====>.........................] - ETA: 6:21 - loss: 4.2678 - regression_loss: 3.2303 - classification_loss: 1.0375
 176/1000 [====>.........................] - ETA: 6:21 - loss: 4.2718 - regression_loss: 3.2329 - classification_loss: 1.0390
 177/1000 [====>.........................] - ETA: 6:20 - loss: 4.2777 - regression_loss: 3.2375 - classification_loss: 1.0401
 178/1000 [====>.........................] - ETA: 6:20 - loss: 4.2537 - regression_loss: 3.2193 - classification_loss: 1.0344
 179/1000 [====>.........................] - ETA: 6:19 - loss: 4.2570 - regression_loss: 3.2217 - classification_loss: 1.0353
 180/1000 [====>.........................] - ETA: 6:19 - loss: 4.2613 - regression_loss: 3.2246 - classification_loss: 1.0367
 181/1000 [====>.........................] - ETA: 6:18 - loss: 4.2706 - regression_loss: 3.2307 - classification_loss: 1.0398
 182/1000 [====>.........................] - ETA: 6:18 - loss: 4.2798 - regression_loss: 3.2390 - classification_loss: 1.0408keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 873 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.016e+03, 2.730e+02, 1.566e+03, 7.170e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 183/1000 [====>.........................] - ETA: 6:17 - loss: 4.2872 - regression_loss: 3.2450 - classification_loss: 1.0422
 184/1000 [====>.........................] - ETA: 6:17 - loss: 4.2951 - regression_loss: 3.2501 - classification_loss: 1.0449
 185/1000 [====>.........................] - ETA: 6:16 - loss: 4.2719 - regression_loss: 3.2325 - classification_loss: 1.0393
 186/1000 [====>.........................] - ETA: 6:16 - loss: 4.2763 - regression_loss: 3.2357 - classification_loss: 1.0406
 187/1000 [====>.........................] - ETA: 6:15 - loss: 4.2812 - regression_loss: 3.2399 - classification_loss: 1.0413
 188/1000 [====>.........................] - ETA: 6:15 - loss: 4.2854 - regression_loss: 3.2426 - classification_loss: 1.0428
 189/1000 [====>.........................] - ETA: 6:14 - loss: 4.2875 - regression_loss: 3.2433 - classification_loss: 1.0442
 190/1000 [====>.........................] - ETA: 6:14 - loss: 4.2920 - regression_loss: 3.2470 - classification_loss: 1.0450
 191/1000 [====>.........................] - ETA: 6:13 - loss: 4.2960 - regression_loss: 3.2503 - classification_loss: 1.0457
 192/1000 [====>.........................] - ETA: 6:13 - loss: 4.3041 - regression_loss: 3.2570 - classification_loss: 1.0471
 193/1000 [====>.........................] - ETA: 6:12 - loss: 4.2819 - regression_loss: 3.2402 - classification_loss: 1.0417
 194/1000 [====>.........................] - ETA: 6:12 - loss: 4.2853 - regression_loss: 3.2427 - classification_loss: 1.0425
 195/1000 [====>.........................] - ETA: 6:11 - loss: 4.2890 - regression_loss: 3.2458 - classification_loss: 1.0432
 196/1000 [====>.........................] - ETA: 6:11 - loss: 4.2900 - regression_loss: 3.2449 - classification_loss: 1.0451
 197/1000 [====>.........................] - ETA: 6:10 - loss: 4.2959 - regression_loss: 3.2461 - classification_loss: 1.0498
 198/1000 [====>.........................] - ETA: 6:10 - loss: 4.3010 - regression_loss: 3.2484 - classification_loss: 1.0526
 199/1000 [====>.........................] - ETA: 6:09 - loss: 4.3018 - regression_loss: 3.2486 - classification_loss: 1.0532
 200/1000 [=====>........................] - ETA: 6:09 - loss: 4.3043 - regression_loss: 3.2501 - classification_loss: 1.0542
 201/1000 [=====>........................] - ETA: 6:08 - loss: 4.3088 - regression_loss: 3.2538 - classification_loss: 1.0551
 202/1000 [=====>........................] - ETA: 6:08 - loss: 4.3124 - regression_loss: 3.2569 - classification_loss: 1.0555keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 367 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.147e+03, 7.430e+02, 1.562e+03, 1.091e+03, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 203/1000 [=====>........................] - ETA: 6:07 - loss: 4.3154 - regression_loss: 3.2591 - classification_loss: 1.0563
 204/1000 [=====>........................] - ETA: 6:07 - loss: 4.3225 - regression_loss: 3.2644 - classification_loss: 1.0580
 205/1000 [=====>........................] - ETA: 6:06 - loss: 4.3014 - regression_loss: 3.2485 - classification_loss: 1.0530
 206/1000 [=====>........................] - ETA: 6:06 - loss: 4.3036 - regression_loss: 3.2495 - classification_loss: 1.0541
 207/1000 [=====>........................] - ETA: 6:05 - loss: 4.3099 - regression_loss: 3.2552 - classification_loss: 1.0547keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 930 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.187e+03, 4.190e+02, 1.591e+03, 7.860e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 208/1000 [=====>........................] - ETA: 6:05 - loss: 4.3159 - regression_loss: 3.2606 - classification_loss: 1.0553
 209/1000 [=====>........................] - ETA: 6:04 - loss: 4.3188 - regression_loss: 3.2620 - classification_loss: 1.0567keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 293 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.193e+03, 2.260e+02, 1.697e+03, 7.160e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 210/1000 [=====>........................] - ETA: 6:04 - loss: 4.3218 - regression_loss: 3.2642 - classification_loss: 1.0575
 211/1000 [=====>........................] - ETA: 6:03 - loss: 4.3281 - regression_loss: 3.2676 - classification_loss: 1.0605
 212/1000 [=====>........................] - ETA: 6:03 - loss: 4.3303 - regression_loss: 3.2689 - classification_loss: 1.0614
 213/1000 [=====>........................] - ETA: 6:02 - loss: 4.3100 - regression_loss: 3.2535 - classification_loss: 1.0565keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 687 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.162e+03, 0.000e+00, 1.531e+03, 2.570e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 214/1000 [=====>........................] - ETA: 6:02 - loss: 4.3161 - regression_loss: 3.2581 - classification_loss: 1.0580keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 751 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.043e+03, 8.910e+02, 1.426e+03, 1.372e+03, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 215/1000 [=====>........................] - ETA: 6:01 - loss: 4.2961 - regression_loss: 3.2429 - classification_loss: 1.0531keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 631 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.064e+03, 0.000e+00, 1.506e+03, 4.200e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 216/1000 [=====>........................] - ETA: 6:01 - loss: 4.3033 - regression_loss: 3.2484 - classification_loss: 1.0549
 217/1000 [=====>........................] - ETA: 6:00 - loss: 4.3062 - regression_loss: 3.2505 - classification_loss: 1.0557
 218/1000 [=====>........................] - ETA: 6:00 - loss: 4.2865 - regression_loss: 3.2356 - classification_loss: 1.0509
 219/1000 [=====>........................] - ETA: 5:59 - loss: 4.2887 - regression_loss: 3.2372 - classification_loss: 1.0515keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 432 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.325e+03, 1.199e+03, 1.481e+03, 1.379e+03, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 220/1000 [=====>........................] - ETA: 5:59 - loss: 4.2693 - regression_loss: 3.2225 - classification_loss: 1.0468
 221/1000 [=====>........................] - ETA: 5:58 - loss: 4.2720 - regression_loss: 3.2245 - classification_loss: 1.0475
 222/1000 [=====>........................] - ETA: 5:58 - loss: 4.2791 - regression_loss: 3.2289 - classification_loss: 1.0502
 223/1000 [=====>........................] - ETA: 5:57 - loss: 4.2827 - regression_loss: 3.2309 - classification_loss: 1.0518keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 134 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.059e+03, 3.690e+02, 1.449e+03, 7.260e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 224/1000 [=====>........................] - ETA: 5:57 - loss: 4.2636 - regression_loss: 3.2165 - classification_loss: 1.0472
 225/1000 [=====>........................] - ETA: 5:56 - loss: 4.2447 - regression_loss: 3.2022 - classification_loss: 1.0426
 226/1000 [=====>........................] - ETA: 5:56 - loss: 4.2260 - regression_loss: 3.1880 - classification_loss: 1.0380
 227/1000 [=====>........................] - ETA: 5:55 - loss: 4.2074 - regression_loss: 3.1739 - classification_loss: 1.0335
 228/1000 [=====>........................] - ETA: 5:55 - loss: 4.2105 - regression_loss: 3.1757 - classification_loss: 1.0349
 229/1000 [=====>........................] - ETA: 5:54 - loss: 4.2151 - regression_loss: 3.1794 - classification_loss: 1.0357
 230/1000 [=====>........................] - ETA: 5:54 - loss: 4.1968 - regression_loss: 3.1656 - classification_loss: 1.0312
 231/1000 [=====>........................] - ETA: 5:53 - loss: 4.2040 - regression_loss: 3.1702 - classification_loss: 1.0338
 232/1000 [=====>........................] - ETA: 5:53 - loss: 4.2070 - regression_loss: 3.1722 - classification_loss: 1.0348
 233/1000 [=====>........................] - ETA: 5:52 - loss: 4.2222 - regression_loss: 3.1849 - classification_loss: 1.0373
 234/1000 [======>.......................] - ETA: 5:52 - loss: 4.2042 - regression_loss: 3.1713 - classification_loss: 1.0329
 235/1000 [======>.......................] - ETA: 5:51 - loss: 4.2065 - regression_loss: 3.1724 - classification_loss: 1.0341
 236/1000 [======>.......................] - ETA: 5:51 - loss: 4.1887 - regression_loss: 3.1590 - classification_loss: 1.0297
 237/1000 [======>.......................] - ETA: 5:50 - loss: 4.1926 - regression_loss: 3.1606 - classification_loss: 1.0321
 238/1000 [======>.......................] - ETA: 5:50 - loss: 4.2007 - regression_loss: 3.1643 - classification_loss: 1.0365keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 252 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([8.280e+02, 8.790e+02, 1.419e+03, 1.315e+03, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 239/1000 [======>.......................] - ETA: 5:49 - loss: 4.2071 - regression_loss: 3.1663 - classification_loss: 1.0408
 240/1000 [======>.......................] - ETA: 5:49 - loss: 4.2162 - regression_loss: 3.1709 - classification_loss: 1.0453
 241/1000 [======>.......................] - ETA: 5:48 - loss: 4.2209 - regression_loss: 3.1748 - classification_loss: 1.0461
 242/1000 [======>.......................] - ETA: 5:48 - loss: 4.2257 - regression_loss: 3.1773 - classification_loss: 1.0484
 243/1000 [======>.......................] - ETA: 5:47 - loss: 4.2285 - regression_loss: 3.1777 - classification_loss: 1.0508
 244/1000 [======>.......................] - ETA: 5:47 - loss: 4.2349 - regression_loss: 3.1822 - classification_loss: 1.0527
 245/1000 [======>.......................] - ETA: 5:46 - loss: 4.2379 - regression_loss: 3.1841 - classification_loss: 1.0538
 246/1000 [======>.......................] - ETA: 5:46 - loss: 4.2400 - regression_loss: 3.1852 - classification_loss: 1.0549
 247/1000 [======>.......................] - ETA: 5:46 - loss: 4.2467 - regression_loss: 3.1900 - classification_loss: 1.0568
 248/1000 [======>.......................] - ETA: 5:45 - loss: 4.2296 - regression_loss: 3.1771 - classification_loss: 1.0525keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 314 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.290e+03, 6.400e+02, 1.481e+03, 8.880e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 249/1000 [======>.......................] - ETA: 5:45 - loss: 4.2126 - regression_loss: 3.1643 - classification_loss: 1.0483
 250/1000 [======>.......................] - ETA: 5:44 - loss: 4.2186 - regression_loss: 3.1665 - classification_loss: 1.0521
 251/1000 [======>.......................] - ETA: 5:44 - loss: 4.2206 - regression_loss: 3.1663 - classification_loss: 1.0543
 252/1000 [======>.......................] - ETA: 5:43 - loss: 4.2243 - regression_loss: 3.1686 - classification_loss: 1.0557keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 533 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.150e+03, 3.800e+02, 1.456e+03, 6.750e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 253/1000 [======>.......................] - ETA: 5:43 - loss: 4.2076 - regression_loss: 3.1561 - classification_loss: 1.0515keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 159 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.284e+03, 6.400e+02, 1.518e+03, 9.170e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 254/1000 [======>.......................] - ETA: 5:42 - loss: 4.2107 - regression_loss: 3.1577 - classification_loss: 1.0530
 255/1000 [======>.......................] - ETA: 5:42 - loss: 4.2151 - regression_loss: 3.1607 - classification_loss: 1.0545
 256/1000 [======>.......................] - ETA: 5:41 - loss: 4.2199 - regression_loss: 3.1639 - classification_loss: 1.0560
 257/1000 [======>.......................] - ETA: 5:41 - loss: 4.2231 - regression_loss: 3.1661 - classification_loss: 1.0570
 258/1000 [======>.......................] - ETA: 5:40 - loss: 4.2258 - regression_loss: 3.1673 - classification_loss: 1.0586
 259/1000 [======>.......................] - ETA: 5:40 - loss: 4.2095 - regression_loss: 3.1550 - classification_loss: 1.0545
 260/1000 [======>.......................] - ETA: 5:39 - loss: 4.2162 - regression_loss: 3.1577 - classification_loss: 1.0585
 261/1000 [======>.......................] - ETA: 5:39 - loss: 4.2204 - regression_loss: 3.1606 - classification_loss: 1.0598keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 732 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.252e+03, 6.210e+02, 1.495e+03, 9.380e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 262/1000 [======>.......................] - ETA: 5:38 - loss: 4.2228 - regression_loss: 3.1620 - classification_loss: 1.0608
 263/1000 [======>.......................] - ETA: 5:38 - loss: 4.2067 - regression_loss: 3.1499 - classification_loss: 1.0568
 264/1000 [======>.......................] - ETA: 5:38 - loss: 4.1908 - regression_loss: 3.1380 - classification_loss: 1.0528keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 774 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([9.900e+02, 1.080e+02, 1.518e+03, 8.040e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 265/1000 [======>.......................] - ETA: 5:37 - loss: 4.1949 - regression_loss: 3.1406 - classification_loss: 1.0543
 266/1000 [======>.......................] - ETA: 5:36 - loss: 4.2019 - regression_loss: 3.1443 - classification_loss: 1.0576
 267/1000 [=======>......................] - ETA: 5:36 - loss: 4.2061 - regression_loss: 3.1465 - classification_loss: 1.0596keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 422 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.112e+03, 3.400e+02, 1.580e+03, 8.430e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 268/1000 [=======>......................] - ETA: 5:36 - loss: 4.2099 - regression_loss: 3.1485 - classification_loss: 1.0614
 269/1000 [=======>......................] - ETA: 5:35 - loss: 4.2142 - regression_loss: 3.1504 - classification_loss: 1.0639
 270/1000 [=======>......................] - ETA: 5:35 - loss: 4.2171 - regression_loss: 3.1525 - classification_loss: 1.0646
 271/1000 [=======>......................] - ETA: 5:34 - loss: 4.2188 - regression_loss: 3.1531 - classification_loss: 1.0657
 272/1000 [=======>......................] - ETA: 5:34 - loss: 4.2033 - regression_loss: 3.1415 - classification_loss: 1.0617
 273/1000 [=======>......................] - ETA: 5:33 - loss: 4.2077 - regression_loss: 3.1451 - classification_loss: 1.0625
 274/1000 [=======>......................] - ETA: 5:33 - loss: 4.2118 - regression_loss: 3.1478 - classification_loss: 1.0639
 275/1000 [=======>......................] - ETA: 5:32 - loss: 4.1964 - regression_loss: 3.1364 - classification_loss: 1.0600
 276/1000 [=======>......................] - ETA: 5:32 - loss: 4.1813 - regression_loss: 3.1250 - classification_loss: 1.0562
 277/1000 [=======>......................] - ETA: 5:31 - loss: 4.1853 - regression_loss: 3.1269 - classification_loss: 1.0584keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 700 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.217e+03, 7.800e+01, 1.410e+03, 3.150e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 278/1000 [=======>......................] - ETA: 5:31 - loss: 4.1702 - regression_loss: 3.1156 - classification_loss: 1.0546
 279/1000 [=======>......................] - ETA: 5:30 - loss: 4.1746 - regression_loss: 3.1194 - classification_loss: 1.0552keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 842 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.241e+03, 1.330e+02, 1.420e+03, 3.740e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 280/1000 [=======>......................] - ETA: 5:30 - loss: 4.1778 - regression_loss: 3.1216 - classification_loss: 1.0562
 281/1000 [=======>......................] - ETA: 5:29 - loss: 4.1815 - regression_loss: 3.1233 - classification_loss: 1.0583keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 174 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.141e+03, 2.590e+02, 1.613e+03, 6.570e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 282/1000 [=======>......................] - ETA: 5:29 - loss: 4.1844 - regression_loss: 3.1253 - classification_loss: 1.0591
 283/1000 [=======>......................] - ETA: 5:28 - loss: 4.1696 - regression_loss: 3.1143 - classification_loss: 1.0554
 284/1000 [=======>......................] - ETA: 5:28 - loss: 4.1727 - regression_loss: 3.1160 - classification_loss: 1.0567
 285/1000 [=======>......................] - ETA: 5:27 - loss: 4.1581 - regression_loss: 3.1051 - classification_loss: 1.0530
 286/1000 [=======>......................] - ETA: 5:27 - loss: 4.1669 - regression_loss: 3.1116 - classification_loss: 1.0553
 287/1000 [=======>......................] - ETA: 5:26 - loss: 4.1701 - regression_loss: 3.1133 - classification_loss: 1.0568
 288/1000 [=======>......................] - ETA: 5:26 - loss: 4.1556 - regression_loss: 3.1025 - classification_loss: 1.0531
 289/1000 [=======>......................] - ETA: 5:26 - loss: 4.1591 - regression_loss: 3.1045 - classification_loss: 1.0546
 290/1000 [=======>......................] - ETA: 5:25 - loss: 4.1447 - regression_loss: 3.0938 - classification_loss: 1.0510
 291/1000 [=======>......................] - ETA: 5:25 - loss: 4.1542 - regression_loss: 3.0981 - classification_loss: 1.0560
 292/1000 [=======>......................] - ETA: 5:24 - loss: 4.1399 - regression_loss: 3.0875 - classification_loss: 1.0524keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 248 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.244e+03, 9.720e+02, 1.505e+03, 1.238e+03, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 293/1000 [=======>......................] - ETA: 5:24 - loss: 4.1522 - regression_loss: 3.0908 - classification_loss: 1.0614
 294/1000 [=======>......................] - ETA: 5:23 - loss: 4.1570 - regression_loss: 3.0929 - classification_loss: 1.0640
 295/1000 [=======>......................] - ETA: 5:23 - loss: 4.1627 - regression_loss: 3.0957 - classification_loss: 1.0670
 296/1000 [=======>......................] - ETA: 5:22 - loss: 4.1713 - regression_loss: 3.1015 - classification_loss: 1.0698
 297/1000 [=======>......................] - ETA: 5:22 - loss: 4.1791 - regression_loss: 3.1077 - classification_loss: 1.0715
 298/1000 [=======>......................] - ETA: 5:21 - loss: 4.1830 - regression_loss: 3.1101 - classification_loss: 1.0729
 299/1000 [=======>......................] - ETA: 5:21 - loss: 4.1961 - regression_loss: 3.1152 - classification_loss: 1.0809
 300/1000 [========>.....................] - ETA: 5:20 - loss: 4.2012 - regression_loss: 3.1178 - classification_loss: 1.0834
 301/1000 [========>.....................] - ETA: 5:20 - loss: 4.2061 - regression_loss: 3.1207 - classification_loss: 1.0854
 302/1000 [========>.....................] - ETA: 5:19 - loss: 4.1922 - regression_loss: 3.1104 - classification_loss: 1.0818keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 753 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.130e+03, 0.000e+00, 1.539e+03, 3.870e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 303/1000 [========>.....................] - ETA: 5:19 - loss: 4.1783 - regression_loss: 3.1001 - classification_loss: 1.0782keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 194 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.010e+03, 6.980e+02, 1.676e+03, 1.347e+03, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 304/1000 [========>.....................] - ETA: 5:18 - loss: 4.1811 - regression_loss: 3.1021 - classification_loss: 1.0789
 305/1000 [========>.....................] - ETA: 5:18 - loss: 4.1673 - regression_loss: 3.0919 - classification_loss: 1.0754
 306/1000 [========>.....................] - ETA: 5:18 - loss: 4.1735 - regression_loss: 3.0932 - classification_loss: 1.0803
 307/1000 [========>.....................] - ETA: 5:17 - loss: 4.1786 - regression_loss: 3.0960 - classification_loss: 1.0827keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 372 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.063e+03, 0.000e+00, 1.430e+03, 2.710e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 308/1000 [========>.....................] - ETA: 5:17 - loss: 4.1817 - regression_loss: 3.0970 - classification_loss: 1.0847
 309/1000 [========>.....................] - ETA: 5:16 - loss: 4.1859 - regression_loss: 3.0991 - classification_loss: 1.0868
 310/1000 [========>.....................] - ETA: 5:16 - loss: 4.1892 - regression_loss: 3.1013 - classification_loss: 1.0879
 311/1000 [========>.....................] - ETA: 5:15 - loss: 4.1924 - regression_loss: 3.1027 - classification_loss: 1.0897
 312/1000 [========>.....................] - ETA: 5:15 - loss: 4.1955 - regression_loss: 3.1046 - classification_loss: 1.0909
 313/1000 [========>.....................] - ETA: 5:14 - loss: 4.1821 - regression_loss: 3.0947 - classification_loss: 1.0874
 314/1000 [========>.....................] - ETA: 5:14 - loss: 4.1688 - regression_loss: 3.0848 - classification_loss: 1.0839
 315/1000 [========>.....................] - ETA: 5:13 - loss: 4.1722 - regression_loss: 3.0872 - classification_loss: 1.0851keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 804 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.309e+03, 9.070e+02, 1.489e+03, 1.119e+03, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 316/1000 [========>.....................] - ETA: 5:13 - loss: 4.1746 - regression_loss: 3.0885 - classification_loss: 1.0861
 317/1000 [========>.....................] - ETA: 5:12 - loss: 4.1774 - regression_loss: 3.0905 - classification_loss: 1.0870keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 759 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.008e+03, 0.000e+00, 1.706e+03, 5.740e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 318/1000 [========>.....................] - ETA: 5:12 - loss: 4.1643 - regression_loss: 3.0807 - classification_loss: 1.0836
 319/1000 [========>.....................] - ETA: 5:11 - loss: 4.1686 - regression_loss: 3.0827 - classification_loss: 1.0859
 320/1000 [========>.....................] - ETA: 5:11 - loss: 4.1722 - regression_loss: 3.0852 - classification_loss: 1.0871keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 913 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([8.740e+02, 2.250e+02, 1.487e+03, 1.078e+03, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 321/1000 [========>.....................] - ETA: 5:10 - loss: 4.1759 - regression_loss: 3.0880 - classification_loss: 1.0879
 322/1000 [========>.....................] - ETA: 5:10 - loss: 4.1781 - regression_loss: 3.0891 - classification_loss: 1.0890
 323/1000 [========>.....................] - ETA: 5:10 - loss: 4.1826 - regression_loss: 3.0916 - classification_loss: 1.0910
 324/1000 [========>.....................] - ETA: 5:09 - loss: 4.1869 - regression_loss: 3.0954 - classification_loss: 1.0915
 325/1000 [========>.....................] - ETA: 5:09 - loss: 4.1905 - regression_loss: 3.0979 - classification_loss: 1.0926
 326/1000 [========>.....................] - ETA: 5:08 - loss: 4.1776 - regression_loss: 3.0884 - classification_loss: 1.0893
 327/1000 [========>.....................] - ETA: 5:08 - loss: 4.1798 - regression_loss: 3.0891 - classification_loss: 1.0907
 328/1000 [========>.....................] - ETA: 5:07 - loss: 4.1670 - regression_loss: 3.0797 - classification_loss: 1.0874keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 895 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.203e+03, 3.390e+02, 1.563e+03, 7.020e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 329/1000 [========>.....................] - ETA: 5:07 - loss: 4.1735 - regression_loss: 3.0837 - classification_loss: 1.0897
 330/1000 [========>.....................] - ETA: 5:06 - loss: 4.1756 - regression_loss: 3.0854 - classification_loss: 1.0901
 331/1000 [========>.....................] - ETA: 5:06 - loss: 4.1630 - regression_loss: 3.0761 - classification_loss: 1.0868
 332/1000 [========>.....................] - ETA: 5:05 - loss: 4.1657 - regression_loss: 3.0785 - classification_loss: 1.0872keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 52 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.249e+03, 6.190e+02, 1.507e+03, 9.160e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 333/1000 [========>.....................] - ETA: 5:05 - loss: 4.1697 - regression_loss: 3.0808 - classification_loss: 1.0889
 334/1000 [=========>....................] - ETA: 5:04 - loss: 4.1715 - regression_loss: 3.0818 - classification_loss: 1.0897
 335/1000 [=========>....................] - ETA: 5:04 - loss: 4.1773 - regression_loss: 3.0839 - classification_loss: 1.0934
 336/1000 [=========>....................] - ETA: 5:03 - loss: 4.1809 - regression_loss: 3.0859 - classification_loss: 1.0950
 337/1000 [=========>....................] - ETA: 5:03 - loss: 4.1843 - regression_loss: 3.0875 - classification_loss: 1.0968
 338/1000 [=========>....................] - ETA: 5:03 - loss: 4.1867 - regression_loss: 3.0887 - classification_loss: 1.0980
 339/1000 [=========>....................] - ETA: 5:02 - loss: 4.1743 - regression_loss: 3.0796 - classification_loss: 1.0947
 340/1000 [=========>....................] - ETA: 5:02 - loss: 4.1805 - regression_loss: 3.0808 - classification_loss: 1.0997
 341/1000 [=========>....................] - ETA: 5:01 - loss: 4.1823 - regression_loss: 3.0812 - classification_loss: 1.1012
 342/1000 [=========>....................] - ETA: 5:01 - loss: 4.1863 - regression_loss: 3.0837 - classification_loss: 1.1025
 343/1000 [=========>....................] - ETA: 5:00 - loss: 4.1741 - regression_loss: 3.0747 - classification_loss: 1.0993
 344/1000 [=========>....................] - ETA: 5:00 - loss: 4.1752 - regression_loss: 3.0754 - classification_loss: 1.0998
 345/1000 [=========>....................] - ETA: 4:59 - loss: 4.1790 - regression_loss: 3.0783 - classification_loss: 1.1007keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 234 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.023e+03, 7.910e+02, 1.406e+03, 1.287e+03, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 346/1000 [=========>....................] - ETA: 4:59 - loss: 4.1823 - regression_loss: 3.0806 - classification_loss: 1.1017
 347/1000 [=========>....................] - ETA: 4:58 - loss: 4.1849 - regression_loss: 3.0822 - classification_loss: 1.1027
 348/1000 [=========>....................] - ETA: 4:58 - loss: 4.1728 - regression_loss: 3.0733 - classification_loss: 1.0995
 349/1000 [=========>....................] - ETA: 4:57 - loss: 4.1609 - regression_loss: 3.0645 - classification_loss: 1.0964
 350/1000 [=========>....................] - ETA: 4:57 - loss: 4.1653 - regression_loss: 3.0681 - classification_loss: 1.0971
 351/1000 [=========>....................] - ETA: 4:56 - loss: 4.1686 - regression_loss: 3.0700 - classification_loss: 1.0986
 352/1000 [=========>....................] - ETA: 4:56 - loss: 4.1738 - regression_loss: 3.0723 - classification_loss: 1.1015
 353/1000 [=========>....................] - ETA: 4:56 - loss: 4.1761 - regression_loss: 3.0742 - classification_loss: 1.1019
 354/1000 [=========>....................] - ETA: 4:55 - loss: 4.1784 - regression_loss: 3.0756 - classification_loss: 1.1028
 355/1000 [=========>....................] - ETA: 4:55 - loss: 4.1833 - regression_loss: 3.0797 - classification_loss: 1.1036
 356/1000 [=========>....................] - ETA: 4:54 - loss: 4.1716 - regression_loss: 3.0711 - classification_loss: 1.1005
 357/1000 [=========>....................] - ETA: 4:54 - loss: 4.1770 - regression_loss: 3.0757 - classification_loss: 1.1013
 358/1000 [=========>....................] - ETA: 4:53 - loss: 4.1803 - regression_loss: 3.0782 - classification_loss: 1.1021
 359/1000 [=========>....................] - ETA: 4:53 - loss: 4.1837 - regression_loss: 3.0804 - classification_loss: 1.1033
 360/1000 [=========>....................] - ETA: 4:52 - loss: 4.1875 - regression_loss: 3.0832 - classification_loss: 1.1044
 361/1000 [=========>....................] - ETA: 4:52 - loss: 4.1923 - regression_loss: 3.0854 - classification_loss: 1.1070
 362/1000 [=========>....................] - ETA: 4:51 - loss: 4.1973 - regression_loss: 3.0893 - classification_loss: 1.1080
 363/1000 [=========>....................] - ETA: 4:51 - loss: 4.2007 - regression_loss: 3.0917 - classification_loss: 1.1090
 364/1000 [=========>....................] - ETA: 4:50 - loss: 4.2041 - regression_loss: 3.0948 - classification_loss: 1.1093
 365/1000 [=========>....................] - ETA: 4:50 - loss: 4.2071 - regression_loss: 3.0973 - classification_loss: 1.1098
 366/1000 [=========>....................] - ETA: 4:49 - loss: 4.2087 - regression_loss: 3.0980 - classification_loss: 1.1107
 367/1000 [==========>...................] - ETA: 4:49 - loss: 4.2106 - regression_loss: 3.0995 - classification_loss: 1.1111
 368/1000 [==========>...................] - ETA: 4:49 - loss: 4.2144 - regression_loss: 3.1014 - classification_loss: 1.1130
 369/1000 [==========>...................] - ETA: 4:48 - loss: 4.2030 - regression_loss: 3.0930 - classification_loss: 1.1099
 370/1000 [==========>...................] - ETA: 4:48 - loss: 4.2048 - regression_loss: 3.0945 - classification_loss: 1.1103
 371/1000 [==========>...................] - ETA: 4:47 - loss: 4.2057 - regression_loss: 3.0951 - classification_loss: 1.1106
 372/1000 [==========>...................] - ETA: 4:47 - loss: 4.2074 - regression_loss: 3.0966 - classification_loss: 1.1108keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 939 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.000e+03, 3.720e+02, 1.515e+03, 8.560e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 373/1000 [==========>...................] - ETA: 4:46 - loss: 4.2091 - regression_loss: 3.0979 - classification_loss: 1.1112
 374/1000 [==========>...................] - ETA: 4:46 - loss: 4.2124 - regression_loss: 3.1000 - classification_loss: 1.1123
 375/1000 [==========>...................] - ETA: 4:45 - loss: 4.2011 - regression_loss: 3.0918 - classification_loss: 1.1094
 376/1000 [==========>...................] - ETA: 4:45 - loss: 4.2028 - regression_loss: 3.0931 - classification_loss: 1.1097keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 226 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.156e+03, 0.000e+00, 1.477e+03, 3.710e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 377/1000 [==========>...................] - ETA: 4:44 - loss: 4.1917 - regression_loss: 3.0849 - classification_loss: 1.1068
 378/1000 [==========>...................] - ETA: 4:44 - loss: 4.1928 - regression_loss: 3.0857 - classification_loss: 1.1071
 379/1000 [==========>...................] - ETA: 4:43 - loss: 4.1959 - regression_loss: 3.0877 - classification_loss: 1.1082
 380/1000 [==========>...................] - ETA: 4:43 - loss: 4.1848 - regression_loss: 3.0796 - classification_loss: 1.1052
 381/1000 [==========>...................] - ETA: 4:42 - loss: 4.1858 - regression_loss: 3.0799 - classification_loss: 1.1059
 382/1000 [==========>...................] - ETA: 4:42 - loss: 4.1891 - regression_loss: 3.0823 - classification_loss: 1.1068
 383/1000 [==========>...................] - ETA: 4:42 - loss: 4.1782 - regression_loss: 3.0743 - classification_loss: 1.1039keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 841 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.323e+03, 1.100e+01, 1.438e+03, 1.500e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 384/1000 [==========>...................] - ETA: 4:41 - loss: 4.1800 - regression_loss: 3.0757 - classification_loss: 1.1043keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 230 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([9.870e+02, 2.710e+02, 1.703e+03, 1.120e+03, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 385/1000 [==========>...................] - ETA: 4:41 - loss: 4.1824 - regression_loss: 3.0773 - classification_loss: 1.1052
 386/1000 [==========>...................] - ETA: 4:40 - loss: 4.1716 - regression_loss: 3.0693 - classification_loss: 1.1023
 387/1000 [==========>...................] - ETA: 4:40 - loss: 4.1608 - regression_loss: 3.0614 - classification_loss: 1.0994
 388/1000 [==========>...................] - ETA: 4:39 - loss: 4.1626 - regression_loss: 3.0625 - classification_loss: 1.1001
 389/1000 [==========>...................] - ETA: 4:39 - loss: 4.1631 - regression_loss: 3.0625 - classification_loss: 1.1005
 390/1000 [==========>...................] - ETA: 4:38 - loss: 4.1671 - regression_loss: 3.0662 - classification_loss: 1.1009
 391/1000 [==========>...................] - ETA: 4:38 - loss: 4.1697 - regression_loss: 3.0682 - classification_loss: 1.1015
 392/1000 [==========>...................] - ETA: 4:37 - loss: 4.1590 - regression_loss: 3.0604 - classification_loss: 1.0987
 393/1000 [==========>...................] - ETA: 4:37 - loss: 4.1485 - regression_loss: 3.0526 - classification_loss: 1.0959
 394/1000 [==========>...................] - ETA: 4:36 - loss: 4.1379 - regression_loss: 3.0448 - classification_loss: 1.0931
 395/1000 [==========>...................] - ETA: 4:36 - loss: 4.1274 - regression_loss: 3.0371 - classification_loss: 1.0903
 396/1000 [==========>...................] - ETA: 4:36 - loss: 4.1297 - regression_loss: 3.0386 - classification_loss: 1.0911
 397/1000 [==========>...................] - ETA: 4:35 - loss: 4.1322 - regression_loss: 3.0403 - classification_loss: 1.0919
 398/1000 [==========>...................] - ETA: 4:35 - loss: 4.1367 - regression_loss: 3.0416 - classification_loss: 1.0951
 399/1000 [==========>...................] - ETA: 4:34 - loss: 4.1414 - regression_loss: 3.0447 - classification_loss: 1.0967
 400/1000 [===========>..................] - ETA: 4:34 - loss: 4.1435 - regression_loss: 3.0462 - classification_loss: 1.0973keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 213 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([0.000e+00, 1.630e+03, 8.300e+01, 1.803e+03, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 401/1000 [===========>..................] - ETA: 4:33 - loss: 4.1463 - regression_loss: 3.0481 - classification_loss: 1.0983
 402/1000 [===========>..................] - ETA: 4:33 - loss: 4.1473 - regression_loss: 3.0479 - classification_loss: 1.0994
 403/1000 [===========>..................] - ETA: 4:32 - loss: 4.1502 - regression_loss: 3.0504 - classification_loss: 1.0998
 404/1000 [===========>..................] - ETA: 4:32 - loss: 4.1524 - regression_loss: 3.0519 - classification_loss: 1.1005
 405/1000 [===========>..................] - ETA: 4:31 - loss: 4.1569 - regression_loss: 3.0547 - classification_loss: 1.1022
 406/1000 [===========>..................] - ETA: 4:31 - loss: 4.1467 - regression_loss: 3.0472 - classification_loss: 1.0994keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 783 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.174e+03, 9.100e+01, 1.473e+03, 3.850e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 407/1000 [===========>..................] - ETA: 4:31 - loss: 4.1508 - regression_loss: 3.0489 - classification_loss: 1.1019
 408/1000 [===========>..................] - ETA: 4:30 - loss: 4.1584 - regression_loss: 3.0521 - classification_loss: 1.1062
 409/1000 [===========>..................] - ETA: 4:30 - loss: 4.1613 - regression_loss: 3.0543 - classification_loss: 1.1070
 410/1000 [===========>..................] - ETA: 4:29 - loss: 4.1657 - regression_loss: 3.0576 - classification_loss: 1.1081
 411/1000 [===========>..................] - ETA: 4:29 - loss: 4.1556 - regression_loss: 3.0502 - classification_loss: 1.1054keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 163 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([9.900e+02, 1.230e+02, 1.643e+03, 9.870e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 412/1000 [===========>..................] - ETA: 4:28 - loss: 4.1580 - regression_loss: 3.0507 - classification_loss: 1.1072
 413/1000 [===========>..................] - ETA: 4:28 - loss: 4.1616 - regression_loss: 3.0527 - classification_loss: 1.1089keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 794 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.081e+03, 1.145e+03, 1.418e+03, 1.435e+03, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 414/1000 [===========>..................] - ETA: 4:27 - loss: 4.1658 - regression_loss: 3.0552 - classification_loss: 1.1107
 415/1000 [===========>..................] - ETA: 4:27 - loss: 4.1689 - regression_loss: 3.0570 - classification_loss: 1.1119
 416/1000 [===========>..................] - ETA: 4:26 - loss: 4.1719 - regression_loss: 3.0590 - classification_loss: 1.1129keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 549 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.145e+03, 0.000e+00, 1.526e+03, 2.920e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 417/1000 [===========>..................] - ETA: 4:26 - loss: 4.1619 - regression_loss: 3.0517 - classification_loss: 1.1102
 418/1000 [===========>..................] - ETA: 4:25 - loss: 4.1647 - regression_loss: 3.0535 - classification_loss: 1.1111
 419/1000 [===========>..................] - ETA: 4:25 - loss: 4.1672 - regression_loss: 3.0555 - classification_loss: 1.1117
 420/1000 [===========>..................] - ETA: 4:25 - loss: 4.1700 - regression_loss: 3.0580 - classification_loss: 1.1120
 421/1000 [===========>..................] - ETA: 4:24 - loss: 4.1740 - regression_loss: 3.0616 - classification_loss: 1.1124
 422/1000 [===========>..................] - ETA: 4:24 - loss: 4.1641 - regression_loss: 3.0543 - classification_loss: 1.1098keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 210 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([9.460e+02, 9.290e+02, 1.423e+03, 1.391e+03, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 423/1000 [===========>..................] - ETA: 4:23 - loss: 4.1542 - regression_loss: 3.0471 - classification_loss: 1.1071
 424/1000 [===========>..................] - ETA: 4:23 - loss: 4.1444 - regression_loss: 3.0399 - classification_loss: 1.1045
 425/1000 [===========>..................] - ETA: 4:22 - loss: 4.1464 - regression_loss: 3.0416 - classification_loss: 1.1048
 426/1000 [===========>..................] - ETA: 4:22 - loss: 4.1482 - regression_loss: 3.0429 - classification_loss: 1.1053
 427/1000 [===========>..................] - ETA: 4:21 - loss: 4.1385 - regression_loss: 3.0358 - classification_loss: 1.1027
 428/1000 [===========>..................] - ETA: 4:21 - loss: 4.1414 - regression_loss: 3.0370 - classification_loss: 1.1044
 429/1000 [===========>..................] - ETA: 4:20 - loss: 4.1458 - regression_loss: 3.0403 - classification_loss: 1.1055
 430/1000 [===========>..................] - ETA: 4:20 - loss: 4.1485 - regression_loss: 3.0418 - classification_loss: 1.1068
 431/1000 [===========>..................] - ETA: 4:19 - loss: 4.1506 - regression_loss: 3.0435 - classification_loss: 1.1071
 432/1000 [===========>..................] - ETA: 4:19 - loss: 4.1527 - regression_loss: 3.0448 - classification_loss: 1.1079
 433/1000 [===========>..................] - ETA: 4:18 - loss: 4.1432 - regression_loss: 3.0378 - classification_loss: 1.1054
 434/1000 [============>.................] - ETA: 4:18 - loss: 4.1458 - regression_loss: 3.0378 - classification_loss: 1.1080
 435/1000 [============>.................] - ETA: 4:18 - loss: 4.1475 - regression_loss: 3.0392 - classification_loss: 1.1082
 436/1000 [============>.................] - ETA: 4:17 - loss: 4.1503 - regression_loss: 3.0395 - classification_loss: 1.1108
 437/1000 [============>.................] - ETA: 4:17 - loss: 4.1521 - regression_loss: 3.0404 - classification_loss: 1.1117
 438/1000 [============>.................] - ETA: 4:16 - loss: 4.1546 - regression_loss: 3.0425 - classification_loss: 1.1121
 439/1000 [============>.................] - ETA: 4:16 - loss: 4.1582 - regression_loss: 3.0447 - classification_loss: 1.1135
 440/1000 [============>.................] - ETA: 4:15 - loss: 4.1601 - regression_loss: 3.0461 - classification_loss: 1.1141
 441/1000 [============>.................] - ETA: 4:15 - loss: 4.1626 - regression_loss: 3.0471 - classification_loss: 1.1155
 442/1000 [============>.................] - ETA: 4:14 - loss: 4.1683 - regression_loss: 3.0506 - classification_loss: 1.1177
 443/1000 [============>.................] - ETA: 4:14 - loss: 4.1589 - regression_loss: 3.0437 - classification_loss: 1.1152
 444/1000 [============>.................] - ETA: 4:13 - loss: 4.1600 - regression_loss: 3.0446 - classification_loss: 1.1154
 445/1000 [============>.................] - ETA: 4:13 - loss: 4.1634 - regression_loss: 3.0463 - classification_loss: 1.1171
 446/1000 [============>.................] - ETA: 4:12 - loss: 4.1655 - regression_loss: 3.0478 - classification_loss: 1.1176keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 89 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([8.020e+02, 8.030e+02, 1.467e+03, 1.324e+03, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 447/1000 [============>.................] - ETA: 4:12 - loss: 4.1702 - regression_loss: 3.0512 - classification_loss: 1.1190
 448/1000 [============>.................] - ETA: 4:12 - loss: 4.1722 - regression_loss: 3.0526 - classification_loss: 1.1195keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 144 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.234e+03, 3.680e+02, 1.444e+03, 5.290e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 449/1000 [============>.................] - ETA: 4:11 - loss: 4.1743 - regression_loss: 3.0543 - classification_loss: 1.1200
 450/1000 [============>.................] - ETA: 4:11 - loss: 4.1753 - regression_loss: 3.0550 - classification_loss: 1.1203
 451/1000 [============>.................] - ETA: 4:10 - loss: 4.1787 - regression_loss: 3.0571 - classification_loss: 1.1216
 452/1000 [============>.................] - ETA: 4:10 - loss: 4.1805 - regression_loss: 3.0586 - classification_loss: 1.1219
 453/1000 [============>.................] - ETA: 4:09 - loss: 4.1825 - regression_loss: 3.0602 - classification_loss: 1.1223
 454/1000 [============>.................] - ETA: 4:09 - loss: 4.1836 - regression_loss: 3.0609 - classification_loss: 1.1226
 455/1000 [============>.................] - ETA: 4:08 - loss: 4.1860 - regression_loss: 3.0630 - classification_loss: 1.1229
 456/1000 [============>.................] - ETA: 4:08 - loss: 4.1880 - regression_loss: 3.0649 - classification_loss: 1.1231
 457/1000 [============>.................] - ETA: 4:07 - loss: 4.1788 - regression_loss: 3.0582 - classification_loss: 1.1207
 458/1000 [============>.................] - ETA: 4:07 - loss: 4.1811 - regression_loss: 3.0603 - classification_loss: 1.1208
 459/1000 [============>.................] - ETA: 4:07 - loss: 4.1720 - regression_loss: 3.0536 - classification_loss: 1.1184keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 224 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.054e+03, 1.014e+03, 1.528e+03, 1.363e+03, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 460/1000 [============>.................] - ETA: 4:06 - loss: 4.1744 - regression_loss: 3.0557 - classification_loss: 1.1187
 461/1000 [============>.................] - ETA: 4:06 - loss: 4.1758 - regression_loss: 3.0565 - classification_loss: 1.1192
 462/1000 [============>.................] - ETA: 4:05 - loss: 4.1788 - regression_loss: 3.0592 - classification_loss: 1.1196
 463/1000 [============>.................] - ETA: 4:05 - loss: 4.1822 - regression_loss: 3.0624 - classification_loss: 1.1198
 464/1000 [============>.................] - ETA: 4:04 - loss: 4.1837 - regression_loss: 3.0635 - classification_loss: 1.1201
 465/1000 [============>.................] - ETA: 4:04 - loss: 4.1861 - regression_loss: 3.0654 - classification_loss: 1.1207
 466/1000 [============>.................] - ETA: 4:03 - loss: 4.1868 - regression_loss: 3.0660 - classification_loss: 1.1208
 467/1000 [=============>................] - ETA: 4:03 - loss: 4.1889 - regression_loss: 3.0678 - classification_loss: 1.1211
 468/1000 [=============>................] - ETA: 4:02 - loss: 4.1900 - regression_loss: 3.0688 - classification_loss: 1.1213keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 161 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.205e+03, 0.000e+00, 1.417e+03, 2.600e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 469/1000 [=============>................] - ETA: 4:02 - loss: 4.1913 - regression_loss: 3.0698 - classification_loss: 1.1215
 470/1000 [=============>................] - ETA: 4:01 - loss: 4.1824 - regression_loss: 3.0633 - classification_loss: 1.1191keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 229 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.071e+03, 0.000e+00, 1.528e+03, 6.160e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 471/1000 [=============>................] - ETA: 4:01 - loss: 4.1735 - regression_loss: 3.0568 - classification_loss: 1.1167
 472/1000 [=============>................] - ETA: 4:01 - loss: 4.1768 - regression_loss: 3.0589 - classification_loss: 1.1179keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 714 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.221e+03, 4.740e+02, 1.555e+03, 7.480e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 473/1000 [=============>................] - ETA: 4:00 - loss: 4.1681 - regression_loss: 3.0525 - classification_loss: 1.1156
 474/1000 [=============>................] - ETA: 4:00 - loss: 4.1593 - regression_loss: 3.0460 - classification_loss: 1.1133
 475/1000 [=============>................] - ETA: 3:59 - loss: 4.1614 - regression_loss: 3.0479 - classification_loss: 1.1136
 476/1000 [=============>................] - ETA: 3:59 - loss: 4.1628 - regression_loss: 3.0490 - classification_loss: 1.1138
 477/1000 [=============>................] - ETA: 3:58 - loss: 4.1658 - regression_loss: 3.0512 - classification_loss: 1.1146
 478/1000 [=============>................] - ETA: 3:58 - loss: 4.1689 - regression_loss: 3.0539 - classification_loss: 1.1149
 479/1000 [=============>................] - ETA: 3:57 - loss: 4.1602 - regression_loss: 3.0476 - classification_loss: 1.1126
 480/1000 [=============>................] - ETA: 3:57 - loss: 4.1515 - regression_loss: 3.0412 - classification_loss: 1.1103keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 580 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.063e+03, 0.000e+00, 1.628e+03, 4.060e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 481/1000 [=============>................] - ETA: 3:56 - loss: 4.1429 - regression_loss: 3.0349 - classification_loss: 1.1080
 482/1000 [=============>................] - ETA: 3:56 - loss: 4.1448 - regression_loss: 3.0365 - classification_loss: 1.1084
 483/1000 [=============>................] - ETA: 3:55 - loss: 4.1362 - regression_loss: 3.0302 - classification_loss: 1.1061
 484/1000 [=============>................] - ETA: 3:55 - loss: 4.1379 - regression_loss: 3.0314 - classification_loss: 1.1065
 485/1000 [=============>................] - ETA: 3:55 - loss: 4.1401 - regression_loss: 3.0330 - classification_loss: 1.1072
 486/1000 [=============>................] - ETA: 3:54 - loss: 4.1424 - regression_loss: 3.0349 - classification_loss: 1.1075
 487/1000 [=============>................] - ETA: 3:54 - loss: 4.1339 - regression_loss: 3.0286 - classification_loss: 1.1053keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 357 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.152e+03, 7.420e+02, 1.490e+03, 9.580e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 488/1000 [=============>................] - ETA: 3:53 - loss: 4.1359 - regression_loss: 3.0300 - classification_loss: 1.1059
 489/1000 [=============>................] - ETA: 3:53 - loss: 4.1386 - regression_loss: 3.0319 - classification_loss: 1.1066
 490/1000 [=============>................] - ETA: 3:52 - loss: 4.1415 - regression_loss: 3.0340 - classification_loss: 1.1075
 491/1000 [=============>................] - ETA: 3:52 - loss: 4.1330 - regression_loss: 3.0278 - classification_loss: 1.1052keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 31 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.127e+03, 4.100e+02, 1.508e+03, 7.640e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 492/1000 [=============>................] - ETA: 3:51 - loss: 4.1358 - regression_loss: 3.0299 - classification_loss: 1.1059
 493/1000 [=============>................] - ETA: 3:51 - loss: 4.1373 - regression_loss: 3.0307 - classification_loss: 1.1067
 494/1000 [=============>................] - ETA: 3:50 - loss: 4.1412 - regression_loss: 3.0341 - classification_loss: 1.1071
 495/1000 [=============>................] - ETA: 3:50 - loss: 4.1430 - regression_loss: 3.0354 - classification_loss: 1.1076
 496/1000 [=============>................] - ETA: 3:49 - loss: 4.1438 - regression_loss: 3.0357 - classification_loss: 1.1081
 497/1000 [=============>................] - ETA: 3:49 - loss: 4.1447 - regression_loss: 3.0358 - classification_loss: 1.1088
 498/1000 [=============>................] - ETA: 3:49 - loss: 4.1363 - regression_loss: 3.0297 - classification_loss: 1.1066
 499/1000 [=============>................] - ETA: 3:48 - loss: 4.1381 - regression_loss: 3.0296 - classification_loss: 1.1085
 500/1000 [==============>...............] - ETA: 3:48 - loss: 4.1411 - regression_loss: 3.0318 - classification_loss: 1.1092
 501/1000 [==============>...............] - ETA: 3:47 - loss: 4.1429 - regression_loss: 3.0328 - classification_loss: 1.1101
 502/1000 [==============>...............] - ETA: 3:47 - loss: 4.1346 - regression_loss: 3.0268 - classification_loss: 1.1079
 503/1000 [==============>...............] - ETA: 3:46 - loss: 4.1382 - regression_loss: 3.0293 - classification_loss: 1.1090
 504/1000 [==============>...............] - ETA: 3:46 - loss: 4.1300 - regression_loss: 3.0232 - classification_loss: 1.1068
 505/1000 [==============>...............] - ETA: 3:45 - loss: 4.1332 - regression_loss: 3.0255 - classification_loss: 1.1077
 506/1000 [==============>...............] - ETA: 3:45 - loss: 4.1345 - regression_loss: 3.0262 - classification_loss: 1.1083
 507/1000 [==============>...............] - ETA: 3:44 - loss: 4.1389 - regression_loss: 3.0287 - classification_loss: 1.1102
 508/1000 [==============>...............] - ETA: 3:44 - loss: 4.1408 - regression_loss: 3.0301 - classification_loss: 1.1107
 509/1000 [==============>...............] - ETA: 3:44 - loss: 4.1442 - regression_loss: 3.0323 - classification_loss: 1.1119
 510/1000 [==============>...............] - ETA: 3:43 - loss: 4.1457 - regression_loss: 3.0336 - classification_loss: 1.1121
 511/1000 [==============>...............] - ETA: 3:43 - loss: 4.1474 - regression_loss: 3.0347 - classification_loss: 1.1126
 512/1000 [==============>...............] - ETA: 3:42 - loss: 4.1501 - regression_loss: 3.0367 - classification_loss: 1.1134
 513/1000 [==============>...............] - ETA: 3:42 - loss: 4.1527 - regression_loss: 3.0384 - classification_loss: 1.1143
 514/1000 [==============>...............] - ETA: 3:41 - loss: 4.1539 - regression_loss: 3.0392 - classification_loss: 1.1146
 515/1000 [==============>...............] - ETA: 3:41 - loss: 4.1559 - regression_loss: 3.0404 - classification_loss: 1.1155
 516/1000 [==============>...............] - ETA: 3:40 - loss: 4.1576 - regression_loss: 3.0416 - classification_loss: 1.1160
 517/1000 [==============>...............] - ETA: 3:40 - loss: 4.1585 - regression_loss: 3.0417 - classification_loss: 1.1168keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 664 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.125e+03, 3.750e+02, 1.586e+03, 8.060e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 518/1000 [==============>...............] - ETA: 3:39 - loss: 4.1604 - regression_loss: 3.0434 - classification_loss: 1.1170
 519/1000 [==============>...............] - ETA: 3:39 - loss: 4.1610 - regression_loss: 3.0439 - classification_loss: 1.1172
 520/1000 [==============>...............] - ETA: 3:39 - loss: 4.1630 - regression_loss: 3.0446 - classification_loss: 1.1184
 521/1000 [==============>...............] - ETA: 3:38 - loss: 4.1651 - regression_loss: 3.0463 - classification_loss: 1.1188keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 890 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.116e+03, 4.360e+02, 1.505e+03, 7.080e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 522/1000 [==============>...............] - ETA: 3:38 - loss: 4.1669 - regression_loss: 3.0479 - classification_loss: 1.1191
 523/1000 [==============>...............] - ETA: 3:37 - loss: 4.1688 - regression_loss: 3.0494 - classification_loss: 1.1194
 524/1000 [==============>...............] - ETA: 3:37 - loss: 4.1608 - regression_loss: 3.0436 - classification_loss: 1.1173
 525/1000 [==============>...............] - ETA: 3:36 - loss: 4.1627 - regression_loss: 3.0452 - classification_loss: 1.1175
 526/1000 [==============>...............] - ETA: 3:36 - loss: 4.1637 - regression_loss: 3.0458 - classification_loss: 1.1179
 527/1000 [==============>...............] - ETA: 3:35 - loss: 4.1645 - regression_loss: 3.0464 - classification_loss: 1.1181
 528/1000 [==============>...............] - ETA: 3:35 - loss: 4.1566 - regression_loss: 3.0407 - classification_loss: 1.1159
 529/1000 [==============>...............] - ETA: 3:34 - loss: 4.1487 - regression_loss: 3.0349 - classification_loss: 1.1138
 530/1000 [==============>...............] - ETA: 3:34 - loss: 4.1409 - regression_loss: 3.0292 - classification_loss: 1.1118keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 447 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.144e+03, 0.000e+00, 1.565e+03, 3.970e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 531/1000 [==============>...............] - ETA: 3:34 - loss: 4.1448 - regression_loss: 3.0322 - classification_loss: 1.1125
 532/1000 [==============>...............] - ETA: 3:33 - loss: 4.1370 - regression_loss: 3.0265 - classification_loss: 1.1104keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 553 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.010e+03, 0.000e+00, 1.445e+03, 5.020e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 533/1000 [==============>...............] - ETA: 3:33 - loss: 4.1405 - regression_loss: 3.0272 - classification_loss: 1.1133
 534/1000 [===============>..............] - ETA: 3:32 - loss: 4.1417 - regression_loss: 3.0279 - classification_loss: 1.1138
 535/1000 [===============>..............] - ETA: 3:32 - loss: 4.1433 - regression_loss: 3.0292 - classification_loss: 1.1141
 536/1000 [===============>..............] - ETA: 3:31 - loss: 4.1446 - regression_loss: 3.0303 - classification_loss: 1.1143
 537/1000 [===============>..............] - ETA: 3:31 - loss: 4.1468 - regression_loss: 3.0316 - classification_loss: 1.1152
 538/1000 [===============>..............] - ETA: 3:30 - loss: 4.1498 - regression_loss: 3.0333 - classification_loss: 1.1165
 539/1000 [===============>..............] - ETA: 3:30 - loss: 4.1522 - regression_loss: 3.0353 - classification_loss: 1.1169
 540/1000 [===============>..............] - ETA: 3:29 - loss: 4.1541 - regression_loss: 3.0362 - classification_loss: 1.1180
 541/1000 [===============>..............] - ETA: 3:29 - loss: 4.1465 - regression_loss: 3.0306 - classification_loss: 1.1159
 542/1000 [===============>..............] - ETA: 3:28 - loss: 4.1488 - regression_loss: 3.0328 - classification_loss: 1.1160
 543/1000 [===============>..............] - ETA: 3:28 - loss: 4.1411 - regression_loss: 3.0272 - classification_loss: 1.1140keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 366 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.311e+03, 5.000e+00, 1.464e+03, 1.880e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 544/1000 [===============>..............] - ETA: 3:28 - loss: 4.1423 - regression_loss: 3.0272 - classification_loss: 1.1151
 545/1000 [===============>..............] - ETA: 3:27 - loss: 4.1347 - regression_loss: 3.0217 - classification_loss: 1.1131
 546/1000 [===============>..............] - ETA: 3:27 - loss: 4.1272 - regression_loss: 3.0161 - classification_loss: 1.1110
 547/1000 [===============>..............] - ETA: 3:26 - loss: 4.1300 - regression_loss: 3.0183 - classification_loss: 1.1117
 548/1000 [===============>..............] - ETA: 3:26 - loss: 4.1225 - regression_loss: 3.0128 - classification_loss: 1.1097
 549/1000 [===============>..............] - ETA: 3:25 - loss: 4.1245 - regression_loss: 3.0143 - classification_loss: 1.1102
 550/1000 [===============>..............] - ETA: 3:25 - loss: 4.1267 - regression_loss: 3.0157 - classification_loss: 1.1110
 551/1000 [===============>..............] - ETA: 3:24 - loss: 4.1273 - regression_loss: 3.0160 - classification_loss: 1.1113
 552/1000 [===============>..............] - ETA: 3:24 - loss: 4.1292 - regression_loss: 3.0172 - classification_loss: 1.1119
 553/1000 [===============>..............] - ETA: 3:23 - loss: 4.1307 - regression_loss: 3.0182 - classification_loss: 1.1125keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 665 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.196e+03, 6.740e+02, 1.458e+03, 1.006e+03, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 554/1000 [===============>..............] - ETA: 3:23 - loss: 4.1233 - regression_loss: 3.0128 - classification_loss: 1.1105
 555/1000 [===============>..............] - ETA: 3:23 - loss: 4.1278 - regression_loss: 3.0133 - classification_loss: 1.1145
 556/1000 [===============>..............] - ETA: 3:22 - loss: 4.1295 - regression_loss: 3.0144 - classification_loss: 1.1151keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 815 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.135e+03, 2.000e+00, 1.548e+03, 3.710e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 557/1000 [===============>..............] - ETA: 3:22 - loss: 4.1220 - regression_loss: 3.0090 - classification_loss: 1.1130
 558/1000 [===============>..............] - ETA: 3:21 - loss: 4.1244 - regression_loss: 3.0108 - classification_loss: 1.1136
 559/1000 [===============>..............] - ETA: 3:21 - loss: 4.1257 - regression_loss: 3.0108 - classification_loss: 1.1149
 560/1000 [===============>..............] - ETA: 3:20 - loss: 4.1280 - regression_loss: 3.0120 - classification_loss: 1.1159
 561/1000 [===============>..............] - ETA: 3:20 - loss: 4.1312 - regression_loss: 3.0149 - classification_loss: 1.1163
 562/1000 [===============>..............] - ETA: 3:19 - loss: 4.1345 - regression_loss: 3.0168 - classification_loss: 1.1177
 563/1000 [===============>..............] - ETA: 3:19 - loss: 4.1391 - regression_loss: 3.0179 - classification_loss: 1.1211
 564/1000 [===============>..............] - ETA: 3:18 - loss: 4.1317 - regression_loss: 3.0126 - classification_loss: 1.1192
 565/1000 [===============>..............] - ETA: 3:18 - loss: 4.1352 - regression_loss: 3.0151 - classification_loss: 1.1201
 566/1000 [===============>..............] - ETA: 3:17 - loss: 4.1370 - regression_loss: 3.0163 - classification_loss: 1.1206
 567/1000 [================>.............] - ETA: 3:17 - loss: 4.1297 - regression_loss: 3.0110 - classification_loss: 1.1186
 568/1000 [================>.............] - ETA: 3:17 - loss: 4.1330 - regression_loss: 3.0129 - classification_loss: 1.1202
 569/1000 [================>.............] - ETA: 3:16 - loss: 4.1342 - regression_loss: 3.0135 - classification_loss: 1.1207
 570/1000 [================>.............] - ETA: 3:16 - loss: 4.1358 - regression_loss: 3.0146 - classification_loss: 1.1212
 571/1000 [================>.............] - ETA: 3:15 - loss: 4.1285 - regression_loss: 3.0093 - classification_loss: 1.1192
 572/1000 [================>.............] - ETA: 3:15 - loss: 4.1302 - regression_loss: 3.0103 - classification_loss: 1.1198
 573/1000 [================>.............] - ETA: 3:14 - loss: 4.1332 - regression_loss: 3.0125 - classification_loss: 1.1207
 574/1000 [================>.............] - ETA: 3:14 - loss: 4.1260 - regression_loss: 3.0073 - classification_loss: 1.1187
 575/1000 [================>.............] - ETA: 3:13 - loss: 4.1188 - regression_loss: 3.0020 - classification_loss: 1.1168
 576/1000 [================>.............] - ETA: 3:13 - loss: 4.1197 - regression_loss: 3.0024 - classification_loss: 1.1172
 577/1000 [================>.............] - ETA: 3:12 - loss: 4.1222 - regression_loss: 3.0043 - classification_loss: 1.1179
 578/1000 [================>.............] - ETA: 3:12 - loss: 4.1239 - regression_loss: 3.0053 - classification_loss: 1.1186
 579/1000 [================>.............] - ETA: 3:11 - loss: 4.1274 - regression_loss: 3.0075 - classification_loss: 1.1199
 580/1000 [================>.............] - ETA: 3:11 - loss: 4.1288 - regression_loss: 3.0085 - classification_loss: 1.1203
 581/1000 [================>.............] - ETA: 3:11 - loss: 4.1309 - regression_loss: 3.0092 - classification_loss: 1.1217
 582/1000 [================>.............] - ETA: 3:10 - loss: 4.1332 - regression_loss: 3.0101 - classification_loss: 1.1231
 583/1000 [================>.............] - ETA: 3:10 - loss: 4.1346 - regression_loss: 3.0111 - classification_loss: 1.1235
 584/1000 [================>.............] - ETA: 3:09 - loss: 4.1368 - regression_loss: 3.0121 - classification_loss: 1.1246keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 444 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.337e+03, 1.581e+03, 1.463e+03, 1.720e+03, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 585/1000 [================>.............] - ETA: 3:09 - loss: 4.1377 - regression_loss: 3.0120 - classification_loss: 1.1257
 586/1000 [================>.............] - ETA: 3:08 - loss: 4.1389 - regression_loss: 3.0126 - classification_loss: 1.1263
 587/1000 [================>.............] - ETA: 3:08 - loss: 4.1404 - regression_loss: 3.0138 - classification_loss: 1.1266
 588/1000 [================>.............] - ETA: 3:07 - loss: 4.1412 - regression_loss: 3.0143 - classification_loss: 1.1269keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 55 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([9.990e+02, 0.000e+00, 1.404e+03, 3.970e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 589/1000 [================>.............] - ETA: 3:07 - loss: 4.1430 - regression_loss: 3.0159 - classification_loss: 1.1271
 590/1000 [================>.............] - ETA: 3:06 - loss: 4.1454 - regression_loss: 3.0181 - classification_loss: 1.1274
 591/1000 [================>.............] - ETA: 3:06 - loss: 4.1477 - regression_loss: 3.0194 - classification_loss: 1.1282keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 291 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.061e+03, 8.590e+02, 1.413e+03, 1.334e+03, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 592/1000 [================>.............] - ETA: 3:06 - loss: 4.1486 - regression_loss: 3.0200 - classification_loss: 1.1286
 593/1000 [================>.............] - ETA: 3:05 - loss: 4.1505 - regression_loss: 3.0218 - classification_loss: 1.1288
 594/1000 [================>.............] - ETA: 3:05 - loss: 4.1520 - regression_loss: 3.0229 - classification_loss: 1.1291
 595/1000 [================>.............] - ETA: 3:04 - loss: 4.1451 - regression_loss: 3.0178 - classification_loss: 1.1272
 596/1000 [================>.............] - ETA: 3:04 - loss: 4.1459 - regression_loss: 3.0185 - classification_loss: 1.1274
 597/1000 [================>.............] - ETA: 3:03 - loss: 4.1390 - regression_loss: 3.0134 - classification_loss: 1.1255
 598/1000 [================>.............] - ETA: 3:03 - loss: 4.1321 - regression_loss: 3.0084 - classification_loss: 1.1237
 599/1000 [================>.............] - ETA: 3:02 - loss: 4.1252 - regression_loss: 3.0034 - classification_loss: 1.1218
 600/1000 [=================>............] - ETA: 3:02 - loss: 4.1269 - regression_loss: 3.0050 - classification_loss: 1.1220
 601/1000 [=================>............] - ETA: 3:01 - loss: 4.1297 - regression_loss: 3.0071 - classification_loss: 1.1226
 602/1000 [=================>............] - ETA: 3:01 - loss: 4.1228 - regression_loss: 3.0021 - classification_loss: 1.1207
 603/1000 [=================>............] - ETA: 3:00 - loss: 4.1248 - regression_loss: 3.0039 - classification_loss: 1.1209
 604/1000 [=================>............] - ETA: 3:00 - loss: 4.1295 - regression_loss: 3.0060 - classification_loss: 1.1235
 605/1000 [=================>............] - ETA: 3:00 - loss: 4.1305 - regression_loss: 3.0068 - classification_loss: 1.1237
 606/1000 [=================>............] - ETA: 2:59 - loss: 4.1313 - regression_loss: 3.0070 - classification_loss: 1.1243
 607/1000 [=================>............] - ETA: 2:59 - loss: 4.1245 - regression_loss: 3.0020 - classification_loss: 1.1225
 608/1000 [=================>............] - ETA: 2:58 - loss: 4.1271 - regression_loss: 3.0035 - classification_loss: 1.1236keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 654 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([0.000e+00, 1.664e+03, 6.900e+01, 1.785e+03, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 609/1000 [=================>............] - ETA: 2:58 - loss: 4.1284 - regression_loss: 3.0043 - classification_loss: 1.1241keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 23 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.233e+03, 0.000e+00, 1.492e+03, 2.180e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 610/1000 [=================>............] - ETA: 2:57 - loss: 4.1307 - regression_loss: 3.0060 - classification_loss: 1.1247
 611/1000 [=================>............] - ETA: 2:57 - loss: 4.1322 - regression_loss: 3.0070 - classification_loss: 1.1251
 612/1000 [=================>............] - ETA: 2:56 - loss: 4.1413 - regression_loss: 3.0138 - classification_loss: 1.1274
 613/1000 [=================>............] - ETA: 2:56 - loss: 4.1345 - regression_loss: 3.0089 - classification_loss: 1.1256
 614/1000 [=================>............] - ETA: 2:55 - loss: 4.1278 - regression_loss: 3.0040 - classification_loss: 1.1237
 615/1000 [=================>............] - ETA: 2:55 - loss: 4.1297 - regression_loss: 3.0056 - classification_loss: 1.1241
 616/1000 [=================>............] - ETA: 2:55 - loss: 4.1311 - regression_loss: 3.0065 - classification_loss: 1.1246
 617/1000 [=================>............] - ETA: 2:54 - loss: 4.1327 - regression_loss: 3.0077 - classification_loss: 1.1251
 618/1000 [=================>............] - ETA: 2:54 - loss: 4.1343 - regression_loss: 3.0082 - classification_loss: 1.1261
 619/1000 [=================>............] - ETA: 2:53 - loss: 4.1276 - regression_loss: 3.0034 - classification_loss: 1.1243
 620/1000 [=================>............] - ETA: 2:53 - loss: 4.1210 - regression_loss: 2.9985 - classification_loss: 1.1224
 621/1000 [=================>............] - ETA: 2:52 - loss: 4.1234 - regression_loss: 2.9998 - classification_loss: 1.1237
 622/1000 [=================>............] - ETA: 2:52 - loss: 4.1255 - regression_loss: 3.0012 - classification_loss: 1.1243
 623/1000 [=================>............] - ETA: 2:51 - loss: 4.1270 - regression_loss: 3.0020 - classification_loss: 1.1250
 624/1000 [=================>............] - ETA: 2:51 - loss: 4.1204 - regression_loss: 2.9972 - classification_loss: 1.1232
 625/1000 [=================>............] - ETA: 2:50 - loss: 4.1211 - regression_loss: 2.9977 - classification_loss: 1.1234
 626/1000 [=================>............] - ETA: 2:50 - loss: 4.1229 - regression_loss: 2.9985 - classification_loss: 1.1243
 627/1000 [=================>............] - ETA: 2:50 - loss: 4.1246 - regression_loss: 2.9996 - classification_loss: 1.1250
 628/1000 [=================>............] - ETA: 2:49 - loss: 4.1181 - regression_loss: 2.9948 - classification_loss: 1.1232keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 597 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([9.100e+02, 0.000e+00, 1.602e+03, 6.520e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 629/1000 [=================>............] - ETA: 2:49 - loss: 4.1190 - regression_loss: 2.9949 - classification_loss: 1.1241
 630/1000 [=================>............] - ETA: 2:48 - loss: 4.1221 - regression_loss: 2.9964 - classification_loss: 1.1257
 631/1000 [=================>............] - ETA: 2:48 - loss: 4.1245 - regression_loss: 2.9977 - classification_loss: 1.1268
 632/1000 [=================>............] - ETA: 2:47 - loss: 4.1294 - regression_loss: 2.9992 - classification_loss: 1.1302keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 423 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.283e+03, 6.000e+01, 1.449e+03, 2.120e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 633/1000 [=================>............] - ETA: 2:47 - loss: 4.1315 - regression_loss: 3.0001 - classification_loss: 1.1315
 634/1000 [==================>...........] - ETA: 2:46 - loss: 4.1334 - regression_loss: 3.0010 - classification_loss: 1.1324
 635/1000 [==================>...........] - ETA: 2:46 - loss: 4.1348 - regression_loss: 3.0018 - classification_loss: 1.1330
 636/1000 [==================>...........] - ETA: 2:45 - loss: 4.1374 - regression_loss: 3.0036 - classification_loss: 1.1338keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 793 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.203e+03, 0.000e+00, 1.512e+03, 3.030e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 637/1000 [==================>...........] - ETA: 2:45 - loss: 4.1392 - regression_loss: 3.0049 - classification_loss: 1.1343
 638/1000 [==================>...........] - ETA: 2:44 - loss: 4.1410 - regression_loss: 3.0063 - classification_loss: 1.1347
 639/1000 [==================>...........] - ETA: 2:44 - loss: 4.1345 - regression_loss: 3.0016 - classification_loss: 1.1329
 640/1000 [==================>...........] - ETA: 2:44 - loss: 4.1363 - regression_loss: 3.0027 - classification_loss: 1.1336
 641/1000 [==================>...........] - ETA: 2:43 - loss: 4.1375 - regression_loss: 3.0037 - classification_loss: 1.1338
 642/1000 [==================>...........] - ETA: 2:43 - loss: 4.1388 - regression_loss: 3.0048 - classification_loss: 1.1340
 643/1000 [==================>...........] - ETA: 2:42 - loss: 4.1324 - regression_loss: 3.0002 - classification_loss: 1.1322
 644/1000 [==================>...........] - ETA: 2:42 - loss: 4.1339 - regression_loss: 3.0009 - classification_loss: 1.1331
 645/1000 [==================>...........] - ETA: 2:41 - loss: 4.1353 - regression_loss: 3.0021 - classification_loss: 1.1332keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 416 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.092e+03, 3.710e+02, 1.485e+03, 6.950e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 646/1000 [==================>...........] - ETA: 2:41 - loss: 4.1368 - regression_loss: 3.0032 - classification_loss: 1.1336
 647/1000 [==================>...........] - ETA: 2:40 - loss: 4.1304 - regression_loss: 2.9986 - classification_loss: 1.1318
 648/1000 [==================>...........] - ETA: 2:40 - loss: 4.1240 - regression_loss: 2.9939 - classification_loss: 1.1301keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 809 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.051e+03, 3.530e+02, 1.554e+03, 7.960e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 649/1000 [==================>...........] - ETA: 2:39 - loss: 4.1177 - regression_loss: 2.9893 - classification_loss: 1.1284
 650/1000 [==================>...........] - ETA: 2:39 - loss: 4.1193 - regression_loss: 2.9908 - classification_loss: 1.1285keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 319 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([9.870e+02, 4.540e+02, 1.404e+03, 9.870e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 651/1000 [==================>...........] - ETA: 2:39 - loss: 4.1130 - regression_loss: 2.9862 - classification_loss: 1.1267
 652/1000 [==================>...........] - ETA: 2:38 - loss: 4.1143 - regression_loss: 2.9873 - classification_loss: 1.1270keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 437 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.137e+03, 8.240e+02, 1.471e+03, 1.192e+03, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 653/1000 [==================>...........] - ETA: 2:38 - loss: 4.1163 - regression_loss: 2.9886 - classification_loss: 1.1276keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 439 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.150e+03, 1.118e+03, 1.471e+03, 1.373e+03, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 654/1000 [==================>...........] - ETA: 2:37 - loss: 4.1234 - regression_loss: 2.9920 - classification_loss: 1.1314
 655/1000 [==================>...........] - ETA: 2:37 - loss: 4.1250 - regression_loss: 2.9931 - classification_loss: 1.1318keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 966 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.225e+03, 2.480e+02, 1.641e+03, 6.550e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 656/1000 [==================>...........] - ETA: 2:36 - loss: 4.1187 - regression_loss: 2.9886 - classification_loss: 1.1301
 657/1000 [==================>...........] - ETA: 2:36 - loss: 4.1195 - regression_loss: 2.9889 - classification_loss: 1.1306
 658/1000 [==================>...........] - ETA: 2:35 - loss: 4.1227 - regression_loss: 2.9900 - classification_loss: 1.1327
 659/1000 [==================>...........] - ETA: 2:35 - loss: 4.1165 - regression_loss: 2.9855 - classification_loss: 1.1310
 660/1000 [==================>...........] - ETA: 2:34 - loss: 4.1184 - regression_loss: 2.9870 - classification_loss: 1.1314
 661/1000 [==================>...........] - ETA: 2:34 - loss: 4.1122 - regression_loss: 2.9825 - classification_loss: 1.1297
 662/1000 [==================>...........] - ETA: 2:34 - loss: 4.1141 - regression_loss: 2.9838 - classification_loss: 1.1303
 663/1000 [==================>...........] - ETA: 2:33 - loss: 4.1079 - regression_loss: 2.9793 - classification_loss: 1.1286
 664/1000 [==================>...........] - ETA: 2:33 - loss: 4.1017 - regression_loss: 2.9748 - classification_loss: 1.1269
 665/1000 [==================>...........] - ETA: 2:32 - loss: 4.1031 - regression_loss: 2.9760 - classification_loss: 1.1271keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 395 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.267e+03, 3.650e+02, 1.422e+03, 5.030e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 666/1000 [==================>...........] - ETA: 2:32 - loss: 4.0969 - regression_loss: 2.9715 - classification_loss: 1.1254
 667/1000 [===================>..........] - ETA: 2:31 - loss: 4.1044 - regression_loss: 2.9759 - classification_loss: 1.1285
 668/1000 [===================>..........] - ETA: 2:31 - loss: 4.1068 - regression_loss: 2.9772 - classification_loss: 1.1296
 669/1000 [===================>..........] - ETA: 2:30 - loss: 4.1081 - regression_loss: 2.9783 - classification_loss: 1.1298
 670/1000 [===================>..........] - ETA: 2:30 - loss: 4.1171 - regression_loss: 2.9829 - classification_loss: 1.1341
 671/1000 [===================>..........] - ETA: 2:29 - loss: 4.1213 - regression_loss: 2.9849 - classification_loss: 1.1364
 672/1000 [===================>..........] - ETA: 2:29 - loss: 4.1229 - regression_loss: 2.9860 - classification_loss: 1.1369
 673/1000 [===================>..........] - ETA: 2:28 - loss: 4.1261 - regression_loss: 2.9874 - classification_loss: 1.1387
 674/1000 [===================>..........] - ETA: 2:28 - loss: 4.1280 - regression_loss: 2.9890 - classification_loss: 1.1390
 675/1000 [===================>..........] - ETA: 2:28 - loss: 4.1219 - regression_loss: 2.9846 - classification_loss: 1.1373
 676/1000 [===================>..........] - ETA: 2:27 - loss: 4.1158 - regression_loss: 2.9802 - classification_loss: 1.1356
 677/1000 [===================>..........] - ETA: 2:27 - loss: 4.1213 - regression_loss: 2.9833 - classification_loss: 1.1380keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 475 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([7.610e+02, 5.510e+02, 1.409e+03, 1.084e+03, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 678/1000 [===================>..........] - ETA: 2:26 - loss: 4.1228 - regression_loss: 2.9845 - classification_loss: 1.1383
 679/1000 [===================>..........] - ETA: 2:26 - loss: 4.1241 - regression_loss: 2.9855 - classification_loss: 1.1385
 680/1000 [===================>..........] - ETA: 2:25 - loss: 4.1250 - regression_loss: 2.9862 - classification_loss: 1.1387
 681/1000 [===================>..........] - ETA: 2:25 - loss: 4.1279 - regression_loss: 2.9875 - classification_loss: 1.1404
 682/1000 [===================>..........] - ETA: 2:24 - loss: 4.1218 - regression_loss: 2.9831 - classification_loss: 1.1387keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 489 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([8.680e+02, 6.140e+02, 1.501e+03, 1.265e+03, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 683/1000 [===================>..........] - ETA: 2:24 - loss: 4.1246 - regression_loss: 2.9845 - classification_loss: 1.1401
 684/1000 [===================>..........] - ETA: 2:23 - loss: 4.1268 - regression_loss: 2.9862 - classification_loss: 1.1405
 685/1000 [===================>..........] - ETA: 2:23 - loss: 4.1281 - regression_loss: 2.9874 - classification_loss: 1.1407
 686/1000 [===================>..........] - ETA: 2:23 - loss: 4.1297 - regression_loss: 2.9885 - classification_loss: 1.1412
 687/1000 [===================>..........] - ETA: 2:22 - loss: 4.1237 - regression_loss: 2.9841 - classification_loss: 1.1396
 688/1000 [===================>..........] - ETA: 2:22 - loss: 4.1177 - regression_loss: 2.9798 - classification_loss: 1.1379
 689/1000 [===================>..........] - ETA: 2:21 - loss: 4.1193 - regression_loss: 2.9808 - classification_loss: 1.1386
 690/1000 [===================>..........] - ETA: 2:21 - loss: 4.1204 - regression_loss: 2.9811 - classification_loss: 1.1394
 691/1000 [===================>..........] - ETA: 2:20 - loss: 4.1230 - regression_loss: 2.9830 - classification_loss: 1.1400
 692/1000 [===================>..........] - ETA: 2:20 - loss: 4.1245 - regression_loss: 2.9840 - classification_loss: 1.1405keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 172 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.281e+03, 2.700e+01, 1.454e+03, 2.120e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 693/1000 [===================>..........] - ETA: 2:19 - loss: 4.1185 - regression_loss: 2.9797 - classification_loss: 1.1388
 694/1000 [===================>..........] - ETA: 2:19 - loss: 4.1126 - regression_loss: 2.9754 - classification_loss: 1.1372
 695/1000 [===================>..........] - ETA: 2:18 - loss: 4.1134 - regression_loss: 2.9760 - classification_loss: 1.1374
 696/1000 [===================>..........] - ETA: 2:18 - loss: 4.1149 - regression_loss: 2.9769 - classification_loss: 1.1380
 697/1000 [===================>..........] - ETA: 2:18 - loss: 4.1163 - regression_loss: 2.9781 - classification_loss: 1.1382
 698/1000 [===================>..........] - ETA: 2:17 - loss: 4.1186 - regression_loss: 2.9801 - classification_loss: 1.1385
 699/1000 [===================>..........] - ETA: 2:17 - loss: 4.1199 - regression_loss: 2.9806 - classification_loss: 1.1393keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 942 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([9.090e+02, 5.660e+02, 1.437e+03, 1.068e+03, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 700/1000 [====================>.........] - ETA: 2:16 - loss: 4.1212 - regression_loss: 2.9813 - classification_loss: 1.1399
 701/1000 [====================>.........] - ETA: 2:16 - loss: 4.1236 - regression_loss: 2.9831 - classification_loss: 1.1405
 702/1000 [====================>.........] - ETA: 2:15 - loss: 4.1252 - regression_loss: 2.9844 - classification_loss: 1.1408
 703/1000 [====================>.........] - ETA: 2:15 - loss: 4.1193 - regression_loss: 2.9801 - classification_loss: 1.1392keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 281 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.068e+03, 2.570e+02, 1.692e+03, 8.020e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 704/1000 [====================>.........] - ETA: 2:14 - loss: 4.1206 - regression_loss: 2.9810 - classification_loss: 1.1396
 705/1000 [====================>.........] - ETA: 2:14 - loss: 4.1221 - regression_loss: 2.9819 - classification_loss: 1.1401keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 273 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.187e+03, 5.360e+02, 1.551e+03, 9.110e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 706/1000 [====================>.........] - ETA: 2:13 - loss: 4.1229 - regression_loss: 2.9823 - classification_loss: 1.1406
 707/1000 [====================>.........] - ETA: 2:13 - loss: 4.1236 - regression_loss: 2.9824 - classification_loss: 1.1412
 708/1000 [====================>.........] - ETA: 2:12 - loss: 4.1252 - regression_loss: 2.9838 - classification_loss: 1.1414
 709/1000 [====================>.........] - ETA: 2:12 - loss: 4.1194 - regression_loss: 2.9796 - classification_loss: 1.1398
 710/1000 [====================>.........] - ETA: 2:12 - loss: 4.1136 - regression_loss: 2.9754 - classification_loss: 1.1382keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 941 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.225e+03, 3.970e+02, 1.401e+03, 5.660e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 711/1000 [====================>.........] - ETA: 2:11 - loss: 4.1146 - regression_loss: 2.9760 - classification_loss: 1.1386keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 461 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([9.260e+02, 8.430e+02, 1.652e+03, 1.457e+03, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 712/1000 [====================>.........] - ETA: 2:11 - loss: 4.1160 - regression_loss: 2.9770 - classification_loss: 1.1390
 713/1000 [====================>.........] - ETA: 2:10 - loss: 4.1181 - regression_loss: 2.9785 - classification_loss: 1.1396
 714/1000 [====================>.........] - ETA: 2:10 - loss: 4.1123 - regression_loss: 2.9743 - classification_loss: 1.1380
 715/1000 [====================>.........] - ETA: 2:09 - loss: 4.1144 - regression_loss: 2.9757 - classification_loss: 1.1386
 716/1000 [====================>.........] - ETA: 2:09 - loss: 4.1086 - regression_loss: 2.9716 - classification_loss: 1.1370
 717/1000 [====================>.........] - ETA: 2:08 - loss: 4.1110 - regression_loss: 2.9731 - classification_loss: 1.1378
 718/1000 [====================>.........] - ETA: 2:08 - loss: 4.1121 - regression_loss: 2.9739 - classification_loss: 1.1382
 719/1000 [====================>.........] - ETA: 2:07 - loss: 4.1137 - regression_loss: 2.9747 - classification_loss: 1.1390
 720/1000 [====================>.........] - ETA: 2:07 - loss: 4.1151 - regression_loss: 2.9753 - classification_loss: 1.1398
 721/1000 [====================>.........] - ETA: 2:07 - loss: 4.1094 - regression_loss: 2.9712 - classification_loss: 1.1382
 722/1000 [====================>.........] - ETA: 2:06 - loss: 4.1037 - regression_loss: 2.9671 - classification_loss: 1.1366
 723/1000 [====================>.........] - ETA: 2:06 - loss: 4.1047 - regression_loss: 2.9677 - classification_loss: 1.1369
 724/1000 [====================>.........] - ETA: 2:05 - loss: 4.1061 - regression_loss: 2.9686 - classification_loss: 1.1376
 725/1000 [====================>.........] - ETA: 2:05 - loss: 4.1083 - regression_loss: 2.9698 - classification_loss: 1.1385
 726/1000 [====================>.........] - ETA: 2:04 - loss: 4.1093 - regression_loss: 2.9705 - classification_loss: 1.1388
 727/1000 [====================>.........] - ETA: 2:04 - loss: 4.1105 - regression_loss: 2.9714 - classification_loss: 1.1391
 728/1000 [====================>.........] - ETA: 2:03 - loss: 4.1049 - regression_loss: 2.9673 - classification_loss: 1.1376
 729/1000 [====================>.........] - ETA: 2:03 - loss: 4.1098 - regression_loss: 2.9694 - classification_loss: 1.1404keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 520 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.094e+03, 7.110e+02, 1.414e+03, 1.174e+03, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 730/1000 [====================>.........] - ETA: 2:02 - loss: 4.1136 - regression_loss: 2.9724 - classification_loss: 1.1412
 731/1000 [====================>.........] - ETA: 2:02 - loss: 4.1155 - regression_loss: 2.9733 - classification_loss: 1.1422
 732/1000 [====================>.........] - ETA: 2:02 - loss: 4.1099 - regression_loss: 2.9693 - classification_loss: 1.1406keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 348 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.209e+03, 1.000e+01, 1.442e+03, 2.930e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 733/1000 [====================>.........] - ETA: 2:01 - loss: 4.1107 - regression_loss: 2.9698 - classification_loss: 1.1409
 734/1000 [=====================>........] - ETA: 2:01 - loss: 4.1114 - regression_loss: 2.9701 - classification_loss: 1.1413
 735/1000 [=====================>........] - ETA: 2:00 - loss: 4.1123 - regression_loss: 2.9708 - classification_loss: 1.1415
 736/1000 [=====================>........] - ETA: 2:00 - loss: 4.1148 - regression_loss: 2.9727 - classification_loss: 1.1421
 737/1000 [=====================>........] - ETA: 1:59 - loss: 4.1092 - regression_loss: 2.9687 - classification_loss: 1.1405keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 51 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.215e+03, 0.000e+00, 1.468e+03, 2.520e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 738/1000 [=====================>........] - ETA: 1:59 - loss: 4.1103 - regression_loss: 2.9690 - classification_loss: 1.1412
 739/1000 [=====================>........] - ETA: 1:58 - loss: 4.1120 - regression_loss: 2.9698 - classification_loss: 1.1422
 740/1000 [=====================>........] - ETA: 1:58 - loss: 4.1065 - regression_loss: 2.9658 - classification_loss: 1.1407
 741/1000 [=====================>........] - ETA: 1:57 - loss: 4.1083 - regression_loss: 2.9674 - classification_loss: 1.1408
 742/1000 [=====================>........] - ETA: 1:57 - loss: 4.1090 - regression_loss: 2.9681 - classification_loss: 1.1410
 743/1000 [=====================>........] - ETA: 1:57 - loss: 4.1035 - regression_loss: 2.9641 - classification_loss: 1.1394
 744/1000 [=====================>........] - ETA: 1:56 - loss: 4.1054 - regression_loss: 2.9659 - classification_loss: 1.1395
 745/1000 [=====================>........] - ETA: 1:56 - loss: 4.0999 - regression_loss: 2.9619 - classification_loss: 1.1380keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 851 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.095e+03, 2.680e+02, 1.591e+03, 7.190e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 746/1000 [=====================>........] - ETA: 1:55 - loss: 4.1014 - regression_loss: 2.9631 - classification_loss: 1.1384
 747/1000 [=====================>........] - ETA: 1:55 - loss: 4.0959 - regression_loss: 2.9591 - classification_loss: 1.1368
 748/1000 [=====================>........] - ETA: 1:54 - loss: 4.0905 - regression_loss: 2.9551 - classification_loss: 1.1353
 749/1000 [=====================>........] - ETA: 1:54 - loss: 4.0924 - regression_loss: 2.9564 - classification_loss: 1.1359keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 479 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.070e+03, 4.760e+02, 1.419e+03, 8.080e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 750/1000 [=====================>........] - ETA: 1:53 - loss: 4.0943 - regression_loss: 2.9573 - classification_loss: 1.1370
 751/1000 [=====================>........] - ETA: 1:53 - loss: 4.0957 - regression_loss: 2.9578 - classification_loss: 1.1379
 752/1000 [=====================>........] - ETA: 1:52 - loss: 4.0974 - regression_loss: 2.9592 - classification_loss: 1.1382
 753/1000 [=====================>........] - ETA: 1:52 - loss: 4.0998 - regression_loss: 2.9598 - classification_loss: 1.1400
 754/1000 [=====================>........] - ETA: 1:51 - loss: 4.1010 - regression_loss: 2.9607 - classification_loss: 1.1402
 755/1000 [=====================>........] - ETA: 1:51 - loss: 4.1041 - regression_loss: 2.9621 - classification_loss: 1.1421
 756/1000 [=====================>........] - ETA: 1:51 - loss: 4.0987 - regression_loss: 2.9581 - classification_loss: 1.1406
 757/1000 [=====================>........] - ETA: 1:50 - loss: 4.1003 - regression_loss: 2.9596 - classification_loss: 1.1407
 758/1000 [=====================>........] - ETA: 1:50 - loss: 4.1035 - regression_loss: 2.9613 - classification_loss: 1.1422
 759/1000 [=====================>........] - ETA: 1:49 - loss: 4.0981 - regression_loss: 2.9574 - classification_loss: 1.1407
 760/1000 [=====================>........] - ETA: 1:49 - loss: 4.0927 - regression_loss: 2.9535 - classification_loss: 1.1392
 761/1000 [=====================>........] - ETA: 1:48 - loss: 4.0936 - regression_loss: 2.9542 - classification_loss: 1.1394keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 801 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([8.990e+02, 3.420e+02, 1.618e+03, 1.007e+03, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 762/1000 [=====================>........] - ETA: 1:48 - loss: 4.0941 - regression_loss: 2.9543 - classification_loss: 1.1399
 763/1000 [=====================>........] - ETA: 1:47 - loss: 4.0954 - regression_loss: 2.9554 - classification_loss: 1.1400
 764/1000 [=====================>........] - ETA: 1:47 - loss: 4.0973 - regression_loss: 2.9562 - classification_loss: 1.1410
 765/1000 [=====================>........] - ETA: 1:46 - loss: 4.1004 - regression_loss: 2.9577 - classification_loss: 1.1427
 766/1000 [=====================>........] - ETA: 1:46 - loss: 4.0950 - regression_loss: 2.9538 - classification_loss: 1.1412keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 7 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([8.650e+02, 5.390e+02, 1.566e+03, 1.088e+03, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 767/1000 [======================>.......] - ETA: 1:46 - loss: 4.0961 - regression_loss: 2.9542 - classification_loss: 1.1419
 768/1000 [======================>.......] - ETA: 1:45 - loss: 4.0986 - regression_loss: 2.9547 - classification_loss: 1.1439
 769/1000 [======================>.......] - ETA: 1:45 - loss: 4.1028 - regression_loss: 2.9556 - classification_loss: 1.1471
 770/1000 [======================>.......] - ETA: 1:44 - loss: 4.0974 - regression_loss: 2.9518 - classification_loss: 1.1457
 771/1000 [======================>.......] - ETA: 1:44 - loss: 4.0988 - regression_loss: 2.9528 - classification_loss: 1.1461
 772/1000 [======================>.......] - ETA: 1:43 - loss: 4.0935 - regression_loss: 2.9490 - classification_loss: 1.1446keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 542 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.252e+03, 5.160e+02, 1.523e+03, 8.730e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 773/1000 [======================>.......] - ETA: 1:43 - loss: 4.0882 - regression_loss: 2.9451 - classification_loss: 1.1431
 774/1000 [======================>.......] - ETA: 1:42 - loss: 4.0898 - regression_loss: 2.9462 - classification_loss: 1.1436keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 620 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.286e+03, 5.000e+00, 1.422e+03, 1.880e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 775/1000 [======================>.......] - ETA: 1:42 - loss: 4.0907 - regression_loss: 2.9469 - classification_loss: 1.1438
 776/1000 [======================>.......] - ETA: 1:41 - loss: 4.0854 - regression_loss: 2.9431 - classification_loss: 1.1423
 777/1000 [======================>.......] - ETA: 1:41 - loss: 4.0802 - regression_loss: 2.9393 - classification_loss: 1.1409
 778/1000 [======================>.......] - ETA: 1:41 - loss: 4.0812 - regression_loss: 2.9396 - classification_loss: 1.1415
 779/1000 [======================>.......] - ETA: 1:40 - loss: 4.0818 - regression_loss: 2.9401 - classification_loss: 1.1417
 780/1000 [======================>.......] - ETA: 1:40 - loss: 4.0831 - regression_loss: 2.9413 - classification_loss: 1.1418
 781/1000 [======================>.......] - ETA: 1:39 - loss: 4.0779 - regression_loss: 2.9375 - classification_loss: 1.1403
 782/1000 [======================>.......] - ETA: 1:39 - loss: 4.0830 - regression_loss: 2.9399 - classification_loss: 1.1431
 783/1000 [======================>.......] - ETA: 1:38 - loss: 4.0777 - regression_loss: 2.9361 - classification_loss: 1.1416
 784/1000 [======================>.......] - ETA: 1:38 - loss: 4.0787 - regression_loss: 2.9365 - classification_loss: 1.1422
 785/1000 [======================>.......] - ETA: 1:37 - loss: 4.0735 - regression_loss: 2.9328 - classification_loss: 1.1407
 786/1000 [======================>.......] - ETA: 1:37 - loss: 4.0762 - regression_loss: 2.9340 - classification_loss: 1.1422
 787/1000 [======================>.......] - ETA: 1:36 - loss: 4.0786 - regression_loss: 2.9348 - classification_loss: 1.1437
 788/1000 [======================>.......] - ETA: 1:36 - loss: 4.0804 - regression_loss: 2.9361 - classification_loss: 1.1443
 789/1000 [======================>.......] - ETA: 1:36 - loss: 4.0753 - regression_loss: 2.9324 - classification_loss: 1.1428
 790/1000 [======================>.......] - ETA: 1:35 - loss: 4.0779 - regression_loss: 2.9342 - classification_loss: 1.1437
 791/1000 [======================>.......] - ETA: 1:35 - loss: 4.0791 - regression_loss: 2.9350 - classification_loss: 1.1441
 792/1000 [======================>.......] - ETA: 1:34 - loss: 4.0824 - regression_loss: 2.9379 - classification_loss: 1.1445
 793/1000 [======================>.......] - ETA: 1:34 - loss: 4.0840 - regression_loss: 2.9383 - classification_loss: 1.1457
 794/1000 [======================>.......] - ETA: 1:33 - loss: 4.0880 - regression_loss: 2.9393 - classification_loss: 1.1487
 795/1000 [======================>.......] - ETA: 1:33 - loss: 4.0896 - regression_loss: 2.9408 - classification_loss: 1.1488
 796/1000 [======================>.......] - ETA: 1:32 - loss: 4.0904 - regression_loss: 2.9410 - classification_loss: 1.1494
 797/1000 [======================>.......] - ETA: 1:32 - loss: 4.0915 - regression_loss: 2.9412 - classification_loss: 1.1503
 798/1000 [======================>.......] - ETA: 1:31 - loss: 4.0941 - regression_loss: 2.9424 - classification_loss: 1.1517
 799/1000 [======================>.......] - ETA: 1:31 - loss: 4.0890 - regression_loss: 2.9387 - classification_loss: 1.1502
 800/1000 [=======================>......] - ETA: 1:31 - loss: 4.0839 - regression_loss: 2.9351 - classification_loss: 1.1488
 801/1000 [=======================>......] - ETA: 1:30 - loss: 4.0859 - regression_loss: 2.9367 - classification_loss: 1.1492keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 643 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.257e+03, 7.200e+01, 1.415e+03, 2.860e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 802/1000 [=======================>......] - ETA: 1:30 - loss: 4.0871 - regression_loss: 2.9376 - classification_loss: 1.1495
 803/1000 [=======================>......] - ETA: 1:29 - loss: 4.0820 - regression_loss: 2.9340 - classification_loss: 1.1480keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 574 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([9.550e+02, 0.000e+00, 1.505e+03, 8.260e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 804/1000 [=======================>......] - ETA: 1:29 - loss: 4.0829 - regression_loss: 2.9348 - classification_loss: 1.1481
 805/1000 [=======================>......] - ETA: 1:28 - loss: 4.0849 - regression_loss: 2.9365 - classification_loss: 1.1484
 806/1000 [=======================>......] - ETA: 1:28 - loss: 4.0860 - regression_loss: 2.9375 - classification_loss: 1.1485
 807/1000 [=======================>......] - ETA: 1:27 - loss: 4.0871 - regression_loss: 2.9383 - classification_loss: 1.1488keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 640 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([0.000e+00, 1.555e+03, 9.600e+01, 1.828e+03, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 808/1000 [=======================>......] - ETA: 1:27 - loss: 4.0880 - regression_loss: 2.9389 - classification_loss: 1.1491
 809/1000 [=======================>......] - ETA: 1:26 - loss: 4.0897 - regression_loss: 2.9400 - classification_loss: 1.1497
 810/1000 [=======================>......] - ETA: 1:26 - loss: 4.0904 - regression_loss: 2.9406 - classification_loss: 1.1499
 811/1000 [=======================>......] - ETA: 1:26 - loss: 4.0929 - regression_loss: 2.9428 - classification_loss: 1.1501keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 613 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.244e+03, 1.740e+02, 1.623e+03, 5.200e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 812/1000 [=======================>......] - ETA: 1:25 - loss: 4.0878 - regression_loss: 2.9392 - classification_loss: 1.1487
 813/1000 [=======================>......] - ETA: 1:25 - loss: 4.0899 - regression_loss: 2.9407 - classification_loss: 1.1492
 814/1000 [=======================>......] - ETA: 1:24 - loss: 4.0849 - regression_loss: 2.9371 - classification_loss: 1.1478
 815/1000 [=======================>......] - ETA: 1:24 - loss: 4.0868 - regression_loss: 2.9384 - classification_loss: 1.1484
 816/1000 [=======================>......] - ETA: 1:23 - loss: 4.0884 - regression_loss: 2.9397 - classification_loss: 1.1487
 817/1000 [=======================>......] - ETA: 1:23 - loss: 4.0896 - regression_loss: 2.9407 - classification_loss: 1.1489keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 197 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([7.750e+02, 2.520e+02, 1.422e+03, 1.031e+03, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 818/1000 [=======================>......] - ETA: 1:22 - loss: 4.0846 - regression_loss: 2.9371 - classification_loss: 1.1475
 819/1000 [=======================>......] - ETA: 1:22 - loss: 4.0861 - regression_loss: 2.9376 - classification_loss: 1.1485
 820/1000 [=======================>......] - ETA: 1:21 - loss: 4.0876 - regression_loss: 2.9388 - classification_loss: 1.1488
 821/1000 [=======================>......] - ETA: 1:21 - loss: 4.0884 - regression_loss: 2.9393 - classification_loss: 1.1491
 822/1000 [=======================>......] - ETA: 1:21 - loss: 4.0834 - regression_loss: 2.9357 - classification_loss: 1.1477
 823/1000 [=======================>......] - ETA: 1:20 - loss: 4.0846 - regression_loss: 2.9366 - classification_loss: 1.1480keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 577 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.085e+03, 4.650e+02, 1.520e+03, 8.170e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 824/1000 [=======================>......] - ETA: 1:20 - loss: 4.0797 - regression_loss: 2.9330 - classification_loss: 1.1466
 825/1000 [=======================>......] - ETA: 1:19 - loss: 4.0811 - regression_loss: 2.9343 - classification_loss: 1.1468
 826/1000 [=======================>......] - ETA: 1:19 - loss: 4.0831 - regression_loss: 2.9354 - classification_loss: 1.1477
 827/1000 [=======================>......] - ETA: 1:18 - loss: 4.0848 - regression_loss: 2.9366 - classification_loss: 1.1481
 828/1000 [=======================>......] - ETA: 1:18 - loss: 4.0798 - regression_loss: 2.9331 - classification_loss: 1.1467
 829/1000 [=======================>......] - ETA: 1:17 - loss: 4.0811 - regression_loss: 2.9340 - classification_loss: 1.1471
 830/1000 [=======================>......] - ETA: 1:17 - loss: 4.0824 - regression_loss: 2.9350 - classification_loss: 1.1474
 831/1000 [=======================>......] - ETA: 1:16 - loss: 4.0840 - regression_loss: 2.9361 - classification_loss: 1.1479
 832/1000 [=======================>......] - ETA: 1:16 - loss: 4.0866 - regression_loss: 2.9381 - classification_loss: 1.1484keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 806 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([9.360e+02, 0.000e+00, 1.633e+03, 4.920e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 833/1000 [=======================>......] - ETA: 1:16 - loss: 4.0883 - regression_loss: 2.9397 - classification_loss: 1.1486
 834/1000 [========================>.....] - ETA: 1:15 - loss: 4.0834 - regression_loss: 2.9362 - classification_loss: 1.1472
 835/1000 [========================>.....] - ETA: 1:15 - loss: 4.0844 - regression_loss: 2.9366 - classification_loss: 1.1478
 836/1000 [========================>.....] - ETA: 1:14 - loss: 4.0853 - regression_loss: 2.9373 - classification_loss: 1.1480
 837/1000 [========================>.....] - ETA: 1:14 - loss: 4.0804 - regression_loss: 2.9338 - classification_loss: 1.1466
 838/1000 [========================>.....] - ETA: 1:13 - loss: 4.0755 - regression_loss: 2.9303 - classification_loss: 1.1453
 839/1000 [========================>.....] - ETA: 1:13 - loss: 4.0772 - regression_loss: 2.9309 - classification_loss: 1.1463
 840/1000 [========================>.....] - ETA: 1:12 - loss: 4.0780 - regression_loss: 2.9316 - classification_loss: 1.1464
 841/1000 [========================>.....] - ETA: 1:12 - loss: 4.0732 - regression_loss: 2.9282 - classification_loss: 1.1450
 842/1000 [========================>.....] - ETA: 1:11 - loss: 4.0745 - regression_loss: 2.9292 - classification_loss: 1.1453
 843/1000 [========================>.....] - ETA: 1:11 - loss: 4.0697 - regression_loss: 2.9257 - classification_loss: 1.1440
 844/1000 [========================>.....] - ETA: 1:10 - loss: 4.0648 - regression_loss: 2.9222 - classification_loss: 1.1426
 845/1000 [========================>.....] - ETA: 1:10 - loss: 4.0661 - regression_loss: 2.9228 - classification_loss: 1.1432
 846/1000 [========================>.....] - ETA: 1:10 - loss: 4.0678 - regression_loss: 2.9237 - classification_loss: 1.1441
 847/1000 [========================>.....] - ETA: 1:09 - loss: 4.0691 - regression_loss: 2.9248 - classification_loss: 1.1443
 848/1000 [========================>.....] - ETA: 1:09 - loss: 4.0708 - regression_loss: 2.9257 - classification_loss: 1.1451
 849/1000 [========================>.....] - ETA: 1:08 - loss: 4.0738 - regression_loss: 2.9266 - classification_loss: 1.1472keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 614 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.076e+03, 0.000e+00, 1.441e+03, 4.430e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 850/1000 [========================>.....] - ETA: 1:08 - loss: 4.0755 - regression_loss: 2.9279 - classification_loss: 1.1476
 851/1000 [========================>.....] - ETA: 1:07 - loss: 4.0770 - regression_loss: 2.9284 - classification_loss: 1.1486
 852/1000 [========================>.....] - ETA: 1:07 - loss: 4.0722 - regression_loss: 2.9250 - classification_loss: 1.1472
 853/1000 [========================>.....] - ETA: 1:06 - loss: 4.0731 - regression_loss: 2.9257 - classification_loss: 1.1474
 854/1000 [========================>.....] - ETA: 1:06 - loss: 4.0742 - regression_loss: 2.9266 - classification_loss: 1.1476
 855/1000 [========================>.....] - ETA: 1:05 - loss: 4.0756 - regression_loss: 2.9272 - classification_loss: 1.1484
 856/1000 [========================>.....] - ETA: 1:05 - loss: 4.0771 - regression_loss: 2.9279 - classification_loss: 1.1492
 857/1000 [========================>.....] - ETA: 1:05 - loss: 4.0781 - regression_loss: 2.9285 - classification_loss: 1.1495
 858/1000 [========================>.....] - ETA: 1:04 - loss: 4.0790 - regression_loss: 2.9293 - classification_loss: 1.1497
 859/1000 [========================>.....] - ETA: 1:04 - loss: 4.0803 - regression_loss: 2.9302 - classification_loss: 1.1502
 860/1000 [========================>.....] - ETA: 1:03 - loss: 4.0756 - regression_loss: 2.9268 - classification_loss: 1.1488
 861/1000 [========================>.....] - ETA: 1:03 - loss: 4.0768 - regression_loss: 2.9278 - classification_loss: 1.1490
 862/1000 [========================>.....] - ETA: 1:02 - loss: 4.0720 - regression_loss: 2.9244 - classification_loss: 1.1477
 863/1000 [========================>.....] - ETA: 1:02 - loss: 4.0732 - regression_loss: 2.9254 - classification_loss: 1.1478
 864/1000 [========================>.....] - ETA: 1:01 - loss: 4.0745 - regression_loss: 2.9263 - classification_loss: 1.1481keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 838 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([0.000e+00, 1.604e+03, 7.200e+01, 1.780e+03, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 865/1000 [========================>.....] - ETA: 1:01 - loss: 4.0760 - regression_loss: 2.9273 - classification_loss: 1.1488
 866/1000 [========================>.....] - ETA: 1:00 - loss: 4.0767 - regression_loss: 2.9274 - classification_loss: 1.1493
 867/1000 [=========================>....] - ETA: 1:00 - loss: 4.0775 - regression_loss: 2.9281 - classification_loss: 1.1495
 868/1000 [=========================>....] - ETA: 1:00 - loss: 4.0790 - regression_loss: 2.9290 - classification_loss: 1.1500
 869/1000 [=========================>....] - ETA: 59s - loss: 4.0800 - regression_loss: 2.9299 - classification_loss: 1.1501 
 870/1000 [=========================>....] - ETA: 59s - loss: 4.0804 - regression_loss: 2.9300 - classification_loss: 1.1504keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 896 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.092e+03, 4.030e+02, 1.459e+03, 6.980e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 871/1000 [=========================>....] - ETA: 58s - loss: 4.0812 - regression_loss: 2.9305 - classification_loss: 1.1507
 872/1000 [=========================>....] - ETA: 58s - loss: 4.0821 - regression_loss: 2.9311 - classification_loss: 1.1509keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 728 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.209e+03, 1.700e+01, 1.463e+03, 3.020e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 873/1000 [=========================>....] - ETA: 57s - loss: 4.0831 - regression_loss: 2.9320 - classification_loss: 1.1511
 874/1000 [=========================>....] - ETA: 57s - loss: 4.0842 - regression_loss: 2.9329 - classification_loss: 1.1512
 875/1000 [=========================>....] - ETA: 56s - loss: 4.0795 - regression_loss: 2.9296 - classification_loss: 1.1499
 876/1000 [=========================>....] - ETA: 56s - loss: 4.0801 - regression_loss: 2.9298 - classification_loss: 1.1503
 877/1000 [=========================>....] - ETA: 55s - loss: 4.0814 - regression_loss: 2.9306 - classification_loss: 1.1508
 878/1000 [=========================>....] - ETA: 55s - loss: 4.0822 - regression_loss: 2.9313 - classification_loss: 1.1509
 879/1000 [=========================>....] - ETA: 55s - loss: 4.0828 - regression_loss: 2.9317 - classification_loss: 1.1511
 880/1000 [=========================>....] - ETA: 54s - loss: 4.0833 - regression_loss: 2.9320 - classification_loss: 1.1514keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 959 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([9.830e+02, 1.410e+02, 1.593e+03, 9.240e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 881/1000 [=========================>....] - ETA: 54s - loss: 4.0787 - regression_loss: 2.9286 - classification_loss: 1.1501keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 568 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.101e+03, 4.390e+02, 1.461e+03, 7.200e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 882/1000 [=========================>....] - ETA: 53s - loss: 4.0853 - regression_loss: 2.9351 - classification_loss: 1.1502
 883/1000 [=========================>....] - ETA: 53s - loss: 4.0806 - regression_loss: 2.9318 - classification_loss: 1.1489
 884/1000 [=========================>....] - ETA: 52s - loss: 4.0817 - regression_loss: 2.9325 - classification_loss: 1.1492
 885/1000 [=========================>....] - ETA: 52s - loss: 4.0823 - regression_loss: 2.9330 - classification_loss: 1.1493
 886/1000 [=========================>....] - ETA: 51s - loss: 4.0777 - regression_loss: 2.9297 - classification_loss: 1.1480keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 545 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.154e+03, 5.600e+02, 1.436e+03, 8.960e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 887/1000 [=========================>....] - ETA: 51s - loss: 4.0786 - regression_loss: 2.9300 - classification_loss: 1.1486
 888/1000 [=========================>....] - ETA: 50s - loss: 4.0797 - regression_loss: 2.9307 - classification_loss: 1.1489
 889/1000 [=========================>....] - ETA: 50s - loss: 4.0808 - regression_loss: 2.9316 - classification_loss: 1.1493
 890/1000 [=========================>....] - ETA: 50s - loss: 4.0815 - regression_loss: 2.9322 - classification_loss: 1.1493
 891/1000 [=========================>....] - ETA: 49s - loss: 4.0769 - regression_loss: 2.9289 - classification_loss: 1.1480
 892/1000 [=========================>....] - ETA: 49s - loss: 4.0723 - regression_loss: 2.9256 - classification_loss: 1.1467
 893/1000 [=========================>....] - ETA: 48s - loss: 4.0678 - regression_loss: 2.9223 - classification_loss: 1.1455
 894/1000 [=========================>....] - ETA: 48s - loss: 4.0690 - regression_loss: 2.9232 - classification_loss: 1.1458keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 41 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.093e+03, 0.000e+00, 1.517e+03, 4.750e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 895/1000 [=========================>....] - ETA: 47s - loss: 4.0700 - regression_loss: 2.9241 - classification_loss: 1.1459
 896/1000 [=========================>....] - ETA: 47s - loss: 4.0655 - regression_loss: 2.9209 - classification_loss: 1.1446
 897/1000 [=========================>....] - ETA: 46s - loss: 4.0609 - regression_loss: 2.9176 - classification_loss: 1.1433
 898/1000 [=========================>....] - ETA: 46s - loss: 4.0637 - regression_loss: 2.9193 - classification_loss: 1.1444
 899/1000 [=========================>....] - ETA: 45s - loss: 4.0674 - regression_loss: 2.9203 - classification_loss: 1.1471
 900/1000 [==========================>...] - ETA: 45s - loss: 4.0696 - regression_loss: 2.9214 - classification_loss: 1.1482
 901/1000 [==========================>...] - ETA: 45s - loss: 4.0709 - regression_loss: 2.9225 - classification_loss: 1.1485
 902/1000 [==========================>...] - ETA: 44s - loss: 4.0733 - regression_loss: 2.9240 - classification_loss: 1.1493
 903/1000 [==========================>...] - ETA: 44s - loss: 4.0746 - regression_loss: 2.9250 - classification_loss: 1.1496
 904/1000 [==========================>...] - ETA: 43s - loss: 4.0768 - regression_loss: 2.9265 - classification_loss: 1.1503
 905/1000 [==========================>...] - ETA: 43s - loss: 4.0723 - regression_loss: 2.9232 - classification_loss: 1.1490keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 633 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.358e+03, 1.620e+02, 1.442e+03, 2.610e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 906/1000 [==========================>...] - ETA: 42s - loss: 4.0736 - regression_loss: 2.9241 - classification_loss: 1.1495
 907/1000 [==========================>...] - ETA: 42s - loss: 4.0750 - regression_loss: 2.9246 - classification_loss: 1.1504
 908/1000 [==========================>...] - ETA: 41s - loss: 4.0762 - regression_loss: 2.9254 - classification_loss: 1.1509
 909/1000 [==========================>...] - ETA: 41s - loss: 4.0785 - regression_loss: 2.9261 - classification_loss: 1.1524
 910/1000 [==========================>...] - ETA: 40s - loss: 4.0806 - regression_loss: 2.9274 - classification_loss: 1.1532
 911/1000 [==========================>...] - ETA: 40s - loss: 4.0822 - regression_loss: 2.9285 - classification_loss: 1.1536
 912/1000 [==========================>...] - ETA: 40s - loss: 4.0833 - regression_loss: 2.9294 - classification_loss: 1.1539keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 345 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([8.890e+02, 2.180e+02, 1.486e+03, 9.210e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 913/1000 [==========================>...] - ETA: 39s - loss: 4.0843 - regression_loss: 2.9303 - classification_loss: 1.1540
 914/1000 [==========================>...] - ETA: 39s - loss: 4.0860 - regression_loss: 2.9317 - classification_loss: 1.1543
 915/1000 [==========================>...] - ETA: 38s - loss: 4.0876 - regression_loss: 2.9330 - classification_loss: 1.1546
 916/1000 [==========================>...] - ETA: 38s - loss: 4.0831 - regression_loss: 2.9298 - classification_loss: 1.1534
 917/1000 [==========================>...] - ETA: 37s - loss: 4.0841 - regression_loss: 2.9306 - classification_loss: 1.1534
 918/1000 [==========================>...] - ETA: 37s - loss: 4.0847 - regression_loss: 2.9312 - classification_loss: 1.1535keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 402 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.165e+03, 4.000e+01, 1.422e+03, 3.110e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 919/1000 [==========================>...] - ETA: 36s - loss: 4.0882 - regression_loss: 2.9341 - classification_loss: 1.1541
 920/1000 [==========================>...] - ETA: 36s - loss: 4.0892 - regression_loss: 2.9349 - classification_loss: 1.1543
 921/1000 [==========================>...] - ETA: 35s - loss: 4.0907 - regression_loss: 2.9362 - classification_loss: 1.1545
 922/1000 [==========================>...] - ETA: 35s - loss: 4.0919 - regression_loss: 2.9372 - classification_loss: 1.1547
 923/1000 [==========================>...] - ETA: 35s - loss: 4.0875 - regression_loss: 2.9341 - classification_loss: 1.1534
 924/1000 [==========================>...] - ETA: 34s - loss: 4.0889 - regression_loss: 2.9351 - classification_loss: 1.1538
 925/1000 [==========================>...] - ETA: 34s - loss: 4.0899 - regression_loss: 2.9358 - classification_loss: 1.1541keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 650 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.011e+03, 3.000e+01, 1.404e+03, 3.460e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 926/1000 [==========================>...] - ETA: 33s - loss: 4.0904 - regression_loss: 2.9362 - classification_loss: 1.1542keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 53 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.021e+03, 6.780e+02, 1.465e+03, 1.021e+03, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 927/1000 [==========================>...] - ETA: 33s - loss: 4.0913 - regression_loss: 2.9370 - classification_loss: 1.1543
 928/1000 [==========================>...] - ETA: 32s - loss: 4.0921 - regression_loss: 2.9376 - classification_loss: 1.1546
 929/1000 [==========================>...] - ETA: 32s - loss: 4.0878 - regression_loss: 2.9344 - classification_loss: 1.1534
 930/1000 [==========================>...] - ETA: 31s - loss: 4.0883 - regression_loss: 2.9349 - classification_loss: 1.1534keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 971 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([9.890e+02, 0.000e+00, 1.601e+03, 5.190e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 931/1000 [==========================>...] - ETA: 31s - loss: 4.0839 - regression_loss: 2.9317 - classification_loss: 1.1522keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 605 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.215e+03, 0.000e+00, 1.455e+03, 2.640e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 932/1000 [==========================>...] - ETA: 30s - loss: 4.0849 - regression_loss: 2.9325 - classification_loss: 1.1524
 933/1000 [==========================>...] - ETA: 30s - loss: 4.0861 - regression_loss: 2.9335 - classification_loss: 1.1526
 934/1000 [===========================>..] - ETA: 30s - loss: 4.0869 - regression_loss: 2.9343 - classification_loss: 1.1526
 935/1000 [===========================>..] - ETA: 29s - loss: 4.0880 - regression_loss: 2.9353 - classification_loss: 1.1526
 936/1000 [===========================>..] - ETA: 29s - loss: 4.0836 - regression_loss: 2.9322 - classification_loss: 1.1514
 937/1000 [===========================>..] - ETA: 28s - loss: 4.0792 - regression_loss: 2.9291 - classification_loss: 1.1502
 938/1000 [===========================>..] - ETA: 28s - loss: 4.0801 - regression_loss: 2.9298 - classification_loss: 1.1502
 939/1000 [===========================>..] - ETA: 27s - loss: 4.0817 - regression_loss: 2.9314 - classification_loss: 1.1504
 940/1000 [===========================>..] - ETA: 27s - loss: 4.0823 - regression_loss: 2.9317 - classification_loss: 1.1506
 941/1000 [===========================>..] - ETA: 26s - loss: 4.0780 - regression_loss: 2.9286 - classification_loss: 1.1494
 942/1000 [===========================>..] - ETA: 26s - loss: 4.0736 - regression_loss: 2.9255 - classification_loss: 1.1481
 943/1000 [===========================>..] - ETA: 25s - loss: 4.0759 - regression_loss: 2.9275 - classification_loss: 1.1484
 944/1000 [===========================>..] - ETA: 25s - loss: 4.0766 - regression_loss: 2.9281 - classification_loss: 1.1485
 945/1000 [===========================>..] - ETA: 25s - loss: 4.0790 - regression_loss: 2.9297 - classification_loss: 1.1494
 946/1000 [===========================>..] - ETA: 24s - loss: 4.0805 - regression_loss: 2.9310 - classification_loss: 1.1495keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 996 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.224e+03, 1.125e+03, 1.486e+03, 1.339e+03, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 947/1000 [===========================>..] - ETA: 24s - loss: 4.0814 - regression_loss: 2.9319 - classification_loss: 1.1495
 948/1000 [===========================>..] - ETA: 23s - loss: 4.0835 - regression_loss: 2.9334 - classification_loss: 1.1501
 949/1000 [===========================>..] - ETA: 23s - loss: 4.0854 - regression_loss: 2.9351 - classification_loss: 1.1503
 950/1000 [===========================>..] - ETA: 22s - loss: 4.0811 - regression_loss: 2.9320 - classification_loss: 1.1491
 951/1000 [===========================>..] - ETA: 22s - loss: 4.0817 - regression_loss: 2.9324 - classification_loss: 1.1493
 952/1000 [===========================>..] - ETA: 21s - loss: 4.0828 - regression_loss: 2.9333 - classification_loss: 1.1496
 953/1000 [===========================>..] - ETA: 21s - loss: 4.0848 - regression_loss: 2.9346 - classification_loss: 1.1503
 954/1000 [===========================>..] - ETA: 20s - loss: 4.0868 - regression_loss: 2.9352 - classification_loss: 1.1516
 955/1000 [===========================>..] - ETA: 20s - loss: 4.0875 - regression_loss: 2.9358 - classification_loss: 1.1517
 956/1000 [===========================>..] - ETA: 20s - loss: 4.0833 - regression_loss: 2.9328 - classification_loss: 1.1505keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 606 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.118e+03, 3.250e+02, 1.589e+03, 8.330e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 957/1000 [===========================>..] - ETA: 19s - loss: 4.0790 - regression_loss: 2.9297 - classification_loss: 1.1493
 958/1000 [===========================>..] - ETA: 19s - loss: 4.0800 - regression_loss: 2.9306 - classification_loss: 1.1494keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 94 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.232e+03, 1.540e+02, 1.504e+03, 4.350e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 959/1000 [===========================>..] - ETA: 18s - loss: 4.0812 - regression_loss: 2.9315 - classification_loss: 1.1497
 960/1000 [===========================>..] - ETA: 18s - loss: 4.0825 - regression_loss: 2.9323 - classification_loss: 1.1502
 961/1000 [===========================>..] - ETA: 17s - loss: 4.0834 - regression_loss: 2.9326 - classification_loss: 1.1508
 962/1000 [===========================>..] - ETA: 17s - loss: 4.0838 - regression_loss: 2.9327 - classification_loss: 1.1511keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 492 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.221e+03, 9.410e+02, 1.545e+03, 1.276e+03, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 963/1000 [===========================>..] - ETA: 16s - loss: 4.0848 - regression_loss: 2.9335 - classification_loss: 1.1513
 964/1000 [===========================>..] - ETA: 16s - loss: 4.0805 - regression_loss: 2.9304 - classification_loss: 1.1501
 965/1000 [===========================>..] - ETA: 15s - loss: 4.0823 - regression_loss: 2.9313 - classification_loss: 1.1510
 966/1000 [===========================>..] - ETA: 15s - loss: 4.0837 - regression_loss: 2.9325 - classification_loss: 1.1512
 967/1000 [============================>.] - ETA: 15s - loss: 4.0795 - regression_loss: 2.9295 - classification_loss: 1.1500keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 679 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.188e+03, 4.350e+02, 1.514e+03, 7.620e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 968/1000 [============================>.] - ETA: 14s - loss: 4.0820 - regression_loss: 2.9313 - classification_loss: 1.1506
 969/1000 [============================>.] - ETA: 14s - loss: 4.0778 - regression_loss: 2.9283 - classification_loss: 1.1495
 970/1000 [============================>.] - ETA: 13s - loss: 4.0800 - regression_loss: 2.9297 - classification_loss: 1.1503keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 261 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([9.080e+02, 7.650e+02, 1.607e+03, 1.482e+03, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 971/1000 [============================>.] - ETA: 13s - loss: 4.0820 - regression_loss: 2.9311 - classification_loss: 1.1509
 972/1000 [============================>.] - ETA: 12s - loss: 4.0836 - regression_loss: 2.9323 - classification_loss: 1.1513
 973/1000 [============================>.] - ETA: 12s - loss: 4.0794 - regression_loss: 2.9293 - classification_loss: 1.1501
 974/1000 [============================>.] - ETA: 11s - loss: 4.0808 - regression_loss: 2.9301 - classification_loss: 1.1508
 975/1000 [============================>.] - ETA: 11s - loss: 4.0828 - regression_loss: 2.9316 - classification_loss: 1.1513
 976/1000 [============================>.] - ETA: 10s - loss: 4.0835 - regression_loss: 2.9321 - classification_loss: 1.1514
 977/1000 [============================>.] - ETA: 10s - loss: 4.0840 - regression_loss: 2.9324 - classification_loss: 1.1516
 978/1000 [============================>.] - ETA: 10s - loss: 4.0798 - regression_loss: 2.9294 - classification_loss: 1.1504
 979/1000 [============================>.] - ETA: 9s - loss: 4.0806 - regression_loss: 2.9294 - classification_loss: 1.1512 
 980/1000 [============================>.] - ETA: 9s - loss: 4.0813 - regression_loss: 2.9299 - classification_loss: 1.1514
 981/1000 [============================>.] - ETA: 8s - loss: 4.0771 - regression_loss: 2.9269 - classification_loss: 1.1502
 982/1000 [============================>.] - ETA: 8s - loss: 4.0785 - regression_loss: 2.9282 - classification_loss: 1.1503
 983/1000 [============================>.] - ETA: 7s - loss: 4.0795 - regression_loss: 2.9284 - classification_loss: 1.1511
 984/1000 [============================>.] - ETA: 7s - loss: 4.0808 - regression_loss: 2.9296 - classification_loss: 1.1512
 985/1000 [============================>.] - ETA: 6s - loss: 4.0820 - regression_loss: 2.9305 - classification_loss: 1.1515
 986/1000 [============================>.] - ETA: 6s - loss: 4.0837 - regression_loss: 2.9319 - classification_loss: 1.1518keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:110: UserWarning: Image with id 176 (shape (1750, 1400, 3)) contains the following invalid boxes: [array([1.057e+03, 0.000e+00, 1.460e+03, 4.090e+02, 1.000e+00])].
  [annotations[invalid_index, :] for invalid_index in invalid_indices]

 987/1000 [============================>.] - ETA: 5s - loss: 4.0851 - regression_loss: 2.9328 - classification_loss: 1.1522
 988/1000 [============================>.] - ETA: 5s - loss: 4.0862 - regression_loss: 2.9339 - classification_loss: 1.1523
 989/1000 [============================>.] - ETA: 5s - loss: 4.0864 - regression_loss: 2.9338 - classification_loss: 1.1525
 990/1000 [============================>.] - ETA: 4s - loss: 4.0882 - regression_loss: 2.9350 - classification_loss: 1.1532
 991/1000 [============================>.] - ETA: 4s - loss: 4.0896 - regression_loss: 2.9358 - classification_loss: 1.1538
 992/1000 [============================>.] - ETA: 3s - loss: 4.0904 - regression_loss: 2.9364 - classification_loss: 1.1540
 993/1000 [============================>.] - ETA: 3s - loss: 4.0913 - regression_loss: 2.9372 - classification_loss: 1.1541
 994/1000 [============================>.] - ETA: 2s - loss: 4.0924 - regression_loss: 2.9378 - classification_loss: 1.1546
 995/1000 [============================>.] - ETA: 2s - loss: 4.0931 - regression_loss: 2.9380 - classification_loss: 1.1551
 996/1000 [============================>.] - ETA: 1s - loss: 4.0940 - regression_loss: 2.9387 - classification_loss: 1.1553
 997/1000 [============================>.] - ETA: 1s - loss: 4.0899 - regression_loss: 2.9358 - classification_loss: 1.1541
 998/1000 [============================>.] - ETA: 0s - loss: 4.0858 - regression_loss: 2.9329 - classification_loss: 1.1529
 999/1000 [============================>.] - ETA: 0s - loss: 4.0867 - regression_loss: 2.9337 - classification_loss: 1.1530
1000/1000 [==============================] - 455s 455ms/step - loss: 4.0826 - regression_loss: 2.9307 - classification_loss: 1.1519

Epoch 00001: saving model to ./snapshots/resnet50_csv_01.h5
0/2001/2002/2003/2004/2005/2006/2007/2008/2009/20010/20011/20012/20013/20014/20015/20016/20017/20018/20019/20020/20021/20022/20023/20024/20025/20026/20027/20028/20029/20030/20031/20032/20033/20034/20035/20036/20037/20038/20039/20040/20041/20042/20043/20044/20045/20046/20047/20048/20049/20050/20051/20052/20053/20054/20055/20056/20057/20058/20059/20060/20061/20062/20063/20064/20065/20066/20067/20068/20069/20070/20071/20072/20073/20074/20075/20076/20077/20078/20079/20080/20081/20082/20083/20084/20085/20086/20087/20088/20089/20090/20091/20092/20093/20094/20095/20096/20097/20098/20099/200100/200101/200102/200103/200104/200105/200106/200107/200108/200109/200110/200111/200112/200113/200114/200115/200116/200117/200118/200119/200120/200121/200122/200123/200124/200125/200126/200127/200128/200129/200130/200131/200132/200133/200134/200135/200136/200137/200138/200139/200140/200141/200142/200143/200144/200145/200146/200147/200148/200149/200150/200151/200152/200153/200154/200155/200156/200157/200158/200159/200160/200161/200162/200163/200164/200165/200166/200167/200168/200169/200170/200171/200172/200173/200174/200175/200176/200177/200178/200179/200180/200181/200182/200183/200184/200185/200186/200187/200188/200189/200190/200191/200192/200193/200194/200195/200196/200197/200198/200199/2000/2001/2002/2003/2004/2005/2006/2007/2008/2009/20010/20011/20012/20013/20014/20015/20016/20017/20018/20019/20020/20021/20022/20023/20024/20025/20026/20027/20028/20029/20030/20031/20032/20033/20034/20035/20036/20037/20038/20039/20040/20041/20042/20043/20044/20045/20046/20047/20048/20049/20050/20051/20052/20053/20054/20055/20056/20057/20058/20059/20060/20061/20062/20063/20064/20065/20066/20067/20068/20069/20070/20071/20072/20073/20074/20075/20076/20077/20078/20079/20080/20081/20082/20083/20084/20085/20086/20087/20088/20089/20090/20091/20092/20093/20094/20095/20096/20097/20098/20099/200100/200101/200102/200103/200104/200105/200106/200107/200108/200109/200110/200111/200112/200113/200114/200115/200116/200117/200118/200119/200120/200121/200122/200123/200124/200125/200126/200127/200128/200129/200130/200131/200132/200133/200134/200135/200136/200137/200138/200139/200140/200141/200142/200143/200144/200145/200146/200147/200148/200149/200150/200151/200152/200153/200154/200155/200156/200157/200158/200159/200160/200161/200162/200163/200164/200165/200166/200167/200168/200169/200170/200171/200172/200173/200174/200175/200176/200177/200178/200179/200180/200181/200182/200183/200184/200185/200186/200187/200188/200189/200190/200191/200192/200193/200194/200195/200196/200197/200198/200199/200

M 0.0000
N 0.0000
mAP: 0.0000
Epoch 2/30

   1/1000 [..............................] - ETA: 7:25 - loss: 6.1240 - regression_loss: 3.9893 - classification_loss: 2.1348
   2/1000 [..............................] - ETA: 7:24 - loss: 5.6859 - regression_loss: 3.9617 - classification_loss: 1.7242
   3/1000 [..............................] - ETA: 7:25 - loss: 3.7907 - regression_loss: 2.6411 - classification_loss: 1.1496
   4/1000 [..............................] - ETA: 7:24 - loss: 2.8440 - regression_loss: 1.9808 - classification_loss: 0.8631
   5/1000 [..............................] - ETA: 7:24 - loss: 3.2218 - regression_loss: 2.2673 - classification_loss: 0.9546
   6/1000 [..............................] - ETA: 7:24 - loss: 3.5614 - regression_loss: 2.5531 - classification_loss: 1.0083
   7/1000 [..............................] - ETA: 7:24 - loss: 3.8586 - regression_loss: 2.6400 - classification_loss: 1.2186
   8/1000 [..............................] - ETA: 7:23 - loss: 3.9779 - regression_loss: 2.7402 - classification_loss: 1.2377
   9/1000 [..............................] - ETA: 7:23 - loss: 4.1438 - regression_loss: 2.8667 - classification_loss: 1.2771
  10/1000 [..............................] - ETA: 7:22 - loss: 3.7294 - regression_loss: 2.5800 - classification_loss: 1.1494
  11/1000 [..............................] - ETA: 7:22 - loss: 3.9084 - regression_loss: 2.7344 - classification_loss: 1.1740
  12/1000 [..............................] - ETA: 7:22 - loss: 3.5827 - regression_loss: 2.5066 - classification_loss: 1.0761
  13/1000 [..............................] - ETA: 7:21 - loss: 3.6837 - regression_loss: 2.5590 - classification_loss: 1.1247
  14/1000 [..............................] - ETA: 7:21 - loss: 3.8068 - regression_loss: 2.6313 - classification_loss: 1.1756
  15/1000 [..............................] - ETA: 7:21 - loss: 3.9423 - regression_loss: 2.7478 - classification_loss: 1.1945
  16/1000 [..............................] - ETA: 7:20 - loss: 3.6959 - regression_loss: 2.5760 - classification_loss: 1.1199
  17/1000 [..............................] - ETA: 7:20 - loss: 3.4785 - regression_loss: 2.4245 - classification_loss: 1.0540
  18/1000 [..............................] - ETA: 7:19 - loss: 3.5702 - regression_loss: 2.5016 - classification_loss: 1.0686
  19/1000 [..............................] - ETA: 7:19 - loss: 3.6608 - regression_loss: 2.5647 - classification_loss: 1.0961
  20/1000 [..............................] - ETA: 7:19 - loss: 3.7386 - regression_loss: 2.6059 - classification_loss: 1.1327
  21/1000 [..............................] - ETA: 7:19 - loss: 3.8096 - regression_loss: 2.6585 - classification_loss: 1.1511
  22/1000 [..............................] - ETA: 7:18 - loss: 3.8834 - regression_loss: 2.7131 - classification_loss: 1.1703
  23/1000 [..............................] - ETA: 7:18 - loss: 3.9282 - regression_loss: 2.7463 - classification_loss: 1.1819
  24/1000 [..............................] - ETA: 7:16 - loss: 3.9783 - regression_loss: 2.7824 - classification_loss: 1.1959
  25/1000 [..............................] - ETA: 7:15 - loss: 3.8193 - regression_loss: 2.6711 - classification_loss: 1.1482
  26/1000 [..............................] - ETA: 7:15 - loss: 3.8844 - regression_loss: 2.7065 - classification_loss: 1.1779
  27/1000 [..............................] - ETA: 7:15 - loss: 3.9967 - regression_loss: 2.7793 - classification_loss: 1.2174
  28/1000 [..............................] - ETA: 7:14 - loss: 4.0370 - regression_loss: 2.8055 - classification_loss: 1.2315
  29/1000 [..............................] - ETA: 7:14 - loss: 3.8978 - regression_loss: 2.7087 - classification_loss: 1.1891
  30/1000 [..............................] - ETA: 7:13 - loss: 3.9759 - regression_loss: 2.7734 - classification_loss: 1.2025
  31/1000 [..............................] - ETA: 7:13 - loss: 4.0205 - regression_loss: 2.8054 - classification_loss: 1.2150
  32/1000 [..............................] - ETA: 7:13 - loss: 4.0900 - regression_loss: 2.8549 - classification_loss: 1.2351
  33/1000 [..............................] - ETA: 7:13 - loss: 4.1357 - regression_loss: 2.8918 - classification_loss: 1.2439
  34/1000 [>.............................] - ETA: 7:13 - loss: 4.1965 - regression_loss: 2.9340 - classification_loss: 1.2625
  35/1000 [>.............................] - ETA: 7:12 - loss: 4.0766 - regression_loss: 2.8502 - classification_loss: 1.2264
  36/1000 [>.............................] - ETA: 7:12 - loss: 4.0983 - regression_loss: 2.8631 - classification_loss: 1.2352
  37/1000 [>.............................] - ETA: 7:11 - loss: 3.9876 - regression_loss: 2.7858 - classification_loss: 1.2018
  38/1000 [>.............................] - ETA: 7:11 - loss: 4.0138 - regression_loss: 2.8079 - classification_loss: 1.2059
  39/1000 [>.............................] - ETA: 7:11 - loss: 3.9109 - regression_loss: 2.7359 - classification_loss: 1.1750
  40/1000 [>.............................] - ETA: 7:10 - loss: 3.9361 - regression_loss: 2.7585 - classification_loss: 1.1776
  41/1000 [>.............................] - ETA: 7:10 - loss: 3.9731 - regression_loss: 2.7885 - classification_loss: 1.1846
  42/1000 [>.............................] - ETA: 7:10 - loss: 3.9982 - regression_loss: 2.8106 - classification_loss: 1.1877
  43/1000 [>.............................] - ETA: 7:09 - loss: 4.0249 - regression_loss: 2.8285 - classification_loss: 1.1964
  44/1000 [>.............................] - ETA: 7:09 - loss: 4.0880 - regression_loss: 2.8861 - classification_loss: 1.2019
  45/1000 [>.............................] - ETA: 7:08 - loss: 4.1121 - regression_loss: 2.9049 - classification_loss: 1.2072
  46/1000 [>.............................] - ETA: 7:08 - loss: 4.0227 - regression_loss: 2.8417 - classification_loss: 1.1810
  47/1000 [>.............................] - ETA: 7:08 - loss: 4.0693 - regression_loss: 2.8848 - classification_loss: 1.1844
  48/1000 [>.............................] - ETA: 7:07 - loss: 4.0974 - regression_loss: 2.9083 - classification_loss: 1.1891
  49/1000 [>.............................] - ETA: 7:07 - loss: 4.1351 - regression_loss: 2.9193 - classification_loss: 1.2158
  50/1000 [>.............................] - ETA: 7:07 - loss: 4.1648 - regression_loss: 2.9364 - classification_loss: 1.2284
  51/1000 [>.............................] - ETA: 7:06 - loss: 4.1795 - regression_loss: 2.9474 - classification_loss: 1.2321
  52/1000 [>.............................] - ETA: 7:06 - loss: 4.0991 - regression_loss: 2.8907 - classification_loss: 1.2085
  53/1000 [>.............................] - ETA: 7:05 - loss: 4.1266 - regression_loss: 2.9183 - classification_loss: 1.2083
  54/1000 [>.............................] - ETA: 7:05 - loss: 4.1543 - regression_loss: 2.9335 - classification_loss: 1.2207
  55/1000 [>.............................] - ETA: 7:04 - loss: 4.0788 - regression_loss: 2.8802 - classification_loss: 1.1986
  56/1000 [>.............................] - ETA: 7:04 - loss: 4.1025 - regression_loss: 2.9003 - classification_loss: 1.2022
  57/1000 [>.............................] - ETA: 7:04 - loss: 4.1159 - regression_loss: 2.9137 - classification_loss: 1.2022
  58/1000 [>.............................] - ETA: 7:03 - loss: 4.1291 - regression_loss: 2.9248 - classification_loss: 1.2043
  59/1000 [>.............................] - ETA: 7:03 - loss: 4.1452 - regression_loss: 2.9398 - classification_loss: 1.2054
  60/1000 [>.............................] - ETA: 7:03 - loss: 4.0761 - regression_loss: 2.8908 - classification_loss: 1.1853
  61/1000 [>.............................] - ETA: 7:02 - loss: 4.1012 - regression_loss: 2.9100 - classification_loss: 1.1913
  62/1000 [>.............................] - ETA: 7:01 - loss: 4.1098 - regression_loss: 2.9162 - classification_loss: 1.1935
  63/1000 [>.............................] - ETA: 7:01 - loss: 4.1261 - regression_loss: 2.9319 - classification_loss: 1.1943
  64/1000 [>.............................] - ETA: 7:00 - loss: 4.1443 - regression_loss: 2.9449 - classification_loss: 1.1994
  65/1000 [>.............................] - ETA: 7:00 - loss: 4.0805 - regression_loss: 2.8996 - classification_loss: 1.1809
  66/1000 [>.............................] - ETA: 7:00 - loss: 4.1027 - regression_loss: 2.9078 - classification_loss: 1.1949
  67/1000 [=>............................] - ETA: 6:59 - loss: 4.1214 - regression_loss: 2.9221 - classification_loss: 1.1993
  68/1000 [=>............................] - ETA: 6:59 - loss: 4.1342 - regression_loss: 2.9333 - classification_loss: 1.2009
  69/1000 [=>............................] - ETA: 6:58 - loss: 4.0743 - regression_loss: 2.8908 - classification_loss: 1.1835
  70/1000 [=>............................] - ETA: 6:58 - loss: 4.0838 - regression_loss: 2.8989 - classification_loss: 1.1849
  71/1000 [=>............................] - ETA: 6:57 - loss: 4.1020 - regression_loss: 2.9084 - classification_loss: 1.1935
  72/1000 [=>............................] - ETA: 6:57 - loss: 4.0450 - regression_loss: 2.8680 - classification_loss: 1.1770
  73/1000 [=>............................] - ETA: 6:57 - loss: 4.0571 - regression_loss: 2.8759 - classification_loss: 1.1813
  74/1000 [=>............................] - ETA: 6:56 - loss: 4.0023 - regression_loss: 2.8370 - classification_loss: 1.1653
  75/1000 [=>............................] - ETA: 6:56 - loss: 4.0148 - regression_loss: 2.8461 - classification_loss: 1.1687
  76/1000 [=>............................] - ETA: 6:55 - loss: 4.0332 - regression_loss: 2.8585 - classification_loss: 1.1747
  77/1000 [=>............................] - ETA: 6:55 - loss: 3.9808 - regression_loss: 2.8214 - classification_loss: 1.1594
  78/1000 [=>............................] - ETA: 6:54 - loss: 3.9933 - regression_loss: 2.8324 - classification_loss: 1.1608
  79/1000 [=>............................] - ETA: 6:54 - loss: 4.0166 - regression_loss: 2.8475 - classification_loss: 1.1692
  80/1000 [=>............................] - ETA: 6:54 - loss: 4.0286 - regression_loss: 2.8555 - classification_loss: 1.1731
  81/1000 [=>............................] - ETA: 6:53 - loss: 4.0469 - regression_loss: 2.8691 - classification_loss: 1.1778
  82/1000 [=>............................] - ETA: 6:53 - loss: 4.0518 - regression_loss: 2.8694 - classification_loss: 1.1823
  83/1000 [=>............................] - ETA: 6:52 - loss: 4.0759 - regression_loss: 2.8803 - classification_loss: 1.1957
  84/1000 [=>............................] - ETA: 6:52 - loss: 4.0998 - regression_loss: 2.8983 - classification_loss: 1.2015
  85/1000 [=>............................] - ETA: 6:52 - loss: 4.1058 - regression_loss: 2.9030 - classification_loss: 1.2028
  86/1000 [=>............................] - ETA: 6:51 - loss: 4.0581 - regression_loss: 2.8692 - classification_loss: 1.1889
  87/1000 [=>............................] - ETA: 6:51 - loss: 4.0115 - regression_loss: 2.8363 - classification_loss: 1.1752
  88/1000 [=>............................] - ETA: 6:50 - loss: 4.0214 - regression_loss: 2.8448 - classification_loss: 1.1766
  89/1000 [=>............................] - ETA: 6:50 - loss: 3.9763 - regression_loss: 2.8128 - classification_loss: 1.1634
  90/1000 [=>............................] - ETA: 6:49 - loss: 3.9321 - regression_loss: 2.7816 - classification_loss: 1.1505
  91/1000 [=>............................] - ETA: 6:49 - loss: 3.9452 - regression_loss: 2.7894 - classification_loss: 1.1557
  92/1000 [=>............................] - ETA: 6:48 - loss: 3.9525 - regression_loss: 2.7961 - classification_loss: 1.1564
  93/1000 [=>............................] - ETA: 6:48 - loss: 3.9656 - regression_loss: 2.8035 - classification_loss: 1.1622
  94/1000 [=>............................] - ETA: 6:48 - loss: 3.9802 - regression_loss: 2.8115 - classification_loss: 1.1687
  95/1000 [=>............................] - ETA: 6:47 - loss: 4.0123 - regression_loss: 2.8175 - classification_loss: 1.1948
  96/1000 [=>............................] - ETA: 6:47 - loss: 4.0339 - regression_loss: 2.8320 - classification_loss: 1.2019
  97/1000 [=>............................] - ETA: 6:46 - loss: 4.0431 - regression_loss: 2.8393 - classification_loss: 1.2038
  98/1000 [=>............................] - ETA: 6:46 - loss: 4.0507 - regression_loss: 2.8471 - classification_loss: 1.2037
  99/1000 [=>............................] - ETA: 6:45 - loss: 4.0098 - regression_loss: 2.8183 - classification_loss: 1.1915
 100/1000 [==>...........................] - ETA: 6:45 - loss: 4.0246 - regression_loss: 2.8289 - classification_loss: 1.1957
 101/1000 [==>...........................] - ETA: 6:45 - loss: 4.0309 - regression_loss: 2.8342 - classification_loss: 1.1968
 102/1000 [==>...........................] - ETA: 6:44 - loss: 4.0360 - regression_loss: 2.8387 - classification_loss: 1.1973
 103/1000 [==>...........................] - ETA: 6:44 - loss: 4.0466 - regression_loss: 2.8487 - classification_loss: 1.1978
 104/1000 [==>...........................] - ETA: 6:43 - loss: 4.0077 - regression_loss: 2.8213 - classification_loss: 1.1863
 105/1000 [==>...........................] - ETA: 6:43 - loss: 4.0189 - regression_loss: 2.8280 - classification_loss: 1.1910
 106/1000 [==>...........................] - ETA: 6:42 - loss: 3.9810 - regression_loss: 2.8013 - classification_loss: 1.1798
 107/1000 [==>...........................] - ETA: 6:42 - loss: 3.9439 - regression_loss: 2.7751 - classification_loss: 1.1688
 108/1000 [==>...........................] - ETA: 6:41 - loss: 3.9582 - regression_loss: 2.7813 - classification_loss: 1.1769
 109/1000 [==>...........................] - ETA: 6:41 - loss: 3.9632 - regression_loss: 2.7859 - classification_loss: 1.1774
 110/1000 [==>...........................] - ETA: 6:41 - loss: 3.9272 - regression_loss: 2.7605 - classification_loss: 1.1667
 111/1000 [==>...........................] - ETA: 6:40 - loss: 3.9336 - regression_loss: 2.7628 - classification_loss: 1.1709
 112/1000 [==>...........................] - ETA: 6:40 - loss: 3.9488 - regression_loss: 2.7708 - classification_loss: 1.1780
 113/1000 [==>...........................] - ETA: 6:39 - loss: 3.9139 - regression_loss: 2.7463 - classification_loss: 1.1676
 114/1000 [==>...........................] - ETA: 6:39 - loss: 3.8795 - regression_loss: 2.7222 - classification_loss: 1.1573
 115/1000 [==>...........................] - ETA: 6:38 - loss: 3.8982 - regression_loss: 2.7357 - classification_loss: 1.1625
 116/1000 [==>...........................] - ETA: 6:38 - loss: 3.9083 - regression_loss: 2.7427 - classification_loss: 1.1656
 117/1000 [==>...........................] - ETA: 6:38 - loss: 3.9220 - regression_loss: 2.7503 - classification_loss: 1.1718
 118/1000 [==>...........................] - ETA: 6:37 - loss: 3.8888 - regression_loss: 2.7270 - classification_loss: 1.1618
 119/1000 [==>...........................] - ETA: 6:37 - loss: 3.8967 - regression_loss: 2.7331 - classification_loss: 1.1636
 120/1000 [==>...........................] - ETA: 6:36 - loss: 3.8642 - regression_loss: 2.7103 - classification_loss: 1.1539
 121/1000 [==>...........................] - ETA: 6:35 - loss: 3.8323 - regression_loss: 2.6879 - classification_loss: 1.1444
 122/1000 [==>...........................] - ETA: 6:35 - loss: 3.8009 - regression_loss: 2.6659 - classification_loss: 1.1350
 123/1000 [==>...........................] - ETA: 6:35 - loss: 3.8111 - regression_loss: 2.6733 - classification_loss: 1.1378
 124/1000 [==>...........................] - ETA: 6:34 - loss: 3.8305 - regression_loss: 2.6887 - classification_loss: 1.1417
 125/1000 [==>...........................] - ETA: 6:34 - loss: 3.8357 - regression_loss: 2.6921 - classification_loss: 1.1436
 126/1000 [==>...........................] - ETA: 6:33 - loss: 3.8437 - regression_loss: 2.6979 - classification_loss: 1.1458
 127/1000 [==>...........................] - ETA: 6:33 - loss: 3.8618 - regression_loss: 2.7039 - classification_loss: 1.1578
 128/1000 [==>...........................] - ETA: 6:32 - loss: 3.8764 - regression_loss: 2.7141 - classification_loss: 1.1623
 129/1000 [==>...........................] - ETA: 6:32 - loss: 3.8851 - regression_loss: 2.7210 - classification_loss: 1.1641
 130/1000 [==>...........................] - ETA: 6:31 - loss: 3.8552 - regression_loss: 2.7001 - classification_loss: 1.1552
 131/1000 [==>...........................] - ETA: 6:31 - loss: 3.8614 - regression_loss: 2.7048 - classification_loss: 1.1566
 132/1000 [==>...........................] - ETA: 6:30 - loss: 3.8321 - regression_loss: 2.6843 - classification_loss: 1.1478
 133/1000 [==>...........................] - ETA: 6:30 - loss: 3.8033 - regression_loss: 2.6641 - classification_loss: 1.1392
 134/1000 [===>..........................] - ETA: 6:30 - loss: 3.8164 - regression_loss: 2.6708 - classification_loss: 1.1456
 135/1000 [===>..........................] - ETA: 6:29 - loss: 3.7881 - regression_loss: 2.6510 - classification_loss: 1.1371
 136/1000 [===>..........................] - ETA: 6:29 - loss: 3.7980 - regression_loss: 2.6583 - classification_loss: 1.1398
 137/1000 [===>..........................] - ETA: 6:28 - loss: 3.8076 - regression_loss: 2.6650 - classification_loss: 1.1426
 138/1000 [===>..........................] - ETA: 6:28 - loss: 3.7800 - regression_loss: 2.6456 - classification_loss: 1.1344
 139/1000 [===>..........................] - ETA: 6:27 - loss: 3.7884 - regression_loss: 2.6501 - classification_loss: 1.1384
 140/1000 [===>..........................] - ETA: 6:27 - loss: 3.7993 - regression_loss: 2.6572 - classification_loss: 1.1421
 141/1000 [===>..........................] - ETA: 6:27 - loss: 3.8125 - regression_loss: 2.6617 - classification_loss: 1.1509
 142/1000 [===>..........................] - ETA: 6:26 - loss: 3.8207 - regression_loss: 2.6678 - classification_loss: 1.1529
 143/1000 [===>..........................] - ETA: 6:26 - loss: 3.8350 - regression_loss: 2.6757 - classification_loss: 1.1593
 144/1000 [===>..........................] - ETA: 6:25 - loss: 3.8420 - regression_loss: 2.6796 - classification_loss: 1.1623
 145/1000 [===>..........................] - ETA: 6:25 - loss: 3.8468 - regression_loss: 2.6826 - classification_loss: 1.1642
 146/1000 [===>..........................] - ETA: 6:24 - loss: 3.8525 - regression_loss: 2.6878 - classification_loss: 1.1647
 147/1000 [===>..........................] - ETA: 6:24 - loss: 3.8621 - regression_loss: 2.6969 - classification_loss: 1.1653
 148/1000 [===>..........................] - ETA: 6:24 - loss: 3.8720 - regression_loss: 2.6992 - classification_loss: 1.1728
 149/1000 [===>..........................] - ETA: 6:23 - loss: 3.8807 - regression_loss: 2.7059 - classification_loss: 1.1749
 150/1000 [===>..........................] - ETA: 6:23 - loss: 3.8932 - regression_loss: 2.7147 - classification_loss: 1.1785
 151/1000 [===>..........................] - ETA: 6:22 - loss: 3.8998 - regression_loss: 2.7207 - classification_loss: 1.1791
 152/1000 [===>..........................] - ETA: 6:22 - loss: 3.9054 - regression_loss: 2.7259 - classification_loss: 1.1795
 153/1000 [===>..........................] - ETA: 6:21 - loss: 3.9126 - regression_loss: 2.7316 - classification_loss: 1.1810
 154/1000 [===>..........................] - ETA: 6:21 - loss: 3.9192 - regression_loss: 2.7374 - classification_loss: 1.1818
 155/1000 [===>..........................] - ETA: 6:20 - loss: 3.9272 - regression_loss: 2.7448 - classification_loss: 1.1825
 156/1000 [===>..........................] - ETA: 6:20 - loss: 3.9349 - regression_loss: 2.7508 - classification_loss: 1.1841
 157/1000 [===>..........................] - ETA: 6:20 - loss: 3.9447 - regression_loss: 2.7605 - classification_loss: 1.1842
 158/1000 [===>..........................] - ETA: 6:19 - loss: 3.9484 - regression_loss: 2.7635 - classification_loss: 1.1849
 159/1000 [===>..........................] - ETA: 6:19 - loss: 3.9543 - regression_loss: 2.7665 - classification_loss: 1.1879
 160/1000 [===>..........................] - ETA: 6:18 - loss: 3.9583 - regression_loss: 2.7706 - classification_loss: 1.1878
 161/1000 [===>..........................] - ETA: 6:18 - loss: 3.9668 - regression_loss: 2.7785 - classification_loss: 1.1883
 162/1000 [===>..........................] - ETA: 6:17 - loss: 3.9744 - regression_loss: 2.7859 - classification_loss: 1.1884
 163/1000 [===>..........................] - ETA: 6:17 - loss: 3.9768 - regression_loss: 2.7884 - classification_loss: 1.1884
 164/1000 [===>..........................] - ETA: 6:16 - loss: 3.9525 - regression_loss: 2.7714 - classification_loss: 1.1811
 165/1000 [===>..........................] - ETA: 6:16 - loss: 3.9568 - regression_loss: 2.7758 - classification_loss: 1.1811
 166/1000 [===>..........................] - ETA: 6:16 - loss: 3.9614 - regression_loss: 2.7801 - classification_loss: 1.1813
 167/1000 [====>.........................] - ETA: 6:15 - loss: 3.9665 - regression_loss: 2.7845 - classification_loss: 1.1820
 168/1000 [====>.........................] - ETA: 6:15 - loss: 3.9429 - regression_loss: 2.7679 - classification_loss: 1.1750
 169/1000 [====>.........................] - ETA: 6:14 - loss: 3.9196 - regression_loss: 2.7516 - classification_loss: 1.1680
 170/1000 [====>.........................] - ETA: 6:14 - loss: 3.9251 - regression_loss: 2.7569 - classification_loss: 1.1682
 171/1000 [====>.........................] - ETA: 6:13 - loss: 3.9316 - regression_loss: 2.7628 - classification_loss: 1.1689
 172/1000 [====>.........................] - ETA: 6:13 - loss: 3.9378 - regression_loss: 2.7689 - classification_loss: 1.1689
 173/1000 [====>.........................] - ETA: 6:12 - loss: 3.9494 - regression_loss: 2.7791 - classification_loss: 1.1702
 174/1000 [====>.........................] - ETA: 6:12 - loss: 3.9513 - regression_loss: 2.7804 - classification_loss: 1.1709
 175/1000 [====>.........................] - ETA: 6:12 - loss: 3.9287 - regression_loss: 2.7645 - classification_loss: 1.1642
 176/1000 [====>.........................] - ETA: 6:11 - loss: 3.9332 - regression_loss: 2.7685 - classification_loss: 1.1647
 177/1000 [====>.........................] - ETA: 6:11 - loss: 3.9398 - regression_loss: 2.7743 - classification_loss: 1.1655
 178/1000 [====>.........................] - ETA: 6:10 - loss: 3.9177 - regression_loss: 2.7587 - classification_loss: 1.1589
 179/1000 [====>.........................] - ETA: 6:10 - loss: 3.9280 - regression_loss: 2.7678 - classification_loss: 1.1601
 180/1000 [====>.........................] - ETA: 6:09 - loss: 3.9356 - regression_loss: 2.7745 - classification_loss: 1.1611
 181/1000 [====>.........................] - ETA: 6:09 - loss: 3.9419 - regression_loss: 2.7781 - classification_loss: 1.1638
 182/1000 [====>.........................] - ETA: 6:09 - loss: 3.9479 - regression_loss: 2.7807 - classification_loss: 1.1672
 183/1000 [====>.........................] - ETA: 6:08 - loss: 3.9551 - regression_loss: 2.7869 - classification_loss: 1.1682
 184/1000 [====>.........................] - ETA: 6:08 - loss: 3.9627 - regression_loss: 2.7933 - classification_loss: 1.1694
 185/1000 [====>.........................] - ETA: 6:07 - loss: 3.9413 - regression_loss: 2.7782 - classification_loss: 1.1631
 186/1000 [====>.........................] - ETA: 6:07 - loss: 3.9471 - regression_loss: 2.7831 - classification_loss: 1.1640
 187/1000 [====>.........................] - ETA: 6:06 - loss: 3.9543 - regression_loss: 2.7891 - classification_loss: 1.1652
 188/1000 [====>.........................] - ETA: 6:06 - loss: 3.9611 - regression_loss: 2.7956 - classification_loss: 1.1655
 189/1000 [====>.........................] - ETA: 6:05 - loss: 3.9687 - regression_loss: 2.8012 - classification_loss: 1.1675
 190/1000 [====>.........................] - ETA: 6:05 - loss: 3.9478 - regression_loss: 2.7864 - classification_loss: 1.1613
 191/1000 [====>.........................] - ETA: 6:04 - loss: 3.9559 - regression_loss: 2.7921 - classification_loss: 1.1638
 192/1000 [====>.........................] - ETA: 6:04 - loss: 3.9642 - regression_loss: 2.7994 - classification_loss: 1.1648
 193/1000 [====>.........................] - ETA: 6:04 - loss: 3.9719 - regression_loss: 2.8068 - classification_loss: 1.1651
 194/1000 [====>.........................] - ETA: 6:03 - loss: 3.9751 - regression_loss: 2.8095 - classification_loss: 1.1656
 195/1000 [====>.........................] - ETA: 6:03 - loss: 3.9824 - regression_loss: 2.8155 - classification_loss: 1.1669
 196/1000 [====>.........................] - ETA: 6:02 - loss: 3.9865 - regression_loss: 2.8195 - classification_loss: 1.1670
 197/1000 [====>.........................] - ETA: 6:02 - loss: 3.9879 - regression_loss: 2.8207 - classification_loss: 1.1672
 198/1000 [====>.........................] - ETA: 6:01 - loss: 3.9918 - regression_loss: 2.8242 - classification_loss: 1.1676
 199/1000 [====>.........................] - ETA: 6:01 - loss: 3.9990 - regression_loss: 2.8309 - classification_loss: 1.1680
 200/1000 [=====>........................] - ETA: 6:00 - loss: 4.0090 - regression_loss: 2.8407 - classification_loss: 1.1683
 201/1000 [=====>........................] - ETA: 6:00 - loss: 3.9890 - regression_loss: 2.8265 - classification_loss: 1.1625
 202/1000 [=====>........................] - ETA: 6:00 - loss: 3.9693 - regression_loss: 2.8125 - classification_loss: 1.1567
 203/1000 [=====>........................] - ETA: 5:59 - loss: 3.9818 - regression_loss: 2.8241 - classification_loss: 1.1577
 204/1000 [=====>........................] - ETA: 5:59 - loss: 3.9867 - regression_loss: 2.8282 - classification_loss: 1.1585
 205/1000 [=====>........................] - ETA: 5:58 - loss: 4.0009 - regression_loss: 2.8363 - classification_loss: 1.1646
 206/1000 [=====>........................] - ETA: 5:58 - loss: 4.0052 - regression_loss: 2.8401 - classification_loss: 1.1652
 207/1000 [=====>........................] - ETA: 5:57 - loss: 3.9859 - regression_loss: 2.8263 - classification_loss: 1.1596
 208/1000 [=====>........................] - ETA: 5:57 - loss: 3.9893 - regression_loss: 2.8273 - classification_loss: 1.1620
 209/1000 [=====>........................] - ETA: 5:56 - loss: 3.9938 - regression_loss: 2.8301 - classification_loss: 1.1637
 210/1000 [=====>........................] - ETA: 5:56 - loss: 3.9975 - regression_loss: 2.8304 - classification_loss: 1.1671
 211/1000 [=====>........................] - ETA: 5:55 - loss: 4.0017 - regression_loss: 2.8341 - classification_loss: 1.1675
 212/1000 [=====>........................] - ETA: 5:55 - loss: 3.9828 - regression_loss: 2.8208 - classification_loss: 1.1620
 213/1000 [=====>........................] - ETA: 5:55 - loss: 3.9880 - regression_loss: 2.8241 - classification_loss: 1.1640
 214/1000 [=====>........................] - ETA: 5:54 - loss: 3.9694 - regression_loss: 2.8109 - classification_loss: 1.1586
 215/1000 [=====>........................] - ETA: 5:54 - loss: 3.9759 - regression_loss: 2.8157 - classification_loss: 1.1602
 216/1000 [=====>........................] - ETA: 5:53 - loss: 3.9575 - regression_loss: 2.8027 - classification_loss: 1.1548
 217/1000 [=====>........................] - ETA: 5:53 - loss: 3.9630 - regression_loss: 2.8080 - classification_loss: 1.1550
 218/1000 [=====>........................] - ETA: 5:52 - loss: 3.9677 - regression_loss: 2.8105 - classification_loss: 1.1572
 219/1000 [=====>........................] - ETA: 5:52 - loss: 3.9778 - regression_loss: 2.8174 - classification_loss: 1.1603
 220/1000 [=====>........................] - ETA: 5:52 - loss: 3.9811 - regression_loss: 2.8200 - classification_loss: 1.1611
 221/1000 [=====>........................] - ETA: 5:51 - loss: 3.9851 - regression_loss: 2.8226 - classification_loss: 1.1625
 222/1000 [=====>........................] - ETA: 5:51 - loss: 3.9672 - regression_loss: 2.8099 - classification_loss: 1.1573
 223/1000 [=====>........................] - ETA: 5:50 - loss: 3.9709 - regression_loss: 2.8133 - classification_loss: 1.1576
 224/1000 [=====>........................] - ETA: 5:50 - loss: 3.9782 - regression_loss: 2.8176 - classification_loss: 1.1606
 225/1000 [=====>........................] - ETA: 5:49 - loss: 3.9810 - regression_loss: 2.8202 - classification_loss: 1.1608
 226/1000 [=====>........................] - ETA: 5:49 - loss: 3.9854 - regression_loss: 2.8241 - classification_loss: 1.1613
 227/1000 [=====>........................] - ETA: 5:49 - loss: 3.9907 - regression_loss: 2.8271 - classification_loss: 1.1636
 228/1000 [=====>........................] - ETA: 5:48 - loss: 3.9942 - regression_loss: 2.8296 - classification_loss: 1.1647
 229/1000 [=====>........................] - ETA: 5:48 - loss: 3.9981 - regression_loss: 2.8334 - classification_loss: 1.1647
 230/1000 [=====>........................] - ETA: 5:47 - loss: 4.0024 - regression_loss: 2.8369 - classification_loss: 1.1654
 231/1000 [=====>........................] - ETA: 5:47 - loss: 4.0067 - regression_loss: 2.8403 - classification_loss: 1.1664
 232/1000 [=====>........................] - ETA: 5:46 - loss: 4.0263 - regression_loss: 2.8600 - classification_loss: 1.1664
 233/1000 [=====>........................] - ETA: 5:46 - loss: 4.0310 - regression_loss: 2.8636 - classification_loss: 1.1674
 234/1000 [======>.......................] - ETA: 5:45 - loss: 4.0138 - regression_loss: 2.8513 - classification_loss: 1.1625
 235/1000 [======>.......................] - ETA: 5:45 - loss: 3.9967 - regression_loss: 2.8392 - classification_loss: 1.1575
 236/1000 [======>.......................] - ETA: 5:44 - loss: 4.0010 - regression_loss: 2.8421 - classification_loss: 1.1589
 237/1000 [======>.......................] - ETA: 5:44 - loss: 4.0074 - regression_loss: 2.8479 - classification_loss: 1.1595
 238/1000 [======>.......................] - ETA: 5:43 - loss: 4.0137 - regression_loss: 2.8528 - classification_loss: 1.1608
 239/1000 [======>.......................] - ETA: 5:43 - loss: 4.0174 - regression_loss: 2.8562 - classification_loss: 1.1612
 240/1000 [======>.......................] - ETA: 5:43 - loss: 4.0207 - regression_loss: 2.8593 - classification_loss: 1.1613
 241/1000 [======>.......................] - ETA: 5:42 - loss: 4.0249 - regression_loss: 2.8628 - classification_loss: 1.1622
 242/1000 [======>.......................] - ETA: 5:42 - loss: 4.0083 - regression_loss: 2.8509 - classification_loss: 1.1574
 243/1000 [======>.......................] - ETA: 5:41 - loss: 4.0124 - regression_loss: 2.8548 - classification_loss: 1.1576
 244/1000 [======>.......................] - ETA: 5:41 - loss: 4.0177 - regression_loss: 2.8587 - classification_loss: 1.1590
 245/1000 [======>.......................] - ETA: 5:40 - loss: 4.0195 - regression_loss: 2.8600 - classification_loss: 1.1594
 246/1000 [======>.......................] - ETA: 5:40 - loss: 4.0234 - regression_loss: 2.8637 - classification_loss: 1.1597
 247/1000 [======>.......................] - ETA: 5:39 - loss: 4.0258 - regression_loss: 2.8660 - classification_loss: 1.1597
 248/1000 [======>.......................] - ETA: 5:39 - loss: 4.0312 - regression_loss: 2.8685 - classification_loss: 1.1627
 249/1000 [======>.......................] - ETA: 5:38 - loss: 4.0150 - regression_loss: 2.8569 - classification_loss: 1.1581
 250/1000 [======>.......................] - ETA: 5:38 - loss: 4.0209 - regression_loss: 2.8623 - classification_loss: 1.1586
 251/1000 [======>.......................] - ETA: 5:38 - loss: 4.0285 - regression_loss: 2.8696 - classification_loss: 1.1589
 252/1000 [======>.......................] - ETA: 5:37 - loss: 4.0309 - regression_loss: 2.8702 - classification_loss: 1.1607
 253/1000 [======>.......................] - ETA: 5:37 - loss: 4.0382 - regression_loss: 2.8730 - classification_loss: 1.1652
 254/1000 [======>.......................] - ETA: 5:36 - loss: 4.0223 - regression_loss: 2.8617 - classification_loss: 1.1606
 255/1000 [======>.......................] - ETA: 5:36 - loss: 4.0262 - regression_loss: 2.8637 - classification_loss: 1.1624
 256/1000 [======>.......................] - ETA: 5:35 - loss: 4.0283 - regression_loss: 2.8657 - classification_loss: 1.1626
 257/1000 [======>.......................] - ETA: 5:35 - loss: 4.0344 - regression_loss: 2.8696 - classification_loss: 1.1648
 258/1000 [======>.......................] - ETA: 5:34 - loss: 4.0351 - regression_loss: 2.8702 - classification_loss: 1.1649
 259/1000 [======>.......................] - ETA: 5:34 - loss: 4.0195 - regression_loss: 2.8591 - classification_loss: 1.1604
 260/1000 [======>.......................] - ETA: 5:34 - loss: 4.0283 - regression_loss: 2.8669 - classification_loss: 1.1614
 261/1000 [======>.......................] - ETA: 5:33 - loss: 4.0327 - regression_loss: 2.8710 - classification_loss: 1.1617
 262/1000 [======>.......................] - ETA: 5:33 - loss: 4.0395 - regression_loss: 2.8778 - classification_loss: 1.1618
 263/1000 [======>.......................] - ETA: 5:32 - loss: 4.0506 - regression_loss: 2.8856 - classification_loss: 1.1649
 264/1000 [======>.......................] - ETA: 5:32 - loss: 4.0552 - regression_loss: 2.8897 - classification_loss: 1.1655
 265/1000 [======>.......................] - ETA: 5:31 - loss: 4.0563 - regression_loss: 2.8908 - classification_loss: 1.1656
 266/1000 [======>.......................] - ETA: 5:31 - loss: 4.0411 - regression_loss: 2.8799 - classification_loss: 1.1612
 267/1000 [=======>......................] - ETA: 5:30 - loss: 4.0445 - regression_loss: 2.8829 - classification_loss: 1.1615
 268/1000 [=======>......................] - ETA: 5:30 - loss: 4.0294 - regression_loss: 2.8722 - classification_loss: 1.1572
 269/1000 [=======>......................] - ETA: 5:30 - loss: 4.0343 - regression_loss: 2.8757 - classification_loss: 1.1586
 270/1000 [=======>......................] - ETA: 5:29 - loss: 4.0380 - regression_loss: 2.8789 - classification_loss: 1.1591
 271/1000 [=======>......................] - ETA: 5:29 - loss: 4.0231 - regression_loss: 2.8683 - classification_loss: 1.1548
 272/1000 [=======>......................] - ETA: 5:28 - loss: 4.0280 - regression_loss: 2.8724 - classification_loss: 1.1556
 273/1000 [=======>......................] - ETA: 5:28 - loss: 4.0325 - regression_loss: 2.8758 - classification_loss: 1.1567
 274/1000 [=======>......................] - ETA: 5:27 - loss: 4.0378 - regression_loss: 2.8795 - classification_loss: 1.1583
 275/1000 [=======>......................] - ETA: 5:27 - loss: 4.0395 - regression_loss: 2.8811 - classification_loss: 1.1584
 276/1000 [=======>......................] - ETA: 5:26 - loss: 4.0429 - regression_loss: 2.8827 - classification_loss: 1.1602
 277/1000 [=======>......................] - ETA: 5:26 - loss: 4.0473 - regression_loss: 2.8871 - classification_loss: 1.1603
 278/1000 [=======>......................] - ETA: 5:26 - loss: 4.0521 - regression_loss: 2.8907 - classification_loss: 1.1613
 279/1000 [=======>......................] - ETA: 5:25 - loss: 4.0375 - regression_loss: 2.8803 - classification_loss: 1.1572
 280/1000 [=======>......................] - ETA: 5:25 - loss: 4.0395 - regression_loss: 2.8822 - classification_loss: 1.1573
 281/1000 [=======>......................] - ETA: 5:24 - loss: 4.0427 - regression_loss: 2.8844 - classification_loss: 1.1583
 282/1000 [=======>......................] - ETA: 5:24 - loss: 4.0284 - regression_loss: 2.8742 - classification_loss: 1.1542
 283/1000 [=======>......................] - ETA: 5:23 - loss: 4.0313 - regression_loss: 2.8770 - classification_loss: 1.1543
 284/1000 [=======>......................] - ETA: 5:23 - loss: 4.0171 - regression_loss: 2.8669 - classification_loss: 1.1503
 285/1000 [=======>......................] - ETA: 5:22 - loss: 4.0198 - regression_loss: 2.8685 - classification_loss: 1.1513
 286/1000 [=======>......................] - ETA: 5:22 - loss: 4.0213 - regression_loss: 2.8693 - classification_loss: 1.1520
 287/1000 [=======>......................] - ETA: 5:22 - loss: 4.0267 - regression_loss: 2.8740 - classification_loss: 1.1527
 288/1000 [=======>......................] - ETA: 5:21 - loss: 4.0314 - regression_loss: 2.8765 - classification_loss: 1.1550
 289/1000 [=======>......................] - ETA: 5:21 - loss: 4.0343 - regression_loss: 2.8790 - classification_loss: 1.1553
 290/1000 [=======>......................] - ETA: 5:20 - loss: 4.0417 - regression_loss: 2.8834 - classification_loss: 1.1583
 291/1000 [=======>......................] - ETA: 5:20 - loss: 4.0450 - regression_loss: 2.8858 - classification_loss: 1.1592
 292/1000 [=======>......................] - ETA: 5:19 - loss: 4.0473 - regression_loss: 2.8874 - classification_loss: 1.1599
 293/1000 [=======>......................] - ETA: 5:19 - loss: 4.0494 - regression_loss: 2.8882 - classification_loss: 1.1613
 294/1000 [=======>......................] - ETA: 5:18 - loss: 4.0506 - regression_loss: 2.8890 - classification_loss: 1.1616
 295/1000 [=======>......................] - ETA: 5:18 - loss: 4.0574 - regression_loss: 2.8940 - classification_loss: 1.1633
 296/1000 [=======>......................] - ETA: 5:18 - loss: 4.0606 - regression_loss: 2.8963 - classification_loss: 1.1642
 297/1000 [=======>......................] - ETA: 5:17 - loss: 4.0628 - regression_loss: 2.8983 - classification_loss: 1.1645
 298/1000 [=======>......................] - ETA: 5:17 - loss: 4.0491 - regression_loss: 2.8886 - classification_loss: 1.1606
 299/1000 [=======>......................] - ETA: 5:16 - loss: 4.0515 - regression_loss: 2.8907 - classification_loss: 1.1608
 300/1000 [========>.....................] - ETA: 5:16 - loss: 4.0553 - regression_loss: 2.8945 - classification_loss: 1.1608
 301/1000 [========>.....................] - ETA: 5:15 - loss: 4.0585 - regression_loss: 2.8977 - classification_loss: 1.1608
 302/1000 [========>.....................] - ETA: 5:15 - loss: 4.0609 - regression_loss: 2.8999 - classification_loss: 1.1610
 303/1000 [========>.....................] - ETA: 5:14 - loss: 4.0620 - regression_loss: 2.9007 - classification_loss: 1.1614
 304/1000 [========>.....................] - ETA: 5:14 - loss: 4.0631 - regression_loss: 2.9016 - classification_loss: 1.1615
 305/1000 [========>.....................] - ETA: 5:13 - loss: 4.0653 - regression_loss: 2.9036 - classification_loss: 1.1617
 306/1000 [========>.....................] - ETA: 5:13 - loss: 4.0679 - regression_loss: 2.9056 - classification_loss: 1.1623
 307/1000 [========>.....................] - ETA: 5:13 - loss: 4.0547 - regression_loss: 2.8961 - classification_loss: 1.1585
 308/1000 [========>.....................] - ETA: 5:12 - loss: 4.0415 - regression_loss: 2.8867 - classification_loss: 1.1548
 309/1000 [========>.....................] - ETA: 5:12 - loss: 4.0284 - regression_loss: 2.8774 - classification_loss: 1.1510
 310/1000 [========>.....................] - ETA: 5:11 - loss: 4.0300 - regression_loss: 2.8787 - classification_loss: 1.1512
 311/1000 [========>.....................] - ETA: 5:11 - loss: 4.0313 - regression_loss: 2.8792 - classification_loss: 1.1521
 312/1000 [========>.....................] - ETA: 5:10 - loss: 4.0337 - regression_loss: 2.8813 - classification_loss: 1.1523
 313/1000 [========>.....................] - ETA: 5:10 - loss: 4.0353 - regression_loss: 2.8826 - classification_loss: 1.1527
 314/1000 [========>.....................] - ETA: 5:09 - loss: 4.0225 - regression_loss: 2.8734 - classification_loss: 1.1490
 315/1000 [========>.....................] - ETA: 5:09 - loss: 4.0273 - regression_loss: 2.8780 - classification_loss: 1.1492
 316/1000 [========>.....................] - ETA: 5:08 - loss: 4.0298 - regression_loss: 2.8796 - classification_loss: 1.1502
 317/1000 [========>.....................] - ETA: 5:08 - loss: 4.0370 - regression_loss: 2.8847 - classification_loss: 1.1522
 318/1000 [========>.....................] - ETA: 5:07 - loss: 4.0410 - regression_loss: 2.8882 - classification_loss: 1.1527
 319/1000 [========>.....................] - ETA: 5:07 - loss: 4.0430 - regression_loss: 2.8902 - classification_loss: 1.1528
 320/1000 [========>.....................] - ETA: 5:07 - loss: 4.0304 - regression_loss: 2.8812 - classification_loss: 1.1492
 321/1000 [========>.....................] - ETA: 5:06 - loss: 4.0337 - regression_loss: 2.8845 - classification_loss: 1.1492
 322/1000 [========>.....................] - ETA: 5:06 - loss: 4.0370 - regression_loss: 2.8872 - classification_loss: 1.1498
 323/1000 [========>.....................] - ETA: 5:05 - loss: 4.0382 - regression_loss: 2.8876 - classification_loss: 1.1506
 324/1000 [========>.....................] - ETA: 5:05 - loss: 4.0257 - regression_loss: 2.8787 - classification_loss: 1.1470
 325/1000 [========>.....................] - ETA: 5:04 - loss: 4.0290 - regression_loss: 2.8815 - classification_loss: 1.1475
 326/1000 [========>.....................] - ETA: 5:04 - loss: 4.0348 - regression_loss: 2.8858 - classification_loss: 1.1490
 327/1000 [========>.....................] - ETA: 5:03 - loss: 4.0393 - regression_loss: 2.8894 - classification_loss: 1.1499
 328/1000 [========>.....................] - ETA: 5:03 - loss: 4.0271 - regression_loss: 2.8806 - classification_loss: 1.1464
 329/1000 [========>.....................] - ETA: 5:03 - loss: 4.0304 - regression_loss: 2.8833 - classification_loss: 1.1471
 330/1000 [========>.....................] - ETA: 5:02 - loss: 4.0356 - regression_loss: 2.8871 - classification_loss: 1.1485
 331/1000 [========>.....................] - ETA: 5:02 - loss: 4.0367 - regression_loss: 2.8875 - classification_loss: 1.1492
 332/1000 [========>.....................] - ETA: 5:01 - loss: 4.0439 - regression_loss: 2.8911 - classification_loss: 1.1528
 333/1000 [========>.....................] - ETA: 5:01 - loss: 4.0453 - regression_loss: 2.8904 - classification_loss: 1.1549
 334/1000 [=========>....................] - ETA: 5:00 - loss: 4.0484 - regression_loss: 2.8925 - classification_loss: 1.1559
 335/1000 [=========>....................] - ETA: 5:00 - loss: 4.0538 - regression_loss: 2.8965 - classification_loss: 1.1573
 336/1000 [=========>....................] - ETA: 4:59 - loss: 4.0558 - regression_loss: 2.8979 - classification_loss: 1.1579
 337/1000 [=========>....................] - ETA: 4:59 - loss: 4.0587 - regression_loss: 2.9006 - classification_loss: 1.1581
 338/1000 [=========>....................] - ETA: 4:59 - loss: 4.0467 - regression_loss: 2.8920 - classification_loss: 1.1547
 339/1000 [=========>....................] - ETA: 4:58 - loss: 4.0347 - regression_loss: 2.8835 - classification_loss: 1.1513
 340/1000 [=========>....................] - ETA: 4:58 - loss: 4.0229 - regression_loss: 2.8750 - classification_loss: 1.1479
 341/1000 [=========>....................] - ETA: 4:57 - loss: 4.0243 - regression_loss: 2.8763 - classification_loss: 1.1479
 342/1000 [=========>....................] - ETA: 4:57 - loss: 4.0271 - regression_loss: 2.8783 - classification_loss: 1.1488
 343/1000 [=========>....................] - ETA: 4:56 - loss: 4.0273 - regression_loss: 2.8784 - classification_loss: 1.1488
 344/1000 [=========>....................] - ETA: 4:56 - loss: 4.0305 - regression_loss: 2.8816 - classification_loss: 1.1489
 345/1000 [=========>....................] - ETA: 4:55 - loss: 4.0362 - regression_loss: 2.8870 - classification_loss: 1.1492
 346/1000 [=========>....................] - ETA: 4:55 - loss: 4.0377 - regression_loss: 2.8883 - classification_loss: 1.1493
 347/1000 [=========>....................] - ETA: 4:54 - loss: 4.0411 - regression_loss: 2.8914 - classification_loss: 1.1497
 348/1000 [=========>....................] - ETA: 4:54 - loss: 4.0444 - regression_loss: 2.8945 - classification_loss: 1.1499
 349/1000 [=========>....................] - ETA: 4:54 - loss: 4.0461 - regression_loss: 2.8961 - classification_loss: 1.1500
 350/1000 [=========>....................] - ETA: 4:53 - loss: 4.0473 - regression_loss: 2.8971 - classification_loss: 1.1502
 351/1000 [=========>....................] - ETA: 4:53 - loss: 4.0510 - regression_loss: 2.9002 - classification_loss: 1.1508
 352/1000 [=========>....................] - ETA: 4:52 - loss: 4.0395 - regression_loss: 2.8919 - classification_loss: 1.1475
 353/1000 [=========>....................] - ETA: 4:52 - loss: 4.0424 - regression_loss: 2.8948 - classification_loss: 1.1476
 354/1000 [=========>....................] - ETA: 4:51 - loss: 4.0449 - regression_loss: 2.8971 - classification_loss: 1.1478
 355/1000 [=========>....................] - ETA: 4:51 - loss: 4.0481 - regression_loss: 2.8990 - classification_loss: 1.1491
 356/1000 [=========>....................] - ETA: 4:50 - loss: 4.0368 - regression_loss: 2.8909 - classification_loss: 1.1459
 357/1000 [=========>....................] - ETA: 4:50 - loss: 4.0383 - regression_loss: 2.8920 - classification_loss: 1.1463
 358/1000 [=========>....................] - ETA: 4:50 - loss: 4.0432 - regression_loss: 2.8952 - classification_loss: 1.1479
 359/1000 [=========>....................] - ETA: 4:49 - loss: 4.0452 - regression_loss: 2.8970 - classification_loss: 1.1482
 360/1000 [=========>....................] - ETA: 4:49 - loss: 4.0484 - regression_loss: 2.9001 - classification_loss: 1.1483
 361/1000 [=========>....................] - ETA: 4:48 - loss: 4.0508 - regression_loss: 2.9018 - classification_loss: 1.1490
 362/1000 [=========>....................] - ETA: 4:48 - loss: 4.0542 - regression_loss: 2.9047 - classification_loss: 1.1495
 363/1000 [=========>....................] - ETA: 4:47 - loss: 4.0572 - regression_loss: 2.9066 - classification_loss: 1.1506
 364/1000 [=========>....................] - ETA: 4:47 - loss: 4.0461 - regression_loss: 2.8986 - classification_loss: 1.1475
 365/1000 [=========>....................] - ETA: 4:46 - loss: 4.0481 - regression_loss: 2.9006 - classification_loss: 1.1475
 366/1000 [=========>....................] - ETA: 4:46 - loss: 4.0513 - regression_loss: 2.9035 - classification_loss: 1.1478
 367/1000 [==========>...................] - ETA: 4:46 - loss: 4.0527 - regression_loss: 2.9049 - classification_loss: 1.1478
 368/1000 [==========>...................] - ETA: 4:45 - loss: 4.0417 - regression_loss: 2.8970 - classification_loss: 1.1447
 369/1000 [==========>...................] - ETA: 4:45 - loss: 4.0461 - regression_loss: 2.9005 - classification_loss: 1.1456
 370/1000 [==========>...................] - ETA: 4:44 - loss: 4.0540 - regression_loss: 2.9069 - classification_loss: 1.1471
 371/1000 [==========>...................] - ETA: 4:44 - loss: 4.0431 - regression_loss: 2.8990 - classification_loss: 1.1440
 372/1000 [==========>...................] - ETA: 4:43 - loss: 4.0491 - regression_loss: 2.9012 - classification_loss: 1.1479
 373/1000 [==========>...................] - ETA: 4:43 - loss: 4.0515 - regression_loss: 2.9031 - classification_loss: 1.1484
 374/1000 [==========>...................] - ETA: 4:42 - loss: 4.0406 - regression_loss: 2.8953 - classification_loss: 1.1453
 375/1000 [==========>...................] - ETA: 4:42 - loss: 4.0456 - regression_loss: 2.8984 - classification_loss: 1.1473
 376/1000 [==========>...................] - ETA: 4:42 - loss: 4.0472 - regression_loss: 2.8999 - classification_loss: 1.1473
 377/1000 [==========>...................] - ETA: 4:41 - loss: 4.0498 - regression_loss: 2.9016 - classification_loss: 1.1482
 378/1000 [==========>...................] - ETA: 4:41 - loss: 4.0391 - regression_loss: 2.8940 - classification_loss: 1.1452
 379/1000 [==========>...................] - ETA: 4:40 - loss: 4.0285 - regression_loss: 2.8863 - classification_loss: 1.1421
 380/1000 [==========>...................] - ETA: 4:40 - loss: 4.0329 - regression_loss: 2.8899 - classification_loss: 1.1430
 381/1000 [==========>...................] - ETA: 4:39 - loss: 4.0224 - regression_loss: 2.8824 - classification_loss: 1.1400
 382/1000 [==========>...................] - ETA: 4:39 - loss: 4.0270 - regression_loss: 2.8854 - classification_loss: 1.1416
 383/1000 [==========>...................] - ETA: 4:38 - loss: 4.0311 - regression_loss: 2.8891 - classification_loss: 1.1420
 384/1000 [==========>...................] - ETA: 4:38 - loss: 4.0206 - regression_loss: 2.8816 - classification_loss: 1.1391
 385/1000 [==========>...................] - ETA: 4:37 - loss: 4.0226 - regression_loss: 2.8834 - classification_loss: 1.1391
 386/1000 [==========>...................] - ETA: 4:37 - loss: 4.0267 - regression_loss: 2.8866 - classification_loss: 1.1400
 387/1000 [==========>...................] - ETA: 4:37 - loss: 4.0300 - regression_loss: 2.8895 - classification_loss: 1.1405
 388/1000 [==========>...................] - ETA: 4:36 - loss: 4.0196 - regression_loss: 2.8821 - classification_loss: 1.1375
 389/1000 [==========>...................] - ETA: 4:36 - loss: 4.0266 - regression_loss: 2.8848 - classification_loss: 1.1418
 390/1000 [==========>...................] - ETA: 4:35 - loss: 4.0305 - regression_loss: 2.8877 - classification_loss: 1.1428
 391/1000 [==========>...................] - ETA: 4:35 - loss: 4.0202 - regression_loss: 2.8803 - classification_loss: 1.1399
 392/1000 [==========>...................] - ETA: 4:34 - loss: 4.0265 - regression_loss: 2.8838 - classification_loss: 1.1427
 393/1000 [==========>...................] - ETA: 4:34 - loss: 4.0276 - regression_loss: 2.8849 - classification_loss: 1.1428
 394/1000 [==========>...................] - ETA: 4:33 - loss: 4.0174 - regression_loss: 2.8775 - classification_loss: 1.1399
 395/1000 [==========>...................] - ETA: 4:33 - loss: 4.0073 - regression_loss: 2.8702 - classification_loss: 1.1370
 396/1000 [==========>...................] - ETA: 4:32 - loss: 3.9972 - regression_loss: 2.8630 - classification_loss: 1.1342
 397/1000 [==========>...................] - ETA: 4:32 - loss: 4.0012 - regression_loss: 2.8623 - classification_loss: 1.1388
 398/1000 [==========>...................] - ETA: 4:32 - loss: 3.9911 - regression_loss: 2.8552 - classification_loss: 1.1360
 399/1000 [==========>...................] - ETA: 4:31 - loss: 3.9928 - regression_loss: 2.8566 - classification_loss: 1.1362
 400/1000 [===========>..................] - ETA: 4:31 - loss: 3.9946 - regression_loss: 2.8583 - classification_loss: 1.1363
 401/1000 [===========>..................] - ETA: 4:30 - loss: 3.9963 - regression_loss: 2.8597 - classification_loss: 1.1367
 402/1000 [===========>..................] - ETA: 4:30 - loss: 3.9997 - regression_loss: 2.8608 - classification_loss: 1.1389
 403/1000 [===========>..................] - ETA: 4:29 - loss: 4.0046 - regression_loss: 2.8635 - classification_loss: 1.1411
 404/1000 [===========>..................] - ETA: 4:29 - loss: 4.0115 - regression_loss: 2.8672 - classification_loss: 1.1442
 405/1000 [===========>..................] - ETA: 4:28 - loss: 4.0144 - regression_loss: 2.8683 - classification_loss: 1.1461
 406/1000 [===========>..................] - ETA: 4:28 - loss: 4.0045 - regression_loss: 2.8612 - classification_loss: 1.1433
 407/1000 [===========>..................] - ETA: 4:28 - loss: 4.0070 - regression_loss: 2.8621 - classification_loss: 1.1449
 408/1000 [===========>..................] - ETA: 4:27 - loss: 4.0088 - regression_loss: 2.8634 - classification_loss: 1.1454
 409/1000 [===========>..................] - ETA: 4:27 - loss: 4.0151 - regression_loss: 2.8666 - classification_loss: 1.1485
 410/1000 [===========>..................] - ETA: 4:26 - loss: 4.0180 - regression_loss: 2.8695 - classification_loss: 1.1486
 411/1000 [===========>..................] - ETA: 4:26 - loss: 4.0082 - regression_loss: 2.8625 - classification_loss: 1.1458
 412/1000 [===========>..................] - ETA: 4:25 - loss: 4.0108 - regression_loss: 2.8650 - classification_loss: 1.1458
 413/1000 [===========>..................] - ETA: 4:25 - loss: 4.0134 - regression_loss: 2.8666 - classification_loss: 1.1468
 414/1000 [===========>..................] - ETA: 4:24 - loss: 4.0150 - regression_loss: 2.8677 - classification_loss: 1.1472
 415/1000 [===========>..................] - ETA: 4:24 - loss: 4.0183 - regression_loss: 2.8708 - classification_loss: 1.1475
 416/1000 [===========>..................] - ETA: 4:23 - loss: 4.0284 - regression_loss: 2.8807 - classification_loss: 1.1478
 417/1000 [===========>..................] - ETA: 4:23 - loss: 4.0188 - regression_loss: 2.8738 - classification_loss: 1.1450
 418/1000 [===========>..................] - ETA: 4:23 - loss: 4.0218 - regression_loss: 2.8752 - classification_loss: 1.1465
 419/1000 [===========>..................] - ETA: 4:22 - loss: 4.0122 - regression_loss: 2.8684 - classification_loss: 1.1438
 420/1000 [===========>..................] - ETA: 4:22 - loss: 4.0141 - regression_loss: 2.8690 - classification_loss: 1.1451
 421/1000 [===========>..................] - ETA: 4:21 - loss: 4.0174 - regression_loss: 2.8717 - classification_loss: 1.1458
 422/1000 [===========>..................] - ETA: 4:21 - loss: 4.0189 - regression_loss: 2.8730 - classification_loss: 1.1458
 423/1000 [===========>..................] - ETA: 4:20 - loss: 4.0094 - regression_loss: 2.8662 - classification_loss: 1.1431
 424/1000 [===========>..................] - ETA: 4:20 - loss: 4.0121 - regression_loss: 2.8690 - classification_loss: 1.1431
 425/1000 [===========>..................] - ETA: 4:19 - loss: 4.0148 - regression_loss: 2.8717 - classification_loss: 1.1431
 426/1000 [===========>..................] - ETA: 4:19 - loss: 4.0171 - regression_loss: 2.8728 - classification_loss: 1.1443
 427/1000 [===========>..................] - ETA: 4:18 - loss: 4.0201 - regression_loss: 2.8757 - classification_loss: 1.1444
 428/1000 [===========>..................] - ETA: 4:18 - loss: 4.0227 - regression_loss: 2.8781 - classification_loss: 1.1446
 429/1000 [===========>..................] - ETA: 4:18 - loss: 4.0255 - regression_loss: 2.8798 - classification_loss: 1.1457
 430/1000 [===========>..................] - ETA: 4:17 - loss: 4.0299 - regression_loss: 2.8829 - classification_loss: 1.1470
 431/1000 [===========>..................] - ETA: 4:17 - loss: 4.0316 - regression_loss: 2.8841 - classification_loss: 1.1475
 432/1000 [===========>..................] - ETA: 4:16 - loss: 4.0361 - regression_loss: 2.8869 - classification_loss: 1.1492
 433/1000 [===========>..................] - ETA: 4:16 - loss: 4.0374 - regression_loss: 2.8881 - classification_loss: 1.1493
 434/1000 [============>.................] - ETA: 4:15 - loss: 4.0281 - regression_loss: 2.8815 - classification_loss: 1.1466
 435/1000 [============>.................] - ETA: 4:15 - loss: 4.0303 - regression_loss: 2.8834 - classification_loss: 1.1469
 436/1000 [============>.................] - ETA: 4:14 - loss: 4.0340 - regression_loss: 2.8869 - classification_loss: 1.1471
 437/1000 [============>.................] - ETA: 4:14 - loss: 4.0361 - regression_loss: 2.8885 - classification_loss: 1.1476
 438/1000 [============>.................] - ETA: 4:13 - loss: 4.0381 - regression_loss: 2.8895 - classification_loss: 1.1486
 439/1000 [============>.................] - ETA: 4:13 - loss: 4.0405 - regression_loss: 2.8912 - classification_loss: 1.1493
 440/1000 [============>.................] - ETA: 4:13 - loss: 4.0424 - regression_loss: 2.8929 - classification_loss: 1.1495
 441/1000 [============>.................] - ETA: 4:12 - loss: 4.0440 - regression_loss: 2.8936 - classification_loss: 1.1503
 442/1000 [============>.................] - ETA: 4:12 - loss: 4.0479 - regression_loss: 2.8976 - classification_loss: 1.1503
 443/1000 [============>.................] - ETA: 4:11 - loss: 4.0498 - regression_loss: 2.8994 - classification_loss: 1.1504
 444/1000 [============>.................] - ETA: 4:11 - loss: 4.0518 - regression_loss: 2.9007 - classification_loss: 1.1511
 445/1000 [============>.................] - ETA: 4:10 - loss: 4.0538 - regression_loss: 2.9022 - classification_loss: 1.1516
 446/1000 [============>.................] - ETA: 4:10 - loss: 4.0562 - regression_loss: 2.9041 - classification_loss: 1.1522
 447/1000 [============>.................] - ETA: 4:09 - loss: 4.0595 - regression_loss: 2.9071 - classification_loss: 1.1524
 448/1000 [============>.................] - ETA: 4:09 - loss: 4.0618 - regression_loss: 2.9094 - classification_loss: 1.1524
 449/1000 [============>.................] - ETA: 4:09 - loss: 4.0528 - regression_loss: 2.9029 - classification_loss: 1.1499
 450/1000 [============>.................] - ETA: 4:08 - loss: 4.0539 - regression_loss: 2.9040 - classification_loss: 1.1499
 451/1000 [============>.................] - ETA: 4:08 - loss: 4.0546 - regression_loss: 2.9042 - classification_loss: 1.1504
 452/1000 [============>.................] - ETA: 4:07 - loss: 4.0457 - regression_loss: 2.8978 - classification_loss: 1.1479
 453/1000 [============>.................] - ETA: 4:07 - loss: 4.0367 - regression_loss: 2.8914 - classification_loss: 1.1454
 454/1000 [============>.................] - ETA: 4:06 - loss: 4.0279 - regression_loss: 2.8850 - classification_loss: 1.1429
 455/1000 [============>.................] - ETA: 4:06 - loss: 4.0298 - regression_loss: 2.8868 - classification_loss: 1.1430
 456/1000 [============>.................] - ETA: 4:05 - loss: 4.0210 - regression_loss: 2.8805 - classification_loss: 1.1405
 457/1000 [============>.................] - ETA: 4:05 - loss: 4.0122 - regression_loss: 2.8742 - classification_loss: 1.1380
 458/1000 [============>.................] - ETA: 4:04 - loss: 4.0127 - regression_loss: 2.8746 - classification_loss: 1.1380
 459/1000 [============>.................] - ETA: 4:04 - loss: 4.0150 - regression_loss: 2.8766 - classification_loss: 1.1384
 460/1000 [============>.................] - ETA: 4:04 - loss: 4.0169 - regression_loss: 2.8781 - classification_loss: 1.1388
 461/1000 [============>.................] - ETA: 4:03 - loss: 4.0082 - regression_loss: 2.8719 - classification_loss: 1.1363
 462/1000 [============>.................] - ETA: 4:03 - loss: 3.9995 - regression_loss: 2.8656 - classification_loss: 1.1338
 463/1000 [============>.................] - ETA: 4:02 - loss: 4.0005 - regression_loss: 2.8665 - classification_loss: 1.1340
 464/1000 [============>.................] - ETA: 4:02 - loss: 4.0031 - regression_loss: 2.8686 - classification_loss: 1.1345
 465/1000 [============>.................] - ETA: 4:01 - loss: 4.0038 - regression_loss: 2.8683 - classification_loss: 1.1354
 466/1000 [============>.................] - ETA: 4:01 - loss: 4.0049 - regression_loss: 2.8693 - classification_loss: 1.1356
 467/1000 [=============>................] - ETA: 4:00 - loss: 4.0066 - regression_loss: 2.8706 - classification_loss: 1.1360
 468/1000 [=============>................] - ETA: 4:00 - loss: 3.9980 - regression_loss: 2.8644 - classification_loss: 1.1336
 469/1000 [=============>................] - ETA: 4:00 - loss: 4.0032 - regression_loss: 2.8683 - classification_loss: 1.1349
 470/1000 [=============>................] - ETA: 3:59 - loss: 4.0054 - regression_loss: 2.8700 - classification_loss: 1.1354
 471/1000 [=============>................] - ETA: 3:59 - loss: 3.9969 - regression_loss: 2.8639 - classification_loss: 1.1329
 472/1000 [=============>................] - ETA: 3:58 - loss: 3.9995 - regression_loss: 2.8662 - classification_loss: 1.1333
 473/1000 [=============>................] - ETA: 3:58 - loss: 3.9910 - regression_loss: 2.8602 - classification_loss: 1.1309
 474/1000 [=============>................] - ETA: 3:57 - loss: 3.9826 - regression_loss: 2.8541 - classification_loss: 1.1285
 475/1000 [=============>................] - ETA: 3:57 - loss: 3.9832 - regression_loss: 2.8545 - classification_loss: 1.1288
 476/1000 [=============>................] - ETA: 3:56 - loss: 3.9856 - regression_loss: 2.8559 - classification_loss: 1.1297
 477/1000 [=============>................] - ETA: 3:56 - loss: 3.9888 - regression_loss: 2.8583 - classification_loss: 1.1306
 478/1000 [=============>................] - ETA: 3:55 - loss: 3.9908 - regression_loss: 2.8598 - classification_loss: 1.1310
 479/1000 [=============>................] - ETA: 3:55 - loss: 3.9971 - regression_loss: 2.8644 - classification_loss: 1.1327
 480/1000 [=============>................] - ETA: 3:55 - loss: 3.9888 - regression_loss: 2.8585 - classification_loss: 1.1303
 481/1000 [=============>................] - ETA: 3:54 - loss: 3.9918 - regression_loss: 2.8604 - classification_loss: 1.1314
 482/1000 [=============>................] - ETA: 3:54 - loss: 3.9954 - regression_loss: 2.8623 - classification_loss: 1.1332
 483/1000 [=============>................] - ETA: 3:53 - loss: 3.9991 - regression_loss: 2.8642 - classification_loss: 1.1349
 484/1000 [=============>................] - ETA: 3:53 - loss: 4.0009 - regression_loss: 2.8655 - classification_loss: 1.1354
 485/1000 [=============>................] - ETA: 3:52 - loss: 4.0041 - regression_loss: 2.8668 - classification_loss: 1.1372
 486/1000 [=============>................] - ETA: 3:52 - loss: 4.0064 - regression_loss: 2.8687 - classification_loss: 1.1377
 487/1000 [=============>................] - ETA: 3:51 - loss: 3.9982 - regression_loss: 2.8628 - classification_loss: 1.1354
 488/1000 [=============>................] - ETA: 3:51 - loss: 4.0028 - regression_loss: 2.8655 - classification_loss: 1.1374
 489/1000 [=============>................] - ETA: 3:50 - loss: 4.0055 - regression_loss: 2.8670 - classification_loss: 1.1385
 490/1000 [=============>................] - ETA: 3:50 - loss: 4.0099 - regression_loss: 2.8711 - classification_loss: 1.1388
 491/1000 [=============>................] - ETA: 3:50 - loss: 4.0114 - regression_loss: 2.8725 - classification_loss: 1.1388
 492/1000 [=============>................] - ETA: 3:49 - loss: 4.0149 - regression_loss: 2.8756 - classification_loss: 1.1393
 493/1000 [=============>................] - ETA: 3:49 - loss: 4.0068 - regression_loss: 2.8698 - classification_loss: 1.1370
 494/1000 [=============>................] - ETA: 3:48 - loss: 4.0088 - regression_loss: 2.8717 - classification_loss: 1.1371
 495/1000 [=============>................] - ETA: 3:48 - loss: 4.0101 - regression_loss: 2.8729 - classification_loss: 1.1371
 496/1000 [=============>................] - ETA: 3:47 - loss: 4.0140 - regression_loss: 2.8755 - classification_loss: 1.1385
 497/1000 [=============>................] - ETA: 3:47 - loss: 4.0195 - regression_loss: 2.8783 - classification_loss: 1.1412
 498/1000 [=============>................] - ETA: 3:46 - loss: 4.0211 - regression_loss: 2.8793 - classification_loss: 1.1418
 499/1000 [=============>................] - ETA: 3:46 - loss: 4.0225 - regression_loss: 2.8803 - classification_loss: 1.1422
 500/1000 [==============>...............] - ETA: 3:46 - loss: 4.0246 - regression_loss: 2.8822 - classification_loss: 1.1425
 501/1000 [==============>...............] - ETA: 3:45 - loss: 4.0266 - regression_loss: 2.8839 - classification_loss: 1.1427
 502/1000 [==============>...............] - ETA: 3:45 - loss: 4.0275 - regression_loss: 2.8848 - classification_loss: 1.1427
 503/1000 [==============>...............] - ETA: 3:44 - loss: 4.0294 - regression_loss: 2.8867 - classification_loss: 1.1428
 504/1000 [==============>...............] - ETA: 3:44 - loss: 4.0317 - regression_loss: 2.8889 - classification_loss: 1.1428
 505/1000 [==============>...............] - ETA: 3:43 - loss: 4.0332 - regression_loss: 2.8898 - classification_loss: 1.1434
 506/1000 [==============>...............] - ETA: 3:43 - loss: 4.0343 - regression_loss: 2.8902 - classification_loss: 1.1441
 507/1000 [==============>...............] - ETA: 3:42 - loss: 4.0264 - regression_loss: 2.8845 - classification_loss: 1.1418
 508/1000 [==============>...............] - ETA: 3:42 - loss: 4.0276 - regression_loss: 2.8857 - classification_loss: 1.1419
 509/1000 [==============>...............] - ETA: 3:41 - loss: 4.0291 - regression_loss: 2.8869 - classification_loss: 1.1422
 510/1000 [==============>...............] - ETA: 3:41 - loss: 4.0305 - regression_loss: 2.8881 - classification_loss: 1.1425
 511/1000 [==============>...............] - ETA: 3:41 - loss: 4.0321 - regression_loss: 2.8896 - classification_loss: 1.1425
 512/1000 [==============>...............] - ETA: 3:40 - loss: 4.0342 - regression_loss: 2.8916 - classification_loss: 1.1426
 513/1000 [==============>...............] - ETA: 3:40 - loss: 4.0355 - regression_loss: 2.8927 - classification_loss: 1.1428
 514/1000 [==============>...............] - ETA: 3:39 - loss: 4.0374 - regression_loss: 2.8946 - classification_loss: 1.1428
 515/1000 [==============>...............] - ETA: 3:39 - loss: 4.0392 - regression_loss: 2.8959 - classification_loss: 1.1432
 516/1000 [==============>...............] - ETA: 3:38 - loss: 4.0408 - regression_loss: 2.8972 - classification_loss: 1.1435
 517/1000 [==============>...............] - ETA: 3:38 - loss: 4.0444 - regression_loss: 2.9007 - classification_loss: 1.1437
 518/1000 [==============>...............] - ETA: 3:37 - loss: 4.0471 - regression_loss: 2.9032 - classification_loss: 1.1439
 519/1000 [==============>...............] - ETA: 3:37 - loss: 4.0499 - regression_loss: 2.9059 - classification_loss: 1.1439
 520/1000 [==============>...............] - ETA: 3:36 - loss: 4.0509 - regression_loss: 2.9067 - classification_loss: 1.1442
 521/1000 [==============>...............] - ETA: 3:36 - loss: 4.0519 - regression_loss: 2.9076 - classification_loss: 1.1443
 522/1000 [==============>...............] - ETA: 3:36 - loss: 4.0531 - regression_loss: 2.9087 - classification_loss: 1.1444
 523/1000 [==============>...............] - ETA: 3:35 - loss: 4.0454 - regression_loss: 2.9031 - classification_loss: 1.1423
 524/1000 [==============>...............] - ETA: 3:35 - loss: 4.0463 - regression_loss: 2.9040 - classification_loss: 1.1423
 525/1000 [==============>...............] - ETA: 3:34 - loss: 4.0386 - regression_loss: 2.8985 - classification_loss: 1.1401
 526/1000 [==============>...............] - ETA: 3:34 - loss: 4.0309 - regression_loss: 2.8930 - classification_loss: 1.1379
 527/1000 [==============>...............] - ETA: 3:33 - loss: 4.0359 - regression_loss: 2.8972 - classification_loss: 1.1387
 528/1000 [==============>...............] - ETA: 3:33 - loss: 4.0373 - regression_loss: 2.8983 - classification_loss: 1.1391
 529/1000 [==============>...............] - ETA: 3:32 - loss: 4.0383 - regression_loss: 2.8990 - classification_loss: 1.1394
 530/1000 [==============>...............] - ETA: 3:32 - loss: 4.0441 - regression_loss: 2.9041 - classification_loss: 1.1400
 531/1000 [==============>...............] - ETA: 3:32 - loss: 4.0365 - regression_loss: 2.8986 - classification_loss: 1.1378
 532/1000 [==============>...............] - ETA: 3:31 - loss: 4.0289 - regression_loss: 2.8932 - classification_loss: 1.1357
 533/1000 [==============>...............] - ETA: 3:31 - loss: 4.0298 - regression_loss: 2.8938 - classification_loss: 1.1360
 534/1000 [===============>..............] - ETA: 3:30 - loss: 4.0303 - regression_loss: 2.8942 - classification_loss: 1.1360
 535/1000 [===============>..............] - ETA: 3:30 - loss: 4.0315 - regression_loss: 2.8954 - classification_loss: 1.1361
 536/1000 [===============>..............] - ETA: 3:29 - loss: 4.0351 - regression_loss: 2.8986 - classification_loss: 1.1365
 537/1000 [===============>..............] - ETA: 3:29 - loss: 4.0276 - regression_loss: 2.8932 - classification_loss: 1.1343
 538/1000 [===============>..............] - ETA: 3:28 - loss: 4.0287 - regression_loss: 2.8941 - classification_loss: 1.1345
 539/1000 [===============>..............] - ETA: 3:28 - loss: 4.0310 - regression_loss: 2.8963 - classification_loss: 1.1346
 540/1000 [===============>..............] - ETA: 3:27 - loss: 4.0333 - regression_loss: 2.8985 - classification_loss: 1.1348
 541/1000 [===============>..............] - ETA: 3:27 - loss: 4.0346 - regression_loss: 2.8997 - classification_loss: 1.1349
 542/1000 [===============>..............] - ETA: 3:27 - loss: 4.0359 - regression_loss: 2.9007 - classification_loss: 1.1352
 543/1000 [===============>..............] - ETA: 3:26 - loss: 4.0284 - regression_loss: 2.8954 - classification_loss: 1.1331
 544/1000 [===============>..............] - ETA: 3:26 - loss: 4.0301 - regression_loss: 2.8968 - classification_loss: 1.1333
 545/1000 [===============>..............] - ETA: 3:25 - loss: 4.0325 - regression_loss: 2.8985 - classification_loss: 1.1340
 546/1000 [===============>..............] - ETA: 3:25 - loss: 4.0251 - regression_loss: 2.8932 - classification_loss: 1.1319
 547/1000 [===============>..............] - ETA: 3:24 - loss: 4.0266 - regression_loss: 2.8943 - classification_loss: 1.1323
 548/1000 [===============>..............] - ETA: 3:24 - loss: 4.0296 - regression_loss: 2.8963 - classification_loss: 1.1333
 549/1000 [===============>..............] - ETA: 3:23 - loss: 4.0321 - regression_loss: 2.8977 - classification_loss: 1.1344
 550/1000 [===============>..............] - ETA: 3:23 - loss: 4.0342 - regression_loss: 2.8996 - classification_loss: 1.1346
 551/1000 [===============>..............] - ETA: 3:22 - loss: 4.0377 - regression_loss: 2.9024 - classification_loss: 1.1353
 552/1000 [===============>..............] - ETA: 3:22 - loss: 4.0415 - regression_loss: 2.9038 - classification_loss: 1.1377
 553/1000 [===============>..............] - ETA: 3:22 - loss: 4.0434 - regression_loss: 2.9057 - classification_loss: 1.1377
 554/1000 [===============>..............] - ETA: 3:21 - loss: 4.0470 - regression_loss: 2.9086 - classification_loss: 1.1385
 555/1000 [===============>..............] - ETA: 3:21 - loss: 4.0493 - regression_loss: 2.9106 - classification_loss: 1.1387
 556/1000 [===============>..............] - ETA: 3:20 - loss: 4.0504 - regression_loss: 2.9116 - classification_loss: 1.1388
 557/1000 [===============>..............] - ETA: 3:20 - loss: 4.0525 - regression_loss: 2.9130 - classification_loss: 1.1394
 558/1000 [===============>..............] - ETA: 3:19 - loss: 4.0544 - regression_loss: 2.9145 - classification_loss: 1.1399
 559/1000 [===============>..............] - ETA: 3:19 - loss: 4.0568 - regression_loss: 2.9164 - classification_loss: 1.1405
 560/1000 [===============>..............] - ETA: 3:18 - loss: 4.0496 - regression_loss: 2.9112 - classification_loss: 1.1384
 561/1000 [===============>..............] - ETA: 3:18 - loss: 4.0424 - regression_loss: 2.9060 - classification_loss: 1.1364
 562/1000 [===============>..............] - ETA: 3:18 - loss: 4.0442 - regression_loss: 2.9077 - classification_loss: 1.1365
 563/1000 [===============>..............] - ETA: 3:17 - loss: 4.0471 - regression_loss: 2.9106 - classification_loss: 1.1366
 564/1000 [===============>..............] - ETA: 3:17 - loss: 4.0480 - regression_loss: 2.9111 - classification_loss: 1.1369
 565/1000 [===============>..............] - ETA: 3:16 - loss: 4.0503 - regression_loss: 2.9130 - classification_loss: 1.1373
 566/1000 [===============>..............] - ETA: 3:16 - loss: 4.0521 - regression_loss: 2.9145 - classification_loss: 1.1376
 567/1000 [================>.............] - ETA: 3:15 - loss: 4.0449 - regression_loss: 2.9094 - classification_loss: 1.1356
 568/1000 [================>.............] - ETA: 3:15 - loss: 4.0459 - regression_loss: 2.9102 - classification_loss: 1.1356
 569/1000 [================>.............] - ETA: 3:14 - loss: 4.0502 - regression_loss: 2.9142 - classification_loss: 1.1360
 570/1000 [================>.............] - ETA: 3:14 - loss: 4.0531 - regression_loss: 2.9160 - classification_loss: 1.1371
 571/1000 [================>.............] - ETA: 3:13 - loss: 4.0544 - regression_loss: 2.9172 - classification_loss: 1.1372
 572/1000 [================>.............] - ETA: 3:13 - loss: 4.0559 - regression_loss: 2.9185 - classification_loss: 1.1374
 573/1000 [================>.............] - ETA: 3:13 - loss: 4.0584 - regression_loss: 2.9206 - classification_loss: 1.1378
 574/1000 [================>.............] - ETA: 3:12 - loss: 4.0616 - regression_loss: 2.9237 - classification_loss: 1.1379
 575/1000 [================>.............] - ETA: 3:12 - loss: 4.0640 - regression_loss: 2.9260 - classification_loss: 1.1379
 576/1000 [================>.............] - ETA: 3:11 - loss: 4.0652 - regression_loss: 2.9267 - classification_loss: 1.1385
 577/1000 [================>.............] - ETA: 3:11 - loss: 4.0669 - regression_loss: 2.9283 - classification_loss: 1.1386
 578/1000 [================>.............] - ETA: 3:10 - loss: 4.0691 - regression_loss: 2.9288 - classification_loss: 1.1403
 579/1000 [================>.............] - ETA: 3:10 - loss: 4.0703 - regression_loss: 2.9294 - classification_loss: 1.1409
 580/1000 [================>.............] - ETA: 3:09 - loss: 4.0723 - regression_loss: 2.9312 - classification_loss: 1.1411
 581/1000 [================>.............] - ETA: 3:09 - loss: 4.0653 - regression_loss: 2.9261 - classification_loss: 1.1392
 582/1000 [================>.............] - ETA: 3:08 - loss: 4.0666 - regression_loss: 2.9273 - classification_loss: 1.1393
 583/1000 [================>.............] - ETA: 3:08 - loss: 4.0683 - regression_loss: 2.9290 - classification_loss: 1.1393
 584/1000 [================>.............] - ETA: 3:08 - loss: 4.0700 - regression_loss: 2.9307 - classification_loss: 1.1393
 585/1000 [================>.............] - ETA: 3:07 - loss: 4.0712 - regression_loss: 2.9313 - classification_loss: 1.1398
 586/1000 [================>.............] - ETA: 3:07 - loss: 4.0742 - regression_loss: 2.9331 - classification_loss: 1.1410
 587/1000 [================>.............] - ETA: 3:06 - loss: 4.0774 - regression_loss: 2.9358 - classification_loss: 1.1416
 588/1000 [================>.............] - ETA: 3:06 - loss: 4.0790 - regression_loss: 2.9370 - classification_loss: 1.1420
 589/1000 [================>.............] - ETA: 3:05 - loss: 4.0804 - regression_loss: 2.9384 - classification_loss: 1.1420
 590/1000 [================>.............] - ETA: 3:05 - loss: 4.0813 - regression_loss: 2.9392 - classification_loss: 1.1422
 591/1000 [================>.............] - ETA: 3:04 - loss: 4.0831 - regression_loss: 2.9409 - classification_loss: 1.1422
 592/1000 [================>.............] - ETA: 3:04 - loss: 4.0841 - regression_loss: 2.9419 - classification_loss: 1.1422
 593/1000 [================>.............] - ETA: 3:03 - loss: 4.0773 - regression_loss: 2.9369 - classification_loss: 1.1403
 594/1000 [================>.............] - ETA: 3:03 - loss: 4.0789 - regression_loss: 2.9385 - classification_loss: 1.1404
 595/1000 [================>.............] - ETA: 3:03 - loss: 4.0811 - regression_loss: 2.9400 - classification_loss: 1.1410
 596/1000 [================>.............] - ETA: 3:02 - loss: 4.0818 - regression_loss: 2.9406 - classification_loss: 1.1412
 597/1000 [================>.............] - ETA: 3:02 - loss: 4.0828 - regression_loss: 2.9412 - classification_loss: 1.1416
 598/1000 [================>.............] - ETA: 3:01 - loss: 4.0839 - regression_loss: 2.9422 - classification_loss: 1.1417
 599/1000 [================>.............] - ETA: 3:01 - loss: 4.0849 - regression_loss: 2.9432 - classification_loss: 1.1417
 600/1000 [=================>............] - ETA: 3:00 - loss: 4.0868 - regression_loss: 2.9450 - classification_loss: 1.1418
 601/1000 [=================>............] - ETA: 3:00 - loss: 4.0906 - regression_loss: 2.9488 - classification_loss: 1.1418
 602/1000 [=================>............] - ETA: 2:59 - loss: 4.0838 - regression_loss: 2.9439 - classification_loss: 1.1399
 603/1000 [=================>............] - ETA: 2:59 - loss: 4.0844 - regression_loss: 2.9440 - classification_loss: 1.1404
 604/1000 [=================>............] - ETA: 2:58 - loss: 4.0852 - regression_loss: 2.9448 - classification_loss: 1.1404
 605/1000 [=================>............] - ETA: 2:58 - loss: 4.0876 - regression_loss: 2.9469 - classification_loss: 1.1407
 606/1000 [=================>............] - ETA: 2:58 - loss: 4.0886 - regression_loss: 2.9477 - classification_loss: 1.1409
 607/1000 [=================>............] - ETA: 2:57 - loss: 4.0891 - regression_loss: 2.9481 - classification_loss: 1.1410
 608/1000 [=================>............] - ETA: 2:57 - loss: 4.0824 - regression_loss: 2.9433 - classification_loss: 1.1391
 609/1000 [=================>............] - ETA: 2:56 - loss: 4.0757 - regression_loss: 2.9385 - classification_loss: 1.1373
 610/1000 [=================>............] - ETA: 2:56 - loss: 4.0691 - regression_loss: 2.9336 - classification_loss: 1.1354
 611/1000 [=================>............] - ETA: 2:55 - loss: 4.0691 - regression_loss: 2.9334 - classification_loss: 1.1356
 612/1000 [=================>............] - ETA: 2:55 - loss: 4.0711 - regression_loss: 2.9347 - classification_loss: 1.1364
 613/1000 [=================>............] - ETA: 2:54 - loss: 4.0730 - regression_loss: 2.9364 - classification_loss: 1.1366
 614/1000 [=================>............] - ETA: 2:54 - loss: 4.0744 - regression_loss: 2.9377 - classification_loss: 1.1367
 615/1000 [=================>............] - ETA: 2:54 - loss: 4.0752 - regression_loss: 2.9384 - classification_loss: 1.1368
 616/1000 [=================>............] - ETA: 2:53 - loss: 4.0785 - regression_loss: 2.9409 - classification_loss: 1.1376
 617/1000 [=================>............] - ETA: 2:53 - loss: 4.0800 - regression_loss: 2.9421 - classification_loss: 1.1379
 618/1000 [=================>............] - ETA: 2:52 - loss: 4.0814 - regression_loss: 2.9434 - classification_loss: 1.1380
 619/1000 [=================>............] - ETA: 2:52 - loss: 4.0824 - regression_loss: 2.9438 - classification_loss: 1.1386
 620/1000 [=================>............] - ETA: 2:51 - loss: 4.0832 - regression_loss: 2.9443 - classification_loss: 1.1389
 621/1000 [=================>............] - ETA: 2:51 - loss: 4.0855 - regression_loss: 2.9461 - classification_loss: 1.1394
 622/1000 [=================>............] - ETA: 2:50 - loss: 4.0860 - regression_loss: 2.9465 - classification_loss: 1.1395
 623/1000 [=================>............] - ETA: 2:50 - loss: 4.0871 - regression_loss: 2.9476 - classification_loss: 1.1395
 624/1000 [=================>............] - ETA: 2:49 - loss: 4.0885 - regression_loss: 2.9484 - classification_loss: 1.1400
 625/1000 [=================>............] - ETA: 2:49 - loss: 4.0819 - regression_loss: 2.9437 - classification_loss: 1.1382
 626/1000 [=================>............] - ETA: 2:49 - loss: 4.0827 - regression_loss: 2.9427 - classification_loss: 1.1400
 627/1000 [=================>............] - ETA: 2:48 - loss: 4.0841 - regression_loss: 2.9438 - classification_loss: 1.1403
 628/1000 [=================>............] - ETA: 2:48 - loss: 4.0861 - regression_loss: 2.9457 - classification_loss: 1.1404
 629/1000 [=================>............] - ETA: 2:47 - loss: 4.0869 - regression_loss: 2.9464 - classification_loss: 1.1405
 630/1000 [=================>............] - ETA: 2:47 - loss: 4.0875 - regression_loss: 2.9468 - classification_loss: 1.1408
 631/1000 [=================>............] - ETA: 2:46 - loss: 4.0889 - regression_loss: 2.9481 - classification_loss: 1.1408
 632/1000 [=================>............] - ETA: 2:46 - loss: 4.0918 - regression_loss: 2.9510 - classification_loss: 1.1408
 633/1000 [=================>............] - ETA: 2:45 - loss: 4.0853 - regression_loss: 2.9464 - classification_loss: 1.1390
 634/1000 [==================>...........] - ETA: 2:45 - loss: 4.0867 - regression_loss: 2.9475 - classification_loss: 1.1392
 635/1000 [==================>...........] - ETA: 2:44 - loss: 4.0802 - regression_loss: 2.9428 - classification_loss: 1.1374
 636/1000 [==================>...........] - ETA: 2:44 - loss: 4.0814 - regression_loss: 2.9439 - classification_loss: 1.1375
 637/1000 [==================>...........] - ETA: 2:44 - loss: 4.0831 - regression_loss: 2.9453 - classification_loss: 1.1377
 638/1000 [==================>...........] - ETA: 2:43 - loss: 4.0855 - regression_loss: 2.9471 - classification_loss: 1.1383
 639/1000 [==================>...........] - ETA: 2:43 - loss: 4.0791 - regression_loss: 2.9425 - classification_loss: 1.1366
 640/1000 [==================>...........] - ETA: 2:42 - loss: 4.0804 - regression_loss: 2.9435 - classification_loss: 1.1369
 641/1000 [==================>...........] - ETA: 2:42 - loss: 4.0817 - regression_loss: 2.9446 - classification_loss: 1.1371
 642/1000 [==================>...........] - ETA: 2:41 - loss: 4.0826 - regression_loss: 2.9448 - classification_loss: 1.1378
 643/1000 [==================>...........] - ETA: 2:41 - loss: 4.0843 - regression_loss: 2.9462 - classification_loss: 1.1382
 644/1000 [==================>...........] - ETA: 2:40 - loss: 4.0855 - regression_loss: 2.9473 - classification_loss: 1.1382
 645/1000 [==================>...........] - ETA: 2:40 - loss: 4.0866 - regression_loss: 2.9483 - classification_loss: 1.1382
 646/1000 [==================>...........] - ETA: 2:40 - loss: 4.0873 - regression_loss: 2.9487 - classification_loss: 1.1386
 647/1000 [==================>...........] - ETA: 2:39 - loss: 4.0880 - regression_loss: 2.9494 - classification_loss: 1.1386
 648/1000 [==================>...........] - ETA: 2:39 - loss: 4.0895 - regression_loss: 2.9503 - classification_loss: 1.1391
 649/1000 [==================>...........] - ETA: 2:38 - loss: 4.0832 - regression_loss: 2.9458 - classification_loss: 1.1374
 650/1000 [==================>...........] - ETA: 2:38 - loss: 4.0769 - regression_loss: 2.9413 - classification_loss: 1.1356
 651/1000 [==================>...........] - ETA: 2:37 - loss: 4.0781 - regression_loss: 2.9422 - classification_loss: 1.1359
 652/1000 [==================>...........] - ETA: 2:37 - loss: 4.0793 - regression_loss: 2.9424 - classification_loss: 1.1368
 653/1000 [==================>...........] - ETA: 2:36 - loss: 4.0805 - regression_loss: 2.9436 - classification_loss: 1.1368
 654/1000 [==================>...........] - ETA: 2:36 - loss: 4.0816 - regression_loss: 2.9446 - classification_loss: 1.1370
 655/1000 [==================>...........] - ETA: 2:35 - loss: 4.0827 - regression_loss: 2.9455 - classification_loss: 1.1372
 656/1000 [==================>...........] - ETA: 2:35 - loss: 4.0874 - regression_loss: 2.9477 - classification_loss: 1.1397
 657/1000 [==================>...........] - ETA: 2:35 - loss: 4.0890 - regression_loss: 2.9489 - classification_loss: 1.1401
 658/1000 [==================>...........] - ETA: 2:34 - loss: 4.0903 - regression_loss: 2.9493 - classification_loss: 1.1410
 659/1000 [==================>...........] - ETA: 2:34 - loss: 4.0921 - regression_loss: 2.9504 - classification_loss: 1.1417
 660/1000 [==================>...........] - ETA: 2:33 - loss: 4.0924 - regression_loss: 2.9505 - classification_loss: 1.1419
 661/1000 [==================>...........] - ETA: 2:33 - loss: 4.0930 - regression_loss: 2.9510 - classification_loss: 1.1421
 662/1000 [==================>...........] - ETA: 2:32 - loss: 4.0941 - regression_loss: 2.9512 - classification_loss: 1.1429
 663/1000 [==================>...........] - ETA: 2:32 - loss: 4.0879 - regression_loss: 2.9468 - classification_loss: 1.1412
 664/1000 [==================>...........] - ETA: 2:31 - loss: 4.0897 - regression_loss: 2.9480 - classification_loss: 1.1417
 665/1000 [==================>...........] - ETA: 2:31 - loss: 4.0835 - regression_loss: 2.9436 - classification_loss: 1.1400
 666/1000 [==================>...........] - ETA: 2:30 - loss: 4.0846 - regression_loss: 2.9444 - classification_loss: 1.1402
 667/1000 [===================>..........] - ETA: 2:30 - loss: 4.0859 - regression_loss: 2.9457 - classification_loss: 1.1402
 668/1000 [===================>..........] - ETA: 2:30 - loss: 4.0872 - regression_loss: 2.9464 - classification_loss: 1.1407
 669/1000 [===================>..........] - ETA: 2:29 - loss: 4.0890 - regression_loss: 2.9474 - classification_loss: 1.1415
 670/1000 [===================>..........] - ETA: 2:29 - loss: 4.0829 - regression_loss: 2.9430 - classification_loss: 1.1398
 671/1000 [===================>..........] - ETA: 2:28 - loss: 4.0768 - regression_loss: 2.9386 - classification_loss: 1.1381
 672/1000 [===================>..........] - ETA: 2:28 - loss: 4.0792 - regression_loss: 2.9402 - classification_loss: 1.1389
 673/1000 [===================>..........] - ETA: 2:27 - loss: 4.0799 - regression_loss: 2.9402 - classification_loss: 1.1397
 674/1000 [===================>..........] - ETA: 2:27 - loss: 4.0816 - regression_loss: 2.9419 - classification_loss: 1.1397
 675/1000 [===================>..........] - ETA: 2:26 - loss: 4.0820 - regression_loss: 2.9422 - classification_loss: 1.1398
 676/1000 [===================>..........] - ETA: 2:26 - loss: 4.0828 - regression_loss: 2.9430 - classification_loss: 1.1398
 677/1000 [===================>..........] - ETA: 2:26 - loss: 4.0857 - regression_loss: 2.9443 - classification_loss: 1.1414
 678/1000 [===================>..........] - ETA: 2:25 - loss: 4.0868 - regression_loss: 2.9453 - classification_loss: 1.1415
 679/1000 [===================>..........] - ETA: 2:25 - loss: 4.0879 - regression_loss: 2.9463 - classification_loss: 1.1416
 680/1000 [===================>..........] - ETA: 2:24 - loss: 4.0892 - regression_loss: 2.9476 - classification_loss: 1.1416
 681/1000 [===================>..........] - ETA: 2:24 - loss: 4.0900 - regression_loss: 2.9478 - classification_loss: 1.1422
 682/1000 [===================>..........] - ETA: 2:23 - loss: 4.0840 - regression_loss: 2.9435 - classification_loss: 1.1405
 683/1000 [===================>..........] - ETA: 2:23 - loss: 4.0852 - regression_loss: 2.9446 - classification_loss: 1.1406
 684/1000 [===================>..........] - ETA: 2:22 - loss: 4.0879 - regression_loss: 2.9461 - classification_loss: 1.1417
 685/1000 [===================>..........] - ETA: 2:22 - loss: 4.0884 - regression_loss: 2.9466 - classification_loss: 1.1418
 686/1000 [===================>..........] - ETA: 2:21 - loss: 4.0901 - regression_loss: 2.9481 - classification_loss: 1.1420
 687/1000 [===================>..........] - ETA: 2:21 - loss: 4.0919 - regression_loss: 2.9494 - classification_loss: 1.1425
 688/1000 [===================>..........] - ETA: 2:21 - loss: 4.0944 - regression_loss: 2.9507 - classification_loss: 1.1436
 689/1000 [===================>..........] - ETA: 2:20 - loss: 4.0960 - regression_loss: 2.9524 - classification_loss: 1.1436
 690/1000 [===================>..........] - ETA: 2:20 - loss: 4.0901 - regression_loss: 2.9481 - classification_loss: 1.1420
 691/1000 [===================>..........] - ETA: 2:19 - loss: 4.0905 - regression_loss: 2.9485 - classification_loss: 1.1420
 692/1000 [===================>..........] - ETA: 2:19 - loss: 4.0910 - regression_loss: 2.9489 - classification_loss: 1.1420
 693/1000 [===================>..........] - ETA: 2:18 - loss: 4.0920 - regression_loss: 2.9499 - classification_loss: 1.1420
 694/1000 [===================>..........] - ETA: 2:18 - loss: 4.0928 - regression_loss: 2.9508 - classification_loss: 1.1420
 695/1000 [===================>..........] - ETA: 2:17 - loss: 4.0940 - regression_loss: 2.9515 - classification_loss: 1.1425
 696/1000 [===================>..........] - ETA: 2:17 - loss: 4.0959 - regression_loss: 2.9524 - classification_loss: 1.1435
 697/1000 [===================>..........] - ETA: 2:16 - loss: 4.0900 - regression_loss: 2.9482 - classification_loss: 1.1418
 698/1000 [===================>..........] - ETA: 2:16 - loss: 4.0911 - regression_loss: 2.9492 - classification_loss: 1.1419
 699/1000 [===================>..........] - ETA: 2:16 - loss: 4.0922 - regression_loss: 2.9503 - classification_loss: 1.1419
 700/1000 [====================>.........] - ETA: 2:15 - loss: 4.0957 - regression_loss: 2.9531 - classification_loss: 1.1426
 701/1000 [====================>.........] - ETA: 2:15 - loss: 4.0966 - regression_loss: 2.9538 - classification_loss: 1.1428
 702/1000 [====================>.........] - ETA: 2:14 - loss: 4.0907 - regression_loss: 2.9496 - classification_loss: 1.1411
 703/1000 [====================>.........] - ETA: 2:14 - loss: 4.0919 - regression_loss: 2.9507 - classification_loss: 1.1412
 704/1000 [====================>.........] - ETA: 2:13 - loss: 4.0861 - regression_loss: 2.9465 - classification_loss: 1.1396
 705/1000 [====================>.........] - ETA: 2:13 - loss: 4.0871 - regression_loss: 2.9474 - classification_loss: 1.1397
 706/1000 [====================>.........] - ETA: 2:12 - loss: 4.0877 - regression_loss: 2.9477 - classification_loss: 1.1399
 707/1000 [====================>.........] - ETA: 2:12 - loss: 4.0882 - regression_loss: 2.9480 - classification_loss: 1.1401
 708/1000 [====================>.........] - ETA: 2:11 - loss: 4.0895 - regression_loss: 2.9493 - classification_loss: 1.1402
 709/1000 [====================>.........] - ETA: 2:11 - loss: 4.0897 - regression_loss: 2.9494 - classification_loss: 1.1403
 710/1000 [====================>.........] - ETA: 2:11 - loss: 4.0919 - regression_loss: 2.9506 - classification_loss: 1.1412
 711/1000 [====================>.........] - ETA: 2:10 - loss: 4.0861 - regression_loss: 2.9465 - classification_loss: 1.1396
 712/1000 [====================>.........] - ETA: 2:10 - loss: 4.0804 - regression_loss: 2.9423 - classification_loss: 1.1380
 713/1000 [====================>.........] - ETA: 2:09 - loss: 4.0834 - regression_loss: 2.9448 - classification_loss: 1.1386
 714/1000 [====================>.........] - ETA: 2:09 - loss: 4.0843 - regression_loss: 2.9456 - classification_loss: 1.1387
 715/1000 [====================>.........] - ETA: 2:08 - loss: 4.0891 - regression_loss: 2.9501 - classification_loss: 1.1389
 716/1000 [====================>.........] - ETA: 2:08 - loss: 4.0894 - regression_loss: 2.9504 - classification_loss: 1.1390
 717/1000 [====================>.........] - ETA: 2:07 - loss: 4.0904 - regression_loss: 2.9513 - classification_loss: 1.1391
 718/1000 [====================>.........] - ETA: 2:07 - loss: 4.0917 - regression_loss: 2.9524 - classification_loss: 1.1393
 719/1000 [====================>.........] - ETA: 2:07 - loss: 4.0926 - regression_loss: 2.9533 - classification_loss: 1.1393
 720/1000 [====================>.........] - ETA: 2:06 - loss: 4.0941 - regression_loss: 2.9545 - classification_loss: 1.1396
 721/1000 [====================>.........] - ETA: 2:06 - loss: 4.0945 - regression_loss: 2.9547 - classification_loss: 1.1398
 722/1000 [====================>.........] - ETA: 2:05 - loss: 4.0960 - regression_loss: 2.9558 - classification_loss: 1.1402
 723/1000 [====================>.........] - ETA: 2:05 - loss: 4.0968 - regression_loss: 2.9566 - classification_loss: 1.1402
 724/1000 [====================>.........] - ETA: 2:04 - loss: 4.0978 - regression_loss: 2.9576 - classification_loss: 1.1403
 725/1000 [====================>.........] - ETA: 2:04 - loss: 4.0982 - regression_loss: 2.9579 - classification_loss: 1.1403
 726/1000 [====================>.........] - ETA: 2:03 - loss: 4.0991 - regression_loss: 2.9588 - classification_loss: 1.1403
 727/1000 [====================>.........] - ETA: 2:03 - loss: 4.0997 - regression_loss: 2.9592 - classification_loss: 1.1405
 728/1000 [====================>.........] - ETA: 2:02 - loss: 4.0941 - regression_loss: 2.9552 - classification_loss: 1.1389
 729/1000 [====================>.........] - ETA: 2:02 - loss: 4.0952 - regression_loss: 2.9563 - classification_loss: 1.1389
 730/1000 [====================>.........] - ETA: 2:02 - loss: 4.0963 - regression_loss: 2.9574 - classification_loss: 1.1390
 731/1000 [====================>.........] - ETA: 2:01 - loss: 4.0969 - regression_loss: 2.9579 - classification_loss: 1.1391
 732/1000 [====================>.........] - ETA: 2:01 - loss: 4.0913 - regression_loss: 2.9538 - classification_loss: 1.1375
 733/1000 [====================>.........] - ETA: 2:00 - loss: 4.0925 - regression_loss: 2.9549 - classification_loss: 1.1375
 734/1000 [=====================>........] - ETA: 2:00 - loss: 4.0944 - regression_loss: 2.9559 - classification_loss: 1.1386
 735/1000 [=====================>........] - ETA: 1:59 - loss: 4.0951 - regression_loss: 2.9564 - classification_loss: 1.1387
 736/1000 [=====================>........] - ETA: 1:59 - loss: 4.0896 - regression_loss: 2.9524 - classification_loss: 1.1371
 737/1000 [=====================>........] - ETA: 1:58 - loss: 4.0900 - regression_loss: 2.9528 - classification_loss: 1.1372
 738/1000 [=====================>........] - ETA: 1:58 - loss: 4.0912 - regression_loss: 2.9537 - classification_loss: 1.1375
 739/1000 [=====================>........] - ETA: 1:57 - loss: 4.0918 - regression_loss: 2.9539 - classification_loss: 1.1379
 740/1000 [=====================>........] - ETA: 1:57 - loss: 4.0923 - regression_loss: 2.9543 - classification_loss: 1.1380
 741/1000 [=====================>........] - ETA: 1:57 - loss: 4.0929 - regression_loss: 2.9546 - classification_loss: 1.1383
 742/1000 [=====================>........] - ETA: 1:56 - loss: 4.0935 - regression_loss: 2.9551 - classification_loss: 1.1384
 743/1000 [=====================>........] - ETA: 1:56 - loss: 4.0941 - regression_loss: 2.9556 - classification_loss: 1.1385
 744/1000 [=====================>........] - ETA: 1:55 - loss: 4.0886 - regression_loss: 2.9516 - classification_loss: 1.1370
 745/1000 [=====================>........] - ETA: 1:55 - loss: 4.0831 - regression_loss: 2.9476 - classification_loss: 1.1355
 746/1000 [=====================>........] - ETA: 1:54 - loss: 4.0843 - regression_loss: 2.9487 - classification_loss: 1.1355
 747/1000 [=====================>........] - ETA: 1:54 - loss: 4.0860 - regression_loss: 2.9505 - classification_loss: 1.1355
 748/1000 [=====================>........] - ETA: 1:53 - loss: 4.0806 - regression_loss: 2.9466 - classification_loss: 1.1340
 749/1000 [=====================>........] - ETA: 1:53 - loss: 4.0816 - regression_loss: 2.9474 - classification_loss: 1.1342
 750/1000 [=====================>........] - ETA: 1:53 - loss: 4.0762 - regression_loss: 2.9434 - classification_loss: 1.1327
 751/1000 [=====================>........] - ETA: 1:52 - loss: 4.0707 - regression_loss: 2.9395 - classification_loss: 1.1312
 752/1000 [=====================>........] - ETA: 1:52 - loss: 4.0738 - regression_loss: 2.9409 - classification_loss: 1.1329
 753/1000 [=====================>........] - ETA: 1:51 - loss: 4.0756 - regression_loss: 2.9414 - classification_loss: 1.1342
 754/1000 [=====================>........] - ETA: 1:51 - loss: 4.0770 - regression_loss: 2.9423 - classification_loss: 1.1346
 755/1000 [=====================>........] - ETA: 1:50 - loss: 4.0775 - regression_loss: 2.9424 - classification_loss: 1.1351
 756/1000 [=====================>........] - ETA: 1:50 - loss: 4.0796 - regression_loss: 2.9437 - classification_loss: 1.1360
 757/1000 [=====================>........] - ETA: 1:49 - loss: 4.0742 - regression_loss: 2.9398 - classification_loss: 1.1345
 758/1000 [=====================>........] - ETA: 1:49 - loss: 4.0750 - regression_loss: 2.9401 - classification_loss: 1.1348
 759/1000 [=====================>........] - ETA: 1:48 - loss: 4.0696 - regression_loss: 2.9363 - classification_loss: 1.1333
 760/1000 [=====================>........] - ETA: 1:48 - loss: 4.0704 - regression_loss: 2.9366 - classification_loss: 1.1338
 761/1000 [=====================>........] - ETA: 1:48 - loss: 4.0725 - regression_loss: 2.9378 - classification_loss: 1.1347
 762/1000 [=====================>........] - ETA: 1:47 - loss: 4.0728 - regression_loss: 2.9380 - classification_loss: 1.1347
 763/1000 [=====================>........] - ETA: 1:47 - loss: 4.0746 - regression_loss: 2.9397 - classification_loss: 1.1350
 764/1000 [=====================>........] - ETA: 1:46 - loss: 4.0693 - regression_loss: 2.9358 - classification_loss: 1.1335
 765/1000 [=====================>........] - ETA: 1:46 - loss: 4.0708 - regression_loss: 2.9368 - classification_loss: 1.1340
 766/1000 [=====================>........] - ETA: 1:45 - loss: 4.0718 - regression_loss: 2.9377 - classification_loss: 1.1341
 767/1000 [======================>.......] - ETA: 1:45 - loss: 4.0734 - regression_loss: 2.9393 - classification_loss: 1.1341
 768/1000 [======================>.......] - ETA: 1:44 - loss: 4.0750 - regression_loss: 2.9404 - classification_loss: 1.1346
 769/1000 [======================>.......] - ETA: 1:44 - loss: 4.0782 - regression_loss: 2.9435 - classification_loss: 1.1347
 770/1000 [======================>.......] - ETA: 1:43 - loss: 4.0785 - regression_loss: 2.9438 - classification_loss: 1.1347
 771/1000 [======================>.......] - ETA: 1:43 - loss: 4.0797 - regression_loss: 2.9447 - classification_loss: 1.1350
 772/1000 [======================>.......] - ETA: 1:43 - loss: 4.0744 - regression_loss: 2.9409 - classification_loss: 1.1335
 773/1000 [======================>.......] - ETA: 1:42 - loss: 4.0691 - regression_loss: 2.9371 - classification_loss: 1.1320
 774/1000 [======================>.......] - ETA: 1:42 - loss: 4.0698 - regression_loss: 2.9377 - classification_loss: 1.1320
 775/1000 [======================>.......] - ETA: 1:41 - loss: 4.0710 - regression_loss: 2.9388 - classification_loss: 1.1322
 776/1000 [======================>.......] - ETA: 1:41 - loss: 4.0716 - regression_loss: 2.9393 - classification_loss: 1.1323
 777/1000 [======================>.......] - ETA: 1:40 - loss: 4.0663 - regression_loss: 2.9355 - classification_loss: 1.1308
 778/1000 [======================>.......] - ETA: 1:40 - loss: 4.0681 - regression_loss: 2.9365 - classification_loss: 1.1317
 779/1000 [======================>.......] - ETA: 1:39 - loss: 4.0686 - regression_loss: 2.9367 - classification_loss: 1.1319
 780/1000 [======================>.......] - ETA: 1:39 - loss: 4.0634 - regression_loss: 2.9330 - classification_loss: 1.1305
 781/1000 [======================>.......] - ETA: 1:38 - loss: 4.0654 - regression_loss: 2.9342 - classification_loss: 1.1312
 782/1000 [======================>.......] - ETA: 1:38 - loss: 4.0671 - regression_loss: 2.9353 - classification_loss: 1.1318
 783/1000 [======================>.......] - ETA: 1:38 - loss: 4.0619 - regression_loss: 2.9315 - classification_loss: 1.1304
 784/1000 [======================>.......] - ETA: 1:37 - loss: 4.0631 - regression_loss: 2.9327 - classification_loss: 1.1304
 785/1000 [======================>.......] - ETA: 1:37 - loss: 4.0652 - regression_loss: 2.9340 - classification_loss: 1.1312
 786/1000 [======================>.......] - ETA: 1:36 - loss: 4.0664 - regression_loss: 2.9346 - classification_loss: 1.1318
 787/1000 [======================>.......] - ETA: 1:36 - loss: 4.0680 - regression_loss: 2.9353 - classification_loss: 1.1327
 788/1000 [======================>.......] - ETA: 1:35 - loss: 4.0700 - regression_loss: 2.9362 - classification_loss: 1.1338
 789/1000 [======================>.......] - ETA: 1:35 - loss: 4.0648 - regression_loss: 2.9324 - classification_loss: 1.1324
 790/1000 [======================>.......] - ETA: 1:34 - loss: 4.0597 - regression_loss: 2.9287 - classification_loss: 1.1310
 791/1000 [======================>.......] - ETA: 1:34 - loss: 4.0613 - regression_loss: 2.9300 - classification_loss: 1.1313
 792/1000 [======================>.......] - ETA: 1:34 - loss: 4.0561 - regression_loss: 2.9263 - classification_loss: 1.1298
 793/1000 [======================>.......] - ETA: 1:33 - loss: 4.0581 - regression_loss: 2.9272 - classification_loss: 1.1309
 794/1000 [======================>.......] - ETA: 1:33 - loss: 4.0585 - regression_loss: 2.9272 - classification_loss: 1.1312
 795/1000 [======================>.......] - ETA: 1:32 - loss: 4.0588 - regression_loss: 2.9270 - classification_loss: 1.1318
 796/1000 [======================>.......] - ETA: 1:32 - loss: 4.0537 - regression_loss: 2.9233 - classification_loss: 1.1303
 797/1000 [======================>.......] - ETA: 1:31 - loss: 4.0486 - regression_loss: 2.9197 - classification_loss: 1.1289
 798/1000 [======================>.......] - ETA: 1:31 - loss: 4.0435 - regression_loss: 2.9160 - classification_loss: 1.1275
 799/1000 [======================>.......] - ETA: 1:30 - loss: 4.0385 - regression_loss: 2.9124 - classification_loss: 1.1261
 800/1000 [=======================>......] - ETA: 1:30 - loss: 4.0401 - regression_loss: 2.9130 - classification_loss: 1.1271
 801/1000 [=======================>......] - ETA: 1:29 - loss: 4.0350 - regression_loss: 2.9093 - classification_loss: 1.1257
 802/1000 [=======================>......] - ETA: 1:29 - loss: 4.0378 - regression_loss: 2.9111 - classification_loss: 1.1266
 803/1000 [=======================>......] - ETA: 1:29 - loss: 4.0393 - regression_loss: 2.9122 - classification_loss: 1.1271
 804/1000 [=======================>......] - ETA: 1:28 - loss: 4.0377 - regression_loss: 2.9097 - classification_loss: 1.1280
 805/1000 [=======================>......] - ETA: 1:28 - loss: 4.0382 - regression_loss: 2.9102 - classification_loss: 1.1280
 806/1000 [=======================>......] - ETA: 1:27 - loss: 4.0332 - regression_loss: 2.9066 - classification_loss: 1.1266
 807/1000 [=======================>......] - ETA: 1:27 - loss: 4.0338 - regression_loss: 2.9070 - classification_loss: 1.1268
 808/1000 [=======================>......] - ETA: 1:26 - loss: 4.0363 - regression_loss: 2.9084 - classification_loss: 1.1279
 809/1000 [=======================>......] - ETA: 1:26 - loss: 4.0377 - regression_loss: 2.9096 - classification_loss: 1.1280
 810/1000 [=======================>......] - ETA: 1:25 - loss: 4.0327 - regression_loss: 2.9061 - classification_loss: 1.1266
 811/1000 [=======================>......] - ETA: 1:25 - loss: 4.0349 - regression_loss: 2.9081 - classification_loss: 1.1268
 812/1000 [=======================>......] - ETA: 1:24 - loss: 4.0299 - regression_loss: 2.9045 - classification_loss: 1.1254
 813/1000 [=======================>......] - ETA: 1:24 - loss: 4.0327 - regression_loss: 2.9053 - classification_loss: 1.1275
 814/1000 [=======================>......] - ETA: 1:24 - loss: 4.0321 - regression_loss: 2.9037 - classification_loss: 1.1284
 815/1000 [=======================>......] - ETA: 1:23 - loss: 4.0344 - regression_loss: 2.9055 - classification_loss: 1.1289
 816/1000 [=======================>......] - ETA: 1:23 - loss: 4.0358 - regression_loss: 2.9058 - classification_loss: 1.1300
 817/1000 [=======================>......] - ETA: 1:22 - loss: 4.0309 - regression_loss: 2.9022 - classification_loss: 1.1286
 818/1000 [=======================>......] - ETA: 1:22 - loss: 4.0259 - regression_loss: 2.8987 - classification_loss: 1.1272
 819/1000 [=======================>......] - ETA: 1:21 - loss: 4.0277 - regression_loss: 2.8993 - classification_loss: 1.1284
 820/1000 [=======================>......] - ETA: 1:21 - loss: 4.0228 - regression_loss: 2.8958 - classification_loss: 1.1270
 821/1000 [=======================>......] - ETA: 1:20 - loss: 4.0235 - regression_loss: 2.8963 - classification_loss: 1.1271
 822/1000 [=======================>......] - ETA: 1:20 - loss: 4.0249 - regression_loss: 2.8975 - classification_loss: 1.1274
 823/1000 [=======================>......] - ETA: 1:20 - loss: 4.0269 - regression_loss: 2.8987 - classification_loss: 1.1282
 824/1000 [=======================>......] - ETA: 1:19 - loss: 4.0279 - regression_loss: 2.8997 - classification_loss: 1.1282
 825/1000 [=======================>......] - ETA: 1:19 - loss: 4.0230 - regression_loss: 2.8962 - classification_loss: 1.1269
 826/1000 [=======================>......] - ETA: 1:18 - loss: 4.0247 - regression_loss: 2.8968 - classification_loss: 1.1279
 827/1000 [=======================>......] - ETA: 1:18 - loss: 4.0198 - regression_loss: 2.8933 - classification_loss: 1.1265
 828/1000 [=======================>......] - ETA: 1:17 - loss: 4.0218 - regression_loss: 2.8943 - classification_loss: 1.1276
 829/1000 [=======================>......] - ETA: 1:17 - loss: 4.0237 - regression_loss: 2.8953 - classification_loss: 1.1284
 830/1000 [=======================>......] - ETA: 1:16 - loss: 4.0264 - regression_loss: 2.8975 - classification_loss: 1.1290
 831/1000 [=======================>......] - ETA: 1:16 - loss: 4.0276 - regression_loss: 2.8985 - classification_loss: 1.1290
 832/1000 [=======================>......] - ETA: 1:15 - loss: 4.0289 - regression_loss: 2.8993 - classification_loss: 1.1295
 833/1000 [=======================>......] - ETA: 1:15 - loss: 4.0298 - regression_loss: 2.8999 - classification_loss: 1.1299
 834/1000 [========================>.....] - ETA: 1:15 - loss: 4.0297 - regression_loss: 2.8995 - classification_loss: 1.1302
 835/1000 [========================>.....] - ETA: 1:14 - loss: 4.0312 - regression_loss: 2.9010 - classification_loss: 1.1303
 836/1000 [========================>.....] - ETA: 1:14 - loss: 4.0319 - regression_loss: 2.9016 - classification_loss: 1.1303
 837/1000 [========================>.....] - ETA: 1:13 - loss: 4.0271 - regression_loss: 2.8982 - classification_loss: 1.1289
 838/1000 [========================>.....] - ETA: 1:13 - loss: 4.0279 - regression_loss: 2.8990 - classification_loss: 1.1289
 839/1000 [========================>.....] - ETA: 1:12 - loss: 4.0283 - regression_loss: 2.8994 - classification_loss: 1.1290
 840/1000 [========================>.....] - ETA: 1:12 - loss: 4.0299 - regression_loss: 2.9009 - classification_loss: 1.1290
 841/1000 [========================>.....] - ETA: 1:11 - loss: 4.0316 - regression_loss: 2.9024 - classification_loss: 1.1292
 842/1000 [========================>.....] - ETA: 1:11 - loss: 4.0320 - regression_loss: 2.9025 - classification_loss: 1.1295
 843/1000 [========================>.....] - ETA: 1:10 - loss: 4.0329 - regression_loss: 2.9030 - classification_loss: 1.1299
 844/1000 [========================>.....] - ETA: 1:10 - loss: 4.0281 - regression_loss: 2.8995 - classification_loss: 1.1286
 845/1000 [========================>.....] - ETA: 1:10 - loss: 4.0286 - regression_loss: 2.8999 - classification_loss: 1.1287
 846/1000 [========================>.....] - ETA: 1:09 - loss: 4.0238 - regression_loss: 2.8965 - classification_loss: 1.1273
 847/1000 [========================>.....] - ETA: 1:09 - loss: 4.0246 - regression_loss: 2.8970 - classification_loss: 1.1276
 848/1000 [========================>.....] - ETA: 1:08 - loss: 4.0263 - regression_loss: 2.8981 - classification_loss: 1.1282
 849/1000 [========================>.....] - ETA: 1:08 - loss: 4.0276 - regression_loss: 2.8990 - classification_loss: 1.1287
 850/1000 [========================>.....] - ETA: 1:07 - loss: 4.0229 - regression_loss: 2.8956 - classification_loss: 1.1274
 851/1000 [========================>.....] - ETA: 1:07 - loss: 4.0182 - regression_loss: 2.8922 - classification_loss: 1.1260
 852/1000 [========================>.....] - ETA: 1:06 - loss: 4.0195 - regression_loss: 2.8930 - classification_loss: 1.1265
 853/1000 [========================>.....] - ETA: 1:06 - loss: 4.0215 - regression_loss: 2.8948 - classification_loss: 1.1268
 854/1000 [========================>.....] - ETA: 1:05 - loss: 4.0232 - regression_loss: 2.8962 - classification_loss: 1.1270
 855/1000 [========================>.....] - ETA: 1:05 - loss: 4.0243 - regression_loss: 2.8969 - classification_loss: 1.1274
 856/1000 [========================>.....] - ETA: 1:05 - loss: 4.0252 - regression_loss: 2.8978 - classification_loss: 1.1275
 857/1000 [========================>.....] - ETA: 1:04 - loss: 4.0272 - regression_loss: 2.8995 - classification_loss: 1.1277
 858/1000 [========================>.....] - ETA: 1:04 - loss: 4.0279 - regression_loss: 2.9002 - classification_loss: 1.1277
 859/1000 [========================>.....] - ETA: 1:03 - loss: 4.0291 - regression_loss: 2.9014 - classification_loss: 1.1277
 860/1000 [========================>.....] - ETA: 1:03 - loss: 4.0244 - regression_loss: 2.8980 - classification_loss: 1.1264
 861/1000 [========================>.....] - ETA: 1:02 - loss: 4.0249 - regression_loss: 2.8984 - classification_loss: 1.1265
 862/1000 [========================>.....] - ETA: 1:02 - loss: 4.0202 - regression_loss: 2.8950 - classification_loss: 1.1252
 863/1000 [========================>.....] - ETA: 1:01 - loss: 4.0218 - regression_loss: 2.8963 - classification_loss: 1.1255
 864/1000 [========================>.....] - ETA: 1:01 - loss: 4.0219 - regression_loss: 2.8964 - classification_loss: 1.1255
 865/1000 [========================>.....] - ETA: 1:01 - loss: 4.0172 - regression_loss: 2.8930 - classification_loss: 1.1242
 866/1000 [========================>.....] - ETA: 1:00 - loss: 4.0179 - regression_loss: 2.8937 - classification_loss: 1.1242
 867/1000 [=========================>....] - ETA: 1:00 - loss: 4.0193 - regression_loss: 2.8947 - classification_loss: 1.1246
 868/1000 [=========================>....] - ETA: 59s - loss: 4.0147 - regression_loss: 2.8914 - classification_loss: 1.1233 
 869/1000 [=========================>....] - ETA: 59s - loss: 4.0157 - regression_loss: 2.8918 - classification_loss: 1.1239
 870/1000 [=========================>....] - ETA: 58s - loss: 4.0184 - regression_loss: 2.8934 - classification_loss: 1.1251
 871/1000 [=========================>....] - ETA: 58s - loss: 4.0138 - regression_loss: 2.8900 - classification_loss: 1.1238
 872/1000 [=========================>....] - ETA: 57s - loss: 4.0149 - regression_loss: 2.8905 - classification_loss: 1.1244
 873/1000 [=========================>....] - ETA: 57s - loss: 4.0103 - regression_loss: 2.8872 - classification_loss: 1.1231
 874/1000 [=========================>....] - ETA: 56s - loss: 4.0119 - regression_loss: 2.8883 - classification_loss: 1.1236
 875/1000 [=========================>....] - ETA: 56s - loss: 4.0134 - regression_loss: 2.8891 - classification_loss: 1.1243
 876/1000 [=========================>....] - ETA: 56s - loss: 4.0150 - regression_loss: 2.8898 - classification_loss: 1.1252
 877/1000 [=========================>....] - ETA: 55s - loss: 4.0169 - regression_loss: 2.8907 - classification_loss: 1.1262
 878/1000 [=========================>....] - ETA: 55s - loss: 4.0187 - regression_loss: 2.8918 - classification_loss: 1.1270
 879/1000 [=========================>....] - ETA: 54s - loss: 4.0203 - regression_loss: 2.8923 - classification_loss: 1.1280
 880/1000 [=========================>....] - ETA: 54s - loss: 4.0230 - regression_loss: 2.8942 - classification_loss: 1.1288
 881/1000 [=========================>....] - ETA: 53s - loss: 4.0256 - regression_loss: 2.8960 - classification_loss: 1.1296
 882/1000 [=========================>....] - ETA: 53s - loss: 4.0261 - regression_loss: 2.8965 - classification_loss: 1.1296
 883/1000 [=========================>....] - ETA: 52s - loss: 4.0272 - regression_loss: 2.8974 - classification_loss: 1.1298
 884/1000 [=========================>....] - ETA: 52s - loss: 4.0279 - regression_loss: 2.8980 - classification_loss: 1.1298
 885/1000 [=========================>....] - ETA: 51s - loss: 4.0233 - regression_loss: 2.8948 - classification_loss: 1.1286
 886/1000 [=========================>....] - ETA: 51s - loss: 4.0245 - regression_loss: 2.8959 - classification_loss: 1.1286
 887/1000 [=========================>....] - ETA: 51s - loss: 4.0200 - regression_loss: 2.8927 - classification_loss: 1.1273
 888/1000 [=========================>....] - ETA: 50s - loss: 4.0209 - regression_loss: 2.8936 - classification_loss: 1.1273
 889/1000 [=========================>....] - ETA: 50s - loss: 4.0164 - regression_loss: 2.8903 - classification_loss: 1.1260
 890/1000 [=========================>....] - ETA: 49s - loss: 4.0170 - regression_loss: 2.8905 - classification_loss: 1.1266
 891/1000 [=========================>....] - ETA: 49s - loss: 4.0181 - regression_loss: 2.8916 - classification_loss: 1.1266
 892/1000 [=========================>....] - ETA: 48s - loss: 4.0186 - regression_loss: 2.8917 - classification_loss: 1.1269
 893/1000 [=========================>....] - ETA: 48s - loss: 4.0200 - regression_loss: 2.8927 - classification_loss: 1.1273
 894/1000 [=========================>....] - ETA: 47s - loss: 4.0217 - regression_loss: 2.8941 - classification_loss: 1.1276
 895/1000 [=========================>....] - ETA: 47s - loss: 4.0172 - regression_loss: 2.8909 - classification_loss: 1.1264
 896/1000 [=========================>....] - ETA: 46s - loss: 4.0187 - regression_loss: 2.8918 - classification_loss: 1.1269
 897/1000 [=========================>....] - ETA: 46s - loss: 4.0192 - regression_loss: 2.8922 - classification_loss: 1.1269
 898/1000 [=========================>....] - ETA: 46s - loss: 4.0199 - regression_loss: 2.8930 - classification_loss: 1.1269
 899/1000 [=========================>....] - ETA: 45s - loss: 4.0217 - regression_loss: 2.8947 - classification_loss: 1.1269
 900/1000 [==========================>...] - ETA: 45s - loss: 4.0227 - regression_loss: 2.8954 - classification_loss: 1.1273
 901/1000 [==========================>...] - ETA: 44s - loss: 4.0238 - regression_loss: 2.8959 - classification_loss: 1.1279
 902/1000 [==========================>...] - ETA: 44s - loss: 4.0246 - regression_loss: 2.8964 - classification_loss: 1.1283
 903/1000 [==========================>...] - ETA: 43s - loss: 4.0262 - regression_loss: 2.8980 - classification_loss: 1.1283
 904/1000 [==========================>...] - ETA: 43s - loss: 4.0263 - regression_loss: 2.8978 - classification_loss: 1.1285
 905/1000 [==========================>...] - ETA: 42s - loss: 4.0275 - regression_loss: 2.8990 - classification_loss: 1.1285
 906/1000 [==========================>...] - ETA: 42s - loss: 4.0279 - regression_loss: 2.8994 - classification_loss: 1.1285
 907/1000 [==========================>...] - ETA: 42s - loss: 4.0290 - regression_loss: 2.9004 - classification_loss: 1.1286
 908/1000 [==========================>...] - ETA: 41s - loss: 4.0298 - regression_loss: 2.9011 - classification_loss: 1.1287
 909/1000 [==========================>...] - ETA: 41s - loss: 4.0254 - regression_loss: 2.8979 - classification_loss: 1.1275
 910/1000 [==========================>...] - ETA: 40s - loss: 4.0261 - regression_loss: 2.8986 - classification_loss: 1.1275
 911/1000 [==========================>...] - ETA: 40s - loss: 4.0270 - regression_loss: 2.8991 - classification_loss: 1.1280
 912/1000 [==========================>...] - ETA: 39s - loss: 4.0277 - regression_loss: 2.8997 - classification_loss: 1.1280
 913/1000 [==========================>...] - ETA: 39s - loss: 4.0233 - regression_loss: 2.8965 - classification_loss: 1.1268
 914/1000 [==========================>...] - ETA: 38s - loss: 4.0261 - regression_loss: 2.8992 - classification_loss: 1.1268
 915/1000 [==========================>...] - ETA: 38s - loss: 4.0217 - regression_loss: 2.8961 - classification_loss: 1.1256
 916/1000 [==========================>...] - ETA: 37s - loss: 4.0220 - regression_loss: 2.8964 - classification_loss: 1.1256
 917/1000 [==========================>...] - ETA: 37s - loss: 4.0225 - regression_loss: 2.8968 - classification_loss: 1.1256
 918/1000 [==========================>...] - ETA: 37s - loss: 4.0234 - regression_loss: 2.8975 - classification_loss: 1.1259
 919/1000 [==========================>...] - ETA: 36s - loss: 4.0242 - regression_loss: 2.8980 - classification_loss: 1.1262
 920/1000 [==========================>...] - ETA: 36s - loss: 4.0251 - regression_loss: 2.8988 - classification_loss: 1.1263
 921/1000 [==========================>...] - ETA: 35s - loss: 4.0260 - regression_loss: 2.8994 - classification_loss: 1.1266
 922/1000 [==========================>...] - ETA: 35s - loss: 4.0273 - regression_loss: 2.9004 - classification_loss: 1.1269
 923/1000 [==========================>...] - ETA: 34s - loss: 4.0281 - regression_loss: 2.9011 - classification_loss: 1.1269
 924/1000 [==========================>...] - ETA: 34s - loss: 4.0287 - regression_loss: 2.9017 - classification_loss: 1.1270
 925/1000 [==========================>...] - ETA: 33s - loss: 4.0243 - regression_loss: 2.8985 - classification_loss: 1.1258
 926/1000 [==========================>...] - ETA: 33s - loss: 4.0200 - regression_loss: 2.8954 - classification_loss: 1.1246
 927/1000 [==========================>...] - ETA: 32s - loss: 4.0156 - regression_loss: 2.8923 - classification_loss: 1.1234
 928/1000 [==========================>...] - ETA: 32s - loss: 4.0170 - regression_loss: 2.8934 - classification_loss: 1.1236
 929/1000 [==========================>...] - ETA: 32s - loss: 4.0179 - regression_loss: 2.8943 - classification_loss: 1.1236
 930/1000 [==========================>...] - ETA: 31s - loss: 4.0197 - regression_loss: 2.8958 - classification_loss: 1.1239
 931/1000 [==========================>...] - ETA: 31s - loss: 4.0213 - regression_loss: 2.8966 - classification_loss: 1.1247
 932/1000 [==========================>...] - ETA: 30s - loss: 4.0233 - regression_loss: 2.8983 - classification_loss: 1.1249
 933/1000 [==========================>...] - ETA: 30s - loss: 4.0246 - regression_loss: 2.8993 - classification_loss: 1.1253
 934/1000 [===========================>..] - ETA: 29s - loss: 4.0259 - regression_loss: 2.8999 - classification_loss: 1.1260
 935/1000 [===========================>..] - ETA: 29s - loss: 4.0262 - regression_loss: 2.9002 - classification_loss: 1.1260
 936/1000 [===========================>..] - ETA: 28s - loss: 4.0219 - regression_loss: 2.8971 - classification_loss: 1.1248
 937/1000 [===========================>..] - ETA: 28s - loss: 4.0225 - regression_loss: 2.8976 - classification_loss: 1.1249
 938/1000 [===========================>..] - ETA: 28s - loss: 4.0182 - regression_loss: 2.8945 - classification_loss: 1.1237
 939/1000 [===========================>..] - ETA: 27s - loss: 4.0197 - regression_loss: 2.8955 - classification_loss: 1.1243
 940/1000 [===========================>..] - ETA: 27s - loss: 4.0208 - regression_loss: 2.8964 - classification_loss: 1.1244
 941/1000 [===========================>..] - ETA: 26s - loss: 4.0165 - regression_loss: 2.8934 - classification_loss: 1.1232
 942/1000 [===========================>..] - ETA: 26s - loss: 4.0123 - regression_loss: 2.8903 - classification_loss: 1.1220
 943/1000 [===========================>..] - ETA: 25s - loss: 4.0080 - regression_loss: 2.8872 - classification_loss: 1.1208
 944/1000 [===========================>..] - ETA: 25s - loss: 4.0038 - regression_loss: 2.8842 - classification_loss: 1.1196
 945/1000 [===========================>..] - ETA: 24s - loss: 3.9995 - regression_loss: 2.8811 - classification_loss: 1.1184
 946/1000 [===========================>..] - ETA: 24s - loss: 4.0012 - regression_loss: 2.8820 - classification_loss: 1.1192
 947/1000 [===========================>..] - ETA: 23s - loss: 4.0025 - regression_loss: 2.8826 - classification_loss: 1.1199
 948/1000 [===========================>..] - ETA: 23s - loss: 4.0031 - regression_loss: 2.8827 - classification_loss: 1.1204
 949/1000 [===========================>..] - ETA: 23s - loss: 4.0060 - regression_loss: 2.8840 - classification_loss: 1.1219
 950/1000 [===========================>..] - ETA: 22s - loss: 4.0017 - regression_loss: 2.8810 - classification_loss: 1.1207
 951/1000 [===========================>..] - ETA: 22s - loss: 4.0036 - regression_loss: 2.8820 - classification_loss: 1.1215
 952/1000 [===========================>..] - ETA: 21s - loss: 4.0043 - regression_loss: 2.8823 - classification_loss: 1.1219
 953/1000 [===========================>..] - ETA: 21s - loss: 4.0051 - regression_loss: 2.8830 - classification_loss: 1.1221
 954/1000 [===========================>..] - ETA: 20s - loss: 4.0064 - regression_loss: 2.8841 - classification_loss: 1.1222
 955/1000 [===========================>..] - ETA: 20s - loss: 4.0080 - regression_loss: 2.8849 - classification_loss: 1.1231
 956/1000 [===========================>..] - ETA: 19s - loss: 4.0105 - regression_loss: 2.8868 - classification_loss: 1.1237
 957/1000 [===========================>..] - ETA: 19s - loss: 4.0115 - regression_loss: 2.8874 - classification_loss: 1.1241
 958/1000 [===========================>..] - ETA: 18s - loss: 4.0073 - regression_loss: 2.8844 - classification_loss: 1.1229
 959/1000 [===========================>..] - ETA: 18s - loss: 4.0031 - regression_loss: 2.8814 - classification_loss: 1.1217
 960/1000 [===========================>..] - ETA: 18s - loss: 4.0042 - regression_loss: 2.8819 - classification_loss: 1.1223
 961/1000 [===========================>..] - ETA: 17s - loss: 4.0054 - regression_loss: 2.8829 - classification_loss: 1.1225
 962/1000 [===========================>..] - ETA: 17s - loss: 4.0067 - regression_loss: 2.8840 - classification_loss: 1.1227
 963/1000 [===========================>..] - ETA: 16s - loss: 4.0086 - regression_loss: 2.8850 - classification_loss: 1.1236
 964/1000 [===========================>..] - ETA: 16s - loss: 4.0100 - regression_loss: 2.8855 - classification_loss: 1.1245
 965/1000 [===========================>..] - ETA: 15s - loss: 4.0107 - regression_loss: 2.8860 - classification_loss: 1.1247
 966/1000 [===========================>..] - ETA: 15s - loss: 4.0114 - regression_loss: 2.8866 - classification_loss: 1.1248
 967/1000 [============================>.] - ETA: 14s - loss: 4.0073 - regression_loss: 2.8836 - classification_loss: 1.1236
 968/1000 [============================>.] - ETA: 14s - loss: 4.0102 - regression_loss: 2.8852 - classification_loss: 1.1250
 969/1000 [============================>.] - ETA: 14s - loss: 4.0124 - regression_loss: 2.8869 - classification_loss: 1.1255
 970/1000 [============================>.] - ETA: 13s - loss: 4.0083 - regression_loss: 2.8840 - classification_loss: 1.1243
 971/1000 [============================>.] - ETA: 13s - loss: 4.0041 - regression_loss: 2.8810 - classification_loss: 1.1231
 972/1000 [============================>.] - ETA: 12s - loss: 4.0000 - regression_loss: 2.8780 - classification_loss: 1.1220
 973/1000 [============================>.] - ETA: 12s - loss: 4.0018 - regression_loss: 2.8789 - classification_loss: 1.1229
 974/1000 [============================>.] - ETA: 11s - loss: 4.0023 - regression_loss: 2.8791 - classification_loss: 1.1231
 975/1000 [============================>.] - ETA: 11s - loss: 4.0030 - regression_loss: 2.8796 - classification_loss: 1.1234
 976/1000 [============================>.] - ETA: 10s - loss: 4.0047 - regression_loss: 2.8806 - classification_loss: 1.1240
 977/1000 [============================>.] - ETA: 10s - loss: 4.0050 - regression_loss: 2.8807 - classification_loss: 1.1243
 978/1000 [============================>.] - ETA: 9s - loss: 4.0059 - regression_loss: 2.8811 - classification_loss: 1.1248 
 979/1000 [============================>.] - ETA: 9s - loss: 4.0066 - regression_loss: 2.8817 - classification_loss: 1.1249
 980/1000 [============================>.] - ETA: 9s - loss: 4.0025 - regression_loss: 2.8787 - classification_loss: 1.1238
 981/1000 [============================>.] - ETA: 8s - loss: 4.0035 - regression_loss: 2.8795 - classification_loss: 1.1240
 982/1000 [============================>.] - ETA: 8s - loss: 4.0045 - regression_loss: 2.8803 - classification_loss: 1.1242
 983/1000 [============================>.] - ETA: 7s - loss: 4.0057 - regression_loss: 2.8815 - classification_loss: 1.1243
 984/1000 [============================>.] - ETA: 7s - loss: 4.0067 - regression_loss: 2.8821 - classification_loss: 1.1246
 985/1000 [============================>.] - ETA: 6s - loss: 4.0075 - regression_loss: 2.8827 - classification_loss: 1.1248
 986/1000 [============================>.] - ETA: 6s - loss: 4.0092 - regression_loss: 2.8839 - classification_loss: 1.1253
 987/1000 [============================>.] - ETA: 5s - loss: 4.0107 - regression_loss: 2.8849 - classification_loss: 1.1258
 988/1000 [============================>.] - ETA: 5s - loss: 4.0116 - regression_loss: 2.8857 - classification_loss: 1.1258
 989/1000 [============================>.] - ETA: 4s - loss: 4.0123 - regression_loss: 2.8865 - classification_loss: 1.1259
 990/1000 [============================>.] - ETA: 4s - loss: 4.0132 - regression_loss: 2.8871 - classification_loss: 1.1261
 991/1000 [============================>.] - ETA: 4s - loss: 4.0092 - regression_loss: 2.8842 - classification_loss: 1.1249
 992/1000 [============================>.] - ETA: 3s - loss: 4.0110 - regression_loss: 2.8859 - classification_loss: 1.1251
 993/1000 [============================>.] - ETA: 3s - loss: 4.0069 - regression_loss: 2.8830 - classification_loss: 1.1240
 994/1000 [============================>.] - ETA: 2s - loss: 4.0077 - regression_loss: 2.8836 - classification_loss: 1.1241
 995/1000 [============================>.] - ETA: 2s - loss: 4.0090 - regression_loss: 2.8847 - classification_loss: 1.1243
 996/1000 [============================>.] - ETA: 1s - loss: 4.0093 - regression_loss: 2.8849 - classification_loss: 1.1244
 997/1000 [============================>.] - ETA: 1s - loss: 4.0053 - regression_loss: 2.8820 - classification_loss: 1.1233
 998/1000 [============================>.] - ETA: 0s - loss: 4.0069 - regression_loss: 2.8832 - classification_loss: 1.1237
 999/1000 [============================>.] - ETA: 0s - loss: 4.0081 - regression_loss: 2.8843 - classification_loss: 1.1238
1000/1000 [==============================] - 452s 452ms/step - loss: 4.0086 - regression_loss: 2.8848 - classification_loss: 1.1239

Epoch 00002: saving model to ./snapshots/resnet50_csv_02.h5
0/2001/2002/2003/2004/2005/2006/2007/2008/2009/20010/20011/20012/20013/20014/20015/20016/20017/20018/20019/20020/20021/20022/20023/20024/20025/20026/20027/20028/20029/20030/20031/20032/20033/20034/20035/20036/20037/20038/20039/20040/20041/20042/20043/20044/20045/20046/20047/20048/20049/20050/20051/20052/20053/20054/20055/20056/20057/20058/20059/20060/20061/20062/20063/20064/20065/20066/20067/20068/20069/20070/20071/20072/20073/20074/20075/20076/20077/20078/20079/20080/20081/20082/20083/20084/20085/20086/20087/20088/20089/20090/20091/20092/20093/20094/20095/20096/20097/20098/20099/200100/200101/200102/200103/200104/200105/200106/200107/200108/200109/200110/200111/200112/200113/200114/200115/200116/200117/200118/200119/200120/200121/200122/200123/200124/200125/200126/200127/200128/200129/200130/200131/200132/200133/200134/200135/200136/200137/200138/200139/200140/200141/200142/200143/200144/200145/200146/200147/200148/200149/200150/200151/200152/200153/200154/200155/200156/200157/200158/200159/200160/200161/200162/200163/200164/200165/200166/200167/200168/200169/200170/200171/200172/200173/200174/200175/200176/200177/200178/200179/200180/200181/200182/200183/200184/200185/200186/200187/200188/200189/200190/200191/200192/200193/200194/200195/200196/200197/200198/200199/2000/2001/2002/2003/2004/2005/2006/2007/2008/2009/20010/20011/20012/20013/20014/20015/20016/20017/20018/20019/20020/20021/20022/20023/20024/20025/20026/20027/20028/20029/20030/20031/20032/20033/20034/20035/20036/20037/20038/20039/20040/20041/20042/20043/20044/20045/20046/20047/20048/20049/20050/20051/20052/20053/20054/20055/20056/20057/20058/20059/20060/20061/20062/20063/20064/20065/20066/20067/20068/20069/20070/20071/20072/20073/20074/20075/20076/20077/20078/20079/20080/20081/20082/20083/20084/20085/20086/20087/20088/20089/20090/20091/20092/20093/20094/20095/20096/20097/20098/20099/200100/200101/200102/200103/200104/200105/200106/200107/200108/200109/200110/200111/200112/200113/200114/200115/200116/200117/200118/200119/200120/200121/200122/200123/200124/200125/200126/200127/200128/200129/200130/200131/200132/200133/200134/200135/200136/200137/200138/200139/200140/200141/200142/200143/200144/200145/200146/200147/200148/200149/200150/200151/200152/200153/200154/200155/200156/200157/200158/200159/200160/200161/200162/200163/200164/200165/200166/200167/200168/200169/200170/200171/200172/200173/200174/200175/200176/200177/200178/200179/200180/200181/200182/200183/200184/200185/200186/200187/200188/200189/200190/200191/200192/200193/200194/200195/200196/200197/200198/200199/200

M 0.0000
N 0.0000
mAP: 0.0000
Epoch 3/30

   1/1000 [..............................] - ETA: 7:22 - loss: 4.6887 - regression_loss: 3.2978 - classification_loss: 1.3909
   2/1000 [..............................] - ETA: 7:20 - loss: 6.0675 - regression_loss: 4.8102 - classification_loss: 1.2573
   3/1000 [..............................] - ETA: 7:25 - loss: 5.7304 - regression_loss: 4.3889 - classification_loss: 1.3415
   4/1000 [..............................] - ETA: 7:26 - loss: 5.4296 - regression_loss: 4.1004 - classification_loss: 1.3292
   5/1000 [..............................] - ETA: 7:28 - loss: 5.3201 - regression_loss: 4.0277 - classification_loss: 1.2924
   6/1000 [..............................] - ETA: 7:28 - loss: 5.2125 - regression_loss: 3.9345 - classification_loss: 1.2780
   7/1000 [..............................] - ETA: 7:27 - loss: 5.1686 - regression_loss: 3.8865 - classification_loss: 1.2821
   8/1000 [..............................] - ETA: 7:28 - loss: 4.5226 - regression_loss: 3.4007 - classification_loss: 1.1219
   9/1000 [..............................] - ETA: 7:27 - loss: 4.4927 - regression_loss: 3.3705 - classification_loss: 1.1222
  10/1000 [..............................] - ETA: 7:27 - loss: 4.5275 - regression_loss: 3.3940 - classification_loss: 1.1335
  11/1000 [..............................] - ETA: 7:27 - loss: 4.1159 - regression_loss: 3.0855 - classification_loss: 1.0305
  12/1000 [..............................] - ETA: 7:27 - loss: 4.1376 - regression_loss: 3.0996 - classification_loss: 1.0380
  13/1000 [..............................] - ETA: 7:26 - loss: 4.1581 - regression_loss: 3.1136 - classification_loss: 1.0445
  14/1000 [..............................] - ETA: 7:25 - loss: 4.2633 - regression_loss: 3.1669 - classification_loss: 1.0965
  15/1000 [..............................] - ETA: 7:24 - loss: 4.3182 - regression_loss: 3.2159 - classification_loss: 1.1023
  16/1000 [..............................] - ETA: 7:23 - loss: 4.0483 - regression_loss: 3.0149 - classification_loss: 1.0335
  17/1000 [..............................] - ETA: 7:23 - loss: 4.0815 - regression_loss: 3.0420 - classification_loss: 1.0395
  18/1000 [..............................] - ETA: 7:23 - loss: 4.1134 - regression_loss: 3.0619 - classification_loss: 1.0515
  19/1000 [..............................] - ETA: 7:22 - loss: 4.1419 - regression_loss: 3.0872 - classification_loss: 1.0547
  20/1000 [..............................] - ETA: 7:22 - loss: 4.1806 - regression_loss: 3.1197 - classification_loss: 1.0609
  21/1000 [..............................] - ETA: 7:21 - loss: 4.2332 - regression_loss: 3.1566 - classification_loss: 1.0766
  22/1000 [..............................] - ETA: 7:21 - loss: 4.2478 - regression_loss: 3.1676 - classification_loss: 1.0802
  23/1000 [..............................] - ETA: 7:21 - loss: 4.2753 - regression_loss: 3.1930 - classification_loss: 1.0823
  24/1000 [..............................] - ETA: 7:20 - loss: 4.2921 - regression_loss: 3.1974 - classification_loss: 1.0947
  25/1000 [..............................] - ETA: 7:20 - loss: 4.1204 - regression_loss: 3.0695 - classification_loss: 1.0509
  26/1000 [..............................] - ETA: 7:20 - loss: 4.1290 - regression_loss: 3.0757 - classification_loss: 1.0533
  27/1000 [..............................] - ETA: 7:19 - loss: 4.2050 - regression_loss: 3.1364 - classification_loss: 1.0686
  28/1000 [..............................] - ETA: 7:19 - loss: 4.2145 - regression_loss: 3.1438 - classification_loss: 1.0707
  29/1000 [..............................] - ETA: 7:18 - loss: 4.2740 - regression_loss: 3.1795 - classification_loss: 1.0945
  30/1000 [..............................] - ETA: 7:17 - loss: 4.3147 - regression_loss: 3.2145 - classification_loss: 1.1003
  31/1000 [..............................] - ETA: 7:17 - loss: 4.3391 - regression_loss: 3.2254 - classification_loss: 1.1138
  32/1000 [..............................] - ETA: 7:16 - loss: 4.3862 - regression_loss: 3.2419 - classification_loss: 1.1444
  33/1000 [..............................] - ETA: 7:16 - loss: 4.4026 - regression_loss: 3.2582 - classification_loss: 1.1445
  34/1000 [>.............................] - ETA: 7:15 - loss: 4.4270 - regression_loss: 3.2835 - classification_loss: 1.1435
  35/1000 [>.............................] - ETA: 7:15 - loss: 4.4313 - regression_loss: 3.2884 - classification_loss: 1.1429
  36/1000 [>.............................] - ETA: 7:15 - loss: 4.4662 - regression_loss: 3.3179 - classification_loss: 1.1482
  37/1000 [>.............................] - ETA: 7:14 - loss: 4.4902 - regression_loss: 3.3277 - classification_loss: 1.1626
  38/1000 [>.............................] - ETA: 7:14 - loss: 4.5000 - regression_loss: 3.3383 - classification_loss: 1.1617
  39/1000 [>.............................] - ETA: 7:13 - loss: 4.5090 - regression_loss: 3.3439 - classification_loss: 1.1651
  40/1000 [>.............................] - ETA: 7:13 - loss: 4.5121 - regression_loss: 3.3434 - classification_loss: 1.1687
  41/1000 [>.............................] - ETA: 7:12 - loss: 4.5205 - regression_loss: 3.3521 - classification_loss: 1.1684
  42/1000 [>.............................] - ETA: 7:12 - loss: 4.5218 - regression_loss: 3.3534 - classification_loss: 1.1684
  43/1000 [>.............................] - ETA: 7:11 - loss: 4.5586 - regression_loss: 3.3668 - classification_loss: 1.1917
  44/1000 [>.............................] - ETA: 7:11 - loss: 4.5654 - regression_loss: 3.3743 - classification_loss: 1.1912
  45/1000 [>.............................] - ETA: 7:10 - loss: 4.5668 - regression_loss: 3.3705 - classification_loss: 1.1962
  46/1000 [>.............................] - ETA: 7:10 - loss: 4.5873 - regression_loss: 3.3920 - classification_loss: 1.1953
  47/1000 [>.............................] - ETA: 7:09 - loss: 4.6154 - regression_loss: 3.4216 - classification_loss: 1.1937
  48/1000 [>.............................] - ETA: 7:09 - loss: 4.5199 - regression_loss: 3.3504 - classification_loss: 1.1696
  49/1000 [>.............................] - ETA: 7:09 - loss: 4.5337 - regression_loss: 3.3569 - classification_loss: 1.1768
  50/1000 [>.............................] - ETA: 7:08 - loss: 4.5485 - regression_loss: 3.3663 - classification_loss: 1.1822
  51/1000 [>.............................] - ETA: 7:08 - loss: 4.5530 - regression_loss: 3.3685 - classification_loss: 1.1845
  52/1000 [>.............................] - ETA: 7:07 - loss: 4.5629 - regression_loss: 3.3731 - classification_loss: 1.1899
  53/1000 [>.............................] - ETA: 7:07 - loss: 4.5653 - regression_loss: 3.3770 - classification_loss: 1.1883
  54/1000 [>.............................] - ETA: 7:07 - loss: 4.4810 - regression_loss: 3.3145 - classification_loss: 1.1665
  55/1000 [>.............................] - ETA: 7:06 - loss: 4.4800 - regression_loss: 3.3144 - classification_loss: 1.1656
  56/1000 [>.............................] - ETA: 7:06 - loss: 4.5003 - regression_loss: 3.3356 - classification_loss: 1.1646
  57/1000 [>.............................] - ETA: 7:05 - loss: 4.4958 - regression_loss: 3.3251 - classification_loss: 1.1707
  58/1000 [>.............................] - ETA: 7:05 - loss: 4.4183 - regression_loss: 3.2678 - classification_loss: 1.1505
  59/1000 [>.............................] - ETA: 7:04 - loss: 4.4339 - regression_loss: 3.2834 - classification_loss: 1.1505
  60/1000 [>.............................] - ETA: 7:04 - loss: 4.4382 - regression_loss: 3.2880 - classification_loss: 1.1503
  61/1000 [>.............................] - ETA: 7:03 - loss: 4.4415 - regression_loss: 3.2918 - classification_loss: 1.1497
  62/1000 [>.............................] - ETA: 7:03 - loss: 4.4413 - regression_loss: 3.2923 - classification_loss: 1.1490
  63/1000 [>.............................] - ETA: 7:02 - loss: 4.4525 - regression_loss: 3.3021 - classification_loss: 1.1504
  64/1000 [>.............................] - ETA: 7:02 - loss: 4.4503 - regression_loss: 3.2986 - classification_loss: 1.1517
  65/1000 [>.............................] - ETA: 7:01 - loss: 4.4544 - regression_loss: 3.3007 - classification_loss: 1.1537
  66/1000 [>.............................] - ETA: 7:01 - loss: 4.4566 - regression_loss: 3.3018 - classification_loss: 1.1548
  67/1000 [=>............................] - ETA: 7:01 - loss: 4.4735 - regression_loss: 3.3159 - classification_loss: 1.1577
  68/1000 [=>............................] - ETA: 7:00 - loss: 4.4790 - regression_loss: 3.3210 - classification_loss: 1.1580
  69/1000 [=>............................] - ETA: 7:00 - loss: 4.4783 - regression_loss: 3.3212 - classification_loss: 1.1572
  70/1000 [=>............................] - ETA: 6:59 - loss: 4.4957 - regression_loss: 3.3358 - classification_loss: 1.1599
  71/1000 [=>............................] - ETA: 6:59 - loss: 4.5082 - regression_loss: 3.3465 - classification_loss: 1.1617
  72/1000 [=>............................] - ETA: 6:58 - loss: 4.5124 - regression_loss: 3.3513 - classification_loss: 1.1611
  73/1000 [=>............................] - ETA: 6:58 - loss: 4.5121 - regression_loss: 3.3511 - classification_loss: 1.1610
  74/1000 [=>............................] - ETA: 6:57 - loss: 4.4511 - regression_loss: 3.3058 - classification_loss: 1.1454
  75/1000 [=>............................] - ETA: 6:57 - loss: 4.4547 - regression_loss: 3.3098 - classification_loss: 1.1449
  76/1000 [=>............................] - ETA: 6:56 - loss: 4.3965 - regression_loss: 3.2663 - classification_loss: 1.1303
  77/1000 [=>............................] - ETA: 6:56 - loss: 4.4013 - regression_loss: 3.2711 - classification_loss: 1.1301
  78/1000 [=>............................] - ETA: 6:55 - loss: 4.4084 - regression_loss: 3.2771 - classification_loss: 1.1313
  79/1000 [=>............................] - ETA: 6:55 - loss: 4.4225 - regression_loss: 3.2888 - classification_loss: 1.1337
  80/1000 [=>............................] - ETA: 6:55 - loss: 4.3674 - regression_loss: 3.2476 - classification_loss: 1.1198
  81/1000 [=>............................] - ETA: 6:54 - loss: 4.3786 - regression_loss: 3.2579 - classification_loss: 1.1207
  82/1000 [=>............................] - ETA: 6:54 - loss: 4.3800 - regression_loss: 3.2575 - classification_loss: 1.1226
  83/1000 [=>............................] - ETA: 6:53 - loss: 4.3895 - regression_loss: 3.2659 - classification_loss: 1.1236
  84/1000 [=>............................] - ETA: 6:53 - loss: 4.3959 - regression_loss: 3.2723 - classification_loss: 1.1236
  85/1000 [=>............................] - ETA: 6:53 - loss: 4.4056 - regression_loss: 3.2823 - classification_loss: 1.1234
  86/1000 [=>............................] - ETA: 6:52 - loss: 4.3545 - regression_loss: 3.2441 - classification_loss: 1.1104
  87/1000 [=>............................] - ETA: 6:52 - loss: 4.3605 - regression_loss: 3.2466 - classification_loss: 1.1140
  88/1000 [=>............................] - ETA: 6:51 - loss: 4.3110 - regression_loss: 3.2097 - classification_loss: 1.1013
  89/1000 [=>............................] - ETA: 6:51 - loss: 4.3150 - regression_loss: 3.2128 - classification_loss: 1.1022
  90/1000 [=>............................] - ETA: 6:50 - loss: 4.3309 - regression_loss: 3.2272 - classification_loss: 1.1037
  91/1000 [=>............................] - ETA: 6:50 - loss: 4.3370 - regression_loss: 3.2294 - classification_loss: 1.1076
  92/1000 [=>............................] - ETA: 6:49 - loss: 4.3389 - regression_loss: 3.2309 - classification_loss: 1.1080
  93/1000 [=>............................] - ETA: 6:49 - loss: 4.2922 - regression_loss: 3.1962 - classification_loss: 1.0960
  94/1000 [=>............................] - ETA: 6:48 - loss: 4.2971 - regression_loss: 3.1997 - classification_loss: 1.0974
  95/1000 [=>............................] - ETA: 6:48 - loss: 4.3051 - regression_loss: 3.2070 - classification_loss: 1.0981
  96/1000 [=>............................] - ETA: 6:47 - loss: 4.3085 - regression_loss: 3.2102 - classification_loss: 1.0983
  97/1000 [=>............................] - ETA: 6:47 - loss: 4.3159 - regression_loss: 3.2167 - classification_loss: 1.0992
  98/1000 [=>............................] - ETA: 6:46 - loss: 4.3204 - regression_loss: 3.2181 - classification_loss: 1.1022
  99/1000 [=>............................] - ETA: 6:46 - loss: 4.3232 - regression_loss: 3.2188 - classification_loss: 1.1044
 100/1000 [==>...........................] - ETA: 6:46 - loss: 4.3300 - regression_loss: 3.2252 - classification_loss: 1.1048
 101/1000 [==>...........................] - ETA: 6:45 - loss: 4.2873 - regression_loss: 3.1933 - classification_loss: 1.0940
 102/1000 [==>...........................] - ETA: 6:45 - loss: 4.2928 - regression_loss: 3.1985 - classification_loss: 1.0943
 103/1000 [==>...........................] - ETA: 6:44 - loss: 4.2934 - regression_loss: 3.1983 - classification_loss: 1.0952
 104/1000 [==>...........................] - ETA: 6:44 - loss: 4.3054 - regression_loss: 3.2072 - classification_loss: 1.0982
 105/1000 [==>...........................] - ETA: 6:44 - loss: 4.3101 - regression_loss: 3.2101 - classification_loss: 1.1000
 106/1000 [==>...........................] - ETA: 6:43 - loss: 4.2694 - regression_loss: 3.1798 - classification_loss: 1.0896
 107/1000 [==>...........................] - ETA: 6:43 - loss: 4.2765 - regression_loss: 3.1835 - classification_loss: 1.0930
 108/1000 [==>...........................] - ETA: 6:42 - loss: 4.2857 - regression_loss: 3.1882 - classification_loss: 1.0975
 109/1000 [==>...........................] - ETA: 6:42 - loss: 4.2464 - regression_loss: 3.1590 - classification_loss: 1.0874
 110/1000 [==>...........................] - ETA: 6:41 - loss: 4.2769 - regression_loss: 3.1873 - classification_loss: 1.0895
 111/1000 [==>...........................] - ETA: 6:41 - loss: 4.2819 - regression_loss: 3.1900 - classification_loss: 1.0919
 112/1000 [==>...........................] - ETA: 6:40 - loss: 4.2852 - regression_loss: 3.1910 - classification_loss: 1.0941
 113/1000 [==>...........................] - ETA: 6:40 - loss: 4.3053 - regression_loss: 3.2032 - classification_loss: 1.1021
 114/1000 [==>...........................] - ETA: 6:40 - loss: 4.3059 - regression_loss: 3.2009 - classification_loss: 1.1050
 115/1000 [==>...........................] - ETA: 6:39 - loss: 4.3108 - regression_loss: 3.2055 - classification_loss: 1.1053
 116/1000 [==>...........................] - ETA: 6:39 - loss: 4.3231 - regression_loss: 3.2154 - classification_loss: 1.1078
 117/1000 [==>...........................] - ETA: 6:38 - loss: 4.2862 - regression_loss: 3.1879 - classification_loss: 1.0983
 118/1000 [==>...........................] - ETA: 6:38 - loss: 4.2499 - regression_loss: 3.1609 - classification_loss: 1.0890
 119/1000 [==>...........................] - ETA: 6:38 - loss: 4.2624 - regression_loss: 3.1732 - classification_loss: 1.0892
 120/1000 [==>...........................] - ETA: 6:37 - loss: 4.2726 - regression_loss: 3.1786 - classification_loss: 1.0941
 121/1000 [==>...........................] - ETA: 6:36 - loss: 4.2792 - regression_loss: 3.1840 - classification_loss: 1.0952
 122/1000 [==>...........................] - ETA: 6:36 - loss: 4.2860 - regression_loss: 3.1900 - classification_loss: 1.0959
 123/1000 [==>...........................] - ETA: 6:36 - loss: 4.2911 - regression_loss: 3.1912 - classification_loss: 1.0999
 124/1000 [==>...........................] - ETA: 6:35 - loss: 4.2565 - regression_loss: 3.1655 - classification_loss: 1.0910
 125/1000 [==>...........................] - ETA: 6:35 - loss: 4.2590 - regression_loss: 3.1672 - classification_loss: 1.0918
 126/1000 [==>...........................] - ETA: 6:34 - loss: 4.2589 - regression_loss: 3.1670 - classification_loss: 1.0920
 127/1000 [==>...........................] - ETA: 6:34 - loss: 4.2632 - regression_loss: 3.1708 - classification_loss: 1.0924
 128/1000 [==>...........................] - ETA: 6:33 - loss: 4.2628 - regression_loss: 3.1675 - classification_loss: 1.0953
 129/1000 [==>...........................] - ETA: 6:33 - loss: 4.2716 - regression_loss: 3.1758 - classification_loss: 1.0957
 130/1000 [==>...........................] - ETA: 6:33 - loss: 4.2387 - regression_loss: 3.1514 - classification_loss: 1.0873
 131/1000 [==>...........................] - ETA: 6:32 - loss: 4.2422 - regression_loss: 3.1546 - classification_loss: 1.0877
 132/1000 [==>...........................] - ETA: 6:32 - loss: 4.2471 - regression_loss: 3.1552 - classification_loss: 1.0920
 133/1000 [==>...........................] - ETA: 6:31 - loss: 4.2157 - regression_loss: 3.1315 - classification_loss: 1.0842
 134/1000 [===>..........................] - ETA: 6:31 - loss: 4.2185 - regression_loss: 3.1335 - classification_loss: 1.0849
 135/1000 [===>..........................] - ETA: 6:30 - loss: 4.1872 - regression_loss: 3.1103 - classification_loss: 1.0769
 136/1000 [===>..........................] - ETA: 6:30 - loss: 4.1881 - regression_loss: 3.1103 - classification_loss: 1.0778
 137/1000 [===>..........................] - ETA: 6:29 - loss: 4.1575 - regression_loss: 3.0876 - classification_loss: 1.0700
 138/1000 [===>..........................] - ETA: 6:29 - loss: 4.1643 - regression_loss: 3.0913 - classification_loss: 1.0730
 139/1000 [===>..........................] - ETA: 6:28 - loss: 4.1629 - regression_loss: 3.0895 - classification_loss: 1.0734
 140/1000 [===>..........................] - ETA: 6:28 - loss: 4.1701 - regression_loss: 3.0964 - classification_loss: 1.0737
 141/1000 [===>..........................] - ETA: 6:27 - loss: 4.1740 - regression_loss: 3.0982 - classification_loss: 1.0757
 142/1000 [===>..........................] - ETA: 6:27 - loss: 4.1736 - regression_loss: 3.0973 - classification_loss: 1.0764
 143/1000 [===>..........................] - ETA: 6:27 - loss: 4.1444 - regression_loss: 3.0756 - classification_loss: 1.0688
 144/1000 [===>..........................] - ETA: 6:26 - loss: 4.1157 - regression_loss: 3.0542 - classification_loss: 1.0614
 145/1000 [===>..........................] - ETA: 6:26 - loss: 4.1256 - regression_loss: 3.0585 - classification_loss: 1.0671
 146/1000 [===>..........................] - ETA: 6:25 - loss: 4.0973 - regression_loss: 3.0375 - classification_loss: 1.0598
 147/1000 [===>..........................] - ETA: 6:25 - loss: 4.0998 - regression_loss: 3.0393 - classification_loss: 1.0604
 148/1000 [===>..........................] - ETA: 6:24 - loss: 4.1004 - regression_loss: 3.0398 - classification_loss: 1.0606
 149/1000 [===>..........................] - ETA: 6:24 - loss: 4.0729 - regression_loss: 3.0194 - classification_loss: 1.0535
 150/1000 [===>..........................] - ETA: 6:23 - loss: 4.0458 - regression_loss: 2.9992 - classification_loss: 1.0465
 151/1000 [===>..........................] - ETA: 6:23 - loss: 4.0505 - regression_loss: 3.0024 - classification_loss: 1.0481
 152/1000 [===>..........................] - ETA: 6:23 - loss: 4.0532 - regression_loss: 3.0029 - classification_loss: 1.0503
 153/1000 [===>..........................] - ETA: 6:22 - loss: 4.0676 - regression_loss: 3.0110 - classification_loss: 1.0566
 154/1000 [===>..........................] - ETA: 6:22 - loss: 4.0788 - regression_loss: 3.0136 - classification_loss: 1.0652
 155/1000 [===>..........................] - ETA: 6:21 - loss: 4.0799 - regression_loss: 3.0139 - classification_loss: 1.0660
 156/1000 [===>..........................] - ETA: 6:21 - loss: 4.0537 - regression_loss: 2.9946 - classification_loss: 1.0591
 157/1000 [===>..........................] - ETA: 6:20 - loss: 4.0627 - regression_loss: 3.0021 - classification_loss: 1.0606
 158/1000 [===>..........................] - ETA: 6:20 - loss: 4.0626 - regression_loss: 3.0014 - classification_loss: 1.0612
 159/1000 [===>..........................] - ETA: 6:19 - loss: 4.0373 - regression_loss: 2.9826 - classification_loss: 1.0547
 160/1000 [===>..........................] - ETA: 6:19 - loss: 4.0120 - regression_loss: 2.9639 - classification_loss: 1.0481
 161/1000 [===>..........................] - ETA: 6:19 - loss: 4.0230 - regression_loss: 2.9678 - classification_loss: 1.0552
 162/1000 [===>..........................] - ETA: 6:18 - loss: 4.0232 - regression_loss: 2.9677 - classification_loss: 1.0555
 163/1000 [===>..........................] - ETA: 6:18 - loss: 4.0264 - regression_loss: 2.9689 - classification_loss: 1.0576
 164/1000 [===>..........................] - ETA: 6:17 - loss: 4.0354 - regression_loss: 2.9736 - classification_loss: 1.0618
 165/1000 [===>..........................] - ETA: 6:17 - loss: 4.0394 - regression_loss: 2.9774 - classification_loss: 1.0620
 166/1000 [===>..........................] - ETA: 6:16 - loss: 4.0479 - regression_loss: 2.9792 - classification_loss: 1.0687
 167/1000 [====>.........................] - ETA: 6:16 - loss: 4.0491 - regression_loss: 2.9802 - classification_loss: 1.0689
 168/1000 [====>.........................] - ETA: 6:15 - loss: 4.0549 - regression_loss: 2.9833 - classification_loss: 1.0716
 169/1000 [====>.........................] - ETA: 6:15 - loss: 4.0625 - regression_loss: 2.9906 - classification_loss: 1.0719
 170/1000 [====>.........................] - ETA: 6:15 - loss: 4.0386 - regression_loss: 2.9730 - classification_loss: 1.0656
 171/1000 [====>.........................] - ETA: 6:14 - loss: 4.0433 - regression_loss: 2.9751 - classification_loss: 1.0683
 172/1000 [====>.........................] - ETA: 6:14 - loss: 4.0554 - regression_loss: 2.9787 - classification_loss: 1.0767
 173/1000 [====>.........................] - ETA: 6:13 - loss: 4.0689 - regression_loss: 2.9887 - classification_loss: 1.0802
 174/1000 [====>.........................] - ETA: 6:13 - loss: 4.0762 - regression_loss: 2.9950 - classification_loss: 1.0811
 175/1000 [====>.........................] - ETA: 6:12 - loss: 4.0755 - regression_loss: 2.9928 - classification_loss: 1.0827
 176/1000 [====>.........................] - ETA: 6:12 - loss: 4.0775 - regression_loss: 2.9948 - classification_loss: 1.0827
 177/1000 [====>.........................] - ETA: 6:11 - loss: 4.0545 - regression_loss: 2.9778 - classification_loss: 1.0766
 178/1000 [====>.........................] - ETA: 6:11 - loss: 4.0607 - regression_loss: 2.9839 - classification_loss: 1.0768
 179/1000 [====>.........................] - ETA: 6:11 - loss: 4.0649 - regression_loss: 2.9861 - classification_loss: 1.0787
 180/1000 [====>.........................] - ETA: 6:10 - loss: 4.0423 - regression_loss: 2.9695 - classification_loss: 1.0727
 181/1000 [====>.........................] - ETA: 6:10 - loss: 4.0440 - regression_loss: 2.9710 - classification_loss: 1.0730
 182/1000 [====>.........................] - ETA: 6:09 - loss: 4.0474 - regression_loss: 2.9730 - classification_loss: 1.0743
 183/1000 [====>.........................] - ETA: 6:09 - loss: 4.0513 - regression_loss: 2.9764 - classification_loss: 1.0749
 184/1000 [====>.........................] - ETA: 6:08 - loss: 4.0540 - regression_loss: 2.9786 - classification_loss: 1.0754
 185/1000 [====>.........................] - ETA: 6:08 - loss: 4.0583 - regression_loss: 2.9824 - classification_loss: 1.0760
 186/1000 [====>.........................] - ETA: 6:07 - loss: 4.0594 - regression_loss: 2.9832 - classification_loss: 1.0762
 187/1000 [====>.........................] - ETA: 6:07 - loss: 4.0645 - regression_loss: 2.9872 - classification_loss: 1.0773
 188/1000 [====>.........................] - ETA: 6:06 - loss: 4.0686 - regression_loss: 2.9910 - classification_loss: 1.0776
 189/1000 [====>.........................] - ETA: 6:06 - loss: 4.0723 - regression_loss: 2.9941 - classification_loss: 1.0782
 190/1000 [====>.........................] - ETA: 6:06 - loss: 4.0775 - regression_loss: 2.9987 - classification_loss: 1.0788
 191/1000 [====>.........................] - ETA: 6:05 - loss: 4.0853 - regression_loss: 3.0051 - classification_loss: 1.0802
 192/1000 [====>.........................] - ETA: 6:05 - loss: 4.0935 - regression_loss: 3.0124 - classification_loss: 1.0810
 193/1000 [====>.........................] - ETA: 6:04 - loss: 4.0957 - regression_loss: 3.0132 - classification_loss: 1.0825
 194/1000 [====>.........................] - ETA: 6:04 - loss: 4.0746 - regression_loss: 2.9976 - classification_loss: 1.0769
 195/1000 [====>.........................] - ETA: 6:03 - loss: 4.0537 - regression_loss: 2.9823 - classification_loss: 1.0714
 196/1000 [====>.........................] - ETA: 6:03 - loss: 4.0579 - regression_loss: 2.9862 - classification_loss: 1.0717
 197/1000 [====>.........................] - ETA: 6:02 - loss: 4.0635 - regression_loss: 2.9914 - classification_loss: 1.0722
 198/1000 [====>.........................] - ETA: 6:02 - loss: 4.0655 - regression_loss: 2.9930 - classification_loss: 1.0725
 199/1000 [====>.........................] - ETA: 6:02 - loss: 4.0661 - regression_loss: 2.9936 - classification_loss: 1.0725
 200/1000 [=====>........................] - ETA: 6:01 - loss: 4.0681 - regression_loss: 2.9952 - classification_loss: 1.0729
 201/1000 [=====>........................] - ETA: 6:01 - loss: 4.0728 - regression_loss: 2.9997 - classification_loss: 1.0731
 202/1000 [=====>........................] - ETA: 6:00 - loss: 4.0911 - regression_loss: 3.0161 - classification_loss: 1.0750
 203/1000 [=====>........................] - ETA: 6:00 - loss: 4.0710 - regression_loss: 3.0012 - classification_loss: 1.0697
 204/1000 [=====>........................] - ETA: 5:59 - loss: 4.0815 - regression_loss: 3.0072 - classification_loss: 1.0743
 205/1000 [=====>........................] - ETA: 5:59 - loss: 4.0856 - regression_loss: 3.0085 - classification_loss: 1.0771
 206/1000 [=====>........................] - ETA: 5:58 - loss: 4.0915 - regression_loss: 3.0142 - classification_loss: 1.0772
 207/1000 [=====>........................] - ETA: 5:58 - loss: 4.0958 - regression_loss: 3.0175 - classification_loss: 1.0783
 208/1000 [=====>........................] - ETA: 5:58 - loss: 4.1010 - regression_loss: 3.0214 - classification_loss: 1.0796
 209/1000 [=====>........................] - ETA: 5:57 - loss: 4.1069 - regression_loss: 3.0274 - classification_loss: 1.0796
 210/1000 [=====>........................] - ETA: 5:57 - loss: 4.1153 - regression_loss: 3.0322 - classification_loss: 1.0831
 211/1000 [=====>........................] - ETA: 5:56 - loss: 4.0958 - regression_loss: 3.0178 - classification_loss: 1.0780
 212/1000 [=====>........................] - ETA: 5:56 - loss: 4.0983 - regression_loss: 3.0199 - classification_loss: 1.0784
 213/1000 [=====>........................] - ETA: 5:55 - loss: 4.0791 - regression_loss: 3.0058 - classification_loss: 1.0734
 214/1000 [=====>........................] - ETA: 5:55 - loss: 4.0793 - regression_loss: 3.0043 - classification_loss: 1.0750
 215/1000 [=====>........................] - ETA: 5:54 - loss: 4.0853 - regression_loss: 3.0080 - classification_loss: 1.0773
 216/1000 [=====>........................] - ETA: 5:54 - loss: 4.0916 - regression_loss: 3.0079 - classification_loss: 1.0837
 217/1000 [=====>........................] - ETA: 5:53 - loss: 4.0728 - regression_loss: 2.9941 - classification_loss: 1.0787
 218/1000 [=====>........................] - ETA: 5:53 - loss: 4.0751 - regression_loss: 2.9962 - classification_loss: 1.0788
 219/1000 [=====>........................] - ETA: 5:53 - loss: 4.0764 - regression_loss: 2.9976 - classification_loss: 1.0788
 220/1000 [=====>........................] - ETA: 5:52 - loss: 4.0579 - regression_loss: 2.9839 - classification_loss: 1.0739
 221/1000 [=====>........................] - ETA: 5:52 - loss: 4.0626 - regression_loss: 2.9880 - classification_loss: 1.0745
 222/1000 [=====>........................] - ETA: 5:51 - loss: 4.0647 - regression_loss: 2.9894 - classification_loss: 1.0753
 223/1000 [=====>........................] - ETA: 5:51 - loss: 4.0669 - regression_loss: 2.9905 - classification_loss: 1.0764
 224/1000 [=====>........................] - ETA: 5:50 - loss: 4.0728 - regression_loss: 2.9942 - classification_loss: 1.0786
 225/1000 [=====>........................] - ETA: 5:50 - loss: 4.0778 - regression_loss: 2.9988 - classification_loss: 1.0790
 226/1000 [=====>........................] - ETA: 5:49 - loss: 4.0819 - regression_loss: 3.0030 - classification_loss: 1.0789
 227/1000 [=====>........................] - ETA: 5:49 - loss: 4.0878 - regression_loss: 3.0063 - classification_loss: 1.0814
 228/1000 [=====>........................] - ETA: 5:49 - loss: 4.0946 - regression_loss: 3.0112 - classification_loss: 1.0834
 229/1000 [=====>........................] - ETA: 5:48 - loss: 4.0962 - regression_loss: 3.0127 - classification_loss: 1.0835
 230/1000 [=====>........................] - ETA: 5:48 - loss: 4.1020 - regression_loss: 3.0186 - classification_loss: 1.0835
 231/1000 [=====>........................] - ETA: 5:47 - loss: 4.1026 - regression_loss: 3.0188 - classification_loss: 1.0838
 232/1000 [=====>........................] - ETA: 5:47 - loss: 4.0850 - regression_loss: 3.0058 - classification_loss: 1.0791
 233/1000 [=====>........................] - ETA: 5:46 - loss: 4.0901 - regression_loss: 3.0070 - classification_loss: 1.0831
 234/1000 [======>.......................] - ETA: 5:46 - loss: 4.0930 - regression_loss: 3.0093 - classification_loss: 1.0836
 235/1000 [======>.......................] - ETA: 5:45 - loss: 4.0979 - regression_loss: 3.0144 - classification_loss: 1.0835
 236/1000 [======>.......................] - ETA: 5:45 - loss: 4.0805 - regression_loss: 3.0016 - classification_loss: 1.0789
 237/1000 [======>.......................] - ETA: 5:44 - loss: 4.0849 - regression_loss: 3.0045 - classification_loss: 1.0804
 238/1000 [======>.......................] - ETA: 5:44 - loss: 4.0903 - regression_loss: 3.0100 - classification_loss: 1.0803
 239/1000 [======>.......................] - ETA: 5:44 - loss: 4.0981 - regression_loss: 3.0169 - classification_loss: 1.0812
 240/1000 [======>.......................] - ETA: 5:43 - loss: 4.1015 - regression_loss: 3.0201 - classification_loss: 1.0815
 241/1000 [======>.......................] - ETA: 5:43 - loss: 4.1064 - regression_loss: 3.0234 - classification_loss: 1.0829
 242/1000 [======>.......................] - ETA: 5:42 - loss: 4.1089 - regression_loss: 3.0253 - classification_loss: 1.0836
 243/1000 [======>.......................] - ETA: 5:42 - loss: 4.1111 - regression_loss: 3.0262 - classification_loss: 1.0849
 244/1000 [======>.......................] - ETA: 5:41 - loss: 4.1144 - regression_loss: 3.0277 - classification_loss: 1.0866
 245/1000 [======>.......................] - ETA: 5:41 - loss: 4.1170 - regression_loss: 3.0305 - classification_loss: 1.0865
 246/1000 [======>.......................] - ETA: 5:40 - loss: 4.1195 - regression_loss: 3.0316 - classification_loss: 1.0878
 247/1000 [======>.......................] - ETA: 5:40 - loss: 4.1232 - regression_loss: 3.0328 - classification_loss: 1.0904
 248/1000 [======>.......................] - ETA: 5:40 - loss: 4.1253 - regression_loss: 3.0346 - classification_loss: 1.0907
 249/1000 [======>.......................] - ETA: 5:39 - loss: 4.1273 - regression_loss: 3.0367 - classification_loss: 1.0905
 250/1000 [======>.......................] - ETA: 5:39 - loss: 4.1291 - regression_loss: 3.0387 - classification_loss: 1.0904
 251/1000 [======>.......................] - ETA: 5:38 - loss: 4.1127 - regression_loss: 3.0266 - classification_loss: 1.0860
 252/1000 [======>.......................] - ETA: 5:38 - loss: 4.0965 - regression_loss: 3.0146 - classification_loss: 1.0819
 253/1000 [======>.......................] - ETA: 5:37 - loss: 4.0975 - regression_loss: 3.0150 - classification_loss: 1.0826
 254/1000 [======>.......................] - ETA: 5:37 - loss: 4.0984 - regression_loss: 3.0149 - classification_loss: 1.0834
 255/1000 [======>.......................] - ETA: 5:36 - loss: 4.0823 - regression_loss: 3.0031 - classification_loss: 1.0792
 256/1000 [======>.......................] - ETA: 5:36 - loss: 4.0856 - regression_loss: 3.0056 - classification_loss: 1.0801
 257/1000 [======>.......................] - ETA: 5:36 - loss: 4.0880 - regression_loss: 3.0079 - classification_loss: 1.0801
 258/1000 [======>.......................] - ETA: 5:35 - loss: 4.0725 - regression_loss: 2.9962 - classification_loss: 1.0763
 259/1000 [======>.......................] - ETA: 5:34 - loss: 4.0750 - regression_loss: 2.9988 - classification_loss: 1.0762
 260/1000 [======>.......................] - ETA: 5:34 - loss: 4.0765 - regression_loss: 3.0003 - classification_loss: 1.0761
 261/1000 [======>.......................] - ETA: 5:34 - loss: 4.0807 - regression_loss: 3.0039 - classification_loss: 1.0768
 262/1000 [======>.......................] - ETA: 5:33 - loss: 4.0842 - regression_loss: 3.0075 - classification_loss: 1.0767
 263/1000 [======>.......................] - ETA: 5:33 - loss: 4.0893 - regression_loss: 3.0114 - classification_loss: 1.0779
 264/1000 [======>.......................] - ETA: 5:32 - loss: 4.0963 - regression_loss: 3.0161 - classification_loss: 1.0803
 265/1000 [======>.......................] - ETA: 5:32 - loss: 4.1012 - regression_loss: 3.0194 - classification_loss: 1.0818
 266/1000 [======>.......................] - ETA: 5:31 - loss: 4.1060 - regression_loss: 3.0215 - classification_loss: 1.0846
 267/1000 [=======>......................] - ETA: 5:31 - loss: 4.1082 - regression_loss: 3.0230 - classification_loss: 1.0851
 268/1000 [=======>......................] - ETA: 5:30 - loss: 4.1109 - regression_loss: 3.0248 - classification_loss: 1.0860
 269/1000 [=======>......................] - ETA: 5:30 - loss: 4.1132 - regression_loss: 3.0255 - classification_loss: 1.0877
 270/1000 [=======>......................] - ETA: 5:29 - loss: 4.1168 - regression_loss: 3.0278 - classification_loss: 1.0889
 271/1000 [=======>......................] - ETA: 5:29 - loss: 4.1170 - regression_loss: 3.0282 - classification_loss: 1.0888
 272/1000 [=======>......................] - ETA: 5:29 - loss: 4.1196 - regression_loss: 3.0300 - classification_loss: 1.0895
 273/1000 [=======>......................] - ETA: 5:28 - loss: 4.1216 - regression_loss: 3.0319 - classification_loss: 1.0897
 274/1000 [=======>......................] - ETA: 5:28 - loss: 4.1067 - regression_loss: 3.0209 - classification_loss: 1.0858
 275/1000 [=======>......................] - ETA: 5:27 - loss: 4.0918 - regression_loss: 3.0099 - classification_loss: 1.0819
 276/1000 [=======>......................] - ETA: 5:27 - loss: 4.0938 - regression_loss: 3.0113 - classification_loss: 1.0824
 277/1000 [=======>......................] - ETA: 5:26 - loss: 4.0790 - regression_loss: 3.0005 - classification_loss: 1.0785
 278/1000 [=======>......................] - ETA: 5:26 - loss: 4.0643 - regression_loss: 2.9897 - classification_loss: 1.0747
 279/1000 [=======>......................] - ETA: 5:25 - loss: 4.0498 - regression_loss: 2.9789 - classification_loss: 1.0708
 280/1000 [=======>......................] - ETA: 5:25 - loss: 4.0543 - regression_loss: 2.9821 - classification_loss: 1.0722
 281/1000 [=======>......................] - ETA: 5:25 - loss: 4.0548 - regression_loss: 2.9817 - classification_loss: 1.0731
 282/1000 [=======>......................] - ETA: 5:24 - loss: 4.0572 - regression_loss: 2.9842 - classification_loss: 1.0731
 283/1000 [=======>......................] - ETA: 5:24 - loss: 4.0592 - regression_loss: 2.9825 - classification_loss: 1.0767
 284/1000 [=======>......................] - ETA: 5:23 - loss: 4.0598 - regression_loss: 2.9823 - classification_loss: 1.0775
 285/1000 [=======>......................] - ETA: 5:23 - loss: 4.0456 - regression_loss: 2.9719 - classification_loss: 1.0737
 286/1000 [=======>......................] - ETA: 5:22 - loss: 4.0314 - regression_loss: 2.9615 - classification_loss: 1.0700
 287/1000 [=======>......................] - ETA: 5:22 - loss: 4.0334 - regression_loss: 2.9625 - classification_loss: 1.0709
 288/1000 [=======>......................] - ETA: 5:21 - loss: 4.0373 - regression_loss: 2.9650 - classification_loss: 1.0723
 289/1000 [=======>......................] - ETA: 5:21 - loss: 4.0407 - regression_loss: 2.9663 - classification_loss: 1.0745
 290/1000 [=======>......................] - ETA: 5:21 - loss: 4.0442 - regression_loss: 2.9675 - classification_loss: 1.0767
 291/1000 [=======>......................] - ETA: 5:20 - loss: 4.0303 - regression_loss: 2.9573 - classification_loss: 1.0730
 292/1000 [=======>......................] - ETA: 5:20 - loss: 4.0165 - regression_loss: 2.9472 - classification_loss: 1.0693
 293/1000 [=======>......................] - ETA: 5:19 - loss: 4.0028 - regression_loss: 2.9371 - classification_loss: 1.0656
 294/1000 [=======>......................] - ETA: 5:19 - loss: 3.9892 - regression_loss: 2.9271 - classification_loss: 1.0620
 295/1000 [=======>......................] - ETA: 5:18 - loss: 3.9926 - regression_loss: 2.9287 - classification_loss: 1.0639
 296/1000 [=======>......................] - ETA: 5:18 - loss: 3.9963 - regression_loss: 2.9309 - classification_loss: 1.0654
 297/1000 [=======>......................] - ETA: 5:17 - loss: 4.0005 - regression_loss: 2.9316 - classification_loss: 1.0689
 298/1000 [=======>......................] - ETA: 5:17 - loss: 4.0045 - regression_loss: 2.9333 - classification_loss: 1.0711
 299/1000 [=======>......................] - ETA: 5:16 - loss: 4.0126 - regression_loss: 2.9381 - classification_loss: 1.0745
 300/1000 [========>.....................] - ETA: 5:16 - loss: 4.0156 - regression_loss: 2.9404 - classification_loss: 1.0752
 301/1000 [========>.....................] - ETA: 5:16 - loss: 4.0200 - regression_loss: 2.9434 - classification_loss: 1.0767
 302/1000 [========>.....................] - ETA: 5:15 - loss: 4.0287 - regression_loss: 2.9482 - classification_loss: 1.0805
 303/1000 [========>.....................] - ETA: 5:15 - loss: 4.0310 - regression_loss: 2.9487 - classification_loss: 1.0823
 304/1000 [========>.....................] - ETA: 5:14 - loss: 4.0356 - regression_loss: 2.9504 - classification_loss: 1.0852
 305/1000 [========>.....................] - ETA: 5:14 - loss: 4.0224 - regression_loss: 2.9407 - classification_loss: 1.0817
 306/1000 [========>.....................] - ETA: 5:13 - loss: 4.0248 - regression_loss: 2.9411 - classification_loss: 1.0837
 307/1000 [========>.....................] - ETA: 5:13 - loss: 4.0286 - regression_loss: 2.9438 - classification_loss: 1.0848
 308/1000 [========>.....................] - ETA: 5:12 - loss: 4.0326 - regression_loss: 2.9465 - classification_loss: 1.0861
 309/1000 [========>.....................] - ETA: 5:12 - loss: 4.0196 - regression_loss: 2.9370 - classification_loss: 1.0826
 310/1000 [========>.....................] - ETA: 5:12 - loss: 4.0066 - regression_loss: 2.9275 - classification_loss: 1.0791
 311/1000 [========>.....................] - ETA: 5:11 - loss: 4.0092 - regression_loss: 2.9302 - classification_loss: 1.0790
 312/1000 [========>.....................] - ETA: 5:11 - loss: 3.9963 - regression_loss: 2.9208 - classification_loss: 1.0756
 313/1000 [========>.....................] - ETA: 5:10 - loss: 3.9979 - regression_loss: 2.9221 - classification_loss: 1.0757
 314/1000 [========>.....................] - ETA: 5:10 - loss: 3.9987 - regression_loss: 2.9231 - classification_loss: 1.0757
 315/1000 [========>.....................] - ETA: 5:09 - loss: 3.9860 - regression_loss: 2.9138 - classification_loss: 1.0722
 316/1000 [========>.....................] - ETA: 5:09 - loss: 3.9896 - regression_loss: 2.9158 - classification_loss: 1.0738
 317/1000 [========>.....................] - ETA: 5:08 - loss: 3.9770 - regression_loss: 2.9066 - classification_loss: 1.0704
 318/1000 [========>.....................] - ETA: 5:08 - loss: 3.9793 - regression_loss: 2.9089 - classification_loss: 1.0704
 319/1000 [========>.....................] - ETA: 5:08 - loss: 3.9807 - regression_loss: 2.9101 - classification_loss: 1.0706
 320/1000 [========>.....................] - ETA: 5:07 - loss: 3.9859 - regression_loss: 2.9144 - classification_loss: 1.0715
 321/1000 [========>.....................] - ETA: 5:07 - loss: 3.9913 - regression_loss: 2.9166 - classification_loss: 1.0748
 322/1000 [========>.....................] - ETA: 5:06 - loss: 3.9940 - regression_loss: 2.9181 - classification_loss: 1.0759
 323/1000 [========>.....................] - ETA: 5:06 - loss: 3.9816 - regression_loss: 2.9091 - classification_loss: 1.0725
 324/1000 [========>.....................] - ETA: 5:05 - loss: 3.9827 - regression_loss: 2.9103 - classification_loss: 1.0725
 325/1000 [========>.....................] - ETA: 5:05 - loss: 3.9705 - regression_loss: 2.9013 - classification_loss: 1.0692
 326/1000 [========>.....................] - ETA: 5:04 - loss: 3.9721 - regression_loss: 2.9011 - classification_loss: 1.0710
 327/1000 [========>.....................] - ETA: 5:04 - loss: 3.9600 - regression_loss: 2.8923 - classification_loss: 1.0677
 328/1000 [========>.....................] - ETA: 5:03 - loss: 3.9479 - regression_loss: 2.8834 - classification_loss: 1.0645
 329/1000 [========>.....................] - ETA: 5:03 - loss: 3.9641 - regression_loss: 2.8939 - classification_loss: 1.0702
 330/1000 [========>.....................] - ETA: 5:03 - loss: 3.9522 - regression_loss: 2.8852 - classification_loss: 1.0671
 331/1000 [========>.....................] - ETA: 5:02 - loss: 3.9403 - regression_loss: 2.8764 - classification_loss: 1.0639
 332/1000 [========>.....................] - ETA: 5:02 - loss: 3.9433 - regression_loss: 2.8778 - classification_loss: 1.0656
 333/1000 [========>.....................] - ETA: 5:01 - loss: 3.9437 - regression_loss: 2.8783 - classification_loss: 1.0654
 334/1000 [=========>....................] - ETA: 5:01 - loss: 3.9508 - regression_loss: 2.8810 - classification_loss: 1.0698
 335/1000 [=========>....................] - ETA: 5:00 - loss: 3.9536 - regression_loss: 2.8836 - classification_loss: 1.0701
 336/1000 [=========>....................] - ETA: 5:00 - loss: 3.9570 - regression_loss: 2.8843 - classification_loss: 1.0727
 337/1000 [=========>....................] - ETA: 4:59 - loss: 3.9602 - regression_loss: 2.8873 - classification_loss: 1.0729
 338/1000 [=========>....................] - ETA: 4:59 - loss: 3.9626 - regression_loss: 2.8896 - classification_loss: 1.0730
 339/1000 [=========>....................] - ETA: 4:59 - loss: 3.9637 - regression_loss: 2.8891 - classification_loss: 1.0746
 340/1000 [=========>....................] - ETA: 4:58 - loss: 3.9681 - regression_loss: 2.8902 - classification_loss: 1.0779
 341/1000 [=========>....................] - ETA: 4:58 - loss: 3.9724 - regression_loss: 2.8926 - classification_loss: 1.0798
 342/1000 [=========>....................] - ETA: 4:57 - loss: 3.9734 - regression_loss: 2.8927 - classification_loss: 1.0806
 343/1000 [=========>....................] - ETA: 4:57 - loss: 3.9780 - regression_loss: 2.8948 - classification_loss: 1.0832
 344/1000 [=========>....................] - ETA: 4:56 - loss: 3.9808 - regression_loss: 2.8967 - classification_loss: 1.0841
 345/1000 [=========>....................] - ETA: 4:56 - loss: 3.9920 - regression_loss: 2.9070 - classification_loss: 1.0850
 346/1000 [=========>....................] - ETA: 4:55 - loss: 3.9958 - regression_loss: 2.9104 - classification_loss: 1.0853
 347/1000 [=========>....................] - ETA: 4:55 - loss: 3.9994 - regression_loss: 2.9141 - classification_loss: 1.0852
 348/1000 [=========>....................] - ETA: 4:54 - loss: 4.0016 - regression_loss: 2.9156 - classification_loss: 1.0860
 349/1000 [=========>....................] - ETA: 4:54 - loss: 4.0043 - regression_loss: 2.9179 - classification_loss: 1.0864
 350/1000 [=========>....................] - ETA: 4:53 - loss: 4.0076 - regression_loss: 2.9208 - classification_loss: 1.0868
 351/1000 [=========>....................] - ETA: 4:53 - loss: 4.0087 - regression_loss: 2.9210 - classification_loss: 1.0877
 352/1000 [=========>....................] - ETA: 4:52 - loss: 3.9973 - regression_loss: 2.9127 - classification_loss: 1.0847
 353/1000 [=========>....................] - ETA: 4:52 - loss: 3.9988 - regression_loss: 2.9142 - classification_loss: 1.0846
 354/1000 [=========>....................] - ETA: 4:51 - loss: 4.0011 - regression_loss: 2.9164 - classification_loss: 1.0847
 355/1000 [=========>....................] - ETA: 4:51 - loss: 4.0045 - regression_loss: 2.9182 - classification_loss: 1.0863
 356/1000 [=========>....................] - ETA: 4:51 - loss: 4.0052 - regression_loss: 2.9188 - classification_loss: 1.0863
 357/1000 [=========>....................] - ETA: 4:50 - loss: 4.0068 - regression_loss: 2.9205 - classification_loss: 1.0863
 358/1000 [=========>....................] - ETA: 4:50 - loss: 4.0111 - regression_loss: 2.9245 - classification_loss: 1.0866
 359/1000 [=========>....................] - ETA: 4:49 - loss: 4.0000 - regression_loss: 2.9163 - classification_loss: 1.0836
 360/1000 [=========>....................] - ETA: 4:49 - loss: 4.0045 - regression_loss: 2.9199 - classification_loss: 1.0846
 361/1000 [=========>....................] - ETA: 4:48 - loss: 4.0068 - regression_loss: 2.9223 - classification_loss: 1.0845
 362/1000 [=========>....................] - ETA: 4:48 - loss: 4.0091 - regression_loss: 2.9243 - classification_loss: 1.0849
 363/1000 [=========>....................] - ETA: 4:47 - loss: 4.0134 - regression_loss: 2.9278 - classification_loss: 1.0856
 364/1000 [=========>....................] - ETA: 4:47 - loss: 4.0130 - regression_loss: 2.9269 - classification_loss: 1.0861
 365/1000 [=========>....................] - ETA: 4:47 - loss: 4.0153 - regression_loss: 2.9288 - classification_loss: 1.0865
 366/1000 [=========>....................] - ETA: 4:46 - loss: 4.0044 - regression_loss: 2.9208 - classification_loss: 1.0836
 367/1000 [==========>...................] - ETA: 4:46 - loss: 4.0095 - regression_loss: 2.9259 - classification_loss: 1.0836
 368/1000 [==========>...................] - ETA: 4:45 - loss: 3.9986 - regression_loss: 2.9179 - classification_loss: 1.0807
 369/1000 [==========>...................] - ETA: 4:45 - loss: 3.9878 - regression_loss: 2.9100 - classification_loss: 1.0778
 370/1000 [==========>...................] - ETA: 4:44 - loss: 3.9902 - regression_loss: 2.9121 - classification_loss: 1.0781
 371/1000 [==========>...................] - ETA: 4:44 - loss: 3.9917 - regression_loss: 2.9137 - classification_loss: 1.0780
 372/1000 [==========>...................] - ETA: 4:43 - loss: 3.9933 - regression_loss: 2.9149 - classification_loss: 1.0784
 373/1000 [==========>...................] - ETA: 4:43 - loss: 3.9946 - regression_loss: 2.9163 - classification_loss: 1.0783
 374/1000 [==========>...................] - ETA: 4:43 - loss: 3.9839 - regression_loss: 2.9085 - classification_loss: 1.0754
 375/1000 [==========>...................] - ETA: 4:42 - loss: 3.9733 - regression_loss: 2.9008 - classification_loss: 1.0726
 376/1000 [==========>...................] - ETA: 4:42 - loss: 3.9775 - regression_loss: 2.9033 - classification_loss: 1.0742
 377/1000 [==========>...................] - ETA: 4:41 - loss: 3.9804 - regression_loss: 2.9057 - classification_loss: 1.0748
 378/1000 [==========>...................] - ETA: 4:41 - loss: 3.9844 - regression_loss: 2.9090 - classification_loss: 1.0754
 379/1000 [==========>...................] - ETA: 4:40 - loss: 3.9874 - regression_loss: 2.9107 - classification_loss: 1.0768
 380/1000 [==========>...................] - ETA: 4:40 - loss: 3.9901 - regression_loss: 2.9123 - classification_loss: 1.0777
 381/1000 [==========>...................] - ETA: 4:39 - loss: 3.9919 - regression_loss: 2.9140 - classification_loss: 1.0779
 382/1000 [==========>...................] - ETA: 4:39 - loss: 3.9928 - regression_loss: 2.9150 - classification_loss: 1.0778
 383/1000 [==========>...................] - ETA: 4:38 - loss: 3.9957 - regression_loss: 2.9170 - classification_loss: 1.0787
 384/1000 [==========>...................] - ETA: 4:38 - loss: 3.9973 - regression_loss: 2.9185 - classification_loss: 1.0788
 385/1000 [==========>...................] - ETA: 4:38 - loss: 4.0030 - regression_loss: 2.9201 - classification_loss: 1.0829
 386/1000 [==========>...................] - ETA: 4:37 - loss: 3.9927 - regression_loss: 2.9126 - classification_loss: 1.0802
 387/1000 [==========>...................] - ETA: 4:37 - loss: 3.9967 - regression_loss: 2.9164 - classification_loss: 1.0803
 388/1000 [==========>...................] - ETA: 4:36 - loss: 3.9976 - regression_loss: 2.9173 - classification_loss: 1.0803
 389/1000 [==========>...................] - ETA: 4:36 - loss: 3.9981 - regression_loss: 2.9180 - classification_loss: 1.0800
 390/1000 [==========>...................] - ETA: 4:35 - loss: 3.9993 - regression_loss: 2.9189 - classification_loss: 1.0804
 391/1000 [==========>...................] - ETA: 4:35 - loss: 4.0038 - regression_loss: 2.9212 - classification_loss: 1.0826
 392/1000 [==========>...................] - ETA: 4:34 - loss: 4.0048 - regression_loss: 2.9224 - classification_loss: 1.0824
 393/1000 [==========>...................] - ETA: 4:34 - loss: 4.0071 - regression_loss: 2.9239 - classification_loss: 1.0833
 394/1000 [==========>...................] - ETA: 4:34 - loss: 4.0097 - regression_loss: 2.9265 - classification_loss: 1.0832
 395/1000 [==========>...................] - ETA: 4:33 - loss: 4.0108 - regression_loss: 2.9269 - classification_loss: 1.0838
 396/1000 [==========>...................] - ETA: 4:33 - loss: 4.0118 - regression_loss: 2.9281 - classification_loss: 1.0837
 397/1000 [==========>...................] - ETA: 4:32 - loss: 4.0017 - regression_loss: 2.9208 - classification_loss: 1.0810
 398/1000 [==========>...................] - ETA: 4:32 - loss: 4.0023 - regression_loss: 2.9208 - classification_loss: 1.0814
 399/1000 [==========>...................] - ETA: 4:31 - loss: 4.0041 - regression_loss: 2.9224 - classification_loss: 1.0817
 400/1000 [===========>..................] - ETA: 4:31 - loss: 4.0072 - regression_loss: 2.9236 - classification_loss: 1.0836
 401/1000 [===========>..................] - ETA: 4:30 - loss: 4.0129 - regression_loss: 2.9281 - classification_loss: 1.0849
 402/1000 [===========>..................] - ETA: 4:30 - loss: 4.0174 - regression_loss: 2.9310 - classification_loss: 1.0864
 403/1000 [===========>..................] - ETA: 4:29 - loss: 4.0262 - regression_loss: 2.9375 - classification_loss: 1.0887
 404/1000 [===========>..................] - ETA: 4:29 - loss: 4.0279 - regression_loss: 2.9394 - classification_loss: 1.0885
 405/1000 [===========>..................] - ETA: 4:29 - loss: 4.0296 - regression_loss: 2.9412 - classification_loss: 1.0885
 406/1000 [===========>..................] - ETA: 4:28 - loss: 4.0307 - regression_loss: 2.9425 - classification_loss: 1.0882
 407/1000 [===========>..................] - ETA: 4:28 - loss: 4.0208 - regression_loss: 2.9353 - classification_loss: 1.0855
 408/1000 [===========>..................] - ETA: 4:27 - loss: 4.0221 - regression_loss: 2.9365 - classification_loss: 1.0856
 409/1000 [===========>..................] - ETA: 4:27 - loss: 4.0263 - regression_loss: 2.9406 - classification_loss: 1.0856
 410/1000 [===========>..................] - ETA: 4:26 - loss: 4.0279 - regression_loss: 2.9426 - classification_loss: 1.0854
 411/1000 [===========>..................] - ETA: 4:26 - loss: 4.0292 - regression_loss: 2.9440 - classification_loss: 1.0852
 412/1000 [===========>..................] - ETA: 4:25 - loss: 4.0318 - regression_loss: 2.9464 - classification_loss: 1.0854
 413/1000 [===========>..................] - ETA: 4:25 - loss: 4.0333 - regression_loss: 2.9473 - classification_loss: 1.0860
 414/1000 [===========>..................] - ETA: 4:24 - loss: 4.0339 - regression_loss: 2.9478 - classification_loss: 1.0861
 415/1000 [===========>..................] - ETA: 4:24 - loss: 4.0361 - regression_loss: 2.9497 - classification_loss: 1.0864
 416/1000 [===========>..................] - ETA: 4:24 - loss: 4.0397 - regression_loss: 2.9514 - classification_loss: 1.0883
 417/1000 [===========>..................] - ETA: 4:23 - loss: 4.0426 - regression_loss: 2.9541 - classification_loss: 1.0885
 418/1000 [===========>..................] - ETA: 4:23 - loss: 4.0440 - regression_loss: 2.9559 - classification_loss: 1.0882
 419/1000 [===========>..................] - ETA: 4:22 - loss: 4.0456 - regression_loss: 2.9571 - classification_loss: 1.0885
 420/1000 [===========>..................] - ETA: 4:22 - loss: 4.0492 - regression_loss: 2.9607 - classification_loss: 1.0885
 421/1000 [===========>..................] - ETA: 4:21 - loss: 4.0498 - regression_loss: 2.9613 - classification_loss: 1.0885
 422/1000 [===========>..................] - ETA: 4:21 - loss: 4.0402 - regression_loss: 2.9543 - classification_loss: 1.0859
 423/1000 [===========>..................] - ETA: 4:20 - loss: 4.0433 - regression_loss: 2.9568 - classification_loss: 1.0864
 424/1000 [===========>..................] - ETA: 4:20 - loss: 4.0337 - regression_loss: 2.9499 - classification_loss: 1.0839
 425/1000 [===========>..................] - ETA: 4:20 - loss: 4.0242 - regression_loss: 2.9429 - classification_loss: 1.0813
 426/1000 [===========>..................] - ETA: 4:19 - loss: 4.0149 - regression_loss: 2.9360 - classification_loss: 1.0789
 427/1000 [===========>..................] - ETA: 4:19 - loss: 4.0170 - regression_loss: 2.9382 - classification_loss: 1.0788
 428/1000 [===========>..................] - ETA: 4:18 - loss: 4.0192 - regression_loss: 2.9393 - classification_loss: 1.0799
 429/1000 [===========>..................] - ETA: 4:18 - loss: 4.0099 - regression_loss: 2.9324 - classification_loss: 1.0774
 430/1000 [===========>..................] - ETA: 4:17 - loss: 4.0006 - regression_loss: 2.9256 - classification_loss: 1.0750
 431/1000 [===========>..................] - ETA: 4:17 - loss: 4.0055 - regression_loss: 2.9297 - classification_loss: 1.0759
 432/1000 [===========>..................] - ETA: 4:16 - loss: 4.0107 - regression_loss: 2.9327 - classification_loss: 1.0780
 433/1000 [===========>..................] - ETA: 4:16 - loss: 4.0145 - regression_loss: 2.9353 - classification_loss: 1.0792
 434/1000 [============>.................] - ETA: 4:15 - loss: 4.0155 - regression_loss: 2.9366 - classification_loss: 1.0790
 435/1000 [============>.................] - ETA: 4:15 - loss: 4.0169 - regression_loss: 2.9380 - classification_loss: 1.0789
 436/1000 [============>.................] - ETA: 4:15 - loss: 4.0193 - regression_loss: 2.9403 - classification_loss: 1.0789
 437/1000 [============>.................] - ETA: 4:14 - loss: 4.0211 - regression_loss: 2.9406 - classification_loss: 1.0805
 438/1000 [============>.................] - ETA: 4:14 - loss: 4.0247 - regression_loss: 2.9422 - classification_loss: 1.0824
 439/1000 [============>.................] - ETA: 4:13 - loss: 4.0155 - regression_loss: 2.9355 - classification_loss: 1.0800
 440/1000 [============>.................] - ETA: 4:13 - loss: 4.0182 - regression_loss: 2.9381 - classification_loss: 1.0802
 441/1000 [============>.................] - ETA: 4:12 - loss: 4.0203 - regression_loss: 2.9394 - classification_loss: 1.0809
 442/1000 [============>.................] - ETA: 4:12 - loss: 4.0211 - regression_loss: 2.9402 - classification_loss: 1.0809
 443/1000 [============>.................] - ETA: 4:11 - loss: 4.0223 - regression_loss: 2.9418 - classification_loss: 1.0805
 444/1000 [============>.................] - ETA: 4:11 - loss: 4.0243 - regression_loss: 2.9434 - classification_loss: 1.0809
 445/1000 [============>.................] - ETA: 4:10 - loss: 4.0273 - regression_loss: 2.9448 - classification_loss: 1.0825
 446/1000 [============>.................] - ETA: 4:10 - loss: 4.0291 - regression_loss: 2.9461 - classification_loss: 1.0831
 447/1000 [============>.................] - ETA: 4:09 - loss: 4.0334 - regression_loss: 2.9496 - classification_loss: 1.0838
 448/1000 [============>.................] - ETA: 4:09 - loss: 4.0366 - regression_loss: 2.9512 - classification_loss: 1.0855
 449/1000 [============>.................] - ETA: 4:09 - loss: 4.0277 - regression_loss: 2.9446 - classification_loss: 1.0831
 450/1000 [============>.................] - ETA: 4:08 - loss: 4.0298 - regression_loss: 2.9451 - classification_loss: 1.0847
 451/1000 [============>.................] - ETA: 4:08 - loss: 4.0208 - regression_loss: 2.9385 - classification_loss: 1.0823
 452/1000 [============>.................] - ETA: 4:07 - loss: 4.0231 - regression_loss: 2.9411 - classification_loss: 1.0819
 453/1000 [============>.................] - ETA: 4:07 - loss: 4.0245 - regression_loss: 2.9423 - classification_loss: 1.0823
 454/1000 [============>.................] - ETA: 4:06 - loss: 4.0157 - regression_loss: 2.9358 - classification_loss: 1.0799
 455/1000 [============>.................] - ETA: 4:06 - loss: 4.0069 - regression_loss: 2.9293 - classification_loss: 1.0775
 456/1000 [============>.................] - ETA: 4:05 - loss: 4.0091 - regression_loss: 2.9314 - classification_loss: 1.0778
 457/1000 [============>.................] - ETA: 4:05 - loss: 4.0003 - regression_loss: 2.9249 - classification_loss: 1.0754
 458/1000 [============>.................] - ETA: 4:05 - loss: 4.0012 - regression_loss: 2.9245 - classification_loss: 1.0767
 459/1000 [============>.................] - ETA: 4:04 - loss: 4.0026 - regression_loss: 2.9252 - classification_loss: 1.0774
 460/1000 [============>.................] - ETA: 4:04 - loss: 4.0034 - regression_loss: 2.9264 - classification_loss: 1.0770
 461/1000 [============>.................] - ETA: 4:03 - loss: 4.0043 - regression_loss: 2.9277 - classification_loss: 1.0766
 462/1000 [============>.................] - ETA: 4:03 - loss: 4.0062 - regression_loss: 2.9291 - classification_loss: 1.0772
 463/1000 [============>.................] - ETA: 4:02 - loss: 4.0078 - regression_loss: 2.9311 - classification_loss: 1.0768
 464/1000 [============>.................] - ETA: 4:02 - loss: 4.0085 - regression_loss: 2.9324 - classification_loss: 1.0761
 465/1000 [============>.................] - ETA: 4:01 - loss: 4.0090 - regression_loss: 2.9333 - classification_loss: 1.0757
 466/1000 [============>.................] - ETA: 4:01 - loss: 4.0004 - regression_loss: 2.9270 - classification_loss: 1.0734
 467/1000 [=============>................] - ETA: 4:00 - loss: 4.0038 - regression_loss: 2.9284 - classification_loss: 1.0754
 468/1000 [=============>................] - ETA: 4:00 - loss: 4.0077 - regression_loss: 2.9302 - classification_loss: 1.0775
 469/1000 [=============>................] - ETA: 4:00 - loss: 4.0103 - regression_loss: 2.9329 - classification_loss: 1.0774
 470/1000 [=============>................] - ETA: 3:59 - loss: 4.0100 - regression_loss: 2.9331 - classification_loss: 1.0770
 471/1000 [=============>................] - ETA: 3:59 - loss: 4.0104 - regression_loss: 2.9343 - classification_loss: 1.0761
 472/1000 [=============>................] - ETA: 3:58 - loss: 4.0133 - regression_loss: 2.9363 - classification_loss: 1.0770
 473/1000 [=============>................] - ETA: 3:58 - loss: 4.0147 - regression_loss: 2.9367 - classification_loss: 1.0780
 474/1000 [=============>................] - ETA: 3:57 - loss: 4.0181 - regression_loss: 2.9390 - classification_loss: 1.0791
 475/1000 [=============>................] - ETA: 3:57 - loss: 4.0197 - regression_loss: 2.9406 - classification_loss: 1.0791
 476/1000 [=============>................] - ETA: 3:56 - loss: 4.0219 - regression_loss: 2.9432 - classification_loss: 1.0787
 477/1000 [=============>................] - ETA: 3:56 - loss: 4.0227 - regression_loss: 2.9433 - classification_loss: 1.0794
 478/1000 [=============>................] - ETA: 3:56 - loss: 4.0143 - regression_loss: 2.9372 - classification_loss: 1.0771
 479/1000 [=============>................] - ETA: 3:55 - loss: 4.0162 - regression_loss: 2.9383 - classification_loss: 1.0779
 480/1000 [=============>................] - ETA: 3:55 - loss: 4.0186 - regression_loss: 2.9411 - classification_loss: 1.0776
 481/1000 [=============>................] - ETA: 3:54 - loss: 4.0195 - regression_loss: 2.9425 - classification_loss: 1.0770
 482/1000 [=============>................] - ETA: 3:54 - loss: 4.0111 - regression_loss: 2.9364 - classification_loss: 1.0748
 483/1000 [=============>................] - ETA: 3:53 - loss: 4.0115 - regression_loss: 2.9372 - classification_loss: 1.0742
 484/1000 [=============>................] - ETA: 3:53 - loss: 4.0132 - regression_loss: 2.9392 - classification_loss: 1.0740
 485/1000 [=============>................] - ETA: 3:52 - loss: 4.0143 - regression_loss: 2.9407 - classification_loss: 1.0736
 486/1000 [=============>................] - ETA: 3:52 - loss: 4.0148 - regression_loss: 2.9419 - classification_loss: 1.0728
 487/1000 [=============>................] - ETA: 3:51 - loss: 4.0165 - regression_loss: 2.9435 - classification_loss: 1.0729
 488/1000 [=============>................] - ETA: 3:51 - loss: 4.0184 - regression_loss: 2.9443 - classification_loss: 1.0741
 489/1000 [=============>................] - ETA: 3:51 - loss: 4.0199 - regression_loss: 2.9453 - classification_loss: 1.0746
 490/1000 [=============>................] - ETA: 3:50 - loss: 4.0213 - regression_loss: 2.9469 - classification_loss: 1.0744
 491/1000 [=============>................] - ETA: 3:50 - loss: 4.0131 - regression_loss: 2.9409 - classification_loss: 1.0722
 492/1000 [=============>................] - ETA: 3:49 - loss: 4.0049 - regression_loss: 2.9349 - classification_loss: 1.0700
 493/1000 [=============>................] - ETA: 3:49 - loss: 4.0051 - regression_loss: 2.9360 - classification_loss: 1.0692
 494/1000 [=============>................] - ETA: 3:48 - loss: 3.9970 - regression_loss: 2.9300 - classification_loss: 1.0670
 495/1000 [=============>................] - ETA: 3:48 - loss: 3.9997 - regression_loss: 2.9319 - classification_loss: 1.0678
 496/1000 [=============>................] - ETA: 3:47 - loss: 4.0017 - regression_loss: 2.9332 - classification_loss: 1.0685
 497/1000 [=============>................] - ETA: 3:47 - loss: 4.0051 - regression_loss: 2.9349 - classification_loss: 1.0702
 498/1000 [=============>................] - ETA: 3:47 - loss: 4.0062 - regression_loss: 2.9362 - classification_loss: 1.0699
 499/1000 [=============>................] - ETA: 3:46 - loss: 3.9982 - regression_loss: 2.9304 - classification_loss: 1.0678
 500/1000 [==============>...............] - ETA: 3:46 - loss: 4.0014 - regression_loss: 2.9318 - classification_loss: 1.0697
 501/1000 [==============>...............] - ETA: 3:45 - loss: 4.0014 - regression_loss: 2.9319 - classification_loss: 1.0695
 502/1000 [==============>...............] - ETA: 3:45 - loss: 3.9934 - regression_loss: 2.9261 - classification_loss: 1.0674
 503/1000 [==============>...............] - ETA: 3:44 - loss: 3.9936 - regression_loss: 2.9270 - classification_loss: 1.0666
 504/1000 [==============>...............] - ETA: 3:44 - loss: 3.9934 - regression_loss: 2.9273 - classification_loss: 1.0661
 505/1000 [==============>...............] - ETA: 3:43 - loss: 3.9857 - regression_loss: 2.9215 - classification_loss: 1.0642
 506/1000 [==============>...............] - ETA: 3:43 - loss: 3.9883 - regression_loss: 2.9223 - classification_loss: 1.0659
 507/1000 [==============>...............] - ETA: 3:42 - loss: 3.9804 - regression_loss: 2.9166 - classification_loss: 1.0638
 508/1000 [==============>...............] - ETA: 3:42 - loss: 3.9810 - regression_loss: 2.9164 - classification_loss: 1.0646
 509/1000 [==============>...............] - ETA: 3:42 - loss: 3.9732 - regression_loss: 2.9107 - classification_loss: 1.0625
 510/1000 [==============>...............] - ETA: 3:41 - loss: 3.9654 - regression_loss: 2.9050 - classification_loss: 1.0604
 511/1000 [==============>...............] - ETA: 3:41 - loss: 3.9666 - regression_loss: 2.9045 - classification_loss: 1.0621
 512/1000 [==============>...............] - ETA: 3:40 - loss: 3.9703 - regression_loss: 2.9061 - classification_loss: 1.0642
 513/1000 [==============>...............] - ETA: 3:40 - loss: 3.9730 - regression_loss: 2.9082 - classification_loss: 1.0648
 514/1000 [==============>...............] - ETA: 3:39 - loss: 3.9653 - regression_loss: 2.9025 - classification_loss: 1.0627
 515/1000 [==============>...............] - ETA: 3:39 - loss: 3.9657 - regression_loss: 2.9038 - classification_loss: 1.0619
 516/1000 [==============>...............] - ETA: 3:38 - loss: 3.9704 - regression_loss: 2.9070 - classification_loss: 1.0634
 517/1000 [==============>...............] - ETA: 3:38 - loss: 3.9709 - regression_loss: 2.9083 - classification_loss: 1.0626
 518/1000 [==============>...............] - ETA: 3:38 - loss: 3.9721 - regression_loss: 2.9088 - classification_loss: 1.0633
 519/1000 [==============>...............] - ETA: 3:37 - loss: 3.9744 - regression_loss: 2.9094 - classification_loss: 1.0650
 520/1000 [==============>...............] - ETA: 3:37 - loss: 3.9668 - regression_loss: 2.9038 - classification_loss: 1.0629
 521/1000 [==============>...............] - ETA: 3:36 - loss: 3.9592 - regression_loss: 2.8983 - classification_loss: 1.0609
 522/1000 [==============>...............] - ETA: 3:36 - loss: 3.9585 - regression_loss: 2.8984 - classification_loss: 1.0601
 523/1000 [==============>...............] - ETA: 3:35 - loss: 3.9608 - regression_loss: 2.8994 - classification_loss: 1.0614
 524/1000 [==============>...............] - ETA: 3:35 - loss: 3.9623 - regression_loss: 2.9009 - classification_loss: 1.0613
 525/1000 [==============>...............] - ETA: 3:34 - loss: 3.9547 - regression_loss: 2.8954 - classification_loss: 1.0593
 526/1000 [==============>...............] - ETA: 3:34 - loss: 3.9576 - regression_loss: 2.8966 - classification_loss: 1.0610
 527/1000 [==============>...............] - ETA: 3:33 - loss: 3.9589 - regression_loss: 2.8974 - classification_loss: 1.0615
 528/1000 [==============>...............] - ETA: 3:33 - loss: 3.9599 - regression_loss: 2.8975 - classification_loss: 1.0624
 529/1000 [==============>...............] - ETA: 3:33 - loss: 3.9524 - regression_loss: 2.8920 - classification_loss: 1.0604
 530/1000 [==============>...............] - ETA: 3:32 - loss: 3.9449 - regression_loss: 2.8866 - classification_loss: 1.0584
 531/1000 [==============>...............] - ETA: 3:32 - loss: 3.9451 - regression_loss: 2.8876 - classification_loss: 1.0576
 532/1000 [==============>...............] - ETA: 3:31 - loss: 3.9486 - regression_loss: 2.8892 - classification_loss: 1.0594
 533/1000 [==============>...............] - ETA: 3:31 - loss: 3.9412 - regression_loss: 2.8838 - classification_loss: 1.0574
 534/1000 [===============>..............] - ETA: 3:30 - loss: 3.9338 - regression_loss: 2.8784 - classification_loss: 1.0554
 535/1000 [===============>..............] - ETA: 3:30 - loss: 3.9370 - regression_loss: 2.8799 - classification_loss: 1.0571
 536/1000 [===============>..............] - ETA: 3:29 - loss: 3.9368 - regression_loss: 2.8803 - classification_loss: 1.0565
 537/1000 [===============>..............] - ETA: 3:29 - loss: 3.9385 - regression_loss: 2.8823 - classification_loss: 1.0562
 538/1000 [===============>..............] - ETA: 3:28 - loss: 3.9312 - regression_loss: 2.8770 - classification_loss: 1.0542
 539/1000 [===============>..............] - ETA: 3:28 - loss: 3.9340 - regression_loss: 2.8783 - classification_loss: 1.0558
 540/1000 [===============>..............] - ETA: 3:28 - loss: 3.9377 - regression_loss: 2.8804 - classification_loss: 1.0573
 541/1000 [===============>..............] - ETA: 3:27 - loss: 3.9304 - regression_loss: 2.8751 - classification_loss: 1.0553
 542/1000 [===============>..............] - ETA: 3:27 - loss: 3.9313 - regression_loss: 2.8765 - classification_loss: 1.0548
 543/1000 [===============>..............] - ETA: 3:26 - loss: 3.9332 - regression_loss: 2.8772 - classification_loss: 1.0561
 544/1000 [===============>..............] - ETA: 3:26 - loss: 3.9355 - regression_loss: 2.8789 - classification_loss: 1.0566
 545/1000 [===============>..............] - ETA: 3:25 - loss: 3.9364 - regression_loss: 2.8806 - classification_loss: 1.0559
 546/1000 [===============>..............] - ETA: 3:25 - loss: 3.9391 - regression_loss: 2.8831 - classification_loss: 1.0560
 547/1000 [===============>..............] - ETA: 3:24 - loss: 3.9409 - regression_loss: 2.8850 - classification_loss: 1.0559
 548/1000 [===============>..............] - ETA: 3:24 - loss: 3.9415 - regression_loss: 2.8862 - classification_loss: 1.0553
 549/1000 [===============>..............] - ETA: 3:23 - loss: 3.9439 - regression_loss: 2.8875 - classification_loss: 1.0564
 550/1000 [===============>..............] - ETA: 3:23 - loss: 3.9367 - regression_loss: 2.8823 - classification_loss: 1.0544
 551/1000 [===============>..............] - ETA: 3:23 - loss: 3.9381 - regression_loss: 2.8840 - classification_loss: 1.0540
 552/1000 [===============>..............] - ETA: 3:22 - loss: 3.9399 - regression_loss: 2.8846 - classification_loss: 1.0553
 553/1000 [===============>..............] - ETA: 3:22 - loss: 3.9443 - regression_loss: 2.8879 - classification_loss: 1.0564
 554/1000 [===============>..............] - ETA: 3:21 - loss: 3.9446 - regression_loss: 2.8889 - classification_loss: 1.0557
 555/1000 [===============>..............] - ETA: 3:21 - loss: 3.9447 - regression_loss: 2.8897 - classification_loss: 1.0550
 556/1000 [===============>..............] - ETA: 3:20 - loss: 3.9466 - regression_loss: 2.8904 - classification_loss: 1.0563
 557/1000 [===============>..............] - ETA: 3:20 - loss: 3.9401 - regression_loss: 2.8852 - classification_loss: 1.0549
 558/1000 [===============>..............] - ETA: 3:19 - loss: 3.9435 - regression_loss: 2.8876 - classification_loss: 1.0559
 559/1000 [===============>..............] - ETA: 3:19 - loss: 3.9438 - regression_loss: 2.8886 - classification_loss: 1.0553
 560/1000 [===============>..............] - ETA: 3:18 - loss: 3.9455 - regression_loss: 2.8900 - classification_loss: 1.0555
 561/1000 [===============>..............] - ETA: 3:18 - loss: 3.9468 - regression_loss: 2.8910 - classification_loss: 1.0558
 562/1000 [===============>..............] - ETA: 3:18 - loss: 3.9478 - regression_loss: 2.8926 - classification_loss: 1.0552
 563/1000 [===============>..............] - ETA: 3:17 - loss: 3.9508 - regression_loss: 2.8952 - classification_loss: 1.0557
 564/1000 [===============>..............] - ETA: 3:17 - loss: 3.9438 - regression_loss: 2.8900 - classification_loss: 1.0538
 565/1000 [===============>..............] - ETA: 3:16 - loss: 3.9369 - regression_loss: 2.8849 - classification_loss: 1.0519
 566/1000 [===============>..............] - ETA: 3:16 - loss: 3.9387 - regression_loss: 2.8863 - classification_loss: 1.0525
 567/1000 [================>.............] - ETA: 3:15 - loss: 3.9405 - regression_loss: 2.8874 - classification_loss: 1.0531
 568/1000 [================>.............] - ETA: 3:15 - loss: 3.9336 - regression_loss: 2.8823 - classification_loss: 1.0513
 569/1000 [================>.............] - ETA: 3:14 - loss: 3.9372 - regression_loss: 2.8857 - classification_loss: 1.0515
 570/1000 [================>.............] - ETA: 3:14 - loss: 3.9389 - regression_loss: 2.8865 - classification_loss: 1.0525
 571/1000 [================>.............] - ETA: 3:13 - loss: 3.9399 - regression_loss: 2.8877 - classification_loss: 1.0522
 572/1000 [================>.............] - ETA: 3:13 - loss: 3.9435 - regression_loss: 2.8904 - classification_loss: 1.0531
 573/1000 [================>.............] - ETA: 3:13 - loss: 3.9474 - regression_loss: 2.8930 - classification_loss: 1.0544
 574/1000 [================>.............] - ETA: 3:12 - loss: 3.9488 - regression_loss: 2.8942 - classification_loss: 1.0547
 575/1000 [================>.............] - ETA: 3:12 - loss: 3.9493 - regression_loss: 2.8952 - classification_loss: 1.0541
 576/1000 [================>.............] - ETA: 3:11 - loss: 3.9507 - regression_loss: 2.8968 - classification_loss: 1.0539
 577/1000 [================>.............] - ETA: 3:11 - loss: 3.9525 - regression_loss: 2.8988 - classification_loss: 1.0537
 578/1000 [================>.............] - ETA: 3:10 - loss: 3.9533 - regression_loss: 2.8992 - classification_loss: 1.0542
 579/1000 [================>.............] - ETA: 3:10 - loss: 3.9548 - regression_loss: 2.9007 - classification_loss: 1.0540
 580/1000 [================>.............] - ETA: 3:09 - loss: 3.9560 - regression_loss: 2.9020 - classification_loss: 1.0539
 581/1000 [================>.............] - ETA: 3:09 - loss: 3.9583 - regression_loss: 2.9040 - classification_loss: 1.0543
 582/1000 [================>.............] - ETA: 3:08 - loss: 3.9613 - regression_loss: 2.9066 - classification_loss: 1.0548
 583/1000 [================>.............] - ETA: 3:08 - loss: 3.9620 - regression_loss: 2.9076 - classification_loss: 1.0544
 584/1000 [================>.............] - ETA: 3:08 - loss: 3.9553 - regression_loss: 2.9027 - classification_loss: 1.0526
 585/1000 [================>.............] - ETA: 3:07 - loss: 3.9571 - regression_loss: 2.9038 - classification_loss: 1.0533
 586/1000 [================>.............] - ETA: 3:07 - loss: 3.9592 - regression_loss: 2.9054 - classification_loss: 1.0538
 587/1000 [================>.............] - ETA: 3:06 - loss: 3.9525 - regression_loss: 2.9005 - classification_loss: 1.0520
 588/1000 [================>.............] - ETA: 3:06 - loss: 3.9539 - regression_loss: 2.9020 - classification_loss: 1.0520
 589/1000 [================>.............] - ETA: 3:05 - loss: 3.9555 - regression_loss: 2.9029 - classification_loss: 1.0525
 590/1000 [================>.............] - ETA: 3:05 - loss: 3.9488 - regression_loss: 2.8980 - classification_loss: 1.0508
 591/1000 [================>.............] - ETA: 3:04 - loss: 3.9506 - regression_loss: 2.8998 - classification_loss: 1.0507
 592/1000 [================>.............] - ETA: 3:04 - loss: 3.9514 - regression_loss: 2.8995 - classification_loss: 1.0519
 593/1000 [================>.............] - ETA: 3:03 - loss: 3.9447 - regression_loss: 2.8947 - classification_loss: 1.0501
 594/1000 [================>.............] - ETA: 3:03 - loss: 3.9480 - regression_loss: 2.8978 - classification_loss: 1.0502
 595/1000 [================>.............] - ETA: 3:03 - loss: 3.9479 - regression_loss: 2.8981 - classification_loss: 1.0497
 596/1000 [================>.............] - ETA: 3:02 - loss: 3.9486 - regression_loss: 2.8983 - classification_loss: 1.0503
 597/1000 [================>.............] - ETA: 3:02 - loss: 3.9420 - regression_loss: 2.8935 - classification_loss: 1.0485
 598/1000 [================>.............] - ETA: 3:01 - loss: 3.9421 - regression_loss: 2.8938 - classification_loss: 1.0483
 599/1000 [================>.............] - ETA: 3:01 - loss: 3.9452 - regression_loss: 2.8956 - classification_loss: 1.0495
 600/1000 [=================>............] - ETA: 3:00 - loss: 3.9491 - regression_loss: 2.8982 - classification_loss: 1.0508
 601/1000 [=================>............] - ETA: 3:00 - loss: 3.9502 - regression_loss: 2.8999 - classification_loss: 1.0502
 602/1000 [=================>............] - ETA: 2:59 - loss: 3.9436 - regression_loss: 2.8951 - classification_loss: 1.0485
 603/1000 [=================>............] - ETA: 2:59 - loss: 3.9465 - regression_loss: 2.8964 - classification_loss: 1.0501
 604/1000 [=================>............] - ETA: 2:59 - loss: 3.9467 - regression_loss: 2.8969 - classification_loss: 1.0499
 605/1000 [=================>............] - ETA: 2:58 - loss: 3.9479 - regression_loss: 2.8978 - classification_loss: 1.0500
 606/1000 [=================>............] - ETA: 2:58 - loss: 3.9494 - regression_loss: 2.8995 - classification_loss: 1.0499
 607/1000 [=================>............] - ETA: 2:57 - loss: 3.9520 - regression_loss: 2.9008 - classification_loss: 1.0512
 608/1000 [=================>............] - ETA: 2:57 - loss: 3.9561 - regression_loss: 2.9046 - classification_loss: 1.0515
 609/1000 [=================>............] - ETA: 2:56 - loss: 3.9575 - regression_loss: 2.9061 - classification_loss: 1.0513
 610/1000 [=================>............] - ETA: 2:56 - loss: 3.9580 - regression_loss: 2.9070 - classification_loss: 1.0510
 611/1000 [=================>............] - ETA: 2:55 - loss: 3.9600 - regression_loss: 2.9077 - classification_loss: 1.0523
 612/1000 [=================>............] - ETA: 2:55 - loss: 3.9623 - regression_loss: 2.9092 - classification_loss: 1.0531
 613/1000 [=================>............] - ETA: 2:54 - loss: 3.9624 - regression_loss: 2.9097 - classification_loss: 1.0528
 614/1000 [=================>............] - ETA: 2:54 - loss: 3.9632 - regression_loss: 2.9107 - classification_loss: 1.0525
 615/1000 [=================>............] - ETA: 2:54 - loss: 3.9572 - regression_loss: 2.9060 - classification_loss: 1.0512
 616/1000 [=================>............] - ETA: 2:53 - loss: 3.9569 - regression_loss: 2.9063 - classification_loss: 1.0506
 617/1000 [=================>............] - ETA: 2:53 - loss: 3.9584 - regression_loss: 2.9065 - classification_loss: 1.0519
 618/1000 [=================>............] - ETA: 2:52 - loss: 3.9603 - regression_loss: 2.9081 - classification_loss: 1.0521
 619/1000 [=================>............] - ETA: 2:52 - loss: 3.9612 - regression_loss: 2.9095 - classification_loss: 1.0517
 620/1000 [=================>............] - ETA: 2:51 - loss: 3.9554 - regression_loss: 2.9048 - classification_loss: 1.0505
 621/1000 [=================>............] - ETA: 2:51 - loss: 3.9490 - regression_loss: 2.9002 - classification_loss: 1.0488
 622/1000 [=================>............] - ETA: 2:50 - loss: 3.9506 - regression_loss: 2.9019 - classification_loss: 1.0487
 623/1000 [=================>............] - ETA: 2:50 - loss: 3.9443 - regression_loss: 2.8972 - classification_loss: 1.0471
 624/1000 [=================>............] - ETA: 2:49 - loss: 3.9462 - regression_loss: 2.8985 - classification_loss: 1.0477
 625/1000 [=================>............] - ETA: 2:49 - loss: 3.9463 - regression_loss: 2.8984 - classification_loss: 1.0479
 626/1000 [=================>............] - ETA: 2:49 - loss: 3.9400 - regression_loss: 2.8937 - classification_loss: 1.0463
 627/1000 [=================>............] - ETA: 2:48 - loss: 3.9418 - regression_loss: 2.8947 - classification_loss: 1.0472
 628/1000 [=================>............] - ETA: 2:48 - loss: 3.9356 - regression_loss: 2.8901 - classification_loss: 1.0455
 629/1000 [=================>............] - ETA: 2:47 - loss: 3.9372 - regression_loss: 2.8916 - classification_loss: 1.0456
 630/1000 [=================>............] - ETA: 2:47 - loss: 3.9378 - regression_loss: 2.8924 - classification_loss: 1.0454
 631/1000 [=================>............] - ETA: 2:46 - loss: 3.9379 - regression_loss: 2.8924 - classification_loss: 1.0456
 632/1000 [=================>............] - ETA: 2:46 - loss: 3.9403 - regression_loss: 2.8935 - classification_loss: 1.0469
 633/1000 [=================>............] - ETA: 2:45 - loss: 3.9415 - regression_loss: 2.8949 - classification_loss: 1.0466
 634/1000 [==================>...........] - ETA: 2:45 - loss: 3.9450 - regression_loss: 2.8969 - classification_loss: 1.0481
 635/1000 [==================>...........] - ETA: 2:45 - loss: 3.9470 - regression_loss: 2.8981 - classification_loss: 1.0489
 636/1000 [==================>...........] - ETA: 2:44 - loss: 3.9471 - regression_loss: 2.8984 - classification_loss: 1.0487
 637/1000 [==================>...........] - ETA: 2:44 - loss: 3.9480 - regression_loss: 2.8995 - classification_loss: 1.0484
 638/1000 [==================>...........] - ETA: 2:43 - loss: 3.9508 - regression_loss: 2.9007 - classification_loss: 1.0501
 639/1000 [==================>...........] - ETA: 2:43 - loss: 3.9524 - regression_loss: 2.9016 - classification_loss: 1.0509
 640/1000 [==================>...........] - ETA: 2:42 - loss: 3.9463 - regression_loss: 2.8970 - classification_loss: 1.0492
 641/1000 [==================>...........] - ETA: 2:42 - loss: 3.9487 - regression_loss: 2.8979 - classification_loss: 1.0508
 642/1000 [==================>...........] - ETA: 2:41 - loss: 3.9499 - regression_loss: 2.8995 - classification_loss: 1.0503
 643/1000 [==================>...........] - ETA: 2:41 - loss: 3.9522 - regression_loss: 2.9010 - classification_loss: 1.0512
 644/1000 [==================>...........] - ETA: 2:40 - loss: 3.9536 - regression_loss: 2.9028 - classification_loss: 1.0508
 645/1000 [==================>...........] - ETA: 2:40 - loss: 3.9547 - regression_loss: 2.9046 - classification_loss: 1.0501
 646/1000 [==================>...........] - ETA: 2:40 - loss: 3.9556 - regression_loss: 2.9047 - classification_loss: 1.0508
 647/1000 [==================>...........] - ETA: 2:39 - loss: 3.9494 - regression_loss: 2.9002 - classification_loss: 1.0492
 648/1000 [==================>...........] - ETA: 2:39 - loss: 3.9433 - regression_loss: 2.8958 - classification_loss: 1.0476
 649/1000 [==================>...........] - ETA: 2:38 - loss: 3.9373 - regression_loss: 2.8913 - classification_loss: 1.0460
 650/1000 [==================>...........] - ETA: 2:38 - loss: 3.9312 - regression_loss: 2.8869 - classification_loss: 1.0444
 651/1000 [==================>...........] - ETA: 2:37 - loss: 3.9326 - regression_loss: 2.8881 - classification_loss: 1.0445
 652/1000 [==================>...........] - ETA: 2:37 - loss: 3.9343 - regression_loss: 2.8889 - classification_loss: 1.0454
 653/1000 [==================>...........] - ETA: 2:36 - loss: 3.9283 - regression_loss: 2.8844 - classification_loss: 1.0438
 654/1000 [==================>...........] - ETA: 2:36 - loss: 3.9247 - regression_loss: 2.8800 - classification_loss: 1.0447
 655/1000 [==================>...........] - ETA: 2:35 - loss: 3.9272 - regression_loss: 2.8818 - classification_loss: 1.0454
 656/1000 [==================>...........] - ETA: 2:35 - loss: 3.9212 - regression_loss: 2.8774 - classification_loss: 1.0438
 657/1000 [==================>...........] - ETA: 2:35 - loss: 3.9152 - regression_loss: 2.8730 - classification_loss: 1.0422
 658/1000 [==================>...........] - ETA: 2:34 - loss: 3.9164 - regression_loss: 2.8745 - classification_loss: 1.0419
 659/1000 [==================>...........] - ETA: 2:34 - loss: 3.9105 - regression_loss: 2.8702 - classification_loss: 1.0403
 660/1000 [==================>...........] - ETA: 2:33 - loss: 3.9148 - regression_loss: 2.8738 - classification_loss: 1.0410
 661/1000 [==================>...........] - ETA: 2:33 - loss: 3.9181 - regression_loss: 2.8749 - classification_loss: 1.0432
 662/1000 [==================>...........] - ETA: 2:32 - loss: 3.9208 - regression_loss: 2.8762 - classification_loss: 1.0446
 663/1000 [==================>...........] - ETA: 2:32 - loss: 3.9212 - regression_loss: 2.8763 - classification_loss: 1.0449
 664/1000 [==================>...........] - ETA: 2:31 - loss: 3.9214 - regression_loss: 2.8769 - classification_loss: 1.0445
 665/1000 [==================>...........] - ETA: 2:31 - loss: 3.9233 - regression_loss: 2.8773 - classification_loss: 1.0460
 666/1000 [==================>...........] - ETA: 2:30 - loss: 3.9175 - regression_loss: 2.8730 - classification_loss: 1.0445
 667/1000 [===================>..........] - ETA: 2:30 - loss: 3.9175 - regression_loss: 2.8735 - classification_loss: 1.0440
 668/1000 [===================>..........] - ETA: 2:30 - loss: 3.9117 - regression_loss: 2.8692 - classification_loss: 1.0425
 669/1000 [===================>..........] - ETA: 2:29 - loss: 3.9135 - regression_loss: 2.8698 - classification_loss: 1.0437
 670/1000 [===================>..........] - ETA: 2:29 - loss: 3.9163 - regression_loss: 2.8709 - classification_loss: 1.0454
 671/1000 [===================>..........] - ETA: 2:28 - loss: 3.9177 - regression_loss: 2.8721 - classification_loss: 1.0455
 672/1000 [===================>..........] - ETA: 2:28 - loss: 3.9118 - regression_loss: 2.8679 - classification_loss: 1.0440
 673/1000 [===================>..........] - ETA: 2:27 - loss: 3.9122 - regression_loss: 2.8683 - classification_loss: 1.0439
 674/1000 [===================>..........] - ETA: 2:27 - loss: 3.9064 - regression_loss: 2.8641 - classification_loss: 1.0423
 675/1000 [===================>..........] - ETA: 2:26 - loss: 3.9087 - regression_loss: 2.8650 - classification_loss: 1.0437
 676/1000 [===================>..........] - ETA: 2:26 - loss: 3.9030 - regression_loss: 2.8608 - classification_loss: 1.0422
 677/1000 [===================>..........] - ETA: 2:26 - loss: 3.8972 - regression_loss: 2.8566 - classification_loss: 1.0406
 678/1000 [===================>..........] - ETA: 2:25 - loss: 3.9002 - regression_loss: 2.8592 - classification_loss: 1.0410
 679/1000 [===================>..........] - ETA: 2:25 - loss: 3.9012 - regression_loss: 2.8593 - classification_loss: 1.0419
 680/1000 [===================>..........] - ETA: 2:24 - loss: 3.9021 - regression_loss: 2.8606 - classification_loss: 1.0416
 681/1000 [===================>..........] - ETA: 2:24 - loss: 3.9021 - regression_loss: 2.8608 - classification_loss: 1.0413
 682/1000 [===================>..........] - ETA: 2:23 - loss: 3.8964 - regression_loss: 2.8566 - classification_loss: 1.0398
 683/1000 [===================>..........] - ETA: 2:23 - loss: 3.8907 - regression_loss: 2.8524 - classification_loss: 1.0383
 684/1000 [===================>..........] - ETA: 2:22 - loss: 3.8924 - regression_loss: 2.8545 - classification_loss: 1.0379
 685/1000 [===================>..........] - ETA: 2:22 - loss: 3.8945 - regression_loss: 2.8550 - classification_loss: 1.0395
 686/1000 [===================>..........] - ETA: 2:21 - loss: 3.8980 - regression_loss: 2.8567 - classification_loss: 1.0413
 687/1000 [===================>..........] - ETA: 2:21 - loss: 3.8994 - regression_loss: 2.8579 - classification_loss: 1.0415
 688/1000 [===================>..........] - ETA: 2:21 - loss: 3.9005 - regression_loss: 2.8593 - classification_loss: 1.0412
 689/1000 [===================>..........] - ETA: 2:20 - loss: 3.9007 - regression_loss: 2.8599 - classification_loss: 1.0409
 690/1000 [===================>..........] - ETA: 2:20 - loss: 3.9036 - regression_loss: 2.8612 - classification_loss: 1.0423
 691/1000 [===================>..........] - ETA: 2:19 - loss: 3.9047 - regression_loss: 2.8624 - classification_loss: 1.0423
 692/1000 [===================>..........] - ETA: 2:19 - loss: 3.9060 - regression_loss: 2.8626 - classification_loss: 1.0434
 693/1000 [===================>..........] - ETA: 2:18 - loss: 3.9066 - regression_loss: 2.8636 - classification_loss: 1.0430
 694/1000 [===================>..........] - ETA: 2:18 - loss: 3.9077 - regression_loss: 2.8650 - classification_loss: 1.0427
 695/1000 [===================>..........] - ETA: 2:17 - loss: 3.9070 - regression_loss: 2.8609 - classification_loss: 1.0461
 696/1000 [===================>..........] - ETA: 2:17 - loss: 3.9073 - regression_loss: 2.8616 - classification_loss: 1.0457
 697/1000 [===================>..........] - ETA: 2:16 - loss: 3.9017 - regression_loss: 2.8575 - classification_loss: 1.0442
 698/1000 [===================>..........] - ETA: 2:16 - loss: 3.9026 - regression_loss: 2.8576 - classification_loss: 1.0450
 699/1000 [===================>..........] - ETA: 2:16 - loss: 3.9034 - regression_loss: 2.8582 - classification_loss: 1.0452
 700/1000 [====================>.........] - ETA: 2:15 - loss: 3.9033 - regression_loss: 2.8586 - classification_loss: 1.0447
 701/1000 [====================>.........] - ETA: 2:15 - loss: 3.8977 - regression_loss: 2.8545 - classification_loss: 1.0432
 702/1000 [====================>.........] - ETA: 2:14 - loss: 3.8984 - regression_loss: 2.8556 - classification_loss: 1.0428
 703/1000 [====================>.........] - ETA: 2:14 - loss: 3.8984 - regression_loss: 2.8561 - classification_loss: 1.0424
 704/1000 [====================>.........] - ETA: 2:13 - loss: 3.8987 - regression_loss: 2.8567 - classification_loss: 1.0420
 705/1000 [====================>.........] - ETA: 2:13 - loss: 3.8996 - regression_loss: 2.8572 - classification_loss: 1.0424
 706/1000 [====================>.........] - ETA: 2:12 - loss: 3.9016 - regression_loss: 2.8597 - classification_loss: 1.0419
 707/1000 [====================>.........] - ETA: 2:12 - loss: 3.9015 - regression_loss: 2.8601 - classification_loss: 1.0415
 708/1000 [====================>.........] - ETA: 2:11 - loss: 3.9037 - regression_loss: 2.8616 - classification_loss: 1.0421
 709/1000 [====================>.........] - ETA: 2:11 - loss: 3.9058 - regression_loss: 2.8633 - classification_loss: 1.0425
 710/1000 [====================>.........] - ETA: 2:11 - loss: 3.9060 - regression_loss: 2.8629 - classification_loss: 1.0432
 711/1000 [====================>.........] - ETA: 2:10 - loss: 3.9094 - regression_loss: 2.8656 - classification_loss: 1.0439
 712/1000 [====================>.........] - ETA: 2:10 - loss: 3.9104 - regression_loss: 2.8658 - classification_loss: 1.0446
 713/1000 [====================>.........] - ETA: 2:09 - loss: 3.9125 - regression_loss: 2.8681 - classification_loss: 1.0444
 714/1000 [====================>.........] - ETA: 2:09 - loss: 3.9142 - regression_loss: 2.8693 - classification_loss: 1.0449
 715/1000 [====================>.........] - ETA: 2:08 - loss: 3.9162 - regression_loss: 2.8707 - classification_loss: 1.0455
 716/1000 [====================>.........] - ETA: 2:08 - loss: 3.9177 - regression_loss: 2.8720 - classification_loss: 1.0457
 717/1000 [====================>.........] - ETA: 2:07 - loss: 3.9122 - regression_loss: 2.8680 - classification_loss: 1.0442
 718/1000 [====================>.........] - ETA: 2:07 - loss: 3.9136 - regression_loss: 2.8689 - classification_loss: 1.0447
 719/1000 [====================>.........] - ETA: 2:07 - loss: 3.9154 - regression_loss: 2.8704 - classification_loss: 1.0450
 720/1000 [====================>.........] - ETA: 2:06 - loss: 3.9194 - regression_loss: 2.8734 - classification_loss: 1.0460
 721/1000 [====================>.........] - ETA: 2:06 - loss: 3.9224 - regression_loss: 2.8761 - classification_loss: 1.0463
 722/1000 [====================>.........] - ETA: 2:05 - loss: 3.9239 - regression_loss: 2.8781 - classification_loss: 1.0458
 723/1000 [====================>.........] - ETA: 2:05 - loss: 3.9253 - regression_loss: 2.8798 - classification_loss: 1.0455
 724/1000 [====================>.........] - ETA: 2:04 - loss: 3.9277 - regression_loss: 2.8826 - classification_loss: 1.0452
 725/1000 [====================>.........] - ETA: 2:04 - loss: 3.9281 - regression_loss: 2.8833 - classification_loss: 1.0448
 726/1000 [====================>.........] - ETA: 2:03 - loss: 3.9227 - regression_loss: 2.8794 - classification_loss: 1.0433
 727/1000 [====================>.........] - ETA: 2:03 - loss: 3.9248 - regression_loss: 2.8807 - classification_loss: 1.0441
 728/1000 [====================>.........] - ETA: 2:02 - loss: 3.9258 - regression_loss: 2.8817 - classification_loss: 1.0441
 729/1000 [====================>.........] - ETA: 2:02 - loss: 3.9266 - regression_loss: 2.8824 - classification_loss: 1.0443
 730/1000 [====================>.........] - ETA: 2:02 - loss: 3.9275 - regression_loss: 2.8838 - classification_loss: 1.0436
 731/1000 [====================>.........] - ETA: 2:01 - loss: 3.9298 - regression_loss: 2.8858 - classification_loss: 1.0441
 732/1000 [====================>.........] - ETA: 2:01 - loss: 3.9318 - regression_loss: 2.8874 - classification_loss: 1.0444
 733/1000 [====================>.........] - ETA: 2:00 - loss: 3.9275 - regression_loss: 2.8835 - classification_loss: 1.0440
 734/1000 [=====================>........] - ETA: 2:00 - loss: 3.9275 - regression_loss: 2.8839 - classification_loss: 1.0436
 735/1000 [=====================>........] - ETA: 1:59 - loss: 3.9296 - regression_loss: 2.8866 - classification_loss: 1.0430
 736/1000 [=====================>........] - ETA: 1:59 - loss: 3.9315 - regression_loss: 2.8881 - classification_loss: 1.0434
 737/1000 [=====================>........] - ETA: 1:58 - loss: 3.9321 - regression_loss: 2.8890 - classification_loss: 1.0431
 738/1000 [=====================>........] - ETA: 1:58 - loss: 3.9335 - regression_loss: 2.8905 - classification_loss: 1.0429
 739/1000 [=====================>........] - ETA: 1:57 - loss: 3.9282 - regression_loss: 2.8866 - classification_loss: 1.0415
 740/1000 [=====================>........] - ETA: 1:57 - loss: 3.9299 - regression_loss: 2.8886 - classification_loss: 1.0413
 741/1000 [=====================>........] - ETA: 1:57 - loss: 3.9308 - regression_loss: 2.8892 - classification_loss: 1.0416
 742/1000 [=====================>........] - ETA: 1:56 - loss: 3.9319 - regression_loss: 2.8902 - classification_loss: 1.0417
 743/1000 [=====================>........] - ETA: 1:56 - loss: 3.9281 - regression_loss: 2.8863 - classification_loss: 1.0417
 744/1000 [=====================>........] - ETA: 1:55 - loss: 3.9228 - regression_loss: 2.8824 - classification_loss: 1.0403
 745/1000 [=====================>........] - ETA: 1:55 - loss: 3.9239 - regression_loss: 2.8833 - classification_loss: 1.0406
 746/1000 [=====================>........] - ETA: 1:54 - loss: 3.9194 - regression_loss: 2.8795 - classification_loss: 1.0400
 747/1000 [=====================>........] - ETA: 1:54 - loss: 3.9142 - regression_loss: 2.8756 - classification_loss: 1.0386
 748/1000 [=====================>........] - ETA: 1:53 - loss: 3.9153 - regression_loss: 2.8769 - classification_loss: 1.0383
 749/1000 [=====================>........] - ETA: 1:53 - loss: 3.9164 - regression_loss: 2.8782 - classification_loss: 1.0382
 750/1000 [=====================>........] - ETA: 1:53 - loss: 3.9183 - regression_loss: 2.8797 - classification_loss: 1.0386
 751/1000 [=====================>........] - ETA: 1:52 - loss: 3.9194 - regression_loss: 2.8811 - classification_loss: 1.0383
 752/1000 [=====================>........] - ETA: 1:52 - loss: 3.9142 - regression_loss: 2.8773 - classification_loss: 1.0369
 753/1000 [=====================>........] - ETA: 1:51 - loss: 3.9161 - regression_loss: 2.8793 - classification_loss: 1.0369
 754/1000 [=====================>........] - ETA: 1:51 - loss: 3.9178 - regression_loss: 2.8803 - classification_loss: 1.0375
 755/1000 [=====================>........] - ETA: 1:50 - loss: 3.9187 - regression_loss: 2.8810 - classification_loss: 1.0377
 756/1000 [=====================>........] - ETA: 1:50 - loss: 3.9135 - regression_loss: 2.8772 - classification_loss: 1.0364
 757/1000 [=====================>........] - ETA: 1:49 - loss: 3.9084 - regression_loss: 2.8734 - classification_loss: 1.0350
 758/1000 [=====================>........] - ETA: 1:49 - loss: 3.9035 - regression_loss: 2.8696 - classification_loss: 1.0339
 759/1000 [=====================>........] - ETA: 1:48 - loss: 3.9044 - regression_loss: 2.8705 - classification_loss: 1.0339
 760/1000 [=====================>........] - ETA: 1:48 - loss: 3.9053 - regression_loss: 2.8710 - classification_loss: 1.0343
 761/1000 [=====================>........] - ETA: 1:48 - loss: 3.9066 - regression_loss: 2.8717 - classification_loss: 1.0349
 762/1000 [=====================>........] - ETA: 1:47 - loss: 3.9071 - regression_loss: 2.8723 - classification_loss: 1.0348
 763/1000 [=====================>........] - ETA: 1:47 - loss: 3.9086 - regression_loss: 2.8738 - classification_loss: 1.0347
 764/1000 [=====================>........] - ETA: 1:46 - loss: 3.9104 - regression_loss: 2.8750 - classification_loss: 1.0355
 765/1000 [=====================>........] - ETA: 1:46 - loss: 3.9115 - regression_loss: 2.8754 - classification_loss: 1.0362
 766/1000 [=====================>........] - ETA: 1:45 - loss: 3.9127 - regression_loss: 2.8758 - classification_loss: 1.0369
 767/1000 [======================>.......] - ETA: 1:45 - loss: 3.9149 - regression_loss: 2.8773 - classification_loss: 1.0376
 768/1000 [======================>.......] - ETA: 1:44 - loss: 3.9157 - regression_loss: 2.8782 - classification_loss: 1.0375
 769/1000 [======================>.......] - ETA: 1:44 - loss: 3.9179 - regression_loss: 2.8797 - classification_loss: 1.0382
 770/1000 [======================>.......] - ETA: 1:43 - loss: 3.9128 - regression_loss: 2.8760 - classification_loss: 1.0368
 771/1000 [======================>.......] - ETA: 1:43 - loss: 3.9077 - regression_loss: 2.8722 - classification_loss: 1.0355
 772/1000 [======================>.......] - ETA: 1:43 - loss: 3.9096 - regression_loss: 2.8733 - classification_loss: 1.0363
 773/1000 [======================>.......] - ETA: 1:42 - loss: 3.9100 - regression_loss: 2.8740 - classification_loss: 1.0360
 774/1000 [======================>.......] - ETA: 1:42 - loss: 3.9112 - regression_loss: 2.8754 - classification_loss: 1.0358
 775/1000 [======================>.......] - ETA: 1:41 - loss: 3.9062 - regression_loss: 2.8717 - classification_loss: 1.0345
 776/1000 [======================>.......] - ETA: 1:41 - loss: 3.9073 - regression_loss: 2.8727 - classification_loss: 1.0346
 777/1000 [======================>.......] - ETA: 1:40 - loss: 3.9085 - regression_loss: 2.8740 - classification_loss: 1.0345
 778/1000 [======================>.......] - ETA: 1:40 - loss: 3.9096 - regression_loss: 2.8749 - classification_loss: 1.0347
 779/1000 [======================>.......] - ETA: 1:39 - loss: 3.9098 - regression_loss: 2.8755 - classification_loss: 1.0344
 780/1000 [======================>.......] - ETA: 1:39 - loss: 3.9117 - regression_loss: 2.8769 - classification_loss: 1.0348
 781/1000 [======================>.......] - ETA: 1:39 - loss: 3.9130 - regression_loss: 2.8776 - classification_loss: 1.0354
 782/1000 [======================>.......] - ETA: 1:38 - loss: 3.9141 - regression_loss: 2.8786 - classification_loss: 1.0355
 783/1000 [======================>.......] - ETA: 1:38 - loss: 3.9151 - regression_loss: 2.8793 - classification_loss: 1.0357
 784/1000 [======================>.......] - ETA: 1:37 - loss: 3.9160 - regression_loss: 2.8805 - classification_loss: 1.0355
 785/1000 [======================>.......] - ETA: 1:37 - loss: 3.9181 - regression_loss: 2.8816 - classification_loss: 1.0365
 786/1000 [======================>.......] - ETA: 1:36 - loss: 3.9184 - regression_loss: 2.8824 - classification_loss: 1.0360
 787/1000 [======================>.......] - ETA: 1:36 - loss: 3.9206 - regression_loss: 2.8839 - classification_loss: 1.0367
 788/1000 [======================>.......] - ETA: 1:35 - loss: 3.9164 - regression_loss: 2.8802 - classification_loss: 1.0362
 789/1000 [======================>.......] - ETA: 1:35 - loss: 3.9171 - regression_loss: 2.8814 - classification_loss: 1.0357
 790/1000 [======================>.......] - ETA: 1:34 - loss: 3.9187 - regression_loss: 2.8826 - classification_loss: 1.0361
 791/1000 [======================>.......] - ETA: 1:34 - loss: 3.9191 - regression_loss: 2.8835 - classification_loss: 1.0356
 792/1000 [======================>.......] - ETA: 1:34 - loss: 3.9196 - regression_loss: 2.8839 - classification_loss: 1.0357
 793/1000 [======================>.......] - ETA: 1:33 - loss: 3.9216 - regression_loss: 2.8852 - classification_loss: 1.0364
 794/1000 [======================>.......] - ETA: 1:33 - loss: 3.9219 - regression_loss: 2.8858 - classification_loss: 1.0361
 795/1000 [======================>.......] - ETA: 1:32 - loss: 3.9193 - regression_loss: 2.8822 - classification_loss: 1.0372
 796/1000 [======================>.......] - ETA: 1:32 - loss: 3.9202 - regression_loss: 2.8836 - classification_loss: 1.0366
 797/1000 [======================>.......] - ETA: 1:31 - loss: 3.9153 - regression_loss: 2.8800 - classification_loss: 1.0353
 798/1000 [======================>.......] - ETA: 1:31 - loss: 3.9165 - regression_loss: 2.8811 - classification_loss: 1.0354
 799/1000 [======================>.......] - ETA: 1:30 - loss: 3.9171 - regression_loss: 2.8816 - classification_loss: 1.0355
 800/1000 [=======================>......] - ETA: 1:30 - loss: 3.9122 - regression_loss: 2.8780 - classification_loss: 1.0342
 801/1000 [=======================>......] - ETA: 1:29 - loss: 3.9152 - regression_loss: 2.8806 - classification_loss: 1.0346
 802/1000 [=======================>......] - ETA: 1:29 - loss: 3.9164 - regression_loss: 2.8817 - classification_loss: 1.0347
 803/1000 [=======================>......] - ETA: 1:29 - loss: 3.9115 - regression_loss: 2.8781 - classification_loss: 1.0334
 804/1000 [=======================>......] - ETA: 1:28 - loss: 3.9115 - regression_loss: 2.8786 - classification_loss: 1.0329
 805/1000 [=======================>......] - ETA: 1:28 - loss: 3.9125 - regression_loss: 2.8799 - classification_loss: 1.0326
 806/1000 [=======================>......] - ETA: 1:27 - loss: 3.9134 - regression_loss: 2.8807 - classification_loss: 1.0327
 807/1000 [=======================>......] - ETA: 1:27 - loss: 3.9086 - regression_loss: 2.8771 - classification_loss: 1.0314
 808/1000 [=======================>......] - ETA: 1:26 - loss: 3.9088 - regression_loss: 2.8778 - classification_loss: 1.0310
 809/1000 [=======================>......] - ETA: 1:26 - loss: 3.9041 - regression_loss: 2.8743 - classification_loss: 1.0298
 810/1000 [=======================>......] - ETA: 1:25 - loss: 3.9046 - regression_loss: 2.8752 - classification_loss: 1.0294
 811/1000 [=======================>......] - ETA: 1:25 - loss: 3.9053 - regression_loss: 2.8763 - classification_loss: 1.0291
 812/1000 [=======================>......] - ETA: 1:25 - loss: 3.9062 - regression_loss: 2.8772 - classification_loss: 1.0291
 813/1000 [=======================>......] - ETA: 1:24 - loss: 3.9065 - regression_loss: 2.8778 - classification_loss: 1.0287
 814/1000 [=======================>......] - ETA: 1:24 - loss: 3.9069 - regression_loss: 2.8786 - classification_loss: 1.0283
 815/1000 [=======================>......] - ETA: 1:23 - loss: 3.9077 - regression_loss: 2.8787 - classification_loss: 1.0290
 816/1000 [=======================>......] - ETA: 1:23 - loss: 3.9038 - regression_loss: 2.8751 - classification_loss: 1.0287
 817/1000 [=======================>......] - ETA: 1:22 - loss: 3.9041 - regression_loss: 2.8749 - classification_loss: 1.0292
 818/1000 [=======================>......] - ETA: 1:22 - loss: 3.9047 - regression_loss: 2.8757 - classification_loss: 1.0289
 819/1000 [=======================>......] - ETA: 1:21 - loss: 3.9057 - regression_loss: 2.8766 - classification_loss: 1.0292
 820/1000 [=======================>......] - ETA: 1:21 - loss: 3.9067 - regression_loss: 2.8776 - classification_loss: 1.0291
 821/1000 [=======================>......] - ETA: 1:20 - loss: 3.9022 - regression_loss: 2.8741 - classification_loss: 1.0280
 822/1000 [=======================>......] - ETA: 1:20 - loss: 3.8974 - regression_loss: 2.8706 - classification_loss: 1.0268
 823/1000 [=======================>......] - ETA: 1:20 - loss: 3.8986 - regression_loss: 2.8716 - classification_loss: 1.0270
 824/1000 [=======================>......] - ETA: 1:19 - loss: 3.9015 - regression_loss: 2.8729 - classification_loss: 1.0286
 825/1000 [=======================>......] - ETA: 1:19 - loss: 3.9021 - regression_loss: 2.8737 - classification_loss: 1.0284
 826/1000 [=======================>......] - ETA: 1:18 - loss: 3.9046 - regression_loss: 2.8751 - classification_loss: 1.0295
 827/1000 [=======================>......] - ETA: 1:18 - loss: 3.9060 - regression_loss: 2.8768 - classification_loss: 1.0293
 828/1000 [=======================>......] - ETA: 1:17 - loss: 3.9068 - regression_loss: 2.8780 - classification_loss: 1.0289
 829/1000 [=======================>......] - ETA: 1:17 - loss: 3.9107 - regression_loss: 2.8805 - classification_loss: 1.0301
 830/1000 [=======================>......] - ETA: 1:16 - loss: 3.9122 - regression_loss: 2.8815 - classification_loss: 1.0307
 831/1000 [=======================>......] - ETA: 1:16 - loss: 3.9140 - regression_loss: 2.8827 - classification_loss: 1.0313
 832/1000 [=======================>......] - ETA: 1:15 - loss: 3.9148 - regression_loss: 2.8832 - classification_loss: 1.0316
 833/1000 [=======================>......] - ETA: 1:15 - loss: 3.9153 - regression_loss: 2.8838 - classification_loss: 1.0315
 834/1000 [========================>.....] - ETA: 1:15 - loss: 3.9106 - regression_loss: 2.8804 - classification_loss: 1.0302
 835/1000 [========================>.....] - ETA: 1:14 - loss: 3.9115 - regression_loss: 2.8818 - classification_loss: 1.0298
 836/1000 [========================>.....] - ETA: 1:14 - loss: 3.9069 - regression_loss: 2.8783 - classification_loss: 1.0285
 837/1000 [========================>.....] - ETA: 1:13 - loss: 3.9022 - regression_loss: 2.8749 - classification_loss: 1.0273
 838/1000 [========================>.....] - ETA: 1:13 - loss: 3.9027 - regression_loss: 2.8750 - classification_loss: 1.0277
 839/1000 [========================>.....] - ETA: 1:12 - loss: 3.9039 - regression_loss: 2.8756 - classification_loss: 1.0283
 840/1000 [========================>.....] - ETA: 1:12 - loss: 3.9051 - regression_loss: 2.8764 - classification_loss: 1.0288
 841/1000 [========================>.....] - ETA: 1:11 - loss: 3.9005 - regression_loss: 2.8729 - classification_loss: 1.0275
 842/1000 [========================>.....] - ETA: 1:11 - loss: 3.8958 - regression_loss: 2.8695 - classification_loss: 1.0263
 843/1000 [========================>.....] - ETA: 1:11 - loss: 3.8964 - regression_loss: 2.8704 - classification_loss: 1.0260
 844/1000 [========================>.....] - ETA: 1:10 - loss: 3.8918 - regression_loss: 2.8670 - classification_loss: 1.0248
 845/1000 [========================>.....] - ETA: 1:10 - loss: 3.8927 - regression_loss: 2.8676 - classification_loss: 1.0252
 846/1000 [========================>.....] - ETA: 1:09 - loss: 3.8941 - regression_loss: 2.8682 - classification_loss: 1.0259
 847/1000 [========================>.....] - ETA: 1:09 - loss: 3.8943 - regression_loss: 2.8686 - classification_loss: 1.0257
 848/1000 [========================>.....] - ETA: 1:08 - loss: 3.8897 - regression_loss: 2.8652 - classification_loss: 1.0245
 849/1000 [========================>.....] - ETA: 1:08 - loss: 3.8895 - regression_loss: 2.8654 - classification_loss: 1.0241
 850/1000 [========================>.....] - ETA: 1:07 - loss: 3.8906 - regression_loss: 2.8659 - classification_loss: 1.0247
 851/1000 [========================>.....] - ETA: 1:07 - loss: 3.8860 - regression_loss: 2.8625 - classification_loss: 1.0235
 852/1000 [========================>.....] - ETA: 1:06 - loss: 3.8871 - regression_loss: 2.8624 - classification_loss: 1.0247
 853/1000 [========================>.....] - ETA: 1:06 - loss: 3.8896 - regression_loss: 2.8633 - classification_loss: 1.0263
 854/1000 [========================>.....] - ETA: 1:06 - loss: 3.8904 - regression_loss: 2.8641 - classification_loss: 1.0262
 855/1000 [========================>.....] - ETA: 1:05 - loss: 3.8862 - regression_loss: 2.8608 - classification_loss: 1.0254
 856/1000 [========================>.....] - ETA: 1:05 - loss: 3.8885 - regression_loss: 2.8621 - classification_loss: 1.0264
 857/1000 [========================>.....] - ETA: 1:04 - loss: 3.8839 - regression_loss: 2.8587 - classification_loss: 1.0252
 858/1000 [========================>.....] - ETA: 1:04 - loss: 3.8849 - regression_loss: 2.8593 - classification_loss: 1.0255
 859/1000 [========================>.....] - ETA: 1:03 - loss: 3.8846 - regression_loss: 2.8595 - classification_loss: 1.0250
 860/1000 [========================>.....] - ETA: 1:03 - loss: 3.8800 - regression_loss: 2.8562 - classification_loss: 1.0238
 861/1000 [========================>.....] - ETA: 1:02 - loss: 3.8823 - regression_loss: 2.8578 - classification_loss: 1.0245
 862/1000 [========================>.....] - ETA: 1:02 - loss: 3.8833 - regression_loss: 2.8584 - classification_loss: 1.0249
 863/1000 [========================>.....] - ETA: 1:01 - loss: 3.8839 - regression_loss: 2.8591 - classification_loss: 1.0248
 864/1000 [========================>.....] - ETA: 1:01 - loss: 3.8855 - regression_loss: 2.8607 - classification_loss: 1.0249
 865/1000 [========================>.....] - ETA: 1:01 - loss: 3.8873 - regression_loss: 2.8614 - classification_loss: 1.0258
 866/1000 [========================>.....] - ETA: 1:00 - loss: 3.8876 - regression_loss: 2.8621 - classification_loss: 1.0255
 867/1000 [=========================>....] - ETA: 1:00 - loss: 3.8886 - regression_loss: 2.8631 - classification_loss: 1.0255
 868/1000 [=========================>....] - ETA: 59s - loss: 3.8901 - regression_loss: 2.8639 - classification_loss: 1.0262 
 869/1000 [=========================>....] - ETA: 59s - loss: 3.8915 - regression_loss: 2.8648 - classification_loss: 1.0267
 870/1000 [=========================>....] - ETA: 58s - loss: 3.8919 - regression_loss: 2.8651 - classification_loss: 1.0268
 871/1000 [=========================>....] - ETA: 58s - loss: 3.8875 - regression_loss: 2.8618 - classification_loss: 1.0257
 872/1000 [=========================>....] - ETA: 57s - loss: 3.8894 - regression_loss: 2.8629 - classification_loss: 1.0264
 873/1000 [=========================>....] - ETA: 57s - loss: 3.8914 - regression_loss: 2.8643 - classification_loss: 1.0270
 874/1000 [=========================>....] - ETA: 56s - loss: 3.8919 - regression_loss: 2.8653 - classification_loss: 1.0266
 875/1000 [=========================>....] - ETA: 56s - loss: 3.8923 - regression_loss: 2.8662 - classification_loss: 1.0261
 876/1000 [=========================>....] - ETA: 56s - loss: 3.8926 - regression_loss: 2.8663 - classification_loss: 1.0262
 877/1000 [=========================>....] - ETA: 55s - loss: 3.8941 - regression_loss: 2.8672 - classification_loss: 1.0269
 878/1000 [=========================>....] - ETA: 55s - loss: 3.8896 - regression_loss: 2.8639 - classification_loss: 1.0257
 879/1000 [=========================>....] - ETA: 54s - loss: 3.8913 - regression_loss: 2.8652 - classification_loss: 1.0261
 880/1000 [=========================>....] - ETA: 54s - loss: 3.8923 - regression_loss: 2.8658 - classification_loss: 1.0265
 881/1000 [=========================>....] - ETA: 53s - loss: 3.8930 - regression_loss: 2.8670 - classification_loss: 1.0260
 882/1000 [=========================>....] - ETA: 53s - loss: 3.8932 - regression_loss: 2.8672 - classification_loss: 1.0260
 883/1000 [=========================>....] - ETA: 52s - loss: 3.8936 - regression_loss: 2.8678 - classification_loss: 1.0258
 884/1000 [=========================>....] - ETA: 52s - loss: 3.8938 - regression_loss: 2.8683 - classification_loss: 1.0255
 885/1000 [=========================>....] - ETA: 52s - loss: 3.8940 - regression_loss: 2.8689 - classification_loss: 1.0251
 886/1000 [=========================>....] - ETA: 51s - loss: 3.8948 - regression_loss: 2.8696 - classification_loss: 1.0253
 887/1000 [=========================>....] - ETA: 51s - loss: 3.8950 - regression_loss: 2.8702 - classification_loss: 1.0248
 888/1000 [=========================>....] - ETA: 50s - loss: 3.8906 - regression_loss: 2.8670 - classification_loss: 1.0236
 889/1000 [=========================>....] - ETA: 50s - loss: 3.8922 - regression_loss: 2.8683 - classification_loss: 1.0239
 890/1000 [=========================>....] - ETA: 49s - loss: 3.8878 - regression_loss: 2.8651 - classification_loss: 1.0227
 891/1000 [=========================>....] - ETA: 49s - loss: 3.8885 - regression_loss: 2.8661 - classification_loss: 1.0224
 892/1000 [=========================>....] - ETA: 48s - loss: 3.8903 - regression_loss: 2.8675 - classification_loss: 1.0228
 893/1000 [=========================>....] - ETA: 48s - loss: 3.8913 - regression_loss: 2.8688 - classification_loss: 1.0225
 894/1000 [=========================>....] - ETA: 47s - loss: 3.8938 - regression_loss: 2.8704 - classification_loss: 1.0234
 895/1000 [=========================>....] - ETA: 47s - loss: 3.8895 - regression_loss: 2.8672 - classification_loss: 1.0223
 896/1000 [=========================>....] - ETA: 47s - loss: 3.8906 - regression_loss: 2.8685 - classification_loss: 1.0221
 897/1000 [=========================>....] - ETA: 46s - loss: 3.8913 - regression_loss: 2.8689 - classification_loss: 1.0224
 898/1000 [=========================>....] - ETA: 46s - loss: 3.8869 - regression_loss: 2.8657 - classification_loss: 1.0212
 899/1000 [=========================>....] - ETA: 45s - loss: 3.8883 - regression_loss: 2.8666 - classification_loss: 1.0217
 900/1000 [==========================>...] - ETA: 45s - loss: 3.8840 - regression_loss: 2.8635 - classification_loss: 1.0205
 901/1000 [==========================>...] - ETA: 44s - loss: 3.8849 - regression_loss: 2.8639 - classification_loss: 1.0210
 902/1000 [==========================>...] - ETA: 44s - loss: 3.8852 - regression_loss: 2.8645 - classification_loss: 1.0207
 903/1000 [==========================>...] - ETA: 43s - loss: 3.8859 - regression_loss: 2.8652 - classification_loss: 1.0207
 904/1000 [==========================>...] - ETA: 43s - loss: 3.8871 - regression_loss: 2.8661 - classification_loss: 1.0210
 905/1000 [==========================>...] - ETA: 42s - loss: 3.8878 - regression_loss: 2.8662 - classification_loss: 1.0216
 906/1000 [==========================>...] - ETA: 42s - loss: 3.8890 - regression_loss: 2.8676 - classification_loss: 1.0214
 907/1000 [==========================>...] - ETA: 42s - loss: 3.8903 - regression_loss: 2.8690 - classification_loss: 1.0213
 908/1000 [==========================>...] - ETA: 41s - loss: 3.8860 - regression_loss: 2.8658 - classification_loss: 1.0202
 909/1000 [==========================>...] - ETA: 41s - loss: 3.8862 - regression_loss: 2.8663 - classification_loss: 1.0199
 910/1000 [==========================>...] - ETA: 40s - loss: 3.8868 - regression_loss: 2.8671 - classification_loss: 1.0197
 911/1000 [==========================>...] - ETA: 40s - loss: 3.8868 - regression_loss: 2.8675 - classification_loss: 1.0193
 912/1000 [==========================>...] - ETA: 39s - loss: 3.8826 - regression_loss: 2.8644 - classification_loss: 1.0182
 913/1000 [==========================>...] - ETA: 39s - loss: 3.8783 - regression_loss: 2.8613 - classification_loss: 1.0171
 914/1000 [==========================>...] - ETA: 38s - loss: 3.8789 - regression_loss: 2.8621 - classification_loss: 1.0168
 915/1000 [==========================>...] - ETA: 38s - loss: 3.8806 - regression_loss: 2.8641 - classification_loss: 1.0165
 916/1000 [==========================>...] - ETA: 37s - loss: 3.8763 - regression_loss: 2.8610 - classification_loss: 1.0154
 917/1000 [==========================>...] - ETA: 37s - loss: 3.8775 - regression_loss: 2.8614 - classification_loss: 1.0160
 918/1000 [==========================>...] - ETA: 37s - loss: 3.8786 - regression_loss: 2.8625 - classification_loss: 1.0161
 919/1000 [==========================>...] - ETA: 36s - loss: 3.8757 - regression_loss: 2.8594 - classification_loss: 1.0163
 920/1000 [==========================>...] - ETA: 36s - loss: 3.8772 - regression_loss: 2.8599 - classification_loss: 1.0173
 921/1000 [==========================>...] - ETA: 35s - loss: 3.8789 - regression_loss: 2.8610 - classification_loss: 1.0179
 922/1000 [==========================>...] - ETA: 35s - loss: 3.8747 - regression_loss: 2.8579 - classification_loss: 1.0168
 923/1000 [==========================>...] - ETA: 34s - loss: 3.8749 - regression_loss: 2.8583 - classification_loss: 1.0166
 924/1000 [==========================>...] - ETA: 34s - loss: 3.8766 - regression_loss: 2.8592 - classification_loss: 1.0174
 925/1000 [==========================>...] - ETA: 33s - loss: 3.8724 - regression_loss: 2.8561 - classification_loss: 1.0163
 926/1000 [==========================>...] - ETA: 33s - loss: 3.8739 - regression_loss: 2.8567 - classification_loss: 1.0172
 927/1000 [==========================>...] - ETA: 33s - loss: 3.8751 - regression_loss: 2.8571 - classification_loss: 1.0180
 928/1000 [==========================>...] - ETA: 32s - loss: 3.8755 - regression_loss: 2.8579 - classification_loss: 1.0176
 929/1000 [==========================>...] - ETA: 32s - loss: 3.8764 - regression_loss: 2.8583 - classification_loss: 1.0181
 930/1000 [==========================>...] - ETA: 31s - loss: 3.8768 - regression_loss: 2.8588 - classification_loss: 1.0179
 931/1000 [==========================>...] - ETA: 31s - loss: 3.8726 - regression_loss: 2.8558 - classification_loss: 1.0168
 932/1000 [==========================>...] - ETA: 30s - loss: 3.8742 - regression_loss: 2.8566 - classification_loss: 1.0176
 933/1000 [==========================>...] - ETA: 30s - loss: 3.8749 - regression_loss: 2.8576 - classification_loss: 1.0173
 934/1000 [===========================>..] - ETA: 29s - loss: 3.8757 - regression_loss: 2.8587 - classification_loss: 1.0170
 935/1000 [===========================>..] - ETA: 29s - loss: 3.8763 - regression_loss: 2.8596 - classification_loss: 1.0167
 936/1000 [===========================>..] - ETA: 28s - loss: 3.8781 - regression_loss: 2.8609 - classification_loss: 1.0172
 937/1000 [===========================>..] - ETA: 28s - loss: 3.8790 - regression_loss: 2.8623 - classification_loss: 1.0167
 938/1000 [===========================>..] - ETA: 28s - loss: 3.8798 - regression_loss: 2.8627 - classification_loss: 1.0171
 939/1000 [===========================>..] - ETA: 27s - loss: 3.8813 - regression_loss: 2.8637 - classification_loss: 1.0177
 940/1000 [===========================>..] - ETA: 27s - loss: 3.8818 - regression_loss: 2.8642 - classification_loss: 1.0176
 941/1000 [===========================>..] - ETA: 26s - loss: 3.8822 - regression_loss: 2.8642 - classification_loss: 1.0180
 942/1000 [===========================>..] - ETA: 26s - loss: 3.8836 - regression_loss: 2.8648 - classification_loss: 1.0187
 943/1000 [===========================>..] - ETA: 25s - loss: 3.8842 - regression_loss: 2.8657 - classification_loss: 1.0185
 944/1000 [===========================>..] - ETA: 25s - loss: 3.8844 - regression_loss: 2.8660 - classification_loss: 1.0184
 945/1000 [===========================>..] - ETA: 24s - loss: 3.8834 - regression_loss: 2.8630 - classification_loss: 1.0205
 946/1000 [===========================>..] - ETA: 24s - loss: 3.8842 - regression_loss: 2.8640 - classification_loss: 1.0202
 947/1000 [===========================>..] - ETA: 23s - loss: 3.8846 - regression_loss: 2.8644 - classification_loss: 1.0202
 948/1000 [===========================>..] - ETA: 23s - loss: 3.8856 - regression_loss: 2.8657 - classification_loss: 1.0199
 949/1000 [===========================>..] - ETA: 23s - loss: 3.8863 - regression_loss: 2.8666 - classification_loss: 1.0198
 950/1000 [===========================>..] - ETA: 22s - loss: 3.8868 - regression_loss: 2.8674 - classification_loss: 1.0195
 951/1000 [===========================>..] - ETA: 22s - loss: 3.8870 - regression_loss: 2.8679 - classification_loss: 1.0190
 952/1000 [===========================>..] - ETA: 21s - loss: 3.8877 - regression_loss: 2.8684 - classification_loss: 1.0192
 953/1000 [===========================>..] - ETA: 21s - loss: 3.8881 - regression_loss: 2.8693 - classification_loss: 1.0188
 954/1000 [===========================>..] - ETA: 20s - loss: 3.8847 - regression_loss: 2.8663 - classification_loss: 1.0184
 955/1000 [===========================>..] - ETA: 20s - loss: 3.8852 - regression_loss: 2.8671 - classification_loss: 1.0181
 956/1000 [===========================>..] - ETA: 19s - loss: 3.8860 - regression_loss: 2.8679 - classification_loss: 1.0181
 957/1000 [===========================>..] - ETA: 19s - loss: 3.8859 - regression_loss: 2.8683 - classification_loss: 1.0176
 958/1000 [===========================>..] - ETA: 18s - loss: 3.8861 - regression_loss: 2.8687 - classification_loss: 1.0174
 959/1000 [===========================>..] - ETA: 18s - loss: 3.8870 - regression_loss: 2.8697 - classification_loss: 1.0174
 960/1000 [===========================>..] - ETA: 18s - loss: 3.8873 - regression_loss: 2.8701 - classification_loss: 1.0172
 961/1000 [===========================>..] - ETA: 17s - loss: 3.8877 - regression_loss: 2.8705 - classification_loss: 1.0172
 962/1000 [===========================>..] - ETA: 17s - loss: 3.8847 - regression_loss: 2.8675 - classification_loss: 1.0172
 963/1000 [===========================>..] - ETA: 16s - loss: 3.8807 - regression_loss: 2.8645 - classification_loss: 1.0162
 964/1000 [===========================>..] - ETA: 16s - loss: 3.8810 - regression_loss: 2.8648 - classification_loss: 1.0162
 965/1000 [===========================>..] - ETA: 15s - loss: 3.8770 - regression_loss: 2.8618 - classification_loss: 1.0151
 966/1000 [===========================>..] - ETA: 15s - loss: 3.8779 - regression_loss: 2.8626 - classification_loss: 1.0153
 967/1000 [============================>.] - ETA: 14s - loss: 3.8739 - regression_loss: 2.8596 - classification_loss: 1.0143
 968/1000 [============================>.] - ETA: 14s - loss: 3.8741 - regression_loss: 2.8603 - classification_loss: 1.0138
 969/1000 [============================>.] - ETA: 14s - loss: 3.8751 - regression_loss: 2.8610 - classification_loss: 1.0140
 970/1000 [============================>.] - ETA: 13s - loss: 3.8762 - regression_loss: 2.8620 - classification_loss: 1.0143
 971/1000 [============================>.] - ETA: 13s - loss: 3.8764 - regression_loss: 2.8624 - classification_loss: 1.0140
 972/1000 [============================>.] - ETA: 12s - loss: 3.8764 - regression_loss: 2.8629 - classification_loss: 1.0135
 973/1000 [============================>.] - ETA: 12s - loss: 3.8764 - regression_loss: 2.8628 - classification_loss: 1.0136
 974/1000 [============================>.] - ETA: 11s - loss: 3.8768 - regression_loss: 2.8633 - classification_loss: 1.0134
 975/1000 [============================>.] - ETA: 11s - loss: 3.8769 - regression_loss: 2.8637 - classification_loss: 1.0132
 976/1000 [============================>.] - ETA: 10s - loss: 3.8775 - regression_loss: 2.8647 - classification_loss: 1.0129
 977/1000 [============================>.] - ETA: 10s - loss: 3.8801 - regression_loss: 2.8668 - classification_loss: 1.0133
 978/1000 [============================>.] - ETA: 9s - loss: 3.8811 - regression_loss: 2.8675 - classification_loss: 1.0136 
 979/1000 [============================>.] - ETA: 9s - loss: 3.8771 - regression_loss: 2.8645 - classification_loss: 1.0126
 980/1000 [============================>.] - ETA: 9s - loss: 3.8774 - regression_loss: 2.8650 - classification_loss: 1.0123
 981/1000 [============================>.] - ETA: 8s - loss: 3.8784 - regression_loss: 2.8659 - classification_loss: 1.0125
 982/1000 [============================>.] - ETA: 8s - loss: 3.8790 - regression_loss: 2.8662 - classification_loss: 1.0128
 983/1000 [============================>.] - ETA: 7s - loss: 3.8796 - regression_loss: 2.8664 - classification_loss: 1.0132
 984/1000 [============================>.] - ETA: 7s - loss: 3.8759 - regression_loss: 2.8635 - classification_loss: 1.0124
 985/1000 [============================>.] - ETA: 6s - loss: 3.8763 - regression_loss: 2.8640 - classification_loss: 1.0123
 986/1000 [============================>.] - ETA: 6s - loss: 3.8773 - regression_loss: 2.8649 - classification_loss: 1.0124
 987/1000 [============================>.] - ETA: 5s - loss: 3.8795 - regression_loss: 2.8666 - classification_loss: 1.0129
 988/1000 [============================>.] - ETA: 5s - loss: 3.8802 - regression_loss: 2.8672 - classification_loss: 1.0130
 989/1000 [============================>.] - ETA: 4s - loss: 3.8802 - regression_loss: 2.8676 - classification_loss: 1.0126
 990/1000 [============================>.] - ETA: 4s - loss: 3.8801 - regression_loss: 2.8678 - classification_loss: 1.0123
 991/1000 [============================>.] - ETA: 4s - loss: 3.8822 - regression_loss: 2.8692 - classification_loss: 1.0130
 992/1000 [============================>.] - ETA: 3s - loss: 3.8783 - regression_loss: 2.8663 - classification_loss: 1.0120
 993/1000 [============================>.] - ETA: 3s - loss: 3.8750 - regression_loss: 2.8634 - classification_loss: 1.0116
 994/1000 [============================>.] - ETA: 2s - loss: 3.8781 - regression_loss: 2.8663 - classification_loss: 1.0118
 995/1000 [============================>.] - ETA: 2s - loss: 3.8797 - regression_loss: 2.8682 - classification_loss: 1.0115
 996/1000 [============================>.] - ETA: 1s - loss: 3.8799 - regression_loss: 2.8681 - classification_loss: 1.0118
 997/1000 [============================>.] - ETA: 1s - loss: 3.8760 - regression_loss: 2.8653 - classification_loss: 1.0108
 998/1000 [============================>.] - ETA: 0s - loss: 3.8763 - regression_loss: 2.8658 - classification_loss: 1.0105
 999/1000 [============================>.] - ETA: 0s - loss: 3.8775 - regression_loss: 2.8666 - classification_loss: 1.0109
1000/1000 [==============================] - 452s 452ms/step - loss: 3.8736 - regression_loss: 2.8637 - classification_loss: 1.0099

Epoch 00003: saving model to ./snapshots/resnet50_csv_03.h5
0/2001/2002/2003/2004/2005/2006/2007/2008/2009/20010/20011/20012/20013/20014/20015/20016/20017/20018/20019/20020/20021/20022/20023/20024/20025/20026/20027/20028/20029/20030/20031/20032/20033/20034/20035/20036/20037/20038/20039/20040/20041/20042/20043/20044/20045/20046/20047/20048/20049/20050/20051/20052/20053/20054/20055/20056/20057/20058/20059/20060/20061/20062/20063/20064/20065/20066/20067/20068/20069/20070/20071/20072/20073/20074/20075/20076/20077/20078/20079/20080/20081/20082/20083/20084/20085/20086/20087/20088/20089/20090/20091/20092/20093/20094/20095/20096/20097/20098/20099/200100/200101/200102/200103/200104/200105/200106/200107/200108/200109/200110/200111/200112/200113/200114/200115/200116/200117/200118/200119/200120/200121/200122/200123/200124/200125/200126/200127/200128/200129/200130/200131/200132/200133/200134/200135/200136/200137/200138/200139/200140/200141/200142/200143/200144/200145/200146/200147/200148/200149/200150/200151/200152/200153/200154/200155/200156/200157/200158/200159/200160/200161/200162/200163/200164/200165/200166/200167/200168/200169/200170/200171/200172/200173/200174/200175/200176/200177/200178/200179/200180/200181/200182/200183/200184/200185/200186/200187/200188/200189/200190/200191/200192/200193/200194/200195/200196/200197/200198/200199/2000/2001/2002/2003/2004/2005/2006/2007/2008/2009/20010/20011/20012/20013/20014/20015/20016/20017/20018/20019/20020/20021/20022/20023/20024/20025/20026/20027/20028/20029/20030/20031/20032/20033/20034/20035/20036/20037/20038/20039/20040/20041/20042/20043/20044/20045/20046/20047/20048/20049/20050/20051/20052/20053/20054/20055/20056/20057/20058/20059/20060/20061/20062/20063/20064/20065/20066/20067/20068/20069/20070/20071/20072/20073/20074/20075/20076/20077/20078/20079/20080/20081/20082/20083/20084/20085/20086/20087/20088/20089/20090/20091/20092/20093/20094/20095/20096/20097/20098/20099/200100/200101/200102/200103/200104/200105/200106/200107/200108/200109/200110/200111/200112/200113/200114/200115/200116/200117/200118/200119/200120/200121/200122/200123/200124/200125/200126/200127/200128/200129/200130/200131/200132/200133/200134/200135/200136/200137/200138/200139/200140/200141/200142/200143/200144/200145/200146/200147/200148/200149/200150/200151/200152/200153/200154/200155/200156/200157/200158/200159/200160/200161/200162/200163/200164/200165/200166/200167/200168/200169/200170/200171/200172/200173/200174/200175/200176/200177/200178/200179/200180/200181/200182/200183/200184/200185/200186/200187/200188/200189/200190/200191/200192/200193/200194/200195/200196/200197/200198/200199/200

M 0.0034
N 0.0000
mAP: 0.0017
Epoch 4/30

   1/1000 [..............................] - ETA: 7:21 - loss: 4.6484 - regression_loss: 3.5175 - classification_loss: 1.1308
   2/1000 [..............................] - ETA: 7:26 - loss: 4.4961 - regression_loss: 3.2630 - classification_loss: 1.2332
   3/1000 [..............................] - ETA: 7:20 - loss: 4.5393 - regression_loss: 3.4052 - classification_loss: 1.1341
   4/1000 [..............................] - ETA: 7:22 - loss: 4.5743 - regression_loss: 3.4679 - classification_loss: 1.1064
   5/1000 [..............................] - ETA: 7:22 - loss: 4.8325 - regression_loss: 3.6629 - classification_loss: 1.1696
   6/1000 [..............................] - ETA: 7:22 - loss: 4.7889 - regression_loss: 3.5633 - classification_loss: 1.2256
   7/1000 [..............................] - ETA: 7:22 - loss: 4.8426 - regression_loss: 3.5936 - classification_loss: 1.2490
   8/1000 [..............................] - ETA: 7:22 - loss: 4.8048 - regression_loss: 3.5639 - classification_loss: 1.2410
   9/1000 [..............................] - ETA: 7:22 - loss: 4.7564 - regression_loss: 3.5406 - classification_loss: 1.2158
  10/1000 [..............................] - ETA: 7:21 - loss: 4.7178 - regression_loss: 3.5481 - classification_loss: 1.1697
  11/1000 [..............................] - ETA: 7:21 - loss: 4.2890 - regression_loss: 3.2256 - classification_loss: 1.0634
  12/1000 [..............................] - ETA: 7:22 - loss: 4.0906 - regression_loss: 2.9568 - classification_loss: 1.1338
  13/1000 [..............................] - ETA: 7:22 - loss: 4.2322 - regression_loss: 3.0279 - classification_loss: 1.2043
  14/1000 [..............................] - ETA: 7:21 - loss: 4.3024 - regression_loss: 3.0756 - classification_loss: 1.2268
  15/1000 [..............................] - ETA: 7:21 - loss: 4.0156 - regression_loss: 2.8705 - classification_loss: 1.1451
  16/1000 [..............................] - ETA: 7:20 - loss: 4.0620 - regression_loss: 2.9054 - classification_loss: 1.1566
  17/1000 [..............................] - ETA: 7:20 - loss: 4.0503 - regression_loss: 2.9248 - classification_loss: 1.1255
  18/1000 [..............................] - ETA: 7:20 - loss: 4.0659 - regression_loss: 2.9561 - classification_loss: 1.1097
  19/1000 [..............................] - ETA: 7:19 - loss: 4.0843 - regression_loss: 2.9867 - classification_loss: 1.0976
  20/1000 [..............................] - ETA: 7:19 - loss: 4.1161 - regression_loss: 3.0166 - classification_loss: 1.0995
  21/1000 [..............................] - ETA: 7:19 - loss: 3.9548 - regression_loss: 2.8729 - classification_loss: 1.0819
  22/1000 [..............................] - ETA: 7:19 - loss: 3.9831 - regression_loss: 2.9114 - classification_loss: 1.0716
  23/1000 [..............................] - ETA: 7:18 - loss: 4.0132 - regression_loss: 2.9575 - classification_loss: 1.0558
  24/1000 [..............................] - ETA: 7:18 - loss: 3.8460 - regression_loss: 2.8342 - classification_loss: 1.0118
  25/1000 [..............................] - ETA: 7:17 - loss: 3.8571 - regression_loss: 2.8403 - classification_loss: 1.0168
  26/1000 [..............................] - ETA: 7:17 - loss: 3.8775 - regression_loss: 2.8610 - classification_loss: 1.0165
  27/1000 [..............................] - ETA: 7:17 - loss: 3.9117 - regression_loss: 2.9062 - classification_loss: 1.0055
  28/1000 [..............................] - ETA: 7:16 - loss: 3.7832 - regression_loss: 2.8024 - classification_loss: 0.9808
  29/1000 [..............................] - ETA: 7:16 - loss: 3.8382 - regression_loss: 2.8528 - classification_loss: 0.9854
  30/1000 [..............................] - ETA: 7:15 - loss: 3.8836 - regression_loss: 2.8844 - classification_loss: 0.9992
  31/1000 [..............................] - ETA: 7:15 - loss: 3.8969 - regression_loss: 2.9017 - classification_loss: 0.9952
  32/1000 [..............................] - ETA: 7:14 - loss: 3.9019 - regression_loss: 2.9097 - classification_loss: 0.9922
  33/1000 [..............................] - ETA: 7:14 - loss: 3.9143 - regression_loss: 2.9205 - classification_loss: 0.9937
  34/1000 [>.............................] - ETA: 7:13 - loss: 3.9369 - regression_loss: 2.9483 - classification_loss: 0.9887
  35/1000 [>.............................] - ETA: 7:13 - loss: 3.8251 - regression_loss: 2.8640 - classification_loss: 0.9611
  36/1000 [>.............................] - ETA: 7:13 - loss: 3.7313 - regression_loss: 2.7845 - classification_loss: 0.9468
  37/1000 [>.............................] - ETA: 7:12 - loss: 3.7621 - regression_loss: 2.8126 - classification_loss: 0.9495
  38/1000 [>.............................] - ETA: 7:12 - loss: 3.6631 - regression_loss: 2.7386 - classification_loss: 0.9245
  39/1000 [>.............................] - ETA: 7:12 - loss: 3.6876 - regression_loss: 2.7502 - classification_loss: 0.9374
  40/1000 [>.............................] - ETA: 7:11 - loss: 3.7117 - regression_loss: 2.7721 - classification_loss: 0.9396
  41/1000 [>.............................] - ETA: 7:11 - loss: 3.7373 - regression_loss: 2.7999 - classification_loss: 0.9374
  42/1000 [>.............................] - ETA: 7:11 - loss: 3.7534 - regression_loss: 2.8199 - classification_loss: 0.9335
  43/1000 [>.............................] - ETA: 7:10 - loss: 3.7859 - regression_loss: 2.8346 - classification_loss: 0.9512
  44/1000 [>.............................] - ETA: 7:10 - loss: 3.6998 - regression_loss: 2.7702 - classification_loss: 0.9296
  45/1000 [>.............................] - ETA: 7:09 - loss: 3.7170 - regression_loss: 2.7895 - classification_loss: 0.9275
  46/1000 [>.............................] - ETA: 7:09 - loss: 3.7380 - regression_loss: 2.8111 - classification_loss: 0.9269
  47/1000 [>.............................] - ETA: 7:09 - loss: 3.7579 - regression_loss: 2.8229 - classification_loss: 0.9350
  48/1000 [>.............................] - ETA: 7:08 - loss: 3.6798 - regression_loss: 2.7641 - classification_loss: 0.9157
  49/1000 [>.............................] - ETA: 7:08 - loss: 3.7265 - regression_loss: 2.7829 - classification_loss: 0.9435
  50/1000 [>.............................] - ETA: 7:08 - loss: 3.6519 - regression_loss: 2.7273 - classification_loss: 0.9247
  51/1000 [>.............................] - ETA: 7:07 - loss: 3.6675 - regression_loss: 2.7300 - classification_loss: 0.9375
  52/1000 [>.............................] - ETA: 7:07 - loss: 3.6924 - regression_loss: 2.7485 - classification_loss: 0.9439
  53/1000 [>.............................] - ETA: 7:06 - loss: 3.6227 - regression_loss: 2.6966 - classification_loss: 0.9261
  54/1000 [>.............................] - ETA: 7:06 - loss: 3.6303 - regression_loss: 2.7024 - classification_loss: 0.9280
  55/1000 [>.............................] - ETA: 7:05 - loss: 3.6505 - regression_loss: 2.7109 - classification_loss: 0.9395
  56/1000 [>.............................] - ETA: 7:05 - loss: 3.6785 - regression_loss: 2.7329 - classification_loss: 0.9456
  57/1000 [>.............................] - ETA: 7:04 - loss: 3.7011 - regression_loss: 2.7448 - classification_loss: 0.9563
  58/1000 [>.............................] - ETA: 7:04 - loss: 3.7136 - regression_loss: 2.7521 - classification_loss: 0.9615
  59/1000 [>.............................] - ETA: 7:04 - loss: 3.7328 - regression_loss: 2.7734 - classification_loss: 0.9594
  60/1000 [>.............................] - ETA: 7:03 - loss: 3.7544 - regression_loss: 2.7915 - classification_loss: 0.9629
  61/1000 [>.............................] - ETA: 7:03 - loss: 3.6928 - regression_loss: 2.7457 - classification_loss: 0.9471
  62/1000 [>.............................] - ETA: 7:02 - loss: 3.6981 - regression_loss: 2.7483 - classification_loss: 0.9498
  63/1000 [>.............................] - ETA: 7:02 - loss: 3.7084 - regression_loss: 2.7609 - classification_loss: 0.9476
  64/1000 [>.............................] - ETA: 7:02 - loss: 3.7322 - regression_loss: 2.7755 - classification_loss: 0.9567
  65/1000 [>.............................] - ETA: 7:01 - loss: 3.7682 - regression_loss: 2.8099 - classification_loss: 0.9583
  66/1000 [>.............................] - ETA: 7:01 - loss: 3.7792 - regression_loss: 2.8251 - classification_loss: 0.9541
  67/1000 [=>............................] - ETA: 7:00 - loss: 3.8025 - regression_loss: 2.8380 - classification_loss: 0.9645
  68/1000 [=>............................] - ETA: 7:00 - loss: 3.8065 - regression_loss: 2.8480 - classification_loss: 0.9585
  69/1000 [=>............................] - ETA: 7:00 - loss: 3.8272 - regression_loss: 2.8622 - classification_loss: 0.9650
  70/1000 [=>............................] - ETA: 6:59 - loss: 3.8378 - regression_loss: 2.8678 - classification_loss: 0.9700
  71/1000 [=>............................] - ETA: 6:59 - loss: 3.8584 - regression_loss: 2.8786 - classification_loss: 0.9798
  72/1000 [=>............................] - ETA: 6:58 - loss: 3.8656 - regression_loss: 2.8867 - classification_loss: 0.9789
  73/1000 [=>............................] - ETA: 6:58 - loss: 3.8738 - regression_loss: 2.8921 - classification_loss: 0.9817
  74/1000 [=>............................] - ETA: 6:57 - loss: 3.8882 - regression_loss: 2.9031 - classification_loss: 0.9851
  75/1000 [=>............................] - ETA: 6:57 - loss: 3.9018 - regression_loss: 2.9169 - classification_loss: 0.9849
  76/1000 [=>............................] - ETA: 6:56 - loss: 3.8541 - regression_loss: 2.8786 - classification_loss: 0.9755
  77/1000 [=>............................] - ETA: 6:56 - loss: 3.8418 - regression_loss: 2.8412 - classification_loss: 1.0006
  78/1000 [=>............................] - ETA: 6:56 - loss: 3.8477 - regression_loss: 2.8493 - classification_loss: 0.9984
  79/1000 [=>............................] - ETA: 6:55 - loss: 3.8596 - regression_loss: 2.8613 - classification_loss: 0.9983
  80/1000 [=>............................] - ETA: 6:55 - loss: 3.8662 - regression_loss: 2.8726 - classification_loss: 0.9936
  81/1000 [=>............................] - ETA: 6:54 - loss: 3.8745 - regression_loss: 2.8846 - classification_loss: 0.9899
  82/1000 [=>............................] - ETA: 6:54 - loss: 3.8313 - regression_loss: 2.8494 - classification_loss: 0.9819
  83/1000 [=>............................] - ETA: 6:53 - loss: 3.8480 - regression_loss: 2.8675 - classification_loss: 0.9805
  84/1000 [=>............................] - ETA: 6:53 - loss: 3.8023 - regression_loss: 2.8334 - classification_loss: 0.9688
  85/1000 [=>............................] - ETA: 6:52 - loss: 3.8106 - regression_loss: 2.8402 - classification_loss: 0.9704
  86/1000 [=>............................] - ETA: 6:52 - loss: 3.8337 - regression_loss: 2.8595 - classification_loss: 0.9742
  87/1000 [=>............................] - ETA: 6:52 - loss: 3.7897 - regression_loss: 2.8267 - classification_loss: 0.9630
  88/1000 [=>............................] - ETA: 6:51 - loss: 3.7993 - regression_loss: 2.8348 - classification_loss: 0.9645
  89/1000 [=>............................] - ETA: 6:51 - loss: 3.8047 - regression_loss: 2.8427 - classification_loss: 0.9620
  90/1000 [=>............................] - ETA: 6:50 - loss: 3.8223 - regression_loss: 2.8579 - classification_loss: 0.9644
  91/1000 [=>............................] - ETA: 6:50 - loss: 3.7805 - regression_loss: 2.8265 - classification_loss: 0.9540
  92/1000 [=>............................] - ETA: 6:49 - loss: 3.7394 - regression_loss: 2.7958 - classification_loss: 0.9437
  93/1000 [=>............................] - ETA: 6:49 - loss: 3.7654 - regression_loss: 2.8171 - classification_loss: 0.9483
  94/1000 [=>............................] - ETA: 6:48 - loss: 3.7254 - regression_loss: 2.7871 - classification_loss: 0.9382
  95/1000 [=>............................] - ETA: 6:48 - loss: 3.7326 - regression_loss: 2.7936 - classification_loss: 0.9390
  96/1000 [=>............................] - ETA: 6:48 - loss: 3.6938 - regression_loss: 2.7645 - classification_loss: 0.9293
  97/1000 [=>............................] - ETA: 6:47 - loss: 3.6557 - regression_loss: 2.7360 - classification_loss: 0.9197
  98/1000 [=>............................] - ETA: 6:47 - loss: 3.6660 - regression_loss: 2.7434 - classification_loss: 0.9226
  99/1000 [=>............................] - ETA: 6:46 - loss: 3.6720 - regression_loss: 2.7490 - classification_loss: 0.9230
 100/1000 [==>...........................] - ETA: 6:46 - loss: 3.6849 - regression_loss: 2.7568 - classification_loss: 0.9281
 101/1000 [==>...........................] - ETA: 6:45 - loss: 3.6936 - regression_loss: 2.7665 - classification_loss: 0.9271
 102/1000 [==>...........................] - ETA: 6:45 - loss: 3.7129 - regression_loss: 2.7776 - classification_loss: 0.9353
 103/1000 [==>...........................] - ETA: 6:44 - loss: 3.7298 - regression_loss: 2.7928 - classification_loss: 0.9370
 104/1000 [==>...........................] - ETA: 6:44 - loss: 3.6940 - regression_loss: 2.7660 - classification_loss: 0.9280
 105/1000 [==>...........................] - ETA: 6:44 - loss: 3.7068 - regression_loss: 2.7721 - classification_loss: 0.9347
 106/1000 [==>...........................] - ETA: 6:43 - loss: 3.7174 - regression_loss: 2.7829 - classification_loss: 0.9345
 107/1000 [==>...........................] - ETA: 6:43 - loss: 3.7308 - regression_loss: 2.7902 - classification_loss: 0.9406
 108/1000 [==>...........................] - ETA: 6:42 - loss: 3.7440 - regression_loss: 2.7942 - classification_loss: 0.9497
 109/1000 [==>...........................] - ETA: 6:42 - loss: 3.7525 - regression_loss: 2.8013 - classification_loss: 0.9512
 110/1000 [==>...........................] - ETA: 6:41 - loss: 3.7654 - regression_loss: 2.8125 - classification_loss: 0.9529
 111/1000 [==>...........................] - ETA: 6:41 - loss: 3.7714 - regression_loss: 2.8188 - classification_loss: 0.9526
 112/1000 [==>...........................] - ETA: 6:40 - loss: 3.7378 - regression_loss: 2.7936 - classification_loss: 0.9441
 113/1000 [==>...........................] - ETA: 6:40 - loss: 3.7555 - regression_loss: 2.8015 - classification_loss: 0.9541
 114/1000 [==>...........................] - ETA: 6:40 - loss: 3.7226 - regression_loss: 2.7769 - classification_loss: 0.9457
 115/1000 [==>...........................] - ETA: 6:39 - loss: 3.6903 - regression_loss: 2.7527 - classification_loss: 0.9375
 116/1000 [==>...........................] - ETA: 6:39 - loss: 3.6586 - regression_loss: 2.7290 - classification_loss: 0.9296
 117/1000 [==>...........................] - ETA: 6:38 - loss: 3.6759 - regression_loss: 2.7387 - classification_loss: 0.9372
 118/1000 [==>...........................] - ETA: 6:38 - loss: 3.6792 - regression_loss: 2.7438 - classification_loss: 0.9354
 119/1000 [==>...........................] - ETA: 6:37 - loss: 3.6482 - regression_loss: 2.7207 - classification_loss: 0.9275
 120/1000 [==>...........................] - ETA: 6:37 - loss: 3.6554 - regression_loss: 2.7294 - classification_loss: 0.9260
 121/1000 [==>...........................] - ETA: 6:37 - loss: 3.6602 - regression_loss: 2.7360 - classification_loss: 0.9243
 122/1000 [==>...........................] - ETA: 6:36 - loss: 3.6752 - regression_loss: 2.7444 - classification_loss: 0.9308
 123/1000 [==>...........................] - ETA: 6:36 - loss: 3.6793 - regression_loss: 2.7499 - classification_loss: 0.9294
 124/1000 [==>...........................] - ETA: 6:35 - loss: 3.6984 - regression_loss: 2.7648 - classification_loss: 0.9336
 125/1000 [==>...........................] - ETA: 6:35 - loss: 3.6688 - regression_loss: 2.7427 - classification_loss: 0.9261
 126/1000 [==>...........................] - ETA: 6:34 - loss: 3.6848 - regression_loss: 2.7517 - classification_loss: 0.9331
 127/1000 [==>...........................] - ETA: 6:34 - loss: 3.7022 - regression_loss: 2.7615 - classification_loss: 0.9407
 128/1000 [==>...........................] - ETA: 6:34 - loss: 3.6732 - regression_loss: 2.7399 - classification_loss: 0.9333
 129/1000 [==>...........................] - ETA: 6:33 - loss: 3.6883 - regression_loss: 2.7493 - classification_loss: 0.9389
 130/1000 [==>...........................] - ETA: 6:33 - loss: 3.6599 - regression_loss: 2.7282 - classification_loss: 0.9317
 131/1000 [==>...........................] - ETA: 6:32 - loss: 3.6319 - regression_loss: 2.7073 - classification_loss: 0.9246
 132/1000 [==>...........................] - ETA: 6:32 - loss: 3.6044 - regression_loss: 2.6868 - classification_loss: 0.9176
 133/1000 [==>...........................] - ETA: 6:31 - loss: 3.6089 - regression_loss: 2.6918 - classification_loss: 0.9170
 134/1000 [===>..........................] - ETA: 6:31 - loss: 3.6121 - regression_loss: 2.6963 - classification_loss: 0.9158
 135/1000 [===>..........................] - ETA: 6:30 - loss: 3.6184 - regression_loss: 2.7042 - classification_loss: 0.9141
 136/1000 [===>..........................] - ETA: 6:30 - loss: 3.6221 - regression_loss: 2.7085 - classification_loss: 0.9136
 137/1000 [===>..........................] - ETA: 6:29 - loss: 3.6359 - regression_loss: 2.7138 - classification_loss: 0.9220
 138/1000 [===>..........................] - ETA: 6:29 - loss: 3.6498 - regression_loss: 2.7211 - classification_loss: 0.9287
 139/1000 [===>..........................] - ETA: 6:29 - loss: 3.6639 - regression_loss: 2.7272 - classification_loss: 0.9367
 140/1000 [===>..........................] - ETA: 6:28 - loss: 3.6377 - regression_loss: 2.7077 - classification_loss: 0.9300
 141/1000 [===>..........................] - ETA: 6:28 - loss: 3.6530 - regression_loss: 2.7137 - classification_loss: 0.9393
 142/1000 [===>..........................] - ETA: 6:27 - loss: 3.6746 - regression_loss: 2.7309 - classification_loss: 0.9437
 143/1000 [===>..........................] - ETA: 6:27 - loss: 3.6896 - regression_loss: 2.7409 - classification_loss: 0.9487
 144/1000 [===>..........................] - ETA: 6:26 - loss: 3.6951 - regression_loss: 2.7458 - classification_loss: 0.9493
 145/1000 [===>..........................] - ETA: 6:26 - loss: 3.6978 - regression_loss: 2.7496 - classification_loss: 0.9482
 146/1000 [===>..........................] - ETA: 6:25 - loss: 3.7024 - regression_loss: 2.7563 - classification_loss: 0.9461
 147/1000 [===>..........................] - ETA: 6:25 - loss: 3.6786 - regression_loss: 2.7376 - classification_loss: 0.9411
 148/1000 [===>..........................] - ETA: 6:25 - loss: 3.6860 - regression_loss: 2.7391 - classification_loss: 0.9469
 149/1000 [===>..........................] - ETA: 6:24 - loss: 3.6684 - regression_loss: 2.7207 - classification_loss: 0.9477
 150/1000 [===>..........................] - ETA: 6:24 - loss: 3.6816 - regression_loss: 2.7299 - classification_loss: 0.9517
 151/1000 [===>..........................] - ETA: 6:23 - loss: 3.6958 - regression_loss: 2.7400 - classification_loss: 0.9558
 152/1000 [===>..........................] - ETA: 6:23 - loss: 3.7036 - regression_loss: 2.7449 - classification_loss: 0.9587
 153/1000 [===>..........................] - ETA: 6:22 - loss: 3.7092 - regression_loss: 2.7471 - classification_loss: 0.9621
 154/1000 [===>..........................] - ETA: 6:22 - loss: 3.7217 - regression_loss: 2.7540 - classification_loss: 0.9677
 155/1000 [===>..........................] - ETA: 6:22 - loss: 3.7240 - regression_loss: 2.7571 - classification_loss: 0.9669
 156/1000 [===>..........................] - ETA: 6:21 - loss: 3.7287 - regression_loss: 2.7585 - classification_loss: 0.9702
 157/1000 [===>..........................] - ETA: 6:21 - loss: 3.7432 - regression_loss: 2.7692 - classification_loss: 0.9740
 158/1000 [===>..........................] - ETA: 6:20 - loss: 3.7195 - regression_loss: 2.7517 - classification_loss: 0.9678
 159/1000 [===>..........................] - ETA: 6:20 - loss: 3.7257 - regression_loss: 2.7546 - classification_loss: 0.9711
 160/1000 [===>..........................] - ETA: 6:19 - loss: 3.7336 - regression_loss: 2.7606 - classification_loss: 0.9731
 161/1000 [===>..........................] - ETA: 6:19 - loss: 3.7365 - regression_loss: 2.7640 - classification_loss: 0.9725
 162/1000 [===>..........................] - ETA: 6:18 - loss: 3.7393 - regression_loss: 2.7681 - classification_loss: 0.9713
 163/1000 [===>..........................] - ETA: 6:18 - loss: 3.7517 - regression_loss: 2.7766 - classification_loss: 0.9751
 164/1000 [===>..........................] - ETA: 6:17 - loss: 3.7291 - regression_loss: 2.7597 - classification_loss: 0.9694
 165/1000 [===>..........................] - ETA: 6:17 - loss: 3.7065 - regression_loss: 2.7430 - classification_loss: 0.9635
 166/1000 [===>..........................] - ETA: 6:17 - loss: 3.6842 - regression_loss: 2.7264 - classification_loss: 0.9578
 167/1000 [====>.........................] - ETA: 6:16 - loss: 3.6900 - regression_loss: 2.7294 - classification_loss: 0.9605
 168/1000 [====>.........................] - ETA: 6:15 - loss: 3.6954 - regression_loss: 2.7316 - classification_loss: 0.9638
 169/1000 [====>.........................] - ETA: 6:15 - loss: 3.7008 - regression_loss: 2.7352 - classification_loss: 0.9656
 170/1000 [====>.........................] - ETA: 6:15 - loss: 3.7065 - regression_loss: 2.7374 - classification_loss: 0.9691
 171/1000 [====>.........................] - ETA: 6:14 - loss: 3.7114 - regression_loss: 2.7440 - classification_loss: 0.9674
 172/1000 [====>.........................] - ETA: 6:14 - loss: 3.7151 - regression_loss: 2.7451 - classification_loss: 0.9700
 173/1000 [====>.........................] - ETA: 6:13 - loss: 3.7181 - regression_loss: 2.7485 - classification_loss: 0.9696
 174/1000 [====>.........................] - ETA: 6:13 - loss: 3.7420 - regression_loss: 2.7731 - classification_loss: 0.9689
 175/1000 [====>.........................] - ETA: 6:12 - loss: 3.7430 - regression_loss: 2.7730 - classification_loss: 0.9701
 176/1000 [====>.........................] - ETA: 6:12 - loss: 3.7481 - regression_loss: 2.7781 - classification_loss: 0.9700
 177/1000 [====>.........................] - ETA: 6:11 - loss: 3.7269 - regression_loss: 2.7624 - classification_loss: 0.9646
 178/1000 [====>.........................] - ETA: 6:11 - loss: 3.7060 - regression_loss: 2.7469 - classification_loss: 0.9591
 179/1000 [====>.........................] - ETA: 6:10 - loss: 3.7158 - regression_loss: 2.7542 - classification_loss: 0.9617
 180/1000 [====>.........................] - ETA: 6:10 - loss: 3.7208 - regression_loss: 2.7560 - classification_loss: 0.9648
 181/1000 [====>.........................] - ETA: 6:10 - loss: 3.7003 - regression_loss: 2.7408 - classification_loss: 0.9595
 182/1000 [====>.........................] - ETA: 6:09 - loss: 3.6799 - regression_loss: 2.7257 - classification_loss: 0.9542
 183/1000 [====>.........................] - ETA: 6:09 - loss: 3.6821 - regression_loss: 2.7289 - classification_loss: 0.9532
 184/1000 [====>.........................] - ETA: 6:08 - loss: 3.6843 - regression_loss: 2.7310 - classification_loss: 0.9532
 185/1000 [====>.........................] - ETA: 6:08 - loss: 3.6644 - regression_loss: 2.7163 - classification_loss: 0.9481
 186/1000 [====>.........................] - ETA: 6:07 - loss: 3.6471 - regression_loss: 2.7017 - classification_loss: 0.9454
 187/1000 [====>.........................] - ETA: 6:07 - loss: 3.6603 - regression_loss: 2.7114 - classification_loss: 0.9489
 188/1000 [====>.........................] - ETA: 6:07 - loss: 3.6657 - regression_loss: 2.7176 - classification_loss: 0.9481
 189/1000 [====>.........................] - ETA: 6:06 - loss: 3.6463 - regression_loss: 2.7033 - classification_loss: 0.9430
 190/1000 [====>.........................] - ETA: 6:06 - loss: 3.6599 - regression_loss: 2.7129 - classification_loss: 0.9470
 191/1000 [====>.........................] - ETA: 6:05 - loss: 3.6762 - regression_loss: 2.7232 - classification_loss: 0.9530
 192/1000 [====>.........................] - ETA: 6:05 - loss: 3.6776 - regression_loss: 2.7262 - classification_loss: 0.9514
 193/1000 [====>.........................] - ETA: 6:04 - loss: 3.6808 - regression_loss: 2.7251 - classification_loss: 0.9557
 194/1000 [====>.........................] - ETA: 6:04 - loss: 3.6868 - regression_loss: 2.7271 - classification_loss: 0.9598
 195/1000 [====>.........................] - ETA: 6:03 - loss: 3.6902 - regression_loss: 2.7274 - classification_loss: 0.9628
 196/1000 [====>.........................] - ETA: 6:03 - loss: 3.6950 - regression_loss: 2.7314 - classification_loss: 0.9635
 197/1000 [====>.........................] - ETA: 6:02 - loss: 3.6763 - regression_loss: 2.7176 - classification_loss: 0.9587
 198/1000 [====>.........................] - ETA: 6:02 - loss: 3.6785 - regression_loss: 2.7208 - classification_loss: 0.9577
 199/1000 [====>.........................] - ETA: 6:01 - loss: 3.6605 - regression_loss: 2.7072 - classification_loss: 0.9533
 200/1000 [=====>........................] - ETA: 6:01 - loss: 3.6636 - regression_loss: 2.7103 - classification_loss: 0.9533
 201/1000 [=====>........................] - ETA: 6:01 - loss: 3.6454 - regression_loss: 2.6968 - classification_loss: 0.9486
 202/1000 [=====>........................] - ETA: 6:00 - loss: 3.6708 - regression_loss: 2.7116 - classification_loss: 0.9592
 203/1000 [=====>........................] - ETA: 6:00 - loss: 3.6755 - regression_loss: 2.7169 - classification_loss: 0.9586
 204/1000 [=====>........................] - ETA: 5:59 - loss: 3.6823 - regression_loss: 2.7207 - classification_loss: 0.9616
 205/1000 [=====>........................] - ETA: 5:59 - loss: 3.6897 - regression_loss: 2.7241 - classification_loss: 0.9657
 206/1000 [=====>........................] - ETA: 5:58 - loss: 3.6923 - regression_loss: 2.7243 - classification_loss: 0.9680
 207/1000 [=====>........................] - ETA: 5:58 - loss: 3.6745 - regression_loss: 2.7112 - classification_loss: 0.9633
 208/1000 [=====>........................] - ETA: 5:57 - loss: 3.6792 - regression_loss: 2.7144 - classification_loss: 0.9649
 209/1000 [=====>........................] - ETA: 5:57 - loss: 3.6823 - regression_loss: 2.7176 - classification_loss: 0.9647
 210/1000 [=====>........................] - ETA: 5:56 - loss: 3.6864 - regression_loss: 2.7194 - classification_loss: 0.9670
 211/1000 [=====>........................] - ETA: 5:56 - loss: 3.6977 - regression_loss: 2.7263 - classification_loss: 0.9715
 212/1000 [=====>........................] - ETA: 5:55 - loss: 3.6993 - regression_loss: 2.7268 - classification_loss: 0.9726
 213/1000 [=====>........................] - ETA: 5:55 - loss: 3.7018 - regression_loss: 2.7302 - classification_loss: 0.9716
 214/1000 [=====>........................] - ETA: 5:55 - loss: 3.7072 - regression_loss: 2.7343 - classification_loss: 0.9729
 215/1000 [=====>........................] - ETA: 5:54 - loss: 3.7127 - regression_loss: 2.7398 - classification_loss: 0.9729
 216/1000 [=====>........................] - ETA: 5:54 - loss: 3.7128 - regression_loss: 2.7417 - classification_loss: 0.9711
 217/1000 [=====>........................] - ETA: 5:53 - loss: 3.7196 - regression_loss: 2.7496 - classification_loss: 0.9700
 218/1000 [=====>........................] - ETA: 5:53 - loss: 3.7227 - regression_loss: 2.7540 - classification_loss: 0.9688
 219/1000 [=====>........................] - ETA: 5:52 - loss: 3.7057 - regression_loss: 2.7414 - classification_loss: 0.9643
 220/1000 [=====>........................] - ETA: 5:52 - loss: 3.7069 - regression_loss: 2.7441 - classification_loss: 0.9628
 221/1000 [=====>........................] - ETA: 5:51 - loss: 3.7094 - regression_loss: 2.7459 - classification_loss: 0.9635
 222/1000 [=====>........................] - ETA: 5:51 - loss: 3.7095 - regression_loss: 2.7474 - classification_loss: 0.9621
 223/1000 [=====>........................] - ETA: 5:50 - loss: 3.6929 - regression_loss: 2.7351 - classification_loss: 0.9578
 224/1000 [=====>........................] - ETA: 5:50 - loss: 3.6980 - regression_loss: 2.7389 - classification_loss: 0.9590
 225/1000 [=====>........................] - ETA: 5:49 - loss: 3.7006 - regression_loss: 2.7415 - classification_loss: 0.9590
 226/1000 [=====>........................] - ETA: 5:49 - loss: 3.7047 - regression_loss: 2.7464 - classification_loss: 0.9583
 227/1000 [=====>........................] - ETA: 5:49 - loss: 3.7041 - regression_loss: 2.7472 - classification_loss: 0.9569
 228/1000 [=====>........................] - ETA: 5:48 - loss: 3.7117 - regression_loss: 2.7532 - classification_loss: 0.9585
 229/1000 [=====>........................] - ETA: 5:48 - loss: 3.7145 - regression_loss: 2.7550 - classification_loss: 0.9595
 230/1000 [=====>........................] - ETA: 5:47 - loss: 3.7212 - regression_loss: 2.7591 - classification_loss: 0.9620
 231/1000 [=====>........................] - ETA: 5:47 - loss: 3.7389 - regression_loss: 2.7777 - classification_loss: 0.9612
 232/1000 [=====>........................] - ETA: 5:46 - loss: 3.7419 - regression_loss: 2.7812 - classification_loss: 0.9608
 233/1000 [=====>........................] - ETA: 5:46 - loss: 3.7473 - regression_loss: 2.7878 - classification_loss: 0.9595
 234/1000 [======>.......................] - ETA: 5:46 - loss: 3.7525 - regression_loss: 2.7911 - classification_loss: 0.9615
 235/1000 [======>.......................] - ETA: 5:45 - loss: 3.7366 - regression_loss: 2.7792 - classification_loss: 0.9574
 236/1000 [======>.......................] - ETA: 5:45 - loss: 3.7381 - regression_loss: 2.7820 - classification_loss: 0.9561
 237/1000 [======>.......................] - ETA: 5:44 - loss: 3.7400 - regression_loss: 2.7834 - classification_loss: 0.9566
 238/1000 [======>.......................] - ETA: 5:44 - loss: 3.7475 - regression_loss: 2.7918 - classification_loss: 0.9557
 239/1000 [======>.......................] - ETA: 5:43 - loss: 3.7506 - regression_loss: 2.7945 - classification_loss: 0.9561
 240/1000 [======>.......................] - ETA: 5:43 - loss: 3.7539 - regression_loss: 2.7969 - classification_loss: 0.9570
 241/1000 [======>.......................] - ETA: 5:42 - loss: 3.7540 - regression_loss: 2.7970 - classification_loss: 0.9569
 242/1000 [======>.......................] - ETA: 5:42 - loss: 3.7384 - regression_loss: 2.7855 - classification_loss: 0.9530
 243/1000 [======>.......................] - ETA: 5:41 - loss: 3.7420 - regression_loss: 2.7886 - classification_loss: 0.9534
 244/1000 [======>.......................] - ETA: 5:41 - loss: 3.7456 - regression_loss: 2.7933 - classification_loss: 0.9524
 245/1000 [======>.......................] - ETA: 5:41 - loss: 3.7504 - regression_loss: 2.7985 - classification_loss: 0.9519
 246/1000 [======>.......................] - ETA: 5:40 - loss: 3.7561 - regression_loss: 2.8032 - classification_loss: 0.9529
 247/1000 [======>.......................] - ETA: 5:40 - loss: 3.7566 - regression_loss: 2.8051 - classification_loss: 0.9516
 248/1000 [======>.......................] - ETA: 5:39 - loss: 3.7639 - regression_loss: 2.8093 - classification_loss: 0.9545
 249/1000 [======>.......................] - ETA: 5:39 - loss: 3.7491 - regression_loss: 2.7980 - classification_loss: 0.9511
 250/1000 [======>.......................] - ETA: 5:38 - loss: 3.7513 - regression_loss: 2.7980 - classification_loss: 0.9533
 251/1000 [======>.......................] - ETA: 5:38 - loss: 3.7530 - regression_loss: 2.8008 - classification_loss: 0.9523
 252/1000 [======>.......................] - ETA: 5:37 - loss: 3.7382 - regression_loss: 2.7897 - classification_loss: 0.9486
 253/1000 [======>.......................] - ETA: 5:37 - loss: 3.7394 - regression_loss: 2.7897 - classification_loss: 0.9497
 254/1000 [======>.......................] - ETA: 5:36 - loss: 3.7247 - regression_loss: 2.7787 - classification_loss: 0.9460
 255/1000 [======>.......................] - ETA: 5:36 - loss: 3.7273 - regression_loss: 2.7815 - classification_loss: 0.9458
 256/1000 [======>.......................] - ETA: 5:36 - loss: 3.7296 - regression_loss: 2.7838 - classification_loss: 0.9457
 257/1000 [======>.......................] - ETA: 5:35 - loss: 3.7150 - regression_loss: 2.7730 - classification_loss: 0.9420
 258/1000 [======>.......................] - ETA: 5:35 - loss: 3.7179 - regression_loss: 2.7729 - classification_loss: 0.9449
 259/1000 [======>.......................] - ETA: 5:34 - loss: 3.7208 - regression_loss: 2.7766 - classification_loss: 0.9443
 260/1000 [======>.......................] - ETA: 5:34 - loss: 3.7230 - regression_loss: 2.7792 - classification_loss: 0.9438
 261/1000 [======>.......................] - ETA: 5:33 - loss: 3.7264 - regression_loss: 2.7825 - classification_loss: 0.9439
 262/1000 [======>.......................] - ETA: 5:33 - loss: 3.7407 - regression_loss: 2.7895 - classification_loss: 0.9512
 263/1000 [======>.......................] - ETA: 5:32 - loss: 3.7522 - regression_loss: 2.7982 - classification_loss: 0.9540
 264/1000 [======>.......................] - ETA: 5:32 - loss: 3.7537 - regression_loss: 2.7983 - classification_loss: 0.9554
 265/1000 [======>.......................] - ETA: 5:32 - loss: 3.7571 - regression_loss: 2.7997 - classification_loss: 0.9574
 266/1000 [======>.......................] - ETA: 5:31 - loss: 3.7597 - regression_loss: 2.8013 - classification_loss: 0.9583
 267/1000 [=======>......................] - ETA: 5:31 - loss: 3.7456 - regression_loss: 2.7908 - classification_loss: 0.9548
 268/1000 [=======>......................] - ETA: 5:30 - loss: 3.7579 - regression_loss: 2.8006 - classification_loss: 0.9573
 269/1000 [=======>......................] - ETA: 5:30 - loss: 3.7600 - regression_loss: 2.8021 - classification_loss: 0.9578
 270/1000 [=======>......................] - ETA: 5:29 - loss: 3.7646 - regression_loss: 2.8052 - classification_loss: 0.9594
 271/1000 [=======>......................] - ETA: 5:29 - loss: 3.7687 - regression_loss: 2.8083 - classification_loss: 0.9605
 272/1000 [=======>......................] - ETA: 5:28 - loss: 3.7709 - regression_loss: 2.8100 - classification_loss: 0.9609
 273/1000 [=======>......................] - ETA: 5:28 - loss: 3.7770 - regression_loss: 2.8129 - classification_loss: 0.9640
 274/1000 [=======>......................] - ETA: 5:28 - loss: 3.7805 - regression_loss: 2.8155 - classification_loss: 0.9649
 275/1000 [=======>......................] - ETA: 5:27 - loss: 3.7667 - regression_loss: 2.8053 - classification_loss: 0.9615
 276/1000 [=======>......................] - ETA: 5:27 - loss: 3.7687 - regression_loss: 2.8085 - classification_loss: 0.9602
 277/1000 [=======>......................] - ETA: 5:26 - loss: 3.7705 - regression_loss: 2.8095 - classification_loss: 0.9611
 278/1000 [=======>......................] - ETA: 5:26 - loss: 3.7727 - regression_loss: 2.8110 - classification_loss: 0.9616
 279/1000 [=======>......................] - ETA: 5:25 - loss: 3.7592 - regression_loss: 2.8010 - classification_loss: 0.9582
 280/1000 [=======>......................] - ETA: 5:25 - loss: 3.7612 - regression_loss: 2.8026 - classification_loss: 0.9586
 281/1000 [=======>......................] - ETA: 5:24 - loss: 3.7637 - regression_loss: 2.8061 - classification_loss: 0.9576
 282/1000 [=======>......................] - ETA: 5:24 - loss: 3.7544 - regression_loss: 2.7961 - classification_loss: 0.9583
 283/1000 [=======>......................] - ETA: 5:24 - loss: 3.7542 - regression_loss: 2.7975 - classification_loss: 0.9567
 284/1000 [=======>......................] - ETA: 5:23 - loss: 3.7571 - regression_loss: 2.7993 - classification_loss: 0.9577
 285/1000 [=======>......................] - ETA: 5:23 - loss: 3.7604 - regression_loss: 2.8018 - classification_loss: 0.9586
 286/1000 [=======>......................] - ETA: 5:22 - loss: 3.7618 - regression_loss: 2.8040 - classification_loss: 0.9578
 287/1000 [=======>......................] - ETA: 5:22 - loss: 3.7661 - regression_loss: 2.8083 - classification_loss: 0.9578
 288/1000 [=======>......................] - ETA: 5:21 - loss: 3.7690 - regression_loss: 2.8120 - classification_loss: 0.9569
 289/1000 [=======>......................] - ETA: 5:21 - loss: 3.7703 - regression_loss: 2.8140 - classification_loss: 0.9562
 290/1000 [=======>......................] - ETA: 5:20 - loss: 3.7712 - regression_loss: 2.8139 - classification_loss: 0.9573
 291/1000 [=======>......................] - ETA: 5:20 - loss: 3.7711 - regression_loss: 2.8154 - classification_loss: 0.9557
 292/1000 [=======>......................] - ETA: 5:19 - loss: 3.7737 - regression_loss: 2.8174 - classification_loss: 0.9563
 293/1000 [=======>......................] - ETA: 5:19 - loss: 3.7609 - regression_loss: 2.8078 - classification_loss: 0.9531
 294/1000 [=======>......................] - ETA: 5:19 - loss: 3.7610 - regression_loss: 2.8090 - classification_loss: 0.9520
 295/1000 [=======>......................] - ETA: 5:18 - loss: 3.7634 - regression_loss: 2.8118 - classification_loss: 0.9517
 296/1000 [=======>......................] - ETA: 5:18 - loss: 3.7650 - regression_loss: 2.8123 - classification_loss: 0.9527
 297/1000 [=======>......................] - ETA: 5:17 - loss: 3.7653 - regression_loss: 2.8121 - classification_loss: 0.9532
 298/1000 [=======>......................] - ETA: 5:17 - loss: 3.7541 - regression_loss: 2.8027 - classification_loss: 0.9515
 299/1000 [=======>......................] - ETA: 5:16 - loss: 3.7607 - regression_loss: 2.8100 - classification_loss: 0.9507
 300/1000 [========>.....................] - ETA: 5:16 - loss: 3.7649 - regression_loss: 2.8131 - classification_loss: 0.9518
 301/1000 [========>.....................] - ETA: 5:15 - loss: 3.7666 - regression_loss: 2.8140 - classification_loss: 0.9527
 302/1000 [========>.....................] - ETA: 5:15 - loss: 3.7663 - regression_loss: 2.8138 - classification_loss: 0.9526
 303/1000 [========>.....................] - ETA: 5:14 - loss: 3.7667 - regression_loss: 2.8142 - classification_loss: 0.9525
 304/1000 [========>.....................] - ETA: 5:14 - loss: 3.7788 - regression_loss: 2.8050 - classification_loss: 0.9738
 305/1000 [========>.....................] - ETA: 5:14 - loss: 3.7805 - regression_loss: 2.8071 - classification_loss: 0.9734
 306/1000 [========>.....................] - ETA: 5:13 - loss: 3.7808 - regression_loss: 2.8074 - classification_loss: 0.9733
 307/1000 [========>.....................] - ETA: 5:13 - loss: 3.7885 - regression_loss: 2.8161 - classification_loss: 0.9724
 308/1000 [========>.....................] - ETA: 5:12 - loss: 3.7897 - regression_loss: 2.8183 - classification_loss: 0.9714
 309/1000 [========>.....................] - ETA: 5:12 - loss: 3.7908 - regression_loss: 2.8196 - classification_loss: 0.9712
 310/1000 [========>.....................] - ETA: 5:11 - loss: 3.7788 - regression_loss: 2.8105 - classification_loss: 0.9683
 311/1000 [========>.....................] - ETA: 5:11 - loss: 3.7666 - regression_loss: 2.8015 - classification_loss: 0.9652
 312/1000 [========>.....................] - ETA: 5:10 - loss: 3.7700 - regression_loss: 2.8047 - classification_loss: 0.9654
 313/1000 [========>.....................] - ETA: 5:10 - loss: 3.7723 - regression_loss: 2.8072 - classification_loss: 0.9650
 314/1000 [========>.....................] - ETA: 5:09 - loss: 3.7772 - regression_loss: 2.8129 - classification_loss: 0.9643
 315/1000 [========>.....................] - ETA: 5:09 - loss: 3.7801 - regression_loss: 2.8165 - classification_loss: 0.9636
 316/1000 [========>.....................] - ETA: 5:09 - loss: 3.7821 - regression_loss: 2.8189 - classification_loss: 0.9633
 317/1000 [========>.....................] - ETA: 5:08 - loss: 3.7875 - regression_loss: 2.8224 - classification_loss: 0.9651
 318/1000 [========>.....................] - ETA: 5:08 - loss: 3.7911 - regression_loss: 2.8252 - classification_loss: 0.9659
 319/1000 [========>.....................] - ETA: 5:07 - loss: 3.7928 - regression_loss: 2.8274 - classification_loss: 0.9654
 320/1000 [========>.....................] - ETA: 5:07 - loss: 3.7994 - regression_loss: 2.8328 - classification_loss: 0.9666
 321/1000 [========>.....................] - ETA: 5:06 - loss: 3.8026 - regression_loss: 2.8349 - classification_loss: 0.9677
 322/1000 [========>.....................] - ETA: 5:06 - loss: 3.8043 - regression_loss: 2.8368 - classification_loss: 0.9675
 323/1000 [========>.....................] - ETA: 5:05 - loss: 3.8069 - regression_loss: 2.8392 - classification_loss: 0.9677
 324/1000 [========>.....................] - ETA: 5:05 - loss: 3.7952 - regression_loss: 2.8305 - classification_loss: 0.9647
 325/1000 [========>.....................] - ETA: 5:04 - loss: 3.7954 - regression_loss: 2.8304 - classification_loss: 0.9650
 326/1000 [========>.....................] - ETA: 5:04 - loss: 3.7857 - regression_loss: 2.8217 - classification_loss: 0.9641
 327/1000 [========>.....................] - ETA: 5:03 - loss: 3.7889 - regression_loss: 2.8253 - classification_loss: 0.9637
 328/1000 [========>.....................] - ETA: 5:03 - loss: 3.7774 - regression_loss: 2.8167 - classification_loss: 0.9607
 329/1000 [========>.....................] - ETA: 5:03 - loss: 3.7795 - regression_loss: 2.8190 - classification_loss: 0.9604
 330/1000 [========>.....................] - ETA: 5:02 - loss: 3.7802 - regression_loss: 2.8196 - classification_loss: 0.9606
 331/1000 [========>.....................] - ETA: 5:02 - loss: 3.7820 - regression_loss: 2.8220 - classification_loss: 0.9600
 332/1000 [========>.....................] - ETA: 5:01 - loss: 3.7853 - regression_loss: 2.8239 - classification_loss: 0.9614
 333/1000 [========>.....................] - ETA: 5:01 - loss: 3.7864 - regression_loss: 2.8257 - classification_loss: 0.9606
 334/1000 [=========>....................] - ETA: 5:00 - loss: 3.7756 - regression_loss: 2.8172 - classification_loss: 0.9583
 335/1000 [=========>....................] - ETA: 5:00 - loss: 3.7643 - regression_loss: 2.8088 - classification_loss: 0.9555
 336/1000 [=========>....................] - ETA: 4:59 - loss: 3.7657 - regression_loss: 2.8092 - classification_loss: 0.9565
 337/1000 [=========>....................] - ETA: 4:59 - loss: 3.7671 - regression_loss: 2.8114 - classification_loss: 0.9557
 338/1000 [=========>....................] - ETA: 4:59 - loss: 3.7703 - regression_loss: 2.8154 - classification_loss: 0.9550
 339/1000 [=========>....................] - ETA: 4:58 - loss: 3.7724 - regression_loss: 2.8169 - classification_loss: 0.9555
 340/1000 [=========>....................] - ETA: 4:58 - loss: 3.7743 - regression_loss: 2.8168 - classification_loss: 0.9575
 341/1000 [=========>....................] - ETA: 4:57 - loss: 3.7763 - regression_loss: 2.8198 - classification_loss: 0.9565
 342/1000 [=========>....................] - ETA: 4:57 - loss: 3.7661 - regression_loss: 2.8115 - classification_loss: 0.9545
 343/1000 [=========>....................] - ETA: 4:56 - loss: 3.7551 - regression_loss: 2.8033 - classification_loss: 0.9517
 344/1000 [=========>....................] - ETA: 4:56 - loss: 3.7570 - regression_loss: 2.8035 - classification_loss: 0.9535
 345/1000 [=========>....................] - ETA: 4:55 - loss: 3.7461 - regression_loss: 2.7954 - classification_loss: 0.9507
 346/1000 [=========>....................] - ETA: 4:55 - loss: 3.7463 - regression_loss: 2.7962 - classification_loss: 0.9501
 347/1000 [=========>....................] - ETA: 4:54 - loss: 3.7494 - regression_loss: 2.7981 - classification_loss: 0.9513
 348/1000 [=========>....................] - ETA: 4:54 - loss: 3.7387 - regression_loss: 2.7901 - classification_loss: 0.9486
 349/1000 [=========>....................] - ETA: 4:54 - loss: 3.7432 - regression_loss: 2.7931 - classification_loss: 0.9501
 350/1000 [=========>....................] - ETA: 4:53 - loss: 3.7462 - regression_loss: 2.7960 - classification_loss: 0.9502
 351/1000 [=========>....................] - ETA: 4:53 - loss: 3.7494 - regression_loss: 2.7973 - classification_loss: 0.9522
 352/1000 [=========>....................] - ETA: 4:52 - loss: 3.7390 - regression_loss: 2.7893 - classification_loss: 0.9497
 353/1000 [=========>....................] - ETA: 4:52 - loss: 3.7285 - regression_loss: 2.7814 - classification_loss: 0.9471
 354/1000 [=========>....................] - ETA: 4:51 - loss: 3.7297 - regression_loss: 2.7826 - classification_loss: 0.9471
 355/1000 [=========>....................] - ETA: 4:51 - loss: 3.7315 - regression_loss: 2.7825 - classification_loss: 0.9490
 356/1000 [=========>....................] - ETA: 4:50 - loss: 3.7309 - regression_loss: 2.7826 - classification_loss: 0.9483
 357/1000 [=========>....................] - ETA: 4:50 - loss: 3.7347 - regression_loss: 2.7847 - classification_loss: 0.9499
 358/1000 [=========>....................] - ETA: 4:50 - loss: 3.7379 - regression_loss: 2.7863 - classification_loss: 0.9516
 359/1000 [=========>....................] - ETA: 4:49 - loss: 3.7412 - regression_loss: 2.7870 - classification_loss: 0.9542
 360/1000 [=========>....................] - ETA: 4:49 - loss: 3.7439 - regression_loss: 2.7903 - classification_loss: 0.9536
 361/1000 [=========>....................] - ETA: 4:48 - loss: 3.7335 - regression_loss: 2.7826 - classification_loss: 0.9509
 362/1000 [=========>....................] - ETA: 4:48 - loss: 3.7232 - regression_loss: 2.7749 - classification_loss: 0.9483
 363/1000 [=========>....................] - ETA: 4:47 - loss: 3.7236 - regression_loss: 2.7761 - classification_loss: 0.9475
 364/1000 [=========>....................] - ETA: 4:47 - loss: 3.7263 - regression_loss: 2.7765 - classification_loss: 0.9498
 365/1000 [=========>....................] - ETA: 4:46 - loss: 3.7161 - regression_loss: 2.7689 - classification_loss: 0.9472
 366/1000 [=========>....................] - ETA: 4:46 - loss: 3.7172 - regression_loss: 2.7699 - classification_loss: 0.9473
 367/1000 [==========>...................] - ETA: 4:45 - loss: 3.7199 - regression_loss: 2.7711 - classification_loss: 0.9488
 368/1000 [==========>...................] - ETA: 4:45 - loss: 3.7257 - regression_loss: 2.7774 - classification_loss: 0.9482
 369/1000 [==========>...................] - ETA: 4:45 - loss: 3.7302 - regression_loss: 2.7805 - classification_loss: 0.9498
 370/1000 [==========>...................] - ETA: 4:44 - loss: 3.7306 - regression_loss: 2.7796 - classification_loss: 0.9510
 371/1000 [==========>...................] - ETA: 4:44 - loss: 3.7359 - regression_loss: 2.7824 - classification_loss: 0.9535
 372/1000 [==========>...................] - ETA: 4:43 - loss: 3.7375 - regression_loss: 2.7848 - classification_loss: 0.9526
 373/1000 [==========>...................] - ETA: 4:43 - loss: 3.7390 - regression_loss: 2.7860 - classification_loss: 0.9529
 374/1000 [==========>...................] - ETA: 4:42 - loss: 3.7408 - regression_loss: 2.7889 - classification_loss: 0.9519
 375/1000 [==========>...................] - ETA: 4:42 - loss: 3.7464 - regression_loss: 2.7940 - classification_loss: 0.9524
 376/1000 [==========>...................] - ETA: 4:41 - loss: 3.7480 - regression_loss: 2.7952 - classification_loss: 0.9528
 377/1000 [==========>...................] - ETA: 4:41 - loss: 3.7520 - regression_loss: 2.7976 - classification_loss: 0.9543
 378/1000 [==========>...................] - ETA: 4:41 - loss: 3.7540 - regression_loss: 2.8003 - classification_loss: 0.9537
 379/1000 [==========>...................] - ETA: 4:40 - loss: 3.7557 - regression_loss: 2.8006 - classification_loss: 0.9551
 380/1000 [==========>...................] - ETA: 4:40 - loss: 3.7572 - regression_loss: 2.8022 - classification_loss: 0.9550
 381/1000 [==========>...................] - ETA: 4:39 - loss: 3.7596 - regression_loss: 2.8051 - classification_loss: 0.9545
 382/1000 [==========>...................] - ETA: 4:39 - loss: 3.7498 - regression_loss: 2.7978 - classification_loss: 0.9520
 383/1000 [==========>...................] - ETA: 4:38 - loss: 3.7500 - regression_loss: 2.7971 - classification_loss: 0.9530
 384/1000 [==========>...................] - ETA: 4:38 - loss: 3.7439 - regression_loss: 2.7898 - classification_loss: 0.9541
 385/1000 [==========>...................] - ETA: 4:37 - loss: 3.7485 - regression_loss: 2.7923 - classification_loss: 0.9562
 386/1000 [==========>...................] - ETA: 4:37 - loss: 3.7393 - regression_loss: 2.7850 - classification_loss: 0.9543
 387/1000 [==========>...................] - ETA: 4:37 - loss: 3.7419 - regression_loss: 2.7874 - classification_loss: 0.9545
 388/1000 [==========>...................] - ETA: 4:36 - loss: 3.7430 - regression_loss: 2.7876 - classification_loss: 0.9554
 389/1000 [==========>...................] - ETA: 4:36 - loss: 3.7469 - regression_loss: 2.7919 - classification_loss: 0.9550
 390/1000 [==========>...................] - ETA: 4:35 - loss: 3.7492 - regression_loss: 2.7929 - classification_loss: 0.9563
 391/1000 [==========>...................] - ETA: 4:35 - loss: 3.7527 - regression_loss: 2.7958 - classification_loss: 0.9569
 392/1000 [==========>...................] - ETA: 4:34 - loss: 3.7563 - regression_loss: 2.7983 - classification_loss: 0.9580
 393/1000 [==========>...................] - ETA: 4:34 - loss: 3.7468 - regression_loss: 2.7912 - classification_loss: 0.9556
 394/1000 [==========>...................] - ETA: 4:33 - loss: 3.7497 - regression_loss: 2.7927 - classification_loss: 0.9571
 395/1000 [==========>...................] - ETA: 4:33 - loss: 3.7509 - regression_loss: 2.7944 - classification_loss: 0.9565
 396/1000 [==========>...................] - ETA: 4:32 - loss: 3.7524 - regression_loss: 2.7957 - classification_loss: 0.9566
 397/1000 [==========>...................] - ETA: 4:32 - loss: 3.7558 - regression_loss: 2.7991 - classification_loss: 0.9567
 398/1000 [==========>...................] - ETA: 4:32 - loss: 3.7575 - regression_loss: 2.8016 - classification_loss: 0.9560
 399/1000 [==========>...................] - ETA: 4:31 - loss: 3.7604 - regression_loss: 2.8049 - classification_loss: 0.9554
 400/1000 [===========>..................] - ETA: 4:31 - loss: 3.7523 - regression_loss: 2.7979 - classification_loss: 0.9544
 401/1000 [===========>..................] - ETA: 4:30 - loss: 3.7542 - regression_loss: 2.7996 - classification_loss: 0.9547
 402/1000 [===========>..................] - ETA: 4:30 - loss: 3.7449 - regression_loss: 2.7926 - classification_loss: 0.9523
 403/1000 [===========>..................] - ETA: 4:29 - loss: 3.7457 - regression_loss: 2.7938 - classification_loss: 0.9519
 404/1000 [===========>..................] - ETA: 4:29 - loss: 3.7492 - regression_loss: 2.7963 - classification_loss: 0.9529
 405/1000 [===========>..................] - ETA: 4:28 - loss: 3.7529 - regression_loss: 2.7991 - classification_loss: 0.9539
 406/1000 [===========>..................] - ETA: 4:28 - loss: 3.7549 - regression_loss: 2.8008 - classification_loss: 0.9541
 407/1000 [===========>..................] - ETA: 4:27 - loss: 3.7632 - regression_loss: 2.8096 - classification_loss: 0.9536
 408/1000 [===========>..................] - ETA: 4:27 - loss: 3.7645 - regression_loss: 2.8113 - classification_loss: 0.9532
 409/1000 [===========>..................] - ETA: 4:27 - loss: 3.7666 - regression_loss: 2.8138 - classification_loss: 0.9528
 410/1000 [===========>..................] - ETA: 4:26 - loss: 3.7685 - regression_loss: 2.8146 - classification_loss: 0.9538
 411/1000 [===========>..................] - ETA: 4:26 - loss: 3.7694 - regression_loss: 2.8166 - classification_loss: 0.9528
 412/1000 [===========>..................] - ETA: 4:25 - loss: 3.7704 - regression_loss: 2.8175 - classification_loss: 0.9529
 413/1000 [===========>..................] - ETA: 4:25 - loss: 3.7734 - regression_loss: 2.8210 - classification_loss: 0.9523
 414/1000 [===========>..................] - ETA: 4:24 - loss: 3.7643 - regression_loss: 2.8142 - classification_loss: 0.9500
 415/1000 [===========>..................] - ETA: 4:24 - loss: 3.7660 - regression_loss: 2.8162 - classification_loss: 0.9498
 416/1000 [===========>..................] - ETA: 4:23 - loss: 3.7683 - regression_loss: 2.8181 - classification_loss: 0.9501
 417/1000 [===========>..................] - ETA: 4:23 - loss: 3.7690 - regression_loss: 2.8198 - classification_loss: 0.9492
 418/1000 [===========>..................] - ETA: 4:23 - loss: 3.7700 - regression_loss: 2.8209 - classification_loss: 0.9491
 419/1000 [===========>..................] - ETA: 4:22 - loss: 3.7711 - regression_loss: 2.8226 - classification_loss: 0.9485
 420/1000 [===========>..................] - ETA: 4:22 - loss: 3.7719 - regression_loss: 2.8243 - classification_loss: 0.9476
 421/1000 [===========>..................] - ETA: 4:21 - loss: 3.7735 - regression_loss: 2.8251 - classification_loss: 0.9484
 422/1000 [===========>..................] - ETA: 4:21 - loss: 3.7770 - regression_loss: 2.8277 - classification_loss: 0.9492
 423/1000 [===========>..................] - ETA: 4:20 - loss: 3.7782 - regression_loss: 2.8293 - classification_loss: 0.9489
 424/1000 [===========>..................] - ETA: 4:20 - loss: 3.7693 - regression_loss: 2.8226 - classification_loss: 0.9467
 425/1000 [===========>..................] - ETA: 4:19 - loss: 3.7767 - regression_loss: 2.8300 - classification_loss: 0.9468
 426/1000 [===========>..................] - ETA: 4:19 - loss: 3.7777 - regression_loss: 2.8309 - classification_loss: 0.9468
 427/1000 [===========>..................] - ETA: 4:19 - loss: 3.7786 - regression_loss: 2.8308 - classification_loss: 0.9478
 428/1000 [===========>..................] - ETA: 4:18 - loss: 3.7803 - regression_loss: 2.8331 - classification_loss: 0.9473
 429/1000 [===========>..................] - ETA: 4:18 - loss: 3.7804 - regression_loss: 2.8341 - classification_loss: 0.9464
 430/1000 [===========>..................] - ETA: 4:17 - loss: 3.7826 - regression_loss: 2.8359 - classification_loss: 0.9468
 431/1000 [===========>..................] - ETA: 4:17 - loss: 3.7848 - regression_loss: 2.8379 - classification_loss: 0.9469
 432/1000 [===========>..................] - ETA: 4:16 - loss: 3.7856 - regression_loss: 2.8397 - classification_loss: 0.9459
 433/1000 [===========>..................] - ETA: 4:16 - loss: 3.7888 - regression_loss: 2.8417 - classification_loss: 0.9471
 434/1000 [============>.................] - ETA: 4:15 - loss: 3.7910 - regression_loss: 2.8437 - classification_loss: 0.9473
 435/1000 [============>.................] - ETA: 4:15 - loss: 3.7919 - regression_loss: 2.8451 - classification_loss: 0.9468
 436/1000 [============>.................] - ETA: 4:14 - loss: 3.7832 - regression_loss: 2.8386 - classification_loss: 0.9446
 437/1000 [============>.................] - ETA: 4:14 - loss: 3.7867 - regression_loss: 2.8416 - classification_loss: 0.9450
 438/1000 [============>.................] - ETA: 4:14 - loss: 3.7883 - regression_loss: 2.8428 - classification_loss: 0.9455
 439/1000 [============>.................] - ETA: 4:13 - loss: 3.7902 - regression_loss: 2.8453 - classification_loss: 0.9449
 440/1000 [============>.................] - ETA: 4:13 - loss: 3.7924 - regression_loss: 2.8477 - classification_loss: 0.9447
 441/1000 [============>.................] - ETA: 4:12 - loss: 3.7848 - regression_loss: 2.8412 - classification_loss: 0.9436
 442/1000 [============>.................] - ETA: 4:12 - loss: 3.7880 - regression_loss: 2.8434 - classification_loss: 0.9445
 443/1000 [============>.................] - ETA: 4:11 - loss: 3.7910 - regression_loss: 2.8469 - classification_loss: 0.9442
 444/1000 [============>.................] - ETA: 4:11 - loss: 3.7913 - regression_loss: 2.8473 - classification_loss: 0.9440
 445/1000 [============>.................] - ETA: 4:10 - loss: 3.7844 - regression_loss: 2.8409 - classification_loss: 0.9435
 446/1000 [============>.................] - ETA: 4:10 - loss: 3.7761 - regression_loss: 2.8345 - classification_loss: 0.9415
 447/1000 [============>.................] - ETA: 4:09 - loss: 3.7774 - regression_loss: 2.8361 - classification_loss: 0.9413
 448/1000 [============>.................] - ETA: 4:09 - loss: 3.7779 - regression_loss: 2.8359 - classification_loss: 0.9420
 449/1000 [============>.................] - ETA: 4:09 - loss: 3.7784 - regression_loss: 2.8372 - classification_loss: 0.9412
 450/1000 [============>.................] - ETA: 4:08 - loss: 3.7797 - regression_loss: 2.8387 - classification_loss: 0.9411
 451/1000 [============>.................] - ETA: 4:08 - loss: 3.7714 - regression_loss: 2.8324 - classification_loss: 0.9390
 452/1000 [============>.................] - ETA: 4:07 - loss: 3.7718 - regression_loss: 2.8321 - classification_loss: 0.9396
 453/1000 [============>.................] - ETA: 4:07 - loss: 3.7755 - regression_loss: 2.8358 - classification_loss: 0.9397
 454/1000 [============>.................] - ETA: 4:06 - loss: 3.7766 - regression_loss: 2.8372 - classification_loss: 0.9394
 455/1000 [============>.................] - ETA: 4:06 - loss: 3.7762 - regression_loss: 2.8367 - classification_loss: 0.9395
 456/1000 [============>.................] - ETA: 4:05 - loss: 3.7766 - regression_loss: 2.8364 - classification_loss: 0.9401
 457/1000 [============>.................] - ETA: 4:05 - loss: 3.7791 - regression_loss: 2.8394 - classification_loss: 0.9397
 458/1000 [============>.................] - ETA: 4:04 - loss: 3.7709 - regression_loss: 2.8332 - classification_loss: 0.9376
 459/1000 [============>.................] - ETA: 4:04 - loss: 3.7709 - regression_loss: 2.8337 - classification_loss: 0.9372
 460/1000 [============>.................] - ETA: 4:04 - loss: 3.7735 - regression_loss: 2.8357 - classification_loss: 0.9379
 461/1000 [============>.................] - ETA: 4:03 - loss: 3.7654 - regression_loss: 2.8295 - classification_loss: 0.9359
 462/1000 [============>.................] - ETA: 4:03 - loss: 3.7666 - regression_loss: 2.8311 - classification_loss: 0.9355
 463/1000 [============>.................] - ETA: 4:02 - loss: 3.7683 - regression_loss: 2.8331 - classification_loss: 0.9353
 464/1000 [============>.................] - ETA: 4:02 - loss: 3.7697 - regression_loss: 2.8349 - classification_loss: 0.9347
 465/1000 [============>.................] - ETA: 4:01 - loss: 3.7712 - regression_loss: 2.8367 - classification_loss: 0.9345
 466/1000 [============>.................] - ETA: 4:01 - loss: 3.7723 - regression_loss: 2.8381 - classification_loss: 0.9341
 467/1000 [=============>................] - ETA: 4:00 - loss: 3.7717 - regression_loss: 2.8368 - classification_loss: 0.9349
 468/1000 [=============>................] - ETA: 4:00 - loss: 3.7637 - regression_loss: 2.8308 - classification_loss: 0.9330
 469/1000 [=============>................] - ETA: 4:00 - loss: 3.7658 - regression_loss: 2.8316 - classification_loss: 0.9342
 470/1000 [=============>................] - ETA: 3:59 - loss: 3.7654 - regression_loss: 2.8310 - classification_loss: 0.9345
 471/1000 [=============>................] - ETA: 3:59 - loss: 3.7574 - regression_loss: 2.8249 - classification_loss: 0.9325
 472/1000 [=============>................] - ETA: 3:58 - loss: 3.7586 - regression_loss: 2.8264 - classification_loss: 0.9322
 473/1000 [=============>................] - ETA: 3:58 - loss: 3.7590 - regression_loss: 2.8264 - classification_loss: 0.9326
 474/1000 [=============>................] - ETA: 3:57 - loss: 3.7592 - regression_loss: 2.8273 - classification_loss: 0.9320
 475/1000 [=============>................] - ETA: 3:57 - loss: 3.7514 - regression_loss: 2.8213 - classification_loss: 0.9300
 476/1000 [=============>................] - ETA: 3:56 - loss: 3.7513 - regression_loss: 2.8221 - classification_loss: 0.9292
 477/1000 [=============>................] - ETA: 3:56 - loss: 3.7508 - regression_loss: 2.8224 - classification_loss: 0.9285
 478/1000 [=============>................] - ETA: 3:55 - loss: 3.7526 - regression_loss: 2.8245 - classification_loss: 0.9281
 479/1000 [=============>................] - ETA: 3:55 - loss: 3.7553 - regression_loss: 2.8262 - classification_loss: 0.9292
 480/1000 [=============>................] - ETA: 3:55 - loss: 3.7593 - regression_loss: 2.8291 - classification_loss: 0.9301
 481/1000 [=============>................] - ETA: 3:54 - loss: 3.7515 - regression_loss: 2.8232 - classification_loss: 0.9282
 482/1000 [=============>................] - ETA: 3:54 - loss: 3.7547 - regression_loss: 2.8254 - classification_loss: 0.9293
 483/1000 [=============>................] - ETA: 3:53 - loss: 3.7469 - regression_loss: 2.8196 - classification_loss: 0.9274
 484/1000 [=============>................] - ETA: 3:53 - loss: 3.7496 - regression_loss: 2.8222 - classification_loss: 0.9274
 485/1000 [=============>................] - ETA: 3:52 - loss: 3.7512 - regression_loss: 2.8233 - classification_loss: 0.9279
 486/1000 [=============>................] - ETA: 3:52 - loss: 3.7435 - regression_loss: 2.8175 - classification_loss: 0.9260
 487/1000 [=============>................] - ETA: 3:51 - loss: 3.7447 - regression_loss: 2.8179 - classification_loss: 0.9267
 488/1000 [=============>................] - ETA: 3:51 - loss: 3.7451 - regression_loss: 2.8183 - classification_loss: 0.9268
 489/1000 [=============>................] - ETA: 3:50 - loss: 3.7467 - regression_loss: 2.8196 - classification_loss: 0.9270
 490/1000 [=============>................] - ETA: 3:50 - loss: 3.7471 - regression_loss: 2.8209 - classification_loss: 0.9262
 491/1000 [=============>................] - ETA: 3:50 - loss: 3.7476 - regression_loss: 2.8224 - classification_loss: 0.9252
 492/1000 [=============>................] - ETA: 3:49 - loss: 3.7503 - regression_loss: 2.8242 - classification_loss: 0.9261
 493/1000 [=============>................] - ETA: 3:49 - loss: 3.7529 - regression_loss: 2.8258 - classification_loss: 0.9272
 494/1000 [=============>................] - ETA: 3:48 - loss: 3.7565 - regression_loss: 2.8289 - classification_loss: 0.9276
 495/1000 [=============>................] - ETA: 3:48 - loss: 3.7577 - regression_loss: 2.8303 - classification_loss: 0.9274
 496/1000 [=============>................] - ETA: 3:47 - loss: 3.7597 - regression_loss: 2.8318 - classification_loss: 0.9280
 497/1000 [=============>................] - ETA: 3:47 - loss: 3.7599 - regression_loss: 2.8325 - classification_loss: 0.9275
 498/1000 [=============>................] - ETA: 3:46 - loss: 3.7603 - regression_loss: 2.8326 - classification_loss: 0.9277
 499/1000 [=============>................] - ETA: 3:46 - loss: 3.7608 - regression_loss: 2.8341 - classification_loss: 0.9267
 500/1000 [==============>...............] - ETA: 3:45 - loss: 3.7640 - regression_loss: 2.8357 - classification_loss: 0.9282
 501/1000 [==============>...............] - ETA: 3:45 - loss: 3.7642 - regression_loss: 2.8367 - classification_loss: 0.9275
 502/1000 [==============>...............] - ETA: 3:45 - loss: 3.7654 - regression_loss: 2.8384 - classification_loss: 0.9271
 503/1000 [==============>...............] - ETA: 3:44 - loss: 3.7687 - regression_loss: 2.8408 - classification_loss: 0.9279
 504/1000 [==============>...............] - ETA: 3:44 - loss: 3.7613 - regression_loss: 2.8352 - classification_loss: 0.9261
 505/1000 [==============>...............] - ETA: 3:43 - loss: 3.7621 - regression_loss: 2.8363 - classification_loss: 0.9258
 506/1000 [==============>...............] - ETA: 3:43 - loss: 3.7546 - regression_loss: 2.8307 - classification_loss: 0.9240
 507/1000 [==============>...............] - ETA: 3:42 - loss: 3.7554 - regression_loss: 2.8322 - classification_loss: 0.9232
 508/1000 [==============>...............] - ETA: 3:42 - loss: 3.7578 - regression_loss: 2.8334 - classification_loss: 0.9244
 509/1000 [==============>...............] - ETA: 3:41 - loss: 3.7587 - regression_loss: 2.8348 - classification_loss: 0.9239
 510/1000 [==============>...............] - ETA: 3:41 - loss: 3.7591 - regression_loss: 2.8353 - classification_loss: 0.9238
 511/1000 [==============>...............] - ETA: 3:41 - loss: 3.7615 - regression_loss: 2.8376 - classification_loss: 0.9239
 512/1000 [==============>...............] - ETA: 3:40 - loss: 3.7624 - regression_loss: 2.8391 - classification_loss: 0.9233
 513/1000 [==============>...............] - ETA: 3:40 - loss: 3.7653 - regression_loss: 2.8423 - classification_loss: 0.9230
 514/1000 [==============>...............] - ETA: 3:39 - loss: 3.7665 - regression_loss: 2.8443 - classification_loss: 0.9222
 515/1000 [==============>...............] - ETA: 3:39 - loss: 3.7592 - regression_loss: 2.8388 - classification_loss: 0.9204
 516/1000 [==============>...............] - ETA: 3:38 - loss: 3.7615 - regression_loss: 2.8396 - classification_loss: 0.9219
 517/1000 [==============>...............] - ETA: 3:38 - loss: 3.7542 - regression_loss: 2.8341 - classification_loss: 0.9201
 518/1000 [==============>...............] - ETA: 3:37 - loss: 3.7470 - regression_loss: 2.8287 - classification_loss: 0.9183
 519/1000 [==============>...............] - ETA: 3:37 - loss: 3.7398 - regression_loss: 2.8232 - classification_loss: 0.9166
 520/1000 [==============>...............] - ETA: 3:36 - loss: 3.7326 - regression_loss: 2.8178 - classification_loss: 0.9149
 521/1000 [==============>...............] - ETA: 3:36 - loss: 3.7332 - regression_loss: 2.8189 - classification_loss: 0.9144
 522/1000 [==============>...............] - ETA: 3:36 - loss: 3.7349 - regression_loss: 2.8199 - classification_loss: 0.9150
 523/1000 [==============>...............] - ETA: 3:35 - loss: 3.7377 - regression_loss: 2.8210 - classification_loss: 0.9166
 524/1000 [==============>...............] - ETA: 3:35 - loss: 3.7410 - regression_loss: 2.8224 - classification_loss: 0.9185
 525/1000 [==============>...............] - ETA: 3:34 - loss: 3.7425 - regression_loss: 2.8236 - classification_loss: 0.9189
 526/1000 [==============>...............] - ETA: 3:34 - loss: 3.7437 - regression_loss: 2.8243 - classification_loss: 0.9194
 527/1000 [==============>...............] - ETA: 3:33 - loss: 3.7366 - regression_loss: 2.8189 - classification_loss: 0.9176
 528/1000 [==============>...............] - ETA: 3:33 - loss: 3.7376 - regression_loss: 2.8201 - classification_loss: 0.9175
 529/1000 [==============>...............] - ETA: 3:32 - loss: 3.7305 - regression_loss: 2.8148 - classification_loss: 0.9158
 530/1000 [==============>...............] - ETA: 3:32 - loss: 3.7325 - regression_loss: 2.8173 - classification_loss: 0.9152
 531/1000 [==============>...............] - ETA: 3:31 - loss: 3.7255 - regression_loss: 2.8120 - classification_loss: 0.9135
 532/1000 [==============>...............] - ETA: 3:31 - loss: 3.7257 - regression_loss: 2.8127 - classification_loss: 0.9130
 533/1000 [==============>...............] - ETA: 3:31 - loss: 3.7258 - regression_loss: 2.8134 - classification_loss: 0.9124
 534/1000 [===============>..............] - ETA: 3:30 - loss: 3.7281 - regression_loss: 2.8151 - classification_loss: 0.9130
 535/1000 [===============>..............] - ETA: 3:30 - loss: 3.7212 - regression_loss: 2.8099 - classification_loss: 0.9113
 536/1000 [===============>..............] - ETA: 3:29 - loss: 3.7243 - regression_loss: 2.8117 - classification_loss: 0.9125
 537/1000 [===============>..............] - ETA: 3:29 - loss: 3.7173 - regression_loss: 2.8065 - classification_loss: 0.9108
 538/1000 [===============>..............] - ETA: 3:28 - loss: 3.7104 - regression_loss: 2.8013 - classification_loss: 0.9091
 539/1000 [===============>..............] - ETA: 3:28 - loss: 3.7159 - regression_loss: 2.8043 - classification_loss: 0.9116
 540/1000 [===============>..............] - ETA: 3:27 - loss: 3.7167 - regression_loss: 2.8037 - classification_loss: 0.9130
 541/1000 [===============>..............] - ETA: 3:27 - loss: 3.7191 - regression_loss: 2.8059 - classification_loss: 0.9132
 542/1000 [===============>..............] - ETA: 3:26 - loss: 3.7209 - regression_loss: 2.8082 - classification_loss: 0.9126
 543/1000 [===============>..............] - ETA: 3:26 - loss: 3.7226 - regression_loss: 2.8100 - classification_loss: 0.9126
 544/1000 [===============>..............] - ETA: 3:26 - loss: 3.7271 - regression_loss: 2.8128 - classification_loss: 0.9143
 545/1000 [===============>..............] - ETA: 3:25 - loss: 3.7277 - regression_loss: 2.8140 - classification_loss: 0.9137
 546/1000 [===============>..............] - ETA: 3:25 - loss: 3.7209 - regression_loss: 2.8088 - classification_loss: 0.9120
 547/1000 [===============>..............] - ETA: 3:24 - loss: 3.7229 - regression_loss: 2.8110 - classification_loss: 0.9119
 548/1000 [===============>..............] - ETA: 3:24 - loss: 3.7161 - regression_loss: 2.8059 - classification_loss: 0.9102
 549/1000 [===============>..............] - ETA: 3:23 - loss: 3.7233 - regression_loss: 2.8105 - classification_loss: 0.9129
 550/1000 [===============>..............] - ETA: 3:23 - loss: 3.7247 - regression_loss: 2.8117 - classification_loss: 0.9130
 551/1000 [===============>..............] - ETA: 3:22 - loss: 3.7267 - regression_loss: 2.8126 - classification_loss: 0.9140
 552/1000 [===============>..............] - ETA: 3:22 - loss: 3.7199 - regression_loss: 2.8075 - classification_loss: 0.9124
 553/1000 [===============>..............] - ETA: 3:22 - loss: 3.7222 - regression_loss: 2.8082 - classification_loss: 0.9140
 554/1000 [===============>..............] - ETA: 3:21 - loss: 3.7155 - regression_loss: 2.8031 - classification_loss: 0.9124
 555/1000 [===============>..............] - ETA: 3:21 - loss: 3.7158 - regression_loss: 2.8039 - classification_loss: 0.9119
 556/1000 [===============>..............] - ETA: 3:20 - loss: 3.7091 - regression_loss: 2.7989 - classification_loss: 0.9103
 557/1000 [===============>..............] - ETA: 3:20 - loss: 3.7109 - regression_loss: 2.8004 - classification_loss: 0.9105
 558/1000 [===============>..............] - ETA: 3:19 - loss: 3.7120 - regression_loss: 2.8022 - classification_loss: 0.9098
 559/1000 [===============>..............] - ETA: 3:19 - loss: 3.7154 - regression_loss: 2.8035 - classification_loss: 0.9120
 560/1000 [===============>..............] - ETA: 3:18 - loss: 3.7170 - regression_loss: 2.8037 - classification_loss: 0.9132
 561/1000 [===============>..............] - ETA: 3:18 - loss: 3.7191 - regression_loss: 2.8057 - classification_loss: 0.9133
 562/1000 [===============>..............] - ETA: 3:17 - loss: 3.7207 - regression_loss: 2.8073 - classification_loss: 0.9134
 563/1000 [===============>..............] - ETA: 3:17 - loss: 3.7223 - regression_loss: 2.8075 - classification_loss: 0.9147
 564/1000 [===============>..............] - ETA: 3:17 - loss: 3.7157 - regression_loss: 2.8025 - classification_loss: 0.9131
 565/1000 [===============>..............] - ETA: 3:16 - loss: 3.7182 - regression_loss: 2.8043 - classification_loss: 0.9139
 566/1000 [===============>..............] - ETA: 3:16 - loss: 3.7235 - regression_loss: 2.8073 - classification_loss: 0.9162
 567/1000 [================>.............] - ETA: 3:15 - loss: 3.7253 - regression_loss: 2.8098 - classification_loss: 0.9155
 568/1000 [================>.............] - ETA: 3:15 - loss: 3.7188 - regression_loss: 2.8048 - classification_loss: 0.9139
 569/1000 [================>.............] - ETA: 3:14 - loss: 3.7122 - regression_loss: 2.7999 - classification_loss: 0.9123
 570/1000 [================>.............] - ETA: 3:14 - loss: 3.7080 - regression_loss: 2.7950 - classification_loss: 0.9130
 571/1000 [================>.............] - ETA: 3:13 - loss: 3.7088 - regression_loss: 2.7949 - classification_loss: 0.9139
 572/1000 [================>.............] - ETA: 3:13 - loss: 3.7097 - regression_loss: 2.7961 - classification_loss: 0.9136
 573/1000 [================>.............] - ETA: 3:12 - loss: 3.7106 - regression_loss: 2.7975 - classification_loss: 0.9131
 574/1000 [================>.............] - ETA: 3:12 - loss: 3.7126 - regression_loss: 2.7999 - classification_loss: 0.9127
 575/1000 [================>.............] - ETA: 3:12 - loss: 3.7128 - regression_loss: 2.7992 - classification_loss: 0.9136
 576/1000 [================>.............] - ETA: 3:11 - loss: 3.7077 - regression_loss: 2.7944 - classification_loss: 0.9133
 577/1000 [================>.............] - ETA: 3:11 - loss: 3.7089 - regression_loss: 2.7960 - classification_loss: 0.9129
 578/1000 [================>.............] - ETA: 3:10 - loss: 3.7091 - regression_loss: 2.7963 - classification_loss: 0.9128
 579/1000 [================>.............] - ETA: 3:10 - loss: 3.7111 - regression_loss: 2.7979 - classification_loss: 0.9132
 580/1000 [================>.............] - ETA: 3:09 - loss: 3.7124 - regression_loss: 2.7980 - classification_loss: 0.9144
 581/1000 [================>.............] - ETA: 3:09 - loss: 3.7143 - regression_loss: 2.7996 - classification_loss: 0.9146
 582/1000 [================>.............] - ETA: 3:08 - loss: 3.7160 - regression_loss: 2.8012 - classification_loss: 0.9147
 583/1000 [================>.............] - ETA: 3:08 - loss: 3.7170 - regression_loss: 2.8021 - classification_loss: 0.9148
 584/1000 [================>.............] - ETA: 3:08 - loss: 3.7206 - regression_loss: 2.8062 - classification_loss: 0.9144
 585/1000 [================>.............] - ETA: 3:07 - loss: 3.7212 - regression_loss: 2.8069 - classification_loss: 0.9144
 586/1000 [================>.............] - ETA: 3:07 - loss: 3.7223 - regression_loss: 2.8070 - classification_loss: 0.9153
 587/1000 [================>.............] - ETA: 3:06 - loss: 3.7252 - regression_loss: 2.8084 - classification_loss: 0.9167
 588/1000 [================>.............] - ETA: 3:06 - loss: 3.7256 - regression_loss: 2.8094 - classification_loss: 0.9162
 589/1000 [================>.............] - ETA: 3:05 - loss: 3.7263 - regression_loss: 2.8106 - classification_loss: 0.9158
 590/1000 [================>.............] - ETA: 3:05 - loss: 3.7286 - regression_loss: 2.8119 - classification_loss: 0.9167
 591/1000 [================>.............] - ETA: 3:04 - loss: 3.7311 - regression_loss: 2.8135 - classification_loss: 0.9176
 592/1000 [================>.............] - ETA: 3:04 - loss: 3.7327 - regression_loss: 2.8141 - classification_loss: 0.9186
 593/1000 [================>.............] - ETA: 3:03 - loss: 3.7363 - regression_loss: 2.8173 - classification_loss: 0.9191
 594/1000 [================>.............] - ETA: 3:03 - loss: 3.7366 - regression_loss: 2.8181 - classification_loss: 0.9185
 595/1000 [================>.............] - ETA: 3:03 - loss: 3.7303 - regression_loss: 2.8134 - classification_loss: 0.9170
 596/1000 [================>.............] - ETA: 3:02 - loss: 3.7334 - regression_loss: 2.8158 - classification_loss: 0.9175
 597/1000 [================>.............] - ETA: 3:02 - loss: 3.7307 - regression_loss: 2.8111 - classification_loss: 0.9196
 598/1000 [================>.............] - ETA: 3:01 - loss: 3.7322 - regression_loss: 2.8124 - classification_loss: 0.9198
 599/1000 [================>.............] - ETA: 3:01 - loss: 3.7330 - regression_loss: 2.8133 - classification_loss: 0.9197
 600/1000 [=================>............] - ETA: 3:00 - loss: 3.7345 - regression_loss: 2.8147 - classification_loss: 0.9198
 601/1000 [=================>............] - ETA: 3:00 - loss: 3.7283 - regression_loss: 2.8100 - classification_loss: 0.9183
 602/1000 [=================>............] - ETA: 2:59 - loss: 3.7293 - regression_loss: 2.8112 - classification_loss: 0.9181
 603/1000 [=================>............] - ETA: 2:59 - loss: 3.7300 - regression_loss: 2.8125 - classification_loss: 0.9175
 604/1000 [=================>............] - ETA: 2:58 - loss: 3.7311 - regression_loss: 2.8137 - classification_loss: 0.9175
 605/1000 [=================>............] - ETA: 2:58 - loss: 3.7314 - regression_loss: 2.8140 - classification_loss: 0.9174
 606/1000 [=================>............] - ETA: 2:58 - loss: 3.7309 - regression_loss: 2.8094 - classification_loss: 0.9215
 607/1000 [=================>............] - ETA: 2:57 - loss: 3.7254 - regression_loss: 2.8047 - classification_loss: 0.9207
 608/1000 [=================>............] - ETA: 2:57 - loss: 3.7269 - regression_loss: 2.8060 - classification_loss: 0.9209
 609/1000 [=================>............] - ETA: 2:56 - loss: 3.7282 - regression_loss: 2.8078 - classification_loss: 0.9204
 610/1000 [=================>............] - ETA: 2:56 - loss: 3.7221 - regression_loss: 2.8032 - classification_loss: 0.9189
 611/1000 [=================>............] - ETA: 2:55 - loss: 3.7228 - regression_loss: 2.8039 - classification_loss: 0.9188
 612/1000 [=================>............] - ETA: 2:55 - loss: 3.7238 - regression_loss: 2.8056 - classification_loss: 0.9182
 613/1000 [=================>............] - ETA: 2:54 - loss: 3.7252 - regression_loss: 2.8071 - classification_loss: 0.9181
 614/1000 [=================>............] - ETA: 2:54 - loss: 3.7255 - regression_loss: 2.8079 - classification_loss: 0.9176
 615/1000 [=================>............] - ETA: 2:54 - loss: 3.7263 - regression_loss: 2.8089 - classification_loss: 0.9174
 616/1000 [=================>............] - ETA: 2:53 - loss: 3.7202 - regression_loss: 2.8043 - classification_loss: 0.9159
 617/1000 [=================>............] - ETA: 2:53 - loss: 3.7214 - regression_loss: 2.8060 - classification_loss: 0.9154
 618/1000 [=================>............] - ETA: 2:52 - loss: 3.7227 - regression_loss: 2.8077 - classification_loss: 0.9150
 619/1000 [=================>............] - ETA: 2:52 - loss: 3.7242 - regression_loss: 2.8096 - classification_loss: 0.9147
 620/1000 [=================>............] - ETA: 2:51 - loss: 3.7240 - regression_loss: 2.8100 - classification_loss: 0.9140
 621/1000 [=================>............] - ETA: 2:51 - loss: 3.7258 - regression_loss: 2.8120 - classification_loss: 0.9138
 622/1000 [=================>............] - ETA: 2:50 - loss: 3.7274 - regression_loss: 2.8134 - classification_loss: 0.9140
 623/1000 [=================>............] - ETA: 2:50 - loss: 3.7291 - regression_loss: 2.8156 - classification_loss: 0.9135
 624/1000 [=================>............] - ETA: 2:49 - loss: 3.7308 - regression_loss: 2.8164 - classification_loss: 0.9144
 625/1000 [=================>............] - ETA: 2:49 - loss: 3.7316 - regression_loss: 2.8170 - classification_loss: 0.9146
 626/1000 [=================>............] - ETA: 2:49 - loss: 3.7341 - regression_loss: 2.8189 - classification_loss: 0.9152
 627/1000 [=================>............] - ETA: 2:48 - loss: 3.7342 - regression_loss: 2.8195 - classification_loss: 0.9147
 628/1000 [=================>............] - ETA: 2:48 - loss: 3.7282 - regression_loss: 2.8150 - classification_loss: 0.9132
 629/1000 [=================>............] - ETA: 2:47 - loss: 3.7297 - regression_loss: 2.8171 - classification_loss: 0.9126
 630/1000 [=================>............] - ETA: 2:47 - loss: 3.7238 - regression_loss: 2.8126 - classification_loss: 0.9112
 631/1000 [=================>............] - ETA: 2:46 - loss: 3.7240 - regression_loss: 2.8127 - classification_loss: 0.9113
 632/1000 [=================>............] - ETA: 2:46 - loss: 3.7245 - regression_loss: 2.8135 - classification_loss: 0.9110
 633/1000 [=================>............] - ETA: 2:45 - loss: 3.7272 - regression_loss: 2.8153 - classification_loss: 0.9119
 634/1000 [==================>...........] - ETA: 2:45 - loss: 3.7213 - regression_loss: 2.8108 - classification_loss: 0.9105
 635/1000 [==================>...........] - ETA: 2:44 - loss: 3.7211 - regression_loss: 2.8111 - classification_loss: 0.9100
 636/1000 [==================>...........] - ETA: 2:44 - loss: 3.7241 - regression_loss: 2.8130 - classification_loss: 0.9110
 637/1000 [==================>...........] - ETA: 2:44 - loss: 3.7247 - regression_loss: 2.8129 - classification_loss: 0.9118
 638/1000 [==================>...........] - ETA: 2:43 - loss: 3.7296 - regression_loss: 2.8167 - classification_loss: 0.9128
 639/1000 [==================>...........] - ETA: 2:43 - loss: 3.7323 - regression_loss: 2.8187 - classification_loss: 0.9137
 640/1000 [==================>...........] - ETA: 2:42 - loss: 3.7332 - regression_loss: 2.8201 - classification_loss: 0.9131
 641/1000 [==================>...........] - ETA: 2:42 - loss: 3.7344 - regression_loss: 2.8214 - classification_loss: 0.9129
 642/1000 [==================>...........] - ETA: 2:41 - loss: 3.7339 - regression_loss: 2.8217 - classification_loss: 0.9122
 643/1000 [==================>...........] - ETA: 2:41 - loss: 3.7282 - regression_loss: 2.8173 - classification_loss: 0.9108
 644/1000 [==================>...........] - ETA: 2:40 - loss: 3.7224 - regression_loss: 2.8129 - classification_loss: 0.9094
 645/1000 [==================>...........] - ETA: 2:40 - loss: 3.7244 - regression_loss: 2.8147 - classification_loss: 0.9097
 646/1000 [==================>...........] - ETA: 2:39 - loss: 3.7263 - regression_loss: 2.8155 - classification_loss: 0.9109
 647/1000 [==================>...........] - ETA: 2:39 - loss: 3.7206 - regression_loss: 2.8111 - classification_loss: 0.9094
 648/1000 [==================>...........] - ETA: 2:39 - loss: 3.7227 - regression_loss: 2.8119 - classification_loss: 0.9109
 649/1000 [==================>...........] - ETA: 2:38 - loss: 3.7222 - regression_loss: 2.8118 - classification_loss: 0.9105
 650/1000 [==================>...........] - ETA: 2:38 - loss: 3.7165 - regression_loss: 2.8075 - classification_loss: 0.9091
 651/1000 [==================>...........] - ETA: 2:37 - loss: 3.7172 - regression_loss: 2.8087 - classification_loss: 0.9086
 652/1000 [==================>...........] - ETA: 2:37 - loss: 3.7180 - regression_loss: 2.8099 - classification_loss: 0.9081
 653/1000 [==================>...........] - ETA: 2:36 - loss: 3.7187 - regression_loss: 2.8110 - classification_loss: 0.9077
 654/1000 [==================>...........] - ETA: 2:36 - loss: 3.7205 - regression_loss: 2.8120 - classification_loss: 0.9085
 655/1000 [==================>...........] - ETA: 2:35 - loss: 3.7229 - regression_loss: 2.8143 - classification_loss: 0.9086
 656/1000 [==================>...........] - ETA: 2:35 - loss: 3.7172 - regression_loss: 2.8100 - classification_loss: 0.9073
 657/1000 [==================>...........] - ETA: 2:35 - loss: 3.7188 - regression_loss: 2.8110 - classification_loss: 0.9078
 658/1000 [==================>...........] - ETA: 2:34 - loss: 3.7192 - regression_loss: 2.8108 - classification_loss: 0.9084
 659/1000 [==================>...........] - ETA: 2:34 - loss: 3.7213 - regression_loss: 2.8120 - classification_loss: 0.9093
 660/1000 [==================>...........] - ETA: 2:33 - loss: 3.7228 - regression_loss: 2.8134 - classification_loss: 0.9094
 661/1000 [==================>...........] - ETA: 2:33 - loss: 3.7261 - regression_loss: 2.8158 - classification_loss: 0.9103
 662/1000 [==================>...........] - ETA: 2:32 - loss: 3.7258 - regression_loss: 2.8163 - classification_loss: 0.9095
 663/1000 [==================>...........] - ETA: 2:32 - loss: 3.7289 - regression_loss: 2.8185 - classification_loss: 0.9104
 664/1000 [==================>...........] - ETA: 2:31 - loss: 3.7233 - regression_loss: 2.8142 - classification_loss: 0.9090
 665/1000 [==================>...........] - ETA: 2:31 - loss: 3.7238 - regression_loss: 2.8144 - classification_loss: 0.9094
 666/1000 [==================>...........] - ETA: 2:30 - loss: 3.7182 - regression_loss: 2.8102 - classification_loss: 0.9080
 667/1000 [===================>..........] - ETA: 2:30 - loss: 3.7194 - regression_loss: 2.8117 - classification_loss: 0.9077
 668/1000 [===================>..........] - ETA: 2:30 - loss: 3.7193 - regression_loss: 2.8119 - classification_loss: 0.9074
 669/1000 [===================>..........] - ETA: 2:29 - loss: 3.7195 - regression_loss: 2.8115 - classification_loss: 0.9080
 670/1000 [===================>..........] - ETA: 2:29 - loss: 3.7192 - regression_loss: 2.8118 - classification_loss: 0.9074
 671/1000 [===================>..........] - ETA: 2:28 - loss: 3.7204 - regression_loss: 2.8131 - classification_loss: 0.9073
 672/1000 [===================>..........] - ETA: 2:28 - loss: 3.7209 - regression_loss: 2.8136 - classification_loss: 0.9073
 673/1000 [===================>..........] - ETA: 2:27 - loss: 3.7223 - regression_loss: 2.8148 - classification_loss: 0.9075
 674/1000 [===================>..........] - ETA: 2:27 - loss: 3.7246 - regression_loss: 2.8156 - classification_loss: 0.9090
 675/1000 [===================>..........] - ETA: 2:26 - loss: 3.7247 - regression_loss: 2.8157 - classification_loss: 0.9090
 676/1000 [===================>..........] - ETA: 2:26 - loss: 3.7254 - regression_loss: 2.8168 - classification_loss: 0.9086
 677/1000 [===================>..........] - ETA: 2:25 - loss: 3.7283 - regression_loss: 2.8191 - classification_loss: 0.9092
 678/1000 [===================>..........] - ETA: 2:25 - loss: 3.7288 - regression_loss: 2.8190 - classification_loss: 0.9099
 679/1000 [===================>..........] - ETA: 2:25 - loss: 3.7235 - regression_loss: 2.8148 - classification_loss: 0.9087
 680/1000 [===================>..........] - ETA: 2:24 - loss: 3.7234 - regression_loss: 2.8148 - classification_loss: 0.9086
 681/1000 [===================>..........] - ETA: 2:24 - loss: 3.7251 - regression_loss: 2.8155 - classification_loss: 0.9096
 682/1000 [===================>..........] - ETA: 2:23 - loss: 3.7262 - regression_loss: 2.8165 - classification_loss: 0.9097
 683/1000 [===================>..........] - ETA: 2:23 - loss: 3.7272 - regression_loss: 2.8168 - classification_loss: 0.9104
 684/1000 [===================>..........] - ETA: 2:22 - loss: 3.7218 - regression_loss: 2.8127 - classification_loss: 0.9091
 685/1000 [===================>..........] - ETA: 2:22 - loss: 3.7258 - regression_loss: 2.8150 - classification_loss: 0.9108
 686/1000 [===================>..........] - ETA: 2:21 - loss: 3.7266 - regression_loss: 2.8153 - classification_loss: 0.9113
 687/1000 [===================>..........] - ETA: 2:21 - loss: 3.7264 - regression_loss: 2.8156 - classification_loss: 0.9109
 688/1000 [===================>..........] - ETA: 2:21 - loss: 3.7275 - regression_loss: 2.8158 - classification_loss: 0.9117
 689/1000 [===================>..........] - ETA: 2:20 - loss: 3.7288 - regression_loss: 2.8169 - classification_loss: 0.9119
 690/1000 [===================>..........] - ETA: 2:20 - loss: 3.7288 - regression_loss: 2.8166 - classification_loss: 0.9122
 691/1000 [===================>..........] - ETA: 2:19 - loss: 3.7299 - regression_loss: 2.8182 - classification_loss: 0.9117
 692/1000 [===================>..........] - ETA: 2:19 - loss: 3.7310 - regression_loss: 2.8184 - classification_loss: 0.9126
 693/1000 [===================>..........] - ETA: 2:18 - loss: 3.7334 - regression_loss: 2.8201 - classification_loss: 0.9133
 694/1000 [===================>..........] - ETA: 2:18 - loss: 3.7280 - regression_loss: 2.8161 - classification_loss: 0.9120
 695/1000 [===================>..........] - ETA: 2:17 - loss: 3.7292 - regression_loss: 2.8179 - classification_loss: 0.9114
 696/1000 [===================>..........] - ETA: 2:17 - loss: 3.7241 - regression_loss: 2.8138 - classification_loss: 0.9103
 697/1000 [===================>..........] - ETA: 2:16 - loss: 3.7243 - regression_loss: 2.8140 - classification_loss: 0.9103
 698/1000 [===================>..........] - ETA: 2:16 - loss: 3.7236 - regression_loss: 2.8135 - classification_loss: 0.9102
 699/1000 [===================>..........] - ETA: 2:16 - loss: 3.7266 - regression_loss: 2.8149 - classification_loss: 0.9116
 700/1000 [====================>.........] - ETA: 2:15 - loss: 3.7213 - regression_loss: 2.8109 - classification_loss: 0.9103
 701/1000 [====================>.........] - ETA: 2:15 - loss: 3.7250 - regression_loss: 2.8140 - classification_loss: 0.9111
 702/1000 [====================>.........] - ETA: 2:14 - loss: 3.7252 - regression_loss: 2.8138 - classification_loss: 0.9114
 703/1000 [====================>.........] - ETA: 2:14 - loss: 3.7276 - regression_loss: 2.8158 - classification_loss: 0.9118
 704/1000 [====================>.........] - ETA: 2:13 - loss: 3.7224 - regression_loss: 2.8118 - classification_loss: 0.9106
 705/1000 [====================>.........] - ETA: 2:13 - loss: 3.7171 - regression_loss: 2.8078 - classification_loss: 0.9093
 706/1000 [====================>.........] - ETA: 2:12 - loss: 3.7119 - regression_loss: 2.8039 - classification_loss: 0.9080
 707/1000 [====================>.........] - ETA: 2:12 - loss: 3.7140 - regression_loss: 2.8052 - classification_loss: 0.9088
 708/1000 [====================>.........] - ETA: 2:11 - loss: 3.7161 - regression_loss: 2.8057 - classification_loss: 0.9104
 709/1000 [====================>.........] - ETA: 2:11 - loss: 3.7156 - regression_loss: 2.8054 - classification_loss: 0.9102
 710/1000 [====================>.........] - ETA: 2:11 - loss: 3.7169 - regression_loss: 2.8063 - classification_loss: 0.9106
 711/1000 [====================>.........] - ETA: 2:10 - loss: 3.7176 - regression_loss: 2.8069 - classification_loss: 0.9107
 712/1000 [====================>.........] - ETA: 2:10 - loss: 3.7123 - regression_loss: 2.8029 - classification_loss: 0.9094
 713/1000 [====================>.........] - ETA: 2:09 - loss: 3.7071 - regression_loss: 2.7990 - classification_loss: 0.9081
 714/1000 [====================>.........] - ETA: 2:09 - loss: 3.7072 - regression_loss: 2.7985 - classification_loss: 0.9087
 715/1000 [====================>.........] - ETA: 2:08 - loss: 3.7020 - regression_loss: 2.7946 - classification_loss: 0.9074
 716/1000 [====================>.........] - ETA: 2:08 - loss: 3.7048 - regression_loss: 2.7971 - classification_loss: 0.9077
 717/1000 [====================>.........] - ETA: 2:07 - loss: 3.7054 - regression_loss: 2.7979 - classification_loss: 0.9075
 718/1000 [====================>.........] - ETA: 2:07 - loss: 3.7078 - regression_loss: 2.7995 - classification_loss: 0.9083
 719/1000 [====================>.........] - ETA: 2:07 - loss: 3.7107 - regression_loss: 2.8008 - classification_loss: 0.9099
 720/1000 [====================>.........] - ETA: 2:06 - loss: 3.7143 - regression_loss: 2.8029 - classification_loss: 0.9114
 721/1000 [====================>.........] - ETA: 2:06 - loss: 3.7092 - regression_loss: 2.7991 - classification_loss: 0.9101
 722/1000 [====================>.........] - ETA: 2:05 - loss: 3.7040 - regression_loss: 2.7952 - classification_loss: 0.9088
 723/1000 [====================>.........] - ETA: 2:05 - loss: 3.6989 - regression_loss: 2.7913 - classification_loss: 0.9076
 724/1000 [====================>.........] - ETA: 2:04 - loss: 3.6994 - regression_loss: 2.7917 - classification_loss: 0.9077
 725/1000 [====================>.........] - ETA: 2:04 - loss: 3.7024 - regression_loss: 2.7931 - classification_loss: 0.9092
 726/1000 [====================>.........] - ETA: 2:03 - loss: 3.7037 - regression_loss: 2.7940 - classification_loss: 0.9097
 727/1000 [====================>.........] - ETA: 2:03 - loss: 3.7048 - regression_loss: 2.7955 - classification_loss: 0.9093
 728/1000 [====================>.........] - ETA: 2:02 - loss: 3.7064 - regression_loss: 2.7961 - classification_loss: 0.9103
 729/1000 [====================>.........] - ETA: 2:02 - loss: 3.7013 - regression_loss: 2.7923 - classification_loss: 0.9090
 730/1000 [====================>.........] - ETA: 2:02 - loss: 3.7025 - regression_loss: 2.7929 - classification_loss: 0.9096
 731/1000 [====================>.........] - ETA: 2:01 - loss: 3.7034 - regression_loss: 2.7934 - classification_loss: 0.9100
 732/1000 [====================>.........] - ETA: 2:01 - loss: 3.7060 - regression_loss: 2.7946 - classification_loss: 0.9114
 733/1000 [====================>.........] - ETA: 2:00 - loss: 3.7010 - regression_loss: 2.7908 - classification_loss: 0.9102
 734/1000 [=====================>........] - ETA: 2:00 - loss: 3.7018 - regression_loss: 2.7911 - classification_loss: 0.9107
 735/1000 [=====================>........] - ETA: 1:59 - loss: 3.6968 - regression_loss: 2.7873 - classification_loss: 0.9095
 736/1000 [=====================>........] - ETA: 1:59 - loss: 3.6970 - regression_loss: 2.7879 - classification_loss: 0.9091
 737/1000 [=====================>........] - ETA: 1:58 - loss: 3.6986 - regression_loss: 2.7885 - classification_loss: 0.9102
 738/1000 [=====================>........] - ETA: 1:58 - loss: 3.6996 - regression_loss: 2.7896 - classification_loss: 0.9100
 739/1000 [=====================>........] - ETA: 1:57 - loss: 3.7021 - regression_loss: 2.7915 - classification_loss: 0.9106
 740/1000 [=====================>........] - ETA: 1:57 - loss: 3.6971 - regression_loss: 2.7877 - classification_loss: 0.9094
 741/1000 [=====================>........] - ETA: 1:57 - loss: 3.6971 - regression_loss: 2.7878 - classification_loss: 0.9093
 742/1000 [=====================>........] - ETA: 1:56 - loss: 3.6992 - regression_loss: 2.7889 - classification_loss: 0.9103
 743/1000 [=====================>........] - ETA: 1:56 - loss: 3.6943 - regression_loss: 2.7852 - classification_loss: 0.9091
 744/1000 [=====================>........] - ETA: 1:55 - loss: 3.6953 - regression_loss: 2.7853 - classification_loss: 0.9100
 745/1000 [=====================>........] - ETA: 1:55 - loss: 3.6903 - regression_loss: 2.7815 - classification_loss: 0.9088
 746/1000 [=====================>........] - ETA: 1:54 - loss: 3.6915 - regression_loss: 2.7830 - classification_loss: 0.9085
 747/1000 [=====================>........] - ETA: 1:54 - loss: 3.6937 - regression_loss: 2.7848 - classification_loss: 0.9089
 748/1000 [=====================>........] - ETA: 1:53 - loss: 3.6990 - regression_loss: 2.7885 - classification_loss: 0.9104
 749/1000 [=====================>........] - ETA: 1:53 - loss: 3.6940 - regression_loss: 2.7848 - classification_loss: 0.9092
 750/1000 [=====================>........] - ETA: 1:53 - loss: 3.6891 - regression_loss: 2.7811 - classification_loss: 0.9080
 751/1000 [=====================>........] - ETA: 1:52 - loss: 3.6842 - regression_loss: 2.7774 - classification_loss: 0.9068
 752/1000 [=====================>........] - ETA: 1:52 - loss: 3.6863 - regression_loss: 2.7791 - classification_loss: 0.9073
 753/1000 [=====================>........] - ETA: 1:51 - loss: 3.6872 - regression_loss: 2.7804 - classification_loss: 0.9068
 754/1000 [=====================>........] - ETA: 1:51 - loss: 3.6823 - regression_loss: 2.7767 - classification_loss: 0.9056
 755/1000 [=====================>........] - ETA: 1:50 - loss: 3.6830 - regression_loss: 2.7773 - classification_loss: 0.9057
 756/1000 [=====================>........] - ETA: 1:50 - loss: 3.6781 - regression_loss: 2.7736 - classification_loss: 0.9045
 757/1000 [=====================>........] - ETA: 1:49 - loss: 3.6733 - regression_loss: 2.7699 - classification_loss: 0.9033
 758/1000 [=====================>........] - ETA: 1:49 - loss: 3.6684 - regression_loss: 2.7663 - classification_loss: 0.9022
 759/1000 [=====================>........] - ETA: 1:48 - loss: 3.6636 - regression_loss: 2.7626 - classification_loss: 0.9010
 760/1000 [=====================>........] - ETA: 1:48 - loss: 3.6650 - regression_loss: 2.7635 - classification_loss: 0.9015
 761/1000 [=====================>........] - ETA: 1:48 - loss: 3.6667 - regression_loss: 2.7652 - classification_loss: 0.9014
 762/1000 [=====================>........] - ETA: 1:47 - loss: 3.6618 - regression_loss: 2.7616 - classification_loss: 0.9002
 763/1000 [=====================>........] - ETA: 1:47 - loss: 3.6637 - regression_loss: 2.7638 - classification_loss: 0.9000
 764/1000 [=====================>........] - ETA: 1:46 - loss: 3.6637 - regression_loss: 2.7641 - classification_loss: 0.8995
 765/1000 [=====================>........] - ETA: 1:46 - loss: 3.6659 - regression_loss: 2.7651 - classification_loss: 0.9008
 766/1000 [=====================>........] - ETA: 1:45 - loss: 3.6668 - regression_loss: 2.7658 - classification_loss: 0.9010
 767/1000 [======================>.......] - ETA: 1:45 - loss: 3.6620 - regression_loss: 2.7622 - classification_loss: 0.8998
 768/1000 [======================>.......] - ETA: 1:44 - loss: 3.6573 - regression_loss: 2.7586 - classification_loss: 0.8986
 769/1000 [======================>.......] - ETA: 1:44 - loss: 3.6525 - regression_loss: 2.7550 - classification_loss: 0.8975
 770/1000 [======================>.......] - ETA: 1:43 - loss: 3.6543 - regression_loss: 2.7558 - classification_loss: 0.8984
 771/1000 [======================>.......] - ETA: 1:43 - loss: 3.6495 - regression_loss: 2.7523 - classification_loss: 0.8973
 772/1000 [======================>.......] - ETA: 1:43 - loss: 3.6448 - regression_loss: 2.7487 - classification_loss: 0.8961
 773/1000 [======================>.......] - ETA: 1:42 - loss: 3.6455 - regression_loss: 2.7498 - classification_loss: 0.8957
 774/1000 [======================>.......] - ETA: 1:42 - loss: 3.6408 - regression_loss: 2.7462 - classification_loss: 0.8946
 775/1000 [======================>.......] - ETA: 1:41 - loss: 3.6421 - regression_loss: 2.7476 - classification_loss: 0.8946
 776/1000 [======================>.......] - ETA: 1:41 - loss: 3.6375 - regression_loss: 2.7440 - classification_loss: 0.8934
 777/1000 [======================>.......] - ETA: 1:40 - loss: 3.6404 - regression_loss: 2.7434 - classification_loss: 0.8970
 778/1000 [======================>.......] - ETA: 1:40 - loss: 3.6417 - regression_loss: 2.7438 - classification_loss: 0.8979
 779/1000 [======================>.......] - ETA: 1:39 - loss: 3.6441 - regression_loss: 2.7452 - classification_loss: 0.8990
 780/1000 [======================>.......] - ETA: 1:39 - loss: 3.6450 - regression_loss: 2.7454 - classification_loss: 0.8997
 781/1000 [======================>.......] - ETA: 1:38 - loss: 3.6471 - regression_loss: 2.7470 - classification_loss: 0.9001
 782/1000 [======================>.......] - ETA: 1:38 - loss: 3.6470 - regression_loss: 2.7474 - classification_loss: 0.8996
 783/1000 [======================>.......] - ETA: 1:38 - loss: 3.6423 - regression_loss: 2.7439 - classification_loss: 0.8984
 784/1000 [======================>.......] - ETA: 1:37 - loss: 3.6446 - regression_loss: 2.7455 - classification_loss: 0.8991
 785/1000 [======================>.......] - ETA: 1:37 - loss: 3.6445 - regression_loss: 2.7454 - classification_loss: 0.8991
 786/1000 [======================>.......] - ETA: 1:36 - loss: 3.6446 - regression_loss: 2.7456 - classification_loss: 0.8989
 787/1000 [======================>.......] - ETA: 1:36 - loss: 3.6463 - regression_loss: 2.7464 - classification_loss: 0.9000
 788/1000 [======================>.......] - ETA: 1:35 - loss: 3.6478 - regression_loss: 2.7475 - classification_loss: 0.9003
 789/1000 [======================>.......] - ETA: 1:35 - loss: 3.6485 - regression_loss: 2.7486 - classification_loss: 0.8999
 790/1000 [======================>.......] - ETA: 1:34 - loss: 3.6496 - regression_loss: 2.7498 - classification_loss: 0.8998
 791/1000 [======================>.......] - ETA: 1:34 - loss: 3.6515 - regression_loss: 2.7512 - classification_loss: 0.9003
 792/1000 [======================>.......] - ETA: 1:34 - loss: 3.6504 - regression_loss: 2.7478 - classification_loss: 0.9027
 793/1000 [======================>.......] - ETA: 1:33 - loss: 3.6508 - regression_loss: 2.7484 - classification_loss: 0.9024
 794/1000 [======================>.......] - ETA: 1:33 - loss: 3.6462 - regression_loss: 2.7450 - classification_loss: 0.9012
 795/1000 [======================>.......] - ETA: 1:32 - loss: 3.6486 - regression_loss: 2.7463 - classification_loss: 0.9023
 796/1000 [======================>.......] - ETA: 1:32 - loss: 3.6507 - regression_loss: 2.7472 - classification_loss: 0.9035
 797/1000 [======================>.......] - ETA: 1:31 - loss: 3.6535 - regression_loss: 2.7487 - classification_loss: 0.9048
 798/1000 [======================>.......] - ETA: 1:31 - loss: 3.6496 - regression_loss: 2.7452 - classification_loss: 0.9044
 799/1000 [======================>.......] - ETA: 1:30 - loss: 3.6458 - regression_loss: 2.7418 - classification_loss: 0.9040
 800/1000 [=======================>......] - ETA: 1:30 - loss: 3.6475 - regression_loss: 2.7423 - classification_loss: 0.9052
 801/1000 [=======================>......] - ETA: 1:29 - loss: 3.6483 - regression_loss: 2.7433 - classification_loss: 0.9050
 802/1000 [=======================>......] - ETA: 1:29 - loss: 3.6493 - regression_loss: 2.7437 - classification_loss: 0.9055
 803/1000 [=======================>......] - ETA: 1:29 - loss: 3.6494 - regression_loss: 2.7437 - classification_loss: 0.9057
 804/1000 [=======================>......] - ETA: 1:28 - loss: 3.6511 - regression_loss: 2.7448 - classification_loss: 0.9062
 805/1000 [=======================>......] - ETA: 1:28 - loss: 3.6512 - regression_loss: 2.7450 - classification_loss: 0.9062
 806/1000 [=======================>......] - ETA: 1:27 - loss: 3.6519 - regression_loss: 2.7457 - classification_loss: 0.9063
 807/1000 [=======================>......] - ETA: 1:27 - loss: 3.6527 - regression_loss: 2.7464 - classification_loss: 0.9063
 808/1000 [=======================>......] - ETA: 1:26 - loss: 3.6550 - regression_loss: 2.7487 - classification_loss: 0.9063
 809/1000 [=======================>......] - ETA: 1:26 - loss: 3.6559 - regression_loss: 2.7491 - classification_loss: 0.9068
 810/1000 [=======================>......] - ETA: 1:25 - loss: 3.6514 - regression_loss: 2.7457 - classification_loss: 0.9057
 811/1000 [=======================>......] - ETA: 1:25 - loss: 3.6527 - regression_loss: 2.7471 - classification_loss: 0.9056
 812/1000 [=======================>......] - ETA: 1:24 - loss: 3.6535 - regression_loss: 2.7480 - classification_loss: 0.9056
 813/1000 [=======================>......] - ETA: 1:24 - loss: 3.6490 - regression_loss: 2.7446 - classification_loss: 0.9044
 814/1000 [=======================>......] - ETA: 1:24 - loss: 3.6502 - regression_loss: 2.7458 - classification_loss: 0.9044
 815/1000 [=======================>......] - ETA: 1:23 - loss: 3.6457 - regression_loss: 2.7425 - classification_loss: 0.9033
 816/1000 [=======================>......] - ETA: 1:23 - loss: 3.6458 - regression_loss: 2.7429 - classification_loss: 0.9029
 817/1000 [=======================>......] - ETA: 1:22 - loss: 3.6464 - regression_loss: 2.7433 - classification_loss: 0.9031
 818/1000 [=======================>......] - ETA: 1:22 - loss: 3.6476 - regression_loss: 2.7448 - classification_loss: 0.9029
 819/1000 [=======================>......] - ETA: 1:21 - loss: 3.6432 - regression_loss: 2.7414 - classification_loss: 0.9018
 820/1000 [=======================>......] - ETA: 1:21 - loss: 3.6440 - regression_loss: 2.7424 - classification_loss: 0.9016
 821/1000 [=======================>......] - ETA: 1:20 - loss: 3.6396 - regression_loss: 2.7391 - classification_loss: 0.9005
 822/1000 [=======================>......] - ETA: 1:20 - loss: 3.6410 - regression_loss: 2.7397 - classification_loss: 0.9013
 823/1000 [=======================>......] - ETA: 1:20 - loss: 3.6415 - regression_loss: 2.7406 - classification_loss: 0.9009
 824/1000 [=======================>......] - ETA: 1:19 - loss: 3.6433 - regression_loss: 2.7423 - classification_loss: 0.9010
 825/1000 [=======================>......] - ETA: 1:19 - loss: 3.6435 - regression_loss: 2.7427 - classification_loss: 0.9009
 826/1000 [=======================>......] - ETA: 1:18 - loss: 3.6443 - regression_loss: 2.7435 - classification_loss: 0.9008
 827/1000 [=======================>......] - ETA: 1:18 - loss: 3.6446 - regression_loss: 2.7440 - classification_loss: 0.9005
 828/1000 [=======================>......] - ETA: 1:17 - loss: 3.6402 - regression_loss: 2.7407 - classification_loss: 0.8994
 829/1000 [=======================>......] - ETA: 1:17 - loss: 3.6440 - regression_loss: 2.7429 - classification_loss: 0.9011
 830/1000 [=======================>......] - ETA: 1:16 - loss: 3.6465 - regression_loss: 2.7441 - classification_loss: 0.9024
 831/1000 [=======================>......] - ETA: 1:16 - loss: 3.6489 - regression_loss: 2.7448 - classification_loss: 0.9041
 832/1000 [=======================>......] - ETA: 1:15 - loss: 3.6502 - regression_loss: 2.7454 - classification_loss: 0.9048
 833/1000 [=======================>......] - ETA: 1:15 - loss: 3.6514 - regression_loss: 2.7467 - classification_loss: 0.9047
 834/1000 [========================>.....] - ETA: 1:15 - loss: 3.6534 - regression_loss: 2.7480 - classification_loss: 0.9054
 835/1000 [========================>.....] - ETA: 1:14 - loss: 3.6535 - regression_loss: 2.7484 - classification_loss: 0.9052
 836/1000 [========================>.....] - ETA: 1:14 - loss: 3.6535 - regression_loss: 2.7484 - classification_loss: 0.9051
 837/1000 [========================>.....] - ETA: 1:13 - loss: 3.6501 - regression_loss: 2.7452 - classification_loss: 0.9050
 838/1000 [========================>.....] - ETA: 1:13 - loss: 3.6458 - regression_loss: 2.7419 - classification_loss: 0.9039
 839/1000 [========================>.....] - ETA: 1:12 - loss: 3.6469 - regression_loss: 2.7425 - classification_loss: 0.9044
 840/1000 [========================>.....] - ETA: 1:12 - loss: 3.6474 - regression_loss: 2.7434 - classification_loss: 0.9040
 841/1000 [========================>.....] - ETA: 1:11 - loss: 3.6478 - regression_loss: 2.7439 - classification_loss: 0.9038
 842/1000 [========================>.....] - ETA: 1:11 - loss: 3.6491 - regression_loss: 2.7449 - classification_loss: 0.9042
 843/1000 [========================>.....] - ETA: 1:10 - loss: 3.6511 - regression_loss: 2.7459 - classification_loss: 0.9052
 844/1000 [========================>.....] - ETA: 1:10 - loss: 3.6523 - regression_loss: 2.7461 - classification_loss: 0.9062
 845/1000 [========================>.....] - ETA: 1:10 - loss: 3.6532 - regression_loss: 2.7471 - classification_loss: 0.9061
 846/1000 [========================>.....] - ETA: 1:09 - loss: 3.6541 - regression_loss: 2.7480 - classification_loss: 0.9062
 847/1000 [========================>.....] - ETA: 1:09 - loss: 3.6552 - regression_loss: 2.7487 - classification_loss: 0.9065
 848/1000 [========================>.....] - ETA: 1:08 - loss: 3.6561 - regression_loss: 2.7497 - classification_loss: 0.9063
 849/1000 [========================>.....] - ETA: 1:08 - loss: 3.6565 - regression_loss: 2.7502 - classification_loss: 0.9063
 850/1000 [========================>.....] - ETA: 1:07 - loss: 3.6570 - regression_loss: 2.7512 - classification_loss: 0.9058
 851/1000 [========================>.....] - ETA: 1:07 - loss: 3.6578 - regression_loss: 2.7521 - classification_loss: 0.9057
 852/1000 [========================>.....] - ETA: 1:06 - loss: 3.6580 - regression_loss: 2.7520 - classification_loss: 0.9059
 853/1000 [========================>.....] - ETA: 1:06 - loss: 3.6537 - regression_loss: 2.7488 - classification_loss: 0.9049
 854/1000 [========================>.....] - ETA: 1:05 - loss: 3.6538 - regression_loss: 2.7489 - classification_loss: 0.9049
 855/1000 [========================>.....] - ETA: 1:05 - loss: 3.6557 - regression_loss: 2.7506 - classification_loss: 0.9051
 856/1000 [========================>.....] - ETA: 1:05 - loss: 3.6587 - regression_loss: 2.7527 - classification_loss: 0.9060
 857/1000 [========================>.....] - ETA: 1:04 - loss: 3.6596 - regression_loss: 2.7536 - classification_loss: 0.9060
 858/1000 [========================>.....] - ETA: 1:04 - loss: 3.6601 - regression_loss: 2.7537 - classification_loss: 0.9064
 859/1000 [========================>.....] - ETA: 1:03 - loss: 3.6607 - regression_loss: 2.7545 - classification_loss: 0.9062
 860/1000 [========================>.....] - ETA: 1:03 - loss: 3.6612 - regression_loss: 2.7556 - classification_loss: 0.9056
 861/1000 [========================>.....] - ETA: 1:02 - loss: 3.6619 - regression_loss: 2.7565 - classification_loss: 0.9054
 862/1000 [========================>.....] - ETA: 1:02 - loss: 3.6577 - regression_loss: 2.7533 - classification_loss: 0.9044
 863/1000 [========================>.....] - ETA: 1:01 - loss: 3.6705 - regression_loss: 2.7501 - classification_loss: 0.9204
 864/1000 [========================>.....] - ETA: 1:01 - loss: 3.6663 - regression_loss: 2.7469 - classification_loss: 0.9193
 865/1000 [========================>.....] - ETA: 1:01 - loss: 3.6620 - regression_loss: 2.7438 - classification_loss: 0.9183
 866/1000 [========================>.....] - ETA: 1:00 - loss: 3.6619 - regression_loss: 2.7438 - classification_loss: 0.9181
 867/1000 [=========================>....] - ETA: 1:00 - loss: 3.6623 - regression_loss: 2.7447 - classification_loss: 0.9176
 868/1000 [=========================>....] - ETA: 59s - loss: 3.6582 - regression_loss: 2.7415 - classification_loss: 0.9167 
 869/1000 [=========================>....] - ETA: 59s - loss: 3.6584 - regression_loss: 2.7418 - classification_loss: 0.9166
 870/1000 [=========================>....] - ETA: 58s - loss: 3.6598 - regression_loss: 2.7432 - classification_loss: 0.9166
 871/1000 [=========================>....] - ETA: 58s - loss: 3.6600 - regression_loss: 2.7433 - classification_loss: 0.9168
 872/1000 [=========================>....] - ETA: 57s - loss: 3.6611 - regression_loss: 2.7444 - classification_loss: 0.9167
 873/1000 [=========================>....] - ETA: 57s - loss: 3.6612 - regression_loss: 2.7446 - classification_loss: 0.9166
 874/1000 [=========================>....] - ETA: 56s - loss: 3.6620 - regression_loss: 2.7453 - classification_loss: 0.9167
 875/1000 [=========================>....] - ETA: 56s - loss: 3.6619 - regression_loss: 2.7452 - classification_loss: 0.9167
 876/1000 [=========================>....] - ETA: 56s - loss: 3.6628 - regression_loss: 2.7462 - classification_loss: 0.9165
 877/1000 [=========================>....] - ETA: 55s - loss: 3.6647 - regression_loss: 2.7484 - classification_loss: 0.9163
 878/1000 [=========================>....] - ETA: 55s - loss: 3.6646 - regression_loss: 2.7487 - classification_loss: 0.9158
 879/1000 [=========================>....] - ETA: 54s - loss: 3.6652 - regression_loss: 2.7497 - classification_loss: 0.9155
 880/1000 [=========================>....] - ETA: 54s - loss: 3.6612 - regression_loss: 2.7465 - classification_loss: 0.9147
 881/1000 [=========================>....] - ETA: 53s - loss: 3.6615 - regression_loss: 2.7471 - classification_loss: 0.9144
 882/1000 [=========================>....] - ETA: 53s - loss: 3.6574 - regression_loss: 2.7440 - classification_loss: 0.9134
 883/1000 [=========================>....] - ETA: 52s - loss: 3.6584 - regression_loss: 2.7452 - classification_loss: 0.9132
 884/1000 [=========================>....] - ETA: 52s - loss: 3.6543 - regression_loss: 2.7421 - classification_loss: 0.9121
 885/1000 [=========================>....] - ETA: 51s - loss: 3.6554 - regression_loss: 2.7424 - classification_loss: 0.9130
 886/1000 [=========================>....] - ETA: 51s - loss: 3.6555 - regression_loss: 2.7428 - classification_loss: 0.9127
 887/1000 [=========================>....] - ETA: 51s - loss: 3.6514 - regression_loss: 2.7397 - classification_loss: 0.9117
 888/1000 [=========================>....] - ETA: 50s - loss: 3.6535 - regression_loss: 2.7412 - classification_loss: 0.9123
 889/1000 [=========================>....] - ETA: 50s - loss: 3.6558 - regression_loss: 2.7436 - classification_loss: 0.9123
 890/1000 [=========================>....] - ETA: 49s - loss: 3.6576 - regression_loss: 2.7447 - classification_loss: 0.9128
 891/1000 [=========================>....] - ETA: 49s - loss: 3.6575 - regression_loss: 2.7448 - classification_loss: 0.9127
 892/1000 [=========================>....] - ETA: 48s - loss: 3.6581 - regression_loss: 2.7454 - classification_loss: 0.9127
 893/1000 [=========================>....] - ETA: 48s - loss: 3.6601 - regression_loss: 2.7467 - classification_loss: 0.9135
 894/1000 [=========================>....] - ETA: 47s - loss: 3.6599 - regression_loss: 2.7467 - classification_loss: 0.9132
 895/1000 [=========================>....] - ETA: 47s - loss: 3.6597 - regression_loss: 2.7467 - classification_loss: 0.9130
 896/1000 [=========================>....] - ETA: 47s - loss: 3.6604 - regression_loss: 2.7478 - classification_loss: 0.9126
 897/1000 [=========================>....] - ETA: 46s - loss: 3.6608 - regression_loss: 2.7479 - classification_loss: 0.9129
 898/1000 [=========================>....] - ETA: 46s - loss: 3.6618 - regression_loss: 2.7483 - classification_loss: 0.9135
 899/1000 [=========================>....] - ETA: 45s - loss: 3.6577 - regression_loss: 2.7452 - classification_loss: 0.9125
 900/1000 [==========================>...] - ETA: 45s - loss: 3.6607 - regression_loss: 2.7471 - classification_loss: 0.9136
 901/1000 [==========================>...] - ETA: 44s - loss: 3.6566 - regression_loss: 2.7441 - classification_loss: 0.9126
 902/1000 [==========================>...] - ETA: 44s - loss: 3.6575 - regression_loss: 2.7449 - classification_loss: 0.9126
 903/1000 [==========================>...] - ETA: 43s - loss: 3.6598 - regression_loss: 2.7473 - classification_loss: 0.9125
 904/1000 [==========================>...] - ETA: 43s - loss: 3.6601 - regression_loss: 2.7480 - classification_loss: 0.9121
 905/1000 [==========================>...] - ETA: 42s - loss: 3.6600 - regression_loss: 2.7482 - classification_loss: 0.9117
 906/1000 [==========================>...] - ETA: 42s - loss: 3.6559 - regression_loss: 2.7452 - classification_loss: 0.9107
 907/1000 [==========================>...] - ETA: 42s - loss: 3.6563 - regression_loss: 2.7460 - classification_loss: 0.9103
 908/1000 [==========================>...] - ETA: 41s - loss: 3.6564 - regression_loss: 2.7466 - classification_loss: 0.9098
 909/1000 [==========================>...] - ETA: 41s - loss: 3.6583 - regression_loss: 2.7483 - classification_loss: 0.9100
 910/1000 [==========================>...] - ETA: 40s - loss: 3.6585 - regression_loss: 2.7489 - classification_loss: 0.9095
 911/1000 [==========================>...] - ETA: 40s - loss: 3.6590 - regression_loss: 2.7495 - classification_loss: 0.9095
 912/1000 [==========================>...] - ETA: 39s - loss: 3.6604 - regression_loss: 2.7502 - classification_loss: 0.9102
 913/1000 [==========================>...] - ETA: 39s - loss: 3.6624 - regression_loss: 2.7511 - classification_loss: 0.9113
 914/1000 [==========================>...] - ETA: 38s - loss: 3.6636 - regression_loss: 2.7517 - classification_loss: 0.9119
 915/1000 [==========================>...] - ETA: 38s - loss: 3.6645 - regression_loss: 2.7524 - classification_loss: 0.9121
 916/1000 [==========================>...] - ETA: 37s - loss: 3.6650 - regression_loss: 2.7529 - classification_loss: 0.9121
 917/1000 [==========================>...] - ETA: 37s - loss: 3.6670 - regression_loss: 2.7542 - classification_loss: 0.9128
 918/1000 [==========================>...] - ETA: 37s - loss: 3.6689 - regression_loss: 2.7557 - classification_loss: 0.9132
 919/1000 [==========================>...] - ETA: 36s - loss: 3.6705 - regression_loss: 2.7567 - classification_loss: 0.9139
 920/1000 [==========================>...] - ETA: 36s - loss: 3.6716 - regression_loss: 2.7572 - classification_loss: 0.9144
 921/1000 [==========================>...] - ETA: 35s - loss: 3.6676 - regression_loss: 2.7542 - classification_loss: 0.9134
 922/1000 [==========================>...] - ETA: 35s - loss: 3.6678 - regression_loss: 2.7548 - classification_loss: 0.9130
 923/1000 [==========================>...] - ETA: 34s - loss: 3.6686 - regression_loss: 2.7559 - classification_loss: 0.9127
 924/1000 [==========================>...] - ETA: 34s - loss: 3.6691 - regression_loss: 2.7566 - classification_loss: 0.9125
 925/1000 [==========================>...] - ETA: 33s - loss: 3.6690 - regression_loss: 2.7569 - classification_loss: 0.9121
 926/1000 [==========================>...] - ETA: 33s - loss: 3.6705 - regression_loss: 2.7584 - classification_loss: 0.9121
 927/1000 [==========================>...] - ETA: 32s - loss: 3.6707 - regression_loss: 2.7589 - classification_loss: 0.9118
 928/1000 [==========================>...] - ETA: 32s - loss: 3.6707 - regression_loss: 2.7593 - classification_loss: 0.9114
 929/1000 [==========================>...] - ETA: 32s - loss: 3.6667 - regression_loss: 2.7564 - classification_loss: 0.9104
 930/1000 [==========================>...] - ETA: 31s - loss: 3.6683 - regression_loss: 2.7578 - classification_loss: 0.9104
 931/1000 [==========================>...] - ETA: 31s - loss: 3.6682 - regression_loss: 2.7583 - classification_loss: 0.9100
 932/1000 [==========================>...] - ETA: 30s - loss: 3.6691 - regression_loss: 2.7588 - classification_loss: 0.9102
 933/1000 [==========================>...] - ETA: 30s - loss: 3.6707 - regression_loss: 2.7601 - classification_loss: 0.9106
 934/1000 [===========================>..] - ETA: 29s - loss: 3.6710 - regression_loss: 2.7604 - classification_loss: 0.9106
 935/1000 [===========================>..] - ETA: 29s - loss: 3.6713 - regression_loss: 2.7606 - classification_loss: 0.9107
 936/1000 [===========================>..] - ETA: 28s - loss: 3.6738 - regression_loss: 2.7628 - classification_loss: 0.9111
 937/1000 [===========================>..] - ETA: 28s - loss: 3.6699 - regression_loss: 2.7598 - classification_loss: 0.9101
 938/1000 [===========================>..] - ETA: 28s - loss: 3.6695 - regression_loss: 2.7598 - classification_loss: 0.9097
 939/1000 [===========================>..] - ETA: 27s - loss: 3.6656 - regression_loss: 2.7569 - classification_loss: 0.9087
 940/1000 [===========================>..] - ETA: 27s - loss: 3.6681 - regression_loss: 2.7592 - classification_loss: 0.9089
 941/1000 [===========================>..] - ETA: 26s - loss: 3.6684 - regression_loss: 2.7591 - classification_loss: 0.9094
 942/1000 [===========================>..] - ETA: 26s - loss: 3.6690 - regression_loss: 2.7592 - classification_loss: 0.9097
 943/1000 [===========================>..] - ETA: 25s - loss: 3.6728 - regression_loss: 2.7618 - classification_loss: 0.9110
 944/1000 [===========================>..] - ETA: 25s - loss: 3.6719 - regression_loss: 2.7613 - classification_loss: 0.9107
 945/1000 [===========================>..] - ETA: 24s - loss: 3.6729 - regression_loss: 2.7616 - classification_loss: 0.9113
 946/1000 [===========================>..] - ETA: 24s - loss: 3.6738 - regression_loss: 2.7628 - classification_loss: 0.9110
 947/1000 [===========================>..] - ETA: 23s - loss: 3.6756 - regression_loss: 2.7642 - classification_loss: 0.9114
 948/1000 [===========================>..] - ETA: 23s - loss: 3.6763 - regression_loss: 2.7653 - classification_loss: 0.9110
 949/1000 [===========================>..] - ETA: 23s - loss: 3.6724 - regression_loss: 2.7623 - classification_loss: 0.9101
 950/1000 [===========================>..] - ETA: 22s - loss: 3.6685 - regression_loss: 2.7594 - classification_loss: 0.9091
 951/1000 [===========================>..] - ETA: 22s - loss: 3.6647 - regression_loss: 2.7565 - classification_loss: 0.9082
 952/1000 [===========================>..] - ETA: 21s - loss: 3.6656 - regression_loss: 2.7575 - classification_loss: 0.9081
 953/1000 [===========================>..] - ETA: 21s - loss: 3.6662 - regression_loss: 2.7584 - classification_loss: 0.9078
 954/1000 [===========================>..] - ETA: 20s - loss: 3.6624 - regression_loss: 2.7555 - classification_loss: 0.9069
 955/1000 [===========================>..] - ETA: 20s - loss: 3.6630 - regression_loss: 2.7566 - classification_loss: 0.9065
 956/1000 [===========================>..] - ETA: 19s - loss: 3.6594 - regression_loss: 2.7537 - classification_loss: 0.9057
 957/1000 [===========================>..] - ETA: 19s - loss: 3.6624 - regression_loss: 2.7551 - classification_loss: 0.9074
 958/1000 [===========================>..] - ETA: 18s - loss: 3.6624 - regression_loss: 2.7554 - classification_loss: 0.9070
 959/1000 [===========================>..] - ETA: 18s - loss: 3.6634 - regression_loss: 2.7554 - classification_loss: 0.9080
 960/1000 [===========================>..] - ETA: 18s - loss: 3.6636 - regression_loss: 2.7555 - classification_loss: 0.9082
 961/1000 [===========================>..] - ETA: 17s - loss: 3.6673 - regression_loss: 2.7589 - classification_loss: 0.9084
 962/1000 [===========================>..] - ETA: 17s - loss: 3.6634 - regression_loss: 2.7560 - classification_loss: 0.9074
 963/1000 [===========================>..] - ETA: 16s - loss: 3.6663 - regression_loss: 2.7575 - classification_loss: 0.9088
 964/1000 [===========================>..] - ETA: 16s - loss: 3.6663 - regression_loss: 2.7579 - classification_loss: 0.9084
 965/1000 [===========================>..] - ETA: 15s - loss: 3.6684 - regression_loss: 2.7590 - classification_loss: 0.9094
 966/1000 [===========================>..] - ETA: 15s - loss: 3.6646 - regression_loss: 2.7562 - classification_loss: 0.9085
 967/1000 [============================>.] - ETA: 14s - loss: 3.6608 - regression_loss: 2.7533 - classification_loss: 0.9075
 968/1000 [============================>.] - ETA: 14s - loss: 3.6608 - regression_loss: 2.7534 - classification_loss: 0.9074
 969/1000 [============================>.] - ETA: 14s - loss: 3.6570 - regression_loss: 2.7505 - classification_loss: 0.9065
 970/1000 [============================>.] - ETA: 13s - loss: 3.6589 - regression_loss: 2.7515 - classification_loss: 0.9073
 971/1000 [============================>.] - ETA: 13s - loss: 3.6603 - regression_loss: 2.7528 - classification_loss: 0.9075
 972/1000 [============================>.] - ETA: 12s - loss: 3.6619 - regression_loss: 2.7543 - classification_loss: 0.9077
 973/1000 [============================>.] - ETA: 12s - loss: 3.6634 - regression_loss: 2.7555 - classification_loss: 0.9080
 974/1000 [============================>.] - ETA: 11s - loss: 3.6636 - regression_loss: 2.7559 - classification_loss: 0.9078
 975/1000 [============================>.] - ETA: 11s - loss: 3.6643 - regression_loss: 2.7569 - classification_loss: 0.9074
 976/1000 [============================>.] - ETA: 10s - loss: 3.6659 - regression_loss: 2.7588 - classification_loss: 0.9071
 977/1000 [============================>.] - ETA: 10s - loss: 3.6682 - regression_loss: 2.7603 - classification_loss: 0.9079
 978/1000 [============================>.] - ETA: 9s - loss: 3.6683 - regression_loss: 2.7599 - classification_loss: 0.9084 
 979/1000 [============================>.] - ETA: 9s - loss: 3.6703 - regression_loss: 2.7607 - classification_loss: 0.9096
 980/1000 [============================>.] - ETA: 9s - loss: 3.6719 - regression_loss: 2.7615 - classification_loss: 0.9104
 981/1000 [============================>.] - ETA: 8s - loss: 3.6732 - regression_loss: 2.7628 - classification_loss: 0.9105
 982/1000 [============================>.] - ETA: 8s - loss: 3.6742 - regression_loss: 2.7641 - classification_loss: 0.9101
 983/1000 [============================>.] - ETA: 7s - loss: 3.6705 - regression_loss: 2.7613 - classification_loss: 0.9092
 984/1000 [============================>.] - ETA: 7s - loss: 3.6707 - regression_loss: 2.7616 - classification_loss: 0.9091
 985/1000 [============================>.] - ETA: 6s - loss: 3.6711 - regression_loss: 2.7624 - classification_loss: 0.9087
 986/1000 [============================>.] - ETA: 6s - loss: 3.6719 - regression_loss: 2.7630 - classification_loss: 0.9089
 987/1000 [============================>.] - ETA: 5s - loss: 3.6731 - regression_loss: 2.7640 - classification_loss: 0.9091
 988/1000 [============================>.] - ETA: 5s - loss: 3.6746 - regression_loss: 2.7649 - classification_loss: 0.9097
 989/1000 [============================>.] - ETA: 4s - loss: 3.6709 - regression_loss: 2.7621 - classification_loss: 0.9088
 990/1000 [============================>.] - ETA: 4s - loss: 3.6714 - regression_loss: 2.7629 - classification_loss: 0.9084
 991/1000 [============================>.] - ETA: 4s - loss: 3.6716 - regression_loss: 2.7636 - classification_loss: 0.9080
 992/1000 [============================>.] - ETA: 3s - loss: 3.6679 - regression_loss: 2.7609 - classification_loss: 0.9071
 993/1000 [============================>.] - ETA: 3s - loss: 3.6685 - regression_loss: 2.7617 - classification_loss: 0.9067
 994/1000 [============================>.] - ETA: 2s - loss: 3.6697 - regression_loss: 2.7630 - classification_loss: 0.9068
 995/1000 [============================>.] - ETA: 2s - loss: 3.6699 - regression_loss: 2.7628 - classification_loss: 0.9071
 996/1000 [============================>.] - ETA: 1s - loss: 3.6662 - regression_loss: 2.7600 - classification_loss: 0.9062
 997/1000 [============================>.] - ETA: 1s - loss: 3.6666 - regression_loss: 2.7608 - classification_loss: 0.9059
 998/1000 [============================>.] - ETA: 0s - loss: 3.6682 - regression_loss: 2.7619 - classification_loss: 0.9063
 999/1000 [============================>.] - ETA: 0s - loss: 3.6646 - regression_loss: 2.7591 - classification_loss: 0.9054
1000/1000 [==============================] - 452s 452ms/step - loss: 3.6647 - regression_loss: 2.7597 - classification_loss: 0.9050

Epoch 00004: saving model to ./snapshots/resnet50_csv_04.h5
0/2001/2002/2003/2004/2005/2006/2007/2008/2009/20010/20011/20012/20013/20014/20015/20016/20017/20018/20019/20020/20021/20022/20023/20024/20025/20026/20027/20028/20029/20030/20031/20032/20033/20034/20035/20036/20037/20038/20039/20040/20041/20042/20043/20044/20045/20046/20047/20048/20049/20050/20051/20052/20053/20054/20055/20056/20057/20058/20059/20060/20061/20062/20063/20064/20065/20066/20067/20068/20069/20070/20071/20072/20073/20074/20075/20076/20077/20078/20079/20080/20081/20082/20083/20084/20085/20086/20087/20088/20089/20090/20091/20092/20093/20094/20095/20096/20097/20098/20099/200100/200101/200102/200103/200104/200105/200106/200107/200108/200109/200110/200111/200112/200113/200114/200115/200116/200117/200118/200119/200120/200121/200122/200123/200124/200125/200126/200127/200128/200129/200130/200131/200132/200133/200134/200135/200136/200137/200138/200139/200140/200141/200142/200143/200144/200145/200146/200147/200148/200149/200150/200151/200152/200153/200154/200155/200156/200157/200158/200159/200160/200161/200162/200163/200164/200165/200166/200167/200168/200169/200170/200171/200172/200173/200174/200175/200176/200177/200178/200179/200180/200181/200182/200183/200184/200185/200186/200187/200188/200189/200190/200191/200192/200193/200194/200195/200196/200197/200198/200199/2000/2001/2002/2003/2004/2005/2006/2007/2008/2009/20010/20011/20012/20013/20014/20015/20016/20017/20018/20019/20020/20021/20022/20023/20024/20025/20026/20027/20028/20029/20030/20031/20032/20033/20034/20035/20036/20037/20038/20039/20040/20041/20042/20043/20044/20045/20046/20047/20048/20049/20050/20051/20052/20053/20054/20055/20056/20057/20058/20059/20060/20061/20062/20063/20064/20065/20066/20067/20068/20069/20070/20071/20072/20073/20074/20075/20076/20077/20078/20079/20080/20081/20082/20083/20084/20085/20086/20087/20088/20089/20090/20091/20092/20093/20094/20095/20096/20097/20098/20099/200100/200101/200102/200103/200104/200105/200106/200107/200108/200109/200110/200111/200112/200113/200114/200115/200116/200117/200118/200119/200120/200121/200122/200123/200124/200125/200126/200127/200128/200129/200130/200131/200132/200133/200134/200135/200136/200137/200138/200139/200140/200141/200142/200143/200144/200145/200146/200147/200148/200149/200150/200151/200152/200153/200154/200155/200156/200157/200158/200159/200160/200161/200162/200163/200164/200165/200166/200167/200168/200169/200170/200171/200172/200173/200174/200175/200176/200177/200178/200179/200180/200181/200182/200183/200184/200185/200186/200187/200188/200189/200190/200191/200192/200193/200194/200195/200196/200197/200198/200199/200

M 0.0381
N 0.0009
mAP: 0.0195
Epoch 5/30

   1/1000 [..............................] - ETA: 7:16 - loss: 4.5855 - regression_loss: 3.5851 - classification_loss: 1.0004
   2/1000 [..............................] - ETA: 7:23 - loss: 2.2927 - regression_loss: 1.7926 - classification_loss: 0.5002
   3/1000 [..............................] - ETA: 7:25 - loss: 2.9302 - regression_loss: 2.2082 - classification_loss: 0.7220
   4/1000 [..............................] - ETA: 7:25 - loss: 3.1450 - regression_loss: 2.4498 - classification_loss: 0.6952
   5/1000 [..............................] - ETA: 7:26 - loss: 3.3698 - regression_loss: 2.5955 - classification_loss: 0.7743
   6/1000 [..............................] - ETA: 7:26 - loss: 3.6081 - regression_loss: 2.7746 - classification_loss: 0.8335
   7/1000 [..............................] - ETA: 7:25 - loss: 3.7432 - regression_loss: 2.8004 - classification_loss: 0.9428
   8/1000 [..............................] - ETA: 7:26 - loss: 3.8040 - regression_loss: 2.8409 - classification_loss: 0.9631
   9/1000 [..............................] - ETA: 7:26 - loss: 3.8946 - regression_loss: 2.8671 - classification_loss: 1.0275
  10/1000 [..............................] - ETA: 7:25 - loss: 3.5071 - regression_loss: 2.5804 - classification_loss: 0.9267
  11/1000 [..............................] - ETA: 7:24 - loss: 3.6424 - regression_loss: 2.6531 - classification_loss: 0.9893
  12/1000 [..............................] - ETA: 7:23 - loss: 3.6908 - regression_loss: 2.6603 - classification_loss: 1.0305
  13/1000 [..............................] - ETA: 7:23 - loss: 3.4435 - regression_loss: 2.4557 - classification_loss: 0.9878
  14/1000 [..............................] - ETA: 7:23 - loss: 3.5140 - regression_loss: 2.5290 - classification_loss: 0.9851
  15/1000 [..............................] - ETA: 7:22 - loss: 3.2798 - regression_loss: 2.3604 - classification_loss: 0.9194
  16/1000 [..............................] - ETA: 7:22 - loss: 3.3824 - regression_loss: 2.4596 - classification_loss: 0.9228
  17/1000 [..............................] - ETA: 7:22 - loss: 3.1834 - regression_loss: 2.3149 - classification_loss: 0.8685
  18/1000 [..............................] - ETA: 7:21 - loss: 3.1997 - regression_loss: 2.3531 - classification_loss: 0.8466
  19/1000 [..............................] - ETA: 7:21 - loss: 3.2252 - regression_loss: 2.3854 - classification_loss: 0.8398
  20/1000 [..............................] - ETA: 7:21 - loss: 3.2363 - regression_loss: 2.4135 - classification_loss: 0.8229
  21/1000 [..............................] - ETA: 7:21 - loss: 3.3279 - regression_loss: 2.4669 - classification_loss: 0.8610
  22/1000 [..............................] - ETA: 7:20 - loss: 3.3690 - regression_loss: 2.4888 - classification_loss: 0.8802
  23/1000 [..............................] - ETA: 7:20 - loss: 3.2225 - regression_loss: 2.3806 - classification_loss: 0.8419
  24/1000 [..............................] - ETA: 7:19 - loss: 3.0882 - regression_loss: 2.2814 - classification_loss: 0.8068
  25/1000 [..............................] - ETA: 7:19 - loss: 3.1351 - regression_loss: 2.3234 - classification_loss: 0.8117
  26/1000 [..............................] - ETA: 7:18 - loss: 3.2157 - regression_loss: 2.4054 - classification_loss: 0.8103
  27/1000 [..............................] - ETA: 7:18 - loss: 3.3139 - regression_loss: 2.4872 - classification_loss: 0.8268
  28/1000 [..............................] - ETA: 7:17 - loss: 3.1956 - regression_loss: 2.3984 - classification_loss: 0.7972
  29/1000 [..............................] - ETA: 7:17 - loss: 3.2198 - regression_loss: 2.4215 - classification_loss: 0.7983
  30/1000 [..............................] - ETA: 7:15 - loss: 3.1125 - regression_loss: 2.3408 - classification_loss: 0.7716
  31/1000 [..............................] - ETA: 7:15 - loss: 3.1815 - regression_loss: 2.3753 - classification_loss: 0.8061
  32/1000 [..............................] - ETA: 7:14 - loss: 3.2087 - regression_loss: 2.4032 - classification_loss: 0.8055
  33/1000 [..............................] - ETA: 7:13 - loss: 3.2951 - regression_loss: 2.4557 - classification_loss: 0.8394
  34/1000 [>.............................] - ETA: 7:13 - loss: 3.3460 - regression_loss: 2.4864 - classification_loss: 0.8596
  35/1000 [>.............................] - ETA: 7:12 - loss: 3.2720 - regression_loss: 2.4153 - classification_loss: 0.8567
  36/1000 [>.............................] - ETA: 7:12 - loss: 3.2717 - regression_loss: 2.4242 - classification_loss: 0.8474
  37/1000 [>.............................] - ETA: 7:12 - loss: 3.3379 - regression_loss: 2.4761 - classification_loss: 0.8617
  38/1000 [>.............................] - ETA: 7:11 - loss: 3.2501 - regression_loss: 2.4110 - classification_loss: 0.8391
  39/1000 [>.............................] - ETA: 7:11 - loss: 3.2712 - regression_loss: 2.4359 - classification_loss: 0.8353
  40/1000 [>.............................] - ETA: 7:10 - loss: 3.1894 - regression_loss: 2.3750 - classification_loss: 0.8144
  41/1000 [>.............................] - ETA: 7:10 - loss: 3.2198 - regression_loss: 2.3940 - classification_loss: 0.8258
  42/1000 [>.............................] - ETA: 7:10 - loss: 3.2791 - regression_loss: 2.4150 - classification_loss: 0.8641
  43/1000 [>.............................] - ETA: 7:09 - loss: 3.2804 - regression_loss: 2.4219 - classification_loss: 0.8585
  44/1000 [>.............................] - ETA: 7:09 - loss: 3.3116 - regression_loss: 2.4315 - classification_loss: 0.8802
  45/1000 [>.............................] - ETA: 7:09 - loss: 3.3085 - regression_loss: 2.4370 - classification_loss: 0.8715
  46/1000 [>.............................] - ETA: 7:08 - loss: 3.3508 - regression_loss: 2.4641 - classification_loss: 0.8867
  47/1000 [>.............................] - ETA: 7:08 - loss: 3.3566 - regression_loss: 2.4727 - classification_loss: 0.8839
  48/1000 [>.............................] - ETA: 7:07 - loss: 3.3758 - regression_loss: 2.4989 - classification_loss: 0.8769
  49/1000 [>.............................] - ETA: 7:06 - loss: 3.3069 - regression_loss: 2.4479 - classification_loss: 0.8590
  50/1000 [>.............................] - ETA: 7:06 - loss: 3.2408 - regression_loss: 2.3989 - classification_loss: 0.8418
  51/1000 [>.............................] - ETA: 7:05 - loss: 3.1836 - regression_loss: 2.3519 - classification_loss: 0.8318
  52/1000 [>.............................] - ETA: 7:05 - loss: 3.2223 - regression_loss: 2.3857 - classification_loss: 0.8366
  53/1000 [>.............................] - ETA: 7:04 - loss: 3.2448 - regression_loss: 2.4153 - classification_loss: 0.8295
  54/1000 [>.............................] - ETA: 7:04 - loss: 3.2879 - regression_loss: 2.4375 - classification_loss: 0.8504
  55/1000 [>.............................] - ETA: 7:04 - loss: 3.2281 - regression_loss: 2.3932 - classification_loss: 0.8349
  56/1000 [>.............................] - ETA: 7:03 - loss: 3.2652 - regression_loss: 2.4142 - classification_loss: 0.8510
  57/1000 [>.............................] - ETA: 7:03 - loss: 3.2079 - regression_loss: 2.3719 - classification_loss: 0.8360
  58/1000 [>.............................] - ETA: 7:03 - loss: 3.2229 - regression_loss: 2.3830 - classification_loss: 0.8400
  59/1000 [>.............................] - ETA: 7:02 - loss: 3.1683 - regression_loss: 2.3426 - classification_loss: 0.8257
  60/1000 [>.............................] - ETA: 7:02 - loss: 3.2253 - regression_loss: 2.3734 - classification_loss: 0.8519
  61/1000 [>.............................] - ETA: 7:02 - loss: 3.2566 - regression_loss: 2.3882 - classification_loss: 0.8683
  62/1000 [>.............................] - ETA: 7:01 - loss: 3.2659 - regression_loss: 2.4027 - classification_loss: 0.8631
  63/1000 [>.............................] - ETA: 7:01 - loss: 3.2919 - regression_loss: 2.4155 - classification_loss: 0.8764
  64/1000 [>.............................] - ETA: 7:00 - loss: 3.3018 - regression_loss: 2.4308 - classification_loss: 0.8709
  65/1000 [>.............................] - ETA: 7:00 - loss: 3.2510 - regression_loss: 2.3935 - classification_loss: 0.8575
  66/1000 [>.............................] - ETA: 7:00 - loss: 3.2558 - regression_loss: 2.4050 - classification_loss: 0.8509
  67/1000 [=>............................] - ETA: 6:59 - loss: 3.2878 - regression_loss: 2.4241 - classification_loss: 0.8638
  68/1000 [=>............................] - ETA: 6:58 - loss: 3.3250 - regression_loss: 2.4522 - classification_loss: 0.8729
  69/1000 [=>............................] - ETA: 6:58 - loss: 3.3528 - regression_loss: 2.4826 - classification_loss: 0.8702
  70/1000 [=>............................] - ETA: 6:58 - loss: 3.3863 - regression_loss: 2.5201 - classification_loss: 0.8662
  71/1000 [=>............................] - ETA: 6:57 - loss: 3.3413 - regression_loss: 2.4846 - classification_loss: 0.8567
  72/1000 [=>............................] - ETA: 6:57 - loss: 3.3574 - regression_loss: 2.5006 - classification_loss: 0.8568
  73/1000 [=>............................] - ETA: 6:56 - loss: 3.3663 - regression_loss: 2.5133 - classification_loss: 0.8531
  74/1000 [=>............................] - ETA: 6:56 - loss: 3.3758 - regression_loss: 2.5279 - classification_loss: 0.8478
  75/1000 [=>............................] - ETA: 6:56 - loss: 3.4037 - regression_loss: 2.5429 - classification_loss: 0.8608
  76/1000 [=>............................] - ETA: 6:55 - loss: 3.4348 - regression_loss: 2.5584 - classification_loss: 0.8764
  77/1000 [=>............................] - ETA: 6:55 - loss: 3.4508 - regression_loss: 2.5639 - classification_loss: 0.8869
  78/1000 [=>............................] - ETA: 6:54 - loss: 3.4066 - regression_loss: 2.5310 - classification_loss: 0.8755
  79/1000 [=>............................] - ETA: 6:54 - loss: 3.4233 - regression_loss: 2.5376 - classification_loss: 0.8857
  80/1000 [=>............................] - ETA: 6:53 - loss: 3.4282 - regression_loss: 2.5465 - classification_loss: 0.8817
  81/1000 [=>............................] - ETA: 6:53 - loss: 3.4522 - regression_loss: 2.5592 - classification_loss: 0.8931
  82/1000 [=>............................] - ETA: 6:52 - loss: 3.4666 - regression_loss: 2.5768 - classification_loss: 0.8899
  83/1000 [=>............................] - ETA: 6:52 - loss: 3.4249 - regression_loss: 2.5457 - classification_loss: 0.8792
  84/1000 [=>............................] - ETA: 6:52 - loss: 3.4365 - regression_loss: 2.5571 - classification_loss: 0.8794
  85/1000 [=>............................] - ETA: 6:51 - loss: 3.4517 - regression_loss: 2.5663 - classification_loss: 0.8853
  86/1000 [=>............................] - ETA: 6:51 - loss: 3.4564 - regression_loss: 2.5735 - classification_loss: 0.8829
  87/1000 [=>............................] - ETA: 6:50 - loss: 3.4759 - regression_loss: 2.5871 - classification_loss: 0.8888
  88/1000 [=>............................] - ETA: 6:50 - loss: 3.4364 - regression_loss: 2.5577 - classification_loss: 0.8787
  89/1000 [=>............................] - ETA: 6:50 - loss: 3.4546 - regression_loss: 2.5657 - classification_loss: 0.8890
  90/1000 [=>............................] - ETA: 6:49 - loss: 3.4684 - regression_loss: 2.5734 - classification_loss: 0.8950
  91/1000 [=>............................] - ETA: 6:49 - loss: 3.4773 - regression_loss: 2.5834 - classification_loss: 0.8939
  92/1000 [=>............................] - ETA: 6:48 - loss: 3.4952 - regression_loss: 2.6018 - classification_loss: 0.8934
  93/1000 [=>............................] - ETA: 6:48 - loss: 3.5058 - regression_loss: 2.6119 - classification_loss: 0.8939
  94/1000 [=>............................] - ETA: 6:48 - loss: 3.5279 - regression_loss: 2.6210 - classification_loss: 0.9069
  95/1000 [=>............................] - ETA: 6:47 - loss: 3.5403 - regression_loss: 2.6283 - classification_loss: 0.9120
  96/1000 [=>............................] - ETA: 6:47 - loss: 3.5034 - regression_loss: 2.6009 - classification_loss: 0.9025
  97/1000 [=>............................] - ETA: 6:46 - loss: 3.5230 - regression_loss: 2.6142 - classification_loss: 0.9088
  98/1000 [=>............................] - ETA: 6:46 - loss: 3.5347 - regression_loss: 2.6215 - classification_loss: 0.9132
  99/1000 [=>............................] - ETA: 6:45 - loss: 3.5468 - regression_loss: 2.6310 - classification_loss: 0.9158
 100/1000 [==>...........................] - ETA: 6:45 - loss: 3.5551 - regression_loss: 2.6374 - classification_loss: 0.9176
 101/1000 [==>...........................] - ETA: 6:45 - loss: 3.5199 - regression_loss: 2.6113 - classification_loss: 0.9086
 102/1000 [==>...........................] - ETA: 6:44 - loss: 3.5234 - regression_loss: 2.6153 - classification_loss: 0.9081
 103/1000 [==>...........................] - ETA: 6:44 - loss: 3.5199 - regression_loss: 2.6152 - classification_loss: 0.9048
 104/1000 [==>...........................] - ETA: 6:43 - loss: 3.5342 - regression_loss: 2.6222 - classification_loss: 0.9120
 105/1000 [==>...........................] - ETA: 6:43 - loss: 3.5380 - regression_loss: 2.6215 - classification_loss: 0.9165
 106/1000 [==>...........................] - ETA: 6:42 - loss: 3.5417 - regression_loss: 2.6213 - classification_loss: 0.9203
 107/1000 [==>...........................] - ETA: 6:42 - loss: 3.5518 - regression_loss: 2.6267 - classification_loss: 0.9251
 108/1000 [==>...........................] - ETA: 6:42 - loss: 3.5536 - regression_loss: 2.6248 - classification_loss: 0.9288
 109/1000 [==>...........................] - ETA: 6:41 - loss: 3.5605 - regression_loss: 2.6267 - classification_loss: 0.9338
 110/1000 [==>...........................] - ETA: 6:41 - loss: 3.5704 - regression_loss: 2.6375 - classification_loss: 0.9329
 111/1000 [==>...........................] - ETA: 6:40 - loss: 3.5382 - regression_loss: 2.6137 - classification_loss: 0.9245
 112/1000 [==>...........................] - ETA: 6:40 - loss: 3.5410 - regression_loss: 2.6208 - classification_loss: 0.9203
 113/1000 [==>...........................] - ETA: 6:39 - loss: 3.5424 - regression_loss: 2.6258 - classification_loss: 0.9166
 114/1000 [==>...........................] - ETA: 6:39 - loss: 3.5447 - regression_loss: 2.6250 - classification_loss: 0.9197
 115/1000 [==>...........................] - ETA: 6:39 - loss: 3.5519 - regression_loss: 2.6315 - classification_loss: 0.9204
 116/1000 [==>...........................] - ETA: 6:38 - loss: 3.5480 - regression_loss: 2.6320 - classification_loss: 0.9160
 117/1000 [==>...........................] - ETA: 6:38 - loss: 3.5445 - regression_loss: 2.6322 - classification_loss: 0.9123
 118/1000 [==>...........................] - ETA: 6:37 - loss: 3.5145 - regression_loss: 2.6099 - classification_loss: 0.9046
 119/1000 [==>...........................] - ETA: 6:37 - loss: 3.4849 - regression_loss: 2.5880 - classification_loss: 0.8970
 120/1000 [==>...........................] - ETA: 6:37 - loss: 3.4974 - regression_loss: 2.5923 - classification_loss: 0.9051
 121/1000 [==>...........................] - ETA: 6:36 - loss: 3.5012 - regression_loss: 2.5949 - classification_loss: 0.9063
 122/1000 [==>...........................] - ETA: 6:36 - loss: 3.5052 - regression_loss: 2.6007 - classification_loss: 0.9045
 123/1000 [==>...........................] - ETA: 6:35 - loss: 3.4767 - regression_loss: 2.5796 - classification_loss: 0.8972
 124/1000 [==>...........................] - ETA: 6:35 - loss: 3.4875 - regression_loss: 2.5899 - classification_loss: 0.8976
 125/1000 [==>...........................] - ETA: 6:34 - loss: 3.4899 - regression_loss: 2.5946 - classification_loss: 0.8953
 126/1000 [==>...........................] - ETA: 6:34 - loss: 3.4995 - regression_loss: 2.6000 - classification_loss: 0.8994
 127/1000 [==>...........................] - ETA: 6:34 - loss: 3.5042 - regression_loss: 2.6055 - classification_loss: 0.8987
 128/1000 [==>...........................] - ETA: 6:33 - loss: 3.5026 - regression_loss: 2.6076 - classification_loss: 0.8950
 129/1000 [==>...........................] - ETA: 6:33 - loss: 3.5049 - regression_loss: 2.6127 - classification_loss: 0.8922
 130/1000 [==>...........................] - ETA: 6:32 - loss: 3.5148 - regression_loss: 2.6227 - classification_loss: 0.8921
 131/1000 [==>...........................] - ETA: 6:32 - loss: 3.5183 - regression_loss: 2.6279 - classification_loss: 0.8904
 132/1000 [==>...........................] - ETA: 6:31 - loss: 3.5187 - regression_loss: 2.6302 - classification_loss: 0.8885
 133/1000 [==>...........................] - ETA: 6:31 - loss: 3.5188 - regression_loss: 2.6328 - classification_loss: 0.8860
 134/1000 [===>..........................] - ETA: 6:30 - loss: 3.5348 - regression_loss: 2.6486 - classification_loss: 0.8863
 135/1000 [===>..........................] - ETA: 6:30 - loss: 3.5741 - regression_loss: 2.6553 - classification_loss: 0.9189
 136/1000 [===>..........................] - ETA: 6:29 - loss: 3.5614 - regression_loss: 2.6357 - classification_loss: 0.9257
 137/1000 [===>..........................] - ETA: 6:29 - loss: 3.5680 - regression_loss: 2.6411 - classification_loss: 0.9269
 138/1000 [===>..........................] - ETA: 6:29 - loss: 3.5682 - regression_loss: 2.6444 - classification_loss: 0.9238
 139/1000 [===>..........................] - ETA: 6:28 - loss: 3.5746 - regression_loss: 2.6450 - classification_loss: 0.9297
 140/1000 [===>..........................] - ETA: 6:28 - loss: 3.5956 - regression_loss: 2.6635 - classification_loss: 0.9321
 141/1000 [===>..........................] - ETA: 6:27 - loss: 3.5912 - regression_loss: 2.6606 - classification_loss: 0.9306
 142/1000 [===>..........................] - ETA: 6:27 - loss: 3.5940 - regression_loss: 2.6619 - classification_loss: 0.9320
 143/1000 [===>..........................] - ETA: 6:26 - loss: 3.5962 - regression_loss: 2.6677 - classification_loss: 0.9285
 144/1000 [===>..........................] - ETA: 6:26 - loss: 3.6121 - regression_loss: 2.6820 - classification_loss: 0.9301
 145/1000 [===>..........................] - ETA: 6:25 - loss: 3.6130 - regression_loss: 2.6857 - classification_loss: 0.9273
 146/1000 [===>..........................] - ETA: 6:25 - loss: 3.5884 - regression_loss: 2.6673 - classification_loss: 0.9211
 147/1000 [===>..........................] - ETA: 6:25 - loss: 3.5990 - regression_loss: 2.6800 - classification_loss: 0.9190
 148/1000 [===>..........................] - ETA: 6:24 - loss: 3.5759 - regression_loss: 2.6619 - classification_loss: 0.9141
 149/1000 [===>..........................] - ETA: 6:24 - loss: 3.5811 - regression_loss: 2.6682 - classification_loss: 0.9129
 150/1000 [===>..........................] - ETA: 6:23 - loss: 3.5850 - regression_loss: 2.6729 - classification_loss: 0.9121
 151/1000 [===>..........................] - ETA: 6:23 - loss: 3.5877 - regression_loss: 2.6762 - classification_loss: 0.9114
 152/1000 [===>..........................] - ETA: 6:22 - loss: 3.5641 - regression_loss: 2.6586 - classification_loss: 0.9054
 153/1000 [===>..........................] - ETA: 6:22 - loss: 3.5409 - regression_loss: 2.6413 - classification_loss: 0.8996
 154/1000 [===>..........................] - ETA: 6:22 - loss: 3.5421 - regression_loss: 2.6452 - classification_loss: 0.8969
 155/1000 [===>..........................] - ETA: 6:21 - loss: 3.5479 - regression_loss: 2.6495 - classification_loss: 0.8984
 156/1000 [===>..........................] - ETA: 6:21 - loss: 3.5507 - regression_loss: 2.6536 - classification_loss: 0.8971
 157/1000 [===>..........................] - ETA: 6:20 - loss: 3.5281 - regression_loss: 2.6367 - classification_loss: 0.8914
 158/1000 [===>..........................] - ETA: 6:20 - loss: 3.5350 - regression_loss: 2.6419 - classification_loss: 0.8931
 159/1000 [===>..........................] - ETA: 6:19 - loss: 3.5421 - regression_loss: 2.6490 - classification_loss: 0.8931
 160/1000 [===>..........................] - ETA: 6:19 - loss: 3.5404 - regression_loss: 2.6466 - classification_loss: 0.8938
 161/1000 [===>..........................] - ETA: 6:18 - loss: 3.5410 - regression_loss: 2.6483 - classification_loss: 0.8927
 162/1000 [===>..........................] - ETA: 6:18 - loss: 3.5192 - regression_loss: 2.6320 - classification_loss: 0.8872
 163/1000 [===>..........................] - ETA: 6:18 - loss: 3.5240 - regression_loss: 2.6381 - classification_loss: 0.8859
 164/1000 [===>..........................] - ETA: 6:17 - loss: 3.5025 - regression_loss: 2.6220 - classification_loss: 0.8805
 165/1000 [===>..........................] - ETA: 6:17 - loss: 3.5190 - regression_loss: 2.6317 - classification_loss: 0.8872
 166/1000 [===>..........................] - ETA: 6:16 - loss: 3.5258 - regression_loss: 2.6386 - classification_loss: 0.8873
 167/1000 [====>.........................] - ETA: 6:16 - loss: 3.5297 - regression_loss: 2.6390 - classification_loss: 0.8907
 168/1000 [====>.........................] - ETA: 6:15 - loss: 3.5462 - regression_loss: 2.6488 - classification_loss: 0.8975
 169/1000 [====>.........................] - ETA: 6:15 - loss: 3.5466 - regression_loss: 2.6518 - classification_loss: 0.8948
 170/1000 [====>.........................] - ETA: 6:14 - loss: 3.5510 - regression_loss: 2.6571 - classification_loss: 0.8939
 171/1000 [====>.........................] - ETA: 6:14 - loss: 3.5577 - regression_loss: 2.6607 - classification_loss: 0.8970
 172/1000 [====>.........................] - ETA: 6:14 - loss: 3.5371 - regression_loss: 2.6452 - classification_loss: 0.8918
 173/1000 [====>.........................] - ETA: 6:13 - loss: 3.5443 - regression_loss: 2.6538 - classification_loss: 0.8905
 174/1000 [====>.........................] - ETA: 6:13 - loss: 3.5466 - regression_loss: 2.6562 - classification_loss: 0.8904
 175/1000 [====>.........................] - ETA: 6:12 - loss: 3.5281 - regression_loss: 2.6410 - classification_loss: 0.8870
 176/1000 [====>.........................] - ETA: 6:12 - loss: 3.5082 - regression_loss: 2.6260 - classification_loss: 0.8822
 177/1000 [====>.........................] - ETA: 6:11 - loss: 3.5069 - regression_loss: 2.6279 - classification_loss: 0.8790
 178/1000 [====>.........................] - ETA: 6:11 - loss: 3.5054 - regression_loss: 2.6295 - classification_loss: 0.8760
 179/1000 [====>.........................] - ETA: 6:10 - loss: 3.5173 - regression_loss: 2.6370 - classification_loss: 0.8803
 180/1000 [====>.........................] - ETA: 6:10 - loss: 3.5187 - regression_loss: 2.6398 - classification_loss: 0.8789
 181/1000 [====>.........................] - ETA: 6:09 - loss: 3.4992 - regression_loss: 2.6252 - classification_loss: 0.8740
 182/1000 [====>.........................] - ETA: 6:09 - loss: 3.4965 - regression_loss: 2.6251 - classification_loss: 0.8714
 183/1000 [====>.........................] - ETA: 6:08 - loss: 3.5024 - regression_loss: 2.6293 - classification_loss: 0.8731
 184/1000 [====>.........................] - ETA: 6:08 - loss: 3.4833 - regression_loss: 2.6150 - classification_loss: 0.8683
 185/1000 [====>.........................] - ETA: 6:08 - loss: 3.4811 - regression_loss: 2.6152 - classification_loss: 0.8659
 186/1000 [====>.........................] - ETA: 6:07 - loss: 3.4816 - regression_loss: 2.6176 - classification_loss: 0.8640
 187/1000 [====>.........................] - ETA: 6:07 - loss: 3.4937 - regression_loss: 2.6254 - classification_loss: 0.8682
 188/1000 [====>.........................] - ETA: 6:06 - loss: 3.4987 - regression_loss: 2.6302 - classification_loss: 0.8684
 189/1000 [====>.........................] - ETA: 6:06 - loss: 3.4996 - regression_loss: 2.6334 - classification_loss: 0.8662
 190/1000 [====>.........................] - ETA: 6:05 - loss: 3.5000 - regression_loss: 2.6366 - classification_loss: 0.8635
 191/1000 [====>.........................] - ETA: 6:05 - loss: 3.5121 - regression_loss: 2.6228 - classification_loss: 0.8893
 192/1000 [====>.........................] - ETA: 6:04 - loss: 3.4953 - regression_loss: 2.6091 - classification_loss: 0.8862
 193/1000 [====>.........................] - ETA: 6:04 - loss: 3.4908 - regression_loss: 2.6071 - classification_loss: 0.8837
 194/1000 [====>.........................] - ETA: 6:04 - loss: 3.4728 - regression_loss: 2.5936 - classification_loss: 0.8792
 195/1000 [====>.........................] - ETA: 6:03 - loss: 3.4719 - regression_loss: 2.5945 - classification_loss: 0.8774
 196/1000 [====>.........................] - ETA: 6:03 - loss: 3.4542 - regression_loss: 2.5813 - classification_loss: 0.8729
 197/1000 [====>.........................] - ETA: 6:02 - loss: 3.4633 - regression_loss: 2.5856 - classification_loss: 0.8776
 198/1000 [====>.........................] - ETA: 6:02 - loss: 3.4775 - regression_loss: 2.5962 - classification_loss: 0.8813
 199/1000 [====>.........................] - ETA: 6:01 - loss: 3.4910 - regression_loss: 2.6052 - classification_loss: 0.8859
 200/1000 [=====>........................] - ETA: 6:01 - loss: 3.5002 - regression_loss: 2.6154 - classification_loss: 0.8848
 201/1000 [=====>........................] - ETA: 6:00 - loss: 3.4983 - regression_loss: 2.6151 - classification_loss: 0.8832
 202/1000 [=====>........................] - ETA: 6:00 - loss: 3.5070 - regression_loss: 2.6206 - classification_loss: 0.8865
 203/1000 [=====>........................] - ETA: 6:00 - loss: 3.5082 - regression_loss: 2.6219 - classification_loss: 0.8863
 204/1000 [=====>........................] - ETA: 5:59 - loss: 3.5157 - regression_loss: 2.6263 - classification_loss: 0.8894
 205/1000 [=====>........................] - ETA: 5:58 - loss: 3.5258 - regression_loss: 2.6326 - classification_loss: 0.8933
 206/1000 [=====>........................] - ETA: 5:58 - loss: 3.5087 - regression_loss: 2.6198 - classification_loss: 0.8889
 207/1000 [=====>........................] - ETA: 5:57 - loss: 3.5178 - regression_loss: 2.6251 - classification_loss: 0.8926
 208/1000 [=====>........................] - ETA: 5:57 - loss: 3.5213 - regression_loss: 2.6302 - classification_loss: 0.8910
 209/1000 [=====>........................] - ETA: 5:57 - loss: 3.5281 - regression_loss: 2.6341 - classification_loss: 0.8940
 210/1000 [=====>........................] - ETA: 5:56 - loss: 3.5113 - regression_loss: 2.6216 - classification_loss: 0.8897
 211/1000 [=====>........................] - ETA: 5:56 - loss: 3.5120 - regression_loss: 2.6239 - classification_loss: 0.8881
 212/1000 [=====>........................] - ETA: 5:55 - loss: 3.5207 - regression_loss: 2.6323 - classification_loss: 0.8885
 213/1000 [=====>........................] - ETA: 5:55 - loss: 3.5222 - regression_loss: 2.6334 - classification_loss: 0.8888
 214/1000 [=====>........................] - ETA: 5:54 - loss: 3.5231 - regression_loss: 2.6355 - classification_loss: 0.8876
 215/1000 [=====>........................] - ETA: 5:54 - loss: 3.5224 - regression_loss: 2.6370 - classification_loss: 0.8855
 216/1000 [=====>........................] - ETA: 5:53 - loss: 3.5237 - regression_loss: 2.6383 - classification_loss: 0.8854
 217/1000 [=====>........................] - ETA: 5:53 - loss: 3.5276 - regression_loss: 2.6398 - classification_loss: 0.8878
 218/1000 [=====>........................] - ETA: 5:53 - loss: 3.5274 - regression_loss: 2.6415 - classification_loss: 0.8859
 219/1000 [=====>........................] - ETA: 5:52 - loss: 3.5323 - regression_loss: 2.6452 - classification_loss: 0.8871
 220/1000 [=====>........................] - ETA: 5:52 - loss: 3.5326 - regression_loss: 2.6461 - classification_loss: 0.8865
 221/1000 [=====>........................] - ETA: 5:51 - loss: 3.5441 - regression_loss: 2.6532 - classification_loss: 0.8909
 222/1000 [=====>........................] - ETA: 5:51 - loss: 3.5421 - regression_loss: 2.6530 - classification_loss: 0.8891
 223/1000 [=====>........................] - ETA: 5:50 - loss: 3.5441 - regression_loss: 2.6560 - classification_loss: 0.8881
 224/1000 [=====>........................] - ETA: 5:50 - loss: 3.5524 - regression_loss: 2.6617 - classification_loss: 0.8908
 225/1000 [=====>........................] - ETA: 5:49 - loss: 3.5542 - regression_loss: 2.6632 - classification_loss: 0.8910
 226/1000 [=====>........................] - ETA: 5:49 - loss: 3.5563 - regression_loss: 2.6670 - classification_loss: 0.8893
 227/1000 [=====>........................] - ETA: 5:49 - loss: 3.5407 - regression_loss: 2.6552 - classification_loss: 0.8854
 228/1000 [=====>........................] - ETA: 5:48 - loss: 3.5458 - regression_loss: 2.6614 - classification_loss: 0.8845
 229/1000 [=====>........................] - ETA: 5:48 - loss: 3.5307 - regression_loss: 2.6498 - classification_loss: 0.8810
 230/1000 [=====>........................] - ETA: 5:47 - loss: 3.5324 - regression_loss: 2.6514 - classification_loss: 0.8810
 231/1000 [=====>........................] - ETA: 5:47 - loss: 3.5328 - regression_loss: 2.6538 - classification_loss: 0.8790
 232/1000 [=====>........................] - ETA: 5:46 - loss: 3.5423 - regression_loss: 2.6604 - classification_loss: 0.8819
 233/1000 [=====>........................] - ETA: 5:46 - loss: 3.5551 - regression_loss: 2.6707 - classification_loss: 0.8844
 234/1000 [======>.......................] - ETA: 5:45 - loss: 3.5616 - regression_loss: 2.6739 - classification_loss: 0.8878
 235/1000 [======>.......................] - ETA: 5:45 - loss: 3.5663 - regression_loss: 2.6800 - classification_loss: 0.8863
 236/1000 [======>.......................] - ETA: 5:44 - loss: 3.5679 - regression_loss: 2.6803 - classification_loss: 0.8876
 237/1000 [======>.......................] - ETA: 5:44 - loss: 3.5738 - regression_loss: 2.6842 - classification_loss: 0.8896
 238/1000 [======>.......................] - ETA: 5:44 - loss: 3.5786 - regression_loss: 2.6863 - classification_loss: 0.8923
 239/1000 [======>.......................] - ETA: 5:43 - loss: 3.5637 - regression_loss: 2.6751 - classification_loss: 0.8886
 240/1000 [======>.......................] - ETA: 5:43 - loss: 3.5699 - regression_loss: 2.6779 - classification_loss: 0.8920
 241/1000 [======>.......................] - ETA: 5:42 - loss: 3.5708 - regression_loss: 2.6779 - classification_loss: 0.8930
 242/1000 [======>.......................] - ETA: 5:42 - loss: 3.5708 - regression_loss: 2.6791 - classification_loss: 0.8917
 243/1000 [======>.......................] - ETA: 5:41 - loss: 3.5743 - regression_loss: 2.6797 - classification_loss: 0.8946
 244/1000 [======>.......................] - ETA: 5:41 - loss: 3.5729 - regression_loss: 2.6795 - classification_loss: 0.8934
 245/1000 [======>.......................] - ETA: 5:41 - loss: 3.5583 - regression_loss: 2.6686 - classification_loss: 0.8897
 246/1000 [======>.......................] - ETA: 5:40 - loss: 3.5439 - regression_loss: 2.6578 - classification_loss: 0.8861
 247/1000 [======>.......................] - ETA: 5:40 - loss: 3.5418 - regression_loss: 2.6581 - classification_loss: 0.8837
 248/1000 [======>.......................] - ETA: 5:39 - loss: 3.5442 - regression_loss: 2.6621 - classification_loss: 0.8821
 249/1000 [======>.......................] - ETA: 5:39 - loss: 3.5299 - regression_loss: 2.6514 - classification_loss: 0.8786
 250/1000 [======>.......................] - ETA: 5:38 - loss: 3.5327 - regression_loss: 2.6545 - classification_loss: 0.8783
 251/1000 [======>.......................] - ETA: 5:38 - loss: 3.5187 - regression_loss: 2.6439 - classification_loss: 0.8748
 252/1000 [======>.......................] - ETA: 5:37 - loss: 3.5274 - regression_loss: 2.6481 - classification_loss: 0.8793
 253/1000 [======>.......................] - ETA: 5:37 - loss: 3.5135 - regression_loss: 2.6377 - classification_loss: 0.8758
 254/1000 [======>.......................] - ETA: 5:36 - loss: 3.5185 - regression_loss: 2.6391 - classification_loss: 0.8794
 255/1000 [======>.......................] - ETA: 5:36 - loss: 3.5197 - regression_loss: 2.6380 - classification_loss: 0.8817
 256/1000 [======>.......................] - ETA: 5:36 - loss: 3.5213 - regression_loss: 2.6411 - classification_loss: 0.8802
 257/1000 [======>.......................] - ETA: 5:35 - loss: 3.5226 - regression_loss: 2.6429 - classification_loss: 0.8797
 258/1000 [======>.......................] - ETA: 5:35 - loss: 3.5090 - regression_loss: 2.6327 - classification_loss: 0.8763
 259/1000 [======>.......................] - ETA: 5:34 - loss: 3.5082 - regression_loss: 2.6331 - classification_loss: 0.8751
 260/1000 [======>.......................] - ETA: 5:34 - loss: 3.5145 - regression_loss: 2.6394 - classification_loss: 0.8751
 261/1000 [======>.......................] - ETA: 5:33 - loss: 3.5115 - regression_loss: 2.6384 - classification_loss: 0.8731
 262/1000 [======>.......................] - ETA: 5:33 - loss: 3.5156 - regression_loss: 2.6419 - classification_loss: 0.8737
 263/1000 [======>.......................] - ETA: 5:32 - loss: 3.5226 - regression_loss: 2.6449 - classification_loss: 0.8777
 264/1000 [======>.......................] - ETA: 5:32 - loss: 3.5239 - regression_loss: 2.6474 - classification_loss: 0.8766
 265/1000 [======>.......................] - ETA: 5:32 - loss: 3.5262 - regression_loss: 2.6489 - classification_loss: 0.8773
 266/1000 [======>.......................] - ETA: 5:31 - loss: 3.5355 - regression_loss: 2.6540 - classification_loss: 0.8815
 267/1000 [=======>......................] - ETA: 5:31 - loss: 3.5247 - regression_loss: 2.6441 - classification_loss: 0.8807
 268/1000 [=======>......................] - ETA: 5:30 - loss: 3.5325 - regression_loss: 2.6472 - classification_loss: 0.8853
 269/1000 [=======>......................] - ETA: 5:30 - loss: 3.5342 - regression_loss: 2.6502 - classification_loss: 0.8840
 270/1000 [=======>......................] - ETA: 5:29 - loss: 3.5396 - regression_loss: 2.6546 - classification_loss: 0.8850
 271/1000 [=======>......................] - ETA: 5:29 - loss: 3.5379 - regression_loss: 2.6549 - classification_loss: 0.8830
 272/1000 [=======>......................] - ETA: 5:28 - loss: 3.5432 - regression_loss: 2.6563 - classification_loss: 0.8869
 273/1000 [=======>......................] - ETA: 5:28 - loss: 3.5438 - regression_loss: 2.6574 - classification_loss: 0.8863
 274/1000 [=======>......................] - ETA: 5:27 - loss: 3.5474 - regression_loss: 2.6585 - classification_loss: 0.8889
 275/1000 [=======>......................] - ETA: 5:27 - loss: 3.5463 - regression_loss: 2.6593 - classification_loss: 0.8870
 276/1000 [=======>......................] - ETA: 5:27 - loss: 3.5539 - regression_loss: 2.6631 - classification_loss: 0.8908
 277/1000 [=======>......................] - ETA: 5:26 - loss: 3.5411 - regression_loss: 2.6535 - classification_loss: 0.8876
 278/1000 [=======>......................] - ETA: 5:26 - loss: 3.5407 - regression_loss: 2.6544 - classification_loss: 0.8863
 279/1000 [=======>......................] - ETA: 5:25 - loss: 3.5442 - regression_loss: 2.6560 - classification_loss: 0.8882
 280/1000 [=======>......................] - ETA: 5:25 - loss: 3.5502 - regression_loss: 2.6572 - classification_loss: 0.8930
 281/1000 [=======>......................] - ETA: 5:24 - loss: 3.5548 - regression_loss: 2.6602 - classification_loss: 0.8946
 282/1000 [=======>......................] - ETA: 5:24 - loss: 3.5531 - regression_loss: 2.6598 - classification_loss: 0.8933
 283/1000 [=======>......................] - ETA: 5:23 - loss: 3.5543 - regression_loss: 2.6617 - classification_loss: 0.8926
 284/1000 [=======>......................] - ETA: 5:23 - loss: 3.5654 - regression_loss: 2.6693 - classification_loss: 0.8961
 285/1000 [=======>......................] - ETA: 5:23 - loss: 3.5535 - regression_loss: 2.6599 - classification_loss: 0.8936
 286/1000 [=======>......................] - ETA: 5:22 - loss: 3.5593 - regression_loss: 2.6639 - classification_loss: 0.8955
 287/1000 [=======>......................] - ETA: 5:22 - loss: 3.5641 - regression_loss: 2.6659 - classification_loss: 0.8981
 288/1000 [=======>......................] - ETA: 5:21 - loss: 3.5634 - regression_loss: 2.6657 - classification_loss: 0.8977
 289/1000 [=======>......................] - ETA: 5:21 - loss: 3.5511 - regression_loss: 2.6564 - classification_loss: 0.8946
 290/1000 [=======>......................] - ETA: 5:20 - loss: 3.5492 - regression_loss: 2.6559 - classification_loss: 0.8933
 291/1000 [=======>......................] - ETA: 5:20 - loss: 3.5538 - regression_loss: 2.6594 - classification_loss: 0.8943
 292/1000 [=======>......................] - ETA: 5:19 - loss: 3.5557 - regression_loss: 2.6618 - classification_loss: 0.8939
 293/1000 [=======>......................] - ETA: 5:19 - loss: 3.5602 - regression_loss: 2.6669 - classification_loss: 0.8933
 294/1000 [=======>......................] - ETA: 5:18 - loss: 3.5651 - regression_loss: 2.6717 - classification_loss: 0.8934
 295/1000 [=======>......................] - ETA: 5:18 - loss: 3.5712 - regression_loss: 2.6753 - classification_loss: 0.8959
 296/1000 [=======>......................] - ETA: 5:17 - loss: 3.5741 - regression_loss: 2.6789 - classification_loss: 0.8952
 297/1000 [=======>......................] - ETA: 5:17 - loss: 3.5760 - regression_loss: 2.6788 - classification_loss: 0.8973
 298/1000 [=======>......................] - ETA: 5:17 - loss: 3.5782 - regression_loss: 2.6793 - classification_loss: 0.8989
 299/1000 [=======>......................] - ETA: 5:16 - loss: 3.5845 - regression_loss: 2.6838 - classification_loss: 0.9007
 300/1000 [========>.....................] - ETA: 5:16 - loss: 3.5725 - regression_loss: 2.6749 - classification_loss: 0.8977
 301/1000 [========>.....................] - ETA: 5:15 - loss: 3.5745 - regression_loss: 2.6755 - classification_loss: 0.8990
 302/1000 [========>.....................] - ETA: 5:15 - loss: 3.5626 - regression_loss: 2.6666 - classification_loss: 0.8960
 303/1000 [========>.....................] - ETA: 5:14 - loss: 3.5522 - regression_loss: 2.6578 - classification_loss: 0.8944
 304/1000 [========>.....................] - ETA: 5:14 - loss: 3.5566 - regression_loss: 2.6611 - classification_loss: 0.8955
 305/1000 [========>.....................] - ETA: 5:13 - loss: 3.5449 - regression_loss: 2.6524 - classification_loss: 0.8926
 306/1000 [========>.....................] - ETA: 5:13 - loss: 3.5333 - regression_loss: 2.6437 - classification_loss: 0.8897
 307/1000 [========>.....................] - ETA: 5:13 - loss: 3.5378 - regression_loss: 2.6470 - classification_loss: 0.8908
 308/1000 [========>.....................] - ETA: 5:12 - loss: 3.5431 - regression_loss: 2.6516 - classification_loss: 0.8915
 309/1000 [========>.....................] - ETA: 5:12 - loss: 3.5448 - regression_loss: 2.6530 - classification_loss: 0.8918
 310/1000 [========>.....................] - ETA: 5:11 - loss: 3.5498 - regression_loss: 2.6564 - classification_loss: 0.8934
 311/1000 [========>.....................] - ETA: 5:11 - loss: 3.5604 - regression_loss: 2.6673 - classification_loss: 0.8931
 312/1000 [========>.....................] - ETA: 5:10 - loss: 3.5622 - regression_loss: 2.6681 - classification_loss: 0.8941
 313/1000 [========>.....................] - ETA: 5:10 - loss: 3.5508 - regression_loss: 2.6596 - classification_loss: 0.8913
 314/1000 [========>.....................] - ETA: 5:09 - loss: 3.5556 - regression_loss: 2.6616 - classification_loss: 0.8941
 315/1000 [========>.....................] - ETA: 5:09 - loss: 3.5595 - regression_loss: 2.6652 - classification_loss: 0.8944
 316/1000 [========>.....................] - ETA: 5:09 - loss: 3.5483 - regression_loss: 2.6568 - classification_loss: 0.8915
 317/1000 [========>.....................] - ETA: 5:08 - loss: 3.5529 - regression_loss: 2.6609 - classification_loss: 0.8920
 318/1000 [========>.....................] - ETA: 5:08 - loss: 3.5567 - regression_loss: 2.6645 - classification_loss: 0.8922
 319/1000 [========>.....................] - ETA: 5:07 - loss: 3.5588 - regression_loss: 2.6650 - classification_loss: 0.8938
 320/1000 [========>.....................] - ETA: 5:07 - loss: 3.5626 - regression_loss: 2.6670 - classification_loss: 0.8955
 321/1000 [========>.....................] - ETA: 5:06 - loss: 3.5515 - regression_loss: 2.6587 - classification_loss: 0.8928
 322/1000 [========>.....................] - ETA: 5:06 - loss: 3.5405 - regression_loss: 2.6505 - classification_loss: 0.8900
 323/1000 [========>.....................] - ETA: 5:05 - loss: 3.5443 - regression_loss: 2.6542 - classification_loss: 0.8901
 324/1000 [========>.....................] - ETA: 5:05 - loss: 3.5458 - regression_loss: 2.6543 - classification_loss: 0.8915
 325/1000 [========>.....................] - ETA: 5:05 - loss: 3.5453 - regression_loss: 2.6551 - classification_loss: 0.8902
 326/1000 [========>.....................] - ETA: 5:04 - loss: 3.5496 - regression_loss: 2.6603 - classification_loss: 0.8893
 327/1000 [========>.....................] - ETA: 5:04 - loss: 3.5511 - regression_loss: 2.6629 - classification_loss: 0.8883
 328/1000 [========>.....................] - ETA: 5:03 - loss: 3.5565 - regression_loss: 2.6669 - classification_loss: 0.8897
 329/1000 [========>.....................] - ETA: 5:03 - loss: 3.5604 - regression_loss: 2.6710 - classification_loss: 0.8894
 330/1000 [========>.....................] - ETA: 5:02 - loss: 3.5620 - regression_loss: 2.6709 - classification_loss: 0.8911
 331/1000 [========>.....................] - ETA: 5:02 - loss: 3.5513 - regression_loss: 2.6628 - classification_loss: 0.8884
 332/1000 [========>.....................] - ETA: 5:01 - loss: 3.5523 - regression_loss: 2.6625 - classification_loss: 0.8898
 333/1000 [========>.....................] - ETA: 5:01 - loss: 3.5545 - regression_loss: 2.6625 - classification_loss: 0.8920
 334/1000 [=========>....................] - ETA: 5:00 - loss: 3.5581 - regression_loss: 2.6662 - classification_loss: 0.8919
 335/1000 [=========>....................] - ETA: 5:00 - loss: 3.5592 - regression_loss: 2.6682 - classification_loss: 0.8910
 336/1000 [=========>....................] - ETA: 5:00 - loss: 3.5588 - regression_loss: 2.6693 - classification_loss: 0.8895
 337/1000 [=========>....................] - ETA: 4:59 - loss: 3.5620 - regression_loss: 2.6721 - classification_loss: 0.8899
 338/1000 [=========>....................] - ETA: 4:59 - loss: 3.5640 - regression_loss: 2.6737 - classification_loss: 0.8904
 339/1000 [=========>....................] - ETA: 4:58 - loss: 3.5714 - regression_loss: 2.6779 - classification_loss: 0.8935
 340/1000 [=========>....................] - ETA: 4:58 - loss: 3.5609 - regression_loss: 2.6700 - classification_loss: 0.8909
 341/1000 [=========>....................] - ETA: 4:57 - loss: 3.5613 - regression_loss: 2.6715 - classification_loss: 0.8897
 342/1000 [=========>....................] - ETA: 4:57 - loss: 3.5632 - regression_loss: 2.6722 - classification_loss: 0.8910
 343/1000 [=========>....................] - ETA: 4:56 - loss: 3.5651 - regression_loss: 2.6748 - classification_loss: 0.8903
 344/1000 [=========>....................] - ETA: 4:56 - loss: 3.5548 - regression_loss: 2.6671 - classification_loss: 0.8877
 345/1000 [=========>....................] - ETA: 4:56 - loss: 3.5552 - regression_loss: 2.6667 - classification_loss: 0.8885
 346/1000 [=========>....................] - ETA: 4:55 - loss: 3.5594 - regression_loss: 2.6692 - classification_loss: 0.8902
 347/1000 [=========>....................] - ETA: 4:55 - loss: 3.5597 - regression_loss: 2.6707 - classification_loss: 0.8890
 348/1000 [=========>....................] - ETA: 4:54 - loss: 3.5660 - regression_loss: 2.6750 - classification_loss: 0.8910
 349/1000 [=========>....................] - ETA: 4:54 - loss: 3.5668 - regression_loss: 2.6749 - classification_loss: 0.8919
 350/1000 [=========>....................] - ETA: 4:53 - loss: 3.5689 - regression_loss: 2.6762 - classification_loss: 0.8927
 351/1000 [=========>....................] - ETA: 4:53 - loss: 3.5720 - regression_loss: 2.6769 - classification_loss: 0.8951
 352/1000 [=========>....................] - ETA: 4:52 - loss: 3.5619 - regression_loss: 2.6693 - classification_loss: 0.8926
 353/1000 [=========>....................] - ETA: 4:52 - loss: 3.5652 - regression_loss: 2.6706 - classification_loss: 0.8946
 354/1000 [=========>....................] - ETA: 4:51 - loss: 3.5653 - regression_loss: 2.6698 - classification_loss: 0.8955
 355/1000 [=========>....................] - ETA: 4:51 - loss: 3.5676 - regression_loss: 2.6711 - classification_loss: 0.8966
 356/1000 [=========>....................] - ETA: 4:51 - loss: 3.5683 - regression_loss: 2.6714 - classification_loss: 0.8969
 357/1000 [=========>....................] - ETA: 4:50 - loss: 3.5700 - regression_loss: 2.6734 - classification_loss: 0.8967
 358/1000 [=========>....................] - ETA: 4:50 - loss: 3.5748 - regression_loss: 2.6764 - classification_loss: 0.8984
 359/1000 [=========>....................] - ETA: 4:49 - loss: 3.5649 - regression_loss: 2.6689 - classification_loss: 0.8959
 360/1000 [=========>....................] - ETA: 4:49 - loss: 3.5744 - regression_loss: 2.6782 - classification_loss: 0.8963
 361/1000 [=========>....................] - ETA: 4:48 - loss: 3.5754 - regression_loss: 2.6805 - classification_loss: 0.8950
 362/1000 [=========>....................] - ETA: 4:48 - loss: 3.5763 - regression_loss: 2.6817 - classification_loss: 0.8946
 363/1000 [=========>....................] - ETA: 4:47 - loss: 3.5762 - regression_loss: 2.6811 - classification_loss: 0.8951
 364/1000 [=========>....................] - ETA: 4:47 - loss: 3.5782 - regression_loss: 2.6838 - classification_loss: 0.8945
 365/1000 [=========>....................] - ETA: 4:47 - loss: 3.5688 - regression_loss: 2.6764 - classification_loss: 0.8924
 366/1000 [=========>....................] - ETA: 4:46 - loss: 3.5723 - regression_loss: 2.6795 - classification_loss: 0.8928
 367/1000 [==========>...................] - ETA: 4:46 - loss: 3.5763 - regression_loss: 2.6819 - classification_loss: 0.8944
 368/1000 [==========>...................] - ETA: 4:45 - loss: 3.5775 - regression_loss: 2.6837 - classification_loss: 0.8938
 369/1000 [==========>...................] - ETA: 4:45 - loss: 3.5775 - regression_loss: 2.6842 - classification_loss: 0.8933
 370/1000 [==========>...................] - ETA: 4:44 - loss: 3.5678 - regression_loss: 2.6770 - classification_loss: 0.8909
 371/1000 [==========>...................] - ETA: 4:44 - loss: 3.5698 - regression_loss: 2.6785 - classification_loss: 0.8914
 372/1000 [==========>...................] - ETA: 4:43 - loss: 3.5711 - regression_loss: 2.6780 - classification_loss: 0.8931
 373/1000 [==========>...................] - ETA: 4:43 - loss: 3.5711 - regression_loss: 2.6786 - classification_loss: 0.8924
 374/1000 [==========>...................] - ETA: 4:42 - loss: 3.5738 - regression_loss: 2.6794 - classification_loss: 0.8943
 375/1000 [==========>...................] - ETA: 4:42 - loss: 3.5746 - regression_loss: 2.6805 - classification_loss: 0.8941
 376/1000 [==========>...................] - ETA: 4:42 - loss: 3.5651 - regression_loss: 2.6734 - classification_loss: 0.8917
 377/1000 [==========>...................] - ETA: 4:41 - loss: 3.5691 - regression_loss: 2.6766 - classification_loss: 0.8924
 378/1000 [==========>...................] - ETA: 4:41 - loss: 3.5693 - regression_loss: 2.6778 - classification_loss: 0.8915
 379/1000 [==========>...................] - ETA: 4:40 - loss: 3.5626 - regression_loss: 2.6708 - classification_loss: 0.8919
 380/1000 [==========>...................] - ETA: 4:40 - loss: 3.5704 - regression_loss: 2.6764 - classification_loss: 0.8940
 381/1000 [==========>...................] - ETA: 4:39 - loss: 3.5706 - regression_loss: 2.6766 - classification_loss: 0.8940
 382/1000 [==========>...................] - ETA: 4:39 - loss: 3.5703 - regression_loss: 2.6772 - classification_loss: 0.8932
 383/1000 [==========>...................] - ETA: 4:38 - loss: 3.5697 - regression_loss: 2.6775 - classification_loss: 0.8922
 384/1000 [==========>...................] - ETA: 4:38 - loss: 3.5604 - regression_loss: 2.6705 - classification_loss: 0.8898
 385/1000 [==========>...................] - ETA: 4:38 - loss: 3.5605 - regression_loss: 2.6716 - classification_loss: 0.8889
 386/1000 [==========>...................] - ETA: 4:37 - loss: 3.5524 - regression_loss: 2.6647 - classification_loss: 0.8876
 387/1000 [==========>...................] - ETA: 4:37 - loss: 3.5531 - regression_loss: 2.6652 - classification_loss: 0.8879
 388/1000 [==========>...................] - ETA: 4:36 - loss: 3.5532 - regression_loss: 2.6656 - classification_loss: 0.8876
 389/1000 [==========>...................] - ETA: 4:36 - loss: 3.5441 - regression_loss: 2.6587 - classification_loss: 0.8853
 390/1000 [==========>...................] - ETA: 4:35 - loss: 3.5374 - regression_loss: 2.6519 - classification_loss: 0.8855
 391/1000 [==========>...................] - ETA: 4:35 - loss: 3.5385 - regression_loss: 2.6536 - classification_loss: 0.8849
 392/1000 [==========>...................] - ETA: 4:34 - loss: 3.5396 - regression_loss: 2.6549 - classification_loss: 0.8847
 393/1000 [==========>...................] - ETA: 4:34 - loss: 3.5411 - regression_loss: 2.6570 - classification_loss: 0.8841
 394/1000 [==========>...................] - ETA: 4:33 - loss: 3.5321 - regression_loss: 2.6503 - classification_loss: 0.8818
 395/1000 [==========>...................] - ETA: 4:33 - loss: 3.5348 - regression_loss: 2.6521 - classification_loss: 0.8827
 396/1000 [==========>...................] - ETA: 4:33 - loss: 3.5338 - regression_loss: 2.6514 - classification_loss: 0.8824
 397/1000 [==========>...................] - ETA: 4:32 - loss: 3.5367 - regression_loss: 2.6536 - classification_loss: 0.8831
 398/1000 [==========>...................] - ETA: 4:32 - loss: 3.5448 - regression_loss: 2.6586 - classification_loss: 0.8862
 399/1000 [==========>...................] - ETA: 4:31 - loss: 3.5444 - regression_loss: 2.6586 - classification_loss: 0.8858
 400/1000 [===========>..................] - ETA: 4:31 - loss: 3.5355 - regression_loss: 2.6519 - classification_loss: 0.8836
 401/1000 [===========>..................] - ETA: 4:30 - loss: 3.5341 - regression_loss: 2.6514 - classification_loss: 0.8826
 402/1000 [===========>..................] - ETA: 4:30 - loss: 3.5333 - regression_loss: 2.6514 - classification_loss: 0.8819
 403/1000 [===========>..................] - ETA: 4:29 - loss: 3.5342 - regression_loss: 2.6527 - classification_loss: 0.8815
 404/1000 [===========>..................] - ETA: 4:29 - loss: 3.5383 - regression_loss: 2.6544 - classification_loss: 0.8839
 405/1000 [===========>..................] - ETA: 4:28 - loss: 3.5295 - regression_loss: 2.6478 - classification_loss: 0.8817
 406/1000 [===========>..................] - ETA: 4:28 - loss: 3.5319 - regression_loss: 2.6479 - classification_loss: 0.8840
 407/1000 [===========>..................] - ETA: 4:28 - loss: 3.5378 - regression_loss: 2.6510 - classification_loss: 0.8868
 408/1000 [===========>..................] - ETA: 4:27 - loss: 3.5363 - regression_loss: 2.6504 - classification_loss: 0.8859
 409/1000 [===========>..................] - ETA: 4:27 - loss: 3.5276 - regression_loss: 2.6439 - classification_loss: 0.8837
 410/1000 [===========>..................] - ETA: 4:26 - loss: 3.5296 - regression_loss: 2.6437 - classification_loss: 0.8859
 411/1000 [===========>..................] - ETA: 4:26 - loss: 3.5285 - regression_loss: 2.6432 - classification_loss: 0.8853
 412/1000 [===========>..................] - ETA: 4:25 - loss: 3.5261 - regression_loss: 2.6418 - classification_loss: 0.8844
 413/1000 [===========>..................] - ETA: 4:25 - loss: 3.5260 - regression_loss: 2.6417 - classification_loss: 0.8842
 414/1000 [===========>..................] - ETA: 4:24 - loss: 3.5292 - regression_loss: 2.6439 - classification_loss: 0.8852
 415/1000 [===========>..................] - ETA: 4:24 - loss: 3.5287 - regression_loss: 2.6441 - classification_loss: 0.8846
 416/1000 [===========>..................] - ETA: 4:24 - loss: 3.5203 - regression_loss: 2.6377 - classification_loss: 0.8825
 417/1000 [===========>..................] - ETA: 4:23 - loss: 3.5196 - regression_loss: 2.6371 - classification_loss: 0.8825
 418/1000 [===========>..................] - ETA: 4:23 - loss: 3.5191 - regression_loss: 2.6359 - classification_loss: 0.8833
 419/1000 [===========>..................] - ETA: 4:22 - loss: 3.5212 - regression_loss: 2.6377 - classification_loss: 0.8835
 420/1000 [===========>..................] - ETA: 4:22 - loss: 3.5256 - regression_loss: 2.6426 - classification_loss: 0.8830
 421/1000 [===========>..................] - ETA: 4:21 - loss: 3.5291 - regression_loss: 2.6461 - classification_loss: 0.8830
 422/1000 [===========>..................] - ETA: 4:21 - loss: 3.5283 - regression_loss: 2.6462 - classification_loss: 0.8820
 423/1000 [===========>..................] - ETA: 4:20 - loss: 3.5320 - regression_loss: 2.6498 - classification_loss: 0.8822
 424/1000 [===========>..................] - ETA: 4:20 - loss: 3.5334 - regression_loss: 2.6507 - classification_loss: 0.8827
 425/1000 [===========>..................] - ETA: 4:19 - loss: 3.5356 - regression_loss: 2.6531 - classification_loss: 0.8826
 426/1000 [===========>..................] - ETA: 4:19 - loss: 3.5368 - regression_loss: 2.6544 - classification_loss: 0.8824
 427/1000 [===========>..................] - ETA: 4:19 - loss: 3.5413 - regression_loss: 2.6574 - classification_loss: 0.8840
 428/1000 [===========>..................] - ETA: 4:18 - loss: 3.5435 - regression_loss: 2.6594 - classification_loss: 0.8841
 429/1000 [===========>..................] - ETA: 4:18 - loss: 3.5440 - regression_loss: 2.6607 - classification_loss: 0.8833
 430/1000 [===========>..................] - ETA: 4:17 - loss: 3.5448 - regression_loss: 2.6620 - classification_loss: 0.8827
 431/1000 [===========>..................] - ETA: 4:17 - loss: 3.5461 - regression_loss: 2.6638 - classification_loss: 0.8823
 432/1000 [===========>..................] - ETA: 4:16 - loss: 3.5387 - regression_loss: 2.6576 - classification_loss: 0.8810
 433/1000 [===========>..................] - ETA: 4:16 - loss: 3.5452 - regression_loss: 2.6629 - classification_loss: 0.8823
 434/1000 [============>.................] - ETA: 4:15 - loss: 3.5472 - regression_loss: 2.6649 - classification_loss: 0.8824
 435/1000 [============>.................] - ETA: 4:15 - loss: 3.5391 - regression_loss: 2.6587 - classification_loss: 0.8804
 436/1000 [============>.................] - ETA: 4:14 - loss: 3.5399 - regression_loss: 2.6583 - classification_loss: 0.8816
 437/1000 [============>.................] - ETA: 4:14 - loss: 3.5432 - regression_loss: 2.6602 - classification_loss: 0.8830
 438/1000 [============>.................] - ETA: 4:14 - loss: 3.5464 - regression_loss: 2.6633 - classification_loss: 0.8831
 439/1000 [============>.................] - ETA: 4:13 - loss: 3.5461 - regression_loss: 2.6636 - classification_loss: 0.8825
 440/1000 [============>.................] - ETA: 4:13 - loss: 3.5461 - regression_loss: 2.6637 - classification_loss: 0.8824
 441/1000 [============>.................] - ETA: 4:12 - loss: 3.5458 - regression_loss: 2.6635 - classification_loss: 0.8823
 442/1000 [============>.................] - ETA: 4:12 - loss: 3.5467 - regression_loss: 2.6645 - classification_loss: 0.8822
 443/1000 [============>.................] - ETA: 4:11 - loss: 3.5481 - regression_loss: 2.6665 - classification_loss: 0.8816
 444/1000 [============>.................] - ETA: 4:11 - loss: 3.5515 - regression_loss: 2.6699 - classification_loss: 0.8815
 445/1000 [============>.................] - ETA: 4:10 - loss: 3.5437 - regression_loss: 2.6639 - classification_loss: 0.8798
 446/1000 [============>.................] - ETA: 4:10 - loss: 3.5459 - regression_loss: 2.6662 - classification_loss: 0.8797
 447/1000 [============>.................] - ETA: 4:09 - loss: 3.5475 - regression_loss: 2.6669 - classification_loss: 0.8806
 448/1000 [============>.................] - ETA: 4:09 - loss: 3.5480 - regression_loss: 2.6680 - classification_loss: 0.8800
 449/1000 [============>.................] - ETA: 4:09 - loss: 3.5512 - regression_loss: 2.6707 - classification_loss: 0.8806
 450/1000 [============>.................] - ETA: 4:08 - loss: 3.5527 - regression_loss: 2.6715 - classification_loss: 0.8812
 451/1000 [============>.................] - ETA: 4:08 - loss: 3.5547 - regression_loss: 2.6740 - classification_loss: 0.8807
 452/1000 [============>.................] - ETA: 4:07 - loss: 3.5563 - regression_loss: 2.6757 - classification_loss: 0.8806
 453/1000 [============>.................] - ETA: 4:07 - loss: 3.5589 - regression_loss: 2.6781 - classification_loss: 0.8808
 454/1000 [============>.................] - ETA: 4:06 - loss: 3.5583 - regression_loss: 2.6783 - classification_loss: 0.8800
 455/1000 [============>.................] - ETA: 4:06 - loss: 3.5505 - regression_loss: 2.6724 - classification_loss: 0.8781
 456/1000 [============>.................] - ETA: 4:05 - loss: 3.5429 - regression_loss: 2.6666 - classification_loss: 0.8764
 457/1000 [============>.................] - ETA: 4:05 - loss: 3.5442 - regression_loss: 2.6677 - classification_loss: 0.8765
 458/1000 [============>.................] - ETA: 4:05 - loss: 3.5453 - regression_loss: 2.6689 - classification_loss: 0.8764
 459/1000 [============>.................] - ETA: 4:04 - loss: 3.5474 - regression_loss: 2.6703 - classification_loss: 0.8771
 460/1000 [============>.................] - ETA: 4:04 - loss: 3.5512 - regression_loss: 2.6737 - classification_loss: 0.8776
 461/1000 [============>.................] - ETA: 4:03 - loss: 3.5539 - regression_loss: 2.6756 - classification_loss: 0.8783
 462/1000 [============>.................] - ETA: 4:03 - loss: 3.5492 - regression_loss: 2.6698 - classification_loss: 0.8795
 463/1000 [============>.................] - ETA: 4:02 - loss: 3.5478 - regression_loss: 2.6682 - classification_loss: 0.8796
 464/1000 [============>.................] - ETA: 4:02 - loss: 3.5487 - regression_loss: 2.6684 - classification_loss: 0.8802
 465/1000 [============>.................] - ETA: 4:01 - loss: 3.5513 - regression_loss: 2.6716 - classification_loss: 0.8797
 466/1000 [============>.................] - ETA: 4:01 - loss: 3.5437 - regression_loss: 2.6658 - classification_loss: 0.8779
 467/1000 [=============>................] - ETA: 4:00 - loss: 3.5447 - regression_loss: 2.6670 - classification_loss: 0.8777
 468/1000 [=============>................] - ETA: 4:00 - loss: 3.5457 - regression_loss: 2.6682 - classification_loss: 0.8775
 469/1000 [=============>................] - ETA: 4:00 - loss: 3.5459 - regression_loss: 2.6683 - classification_loss: 0.8776
 470/1000 [=============>................] - ETA: 3:59 - loss: 3.5477 - regression_loss: 2.6706 - classification_loss: 0.8771
 471/1000 [=============>................] - ETA: 3:59 - loss: 3.5481 - regression_loss: 2.6709 - classification_loss: 0.8772
 472/1000 [=============>................] - ETA: 3:58 - loss: 3.5496 - regression_loss: 2.6727 - classification_loss: 0.8769
 473/1000 [=============>................] - ETA: 3:58 - loss: 3.5421 - regression_loss: 2.6671 - classification_loss: 0.8750
 474/1000 [=============>................] - ETA: 3:57 - loss: 3.5347 - regression_loss: 2.6615 - classification_loss: 0.8732
 475/1000 [=============>................] - ETA: 3:57 - loss: 3.5355 - regression_loss: 2.6625 - classification_loss: 0.8730
 476/1000 [=============>................] - ETA: 3:56 - loss: 3.5352 - regression_loss: 2.6629 - classification_loss: 0.8723
 477/1000 [=============>................] - ETA: 3:56 - loss: 3.5364 - regression_loss: 2.6641 - classification_loss: 0.8723
 478/1000 [=============>................] - ETA: 3:55 - loss: 3.5357 - regression_loss: 2.6642 - classification_loss: 0.8715
 479/1000 [=============>................] - ETA: 3:55 - loss: 3.5368 - regression_loss: 2.6656 - classification_loss: 0.8712
 480/1000 [=============>................] - ETA: 3:55 - loss: 3.5380 - regression_loss: 2.6672 - classification_loss: 0.8708
 481/1000 [=============>................] - ETA: 3:54 - loss: 3.5308 - regression_loss: 2.6617 - classification_loss: 0.8691
 482/1000 [=============>................] - ETA: 3:54 - loss: 3.5334 - regression_loss: 2.6640 - classification_loss: 0.8694
 483/1000 [=============>................] - ETA: 3:53 - loss: 3.5338 - regression_loss: 2.6644 - classification_loss: 0.8694
 484/1000 [=============>................] - ETA: 3:53 - loss: 3.5349 - regression_loss: 2.6660 - classification_loss: 0.8690
 485/1000 [=============>................] - ETA: 3:52 - loss: 3.5372 - regression_loss: 2.6665 - classification_loss: 0.8707
 486/1000 [=============>................] - ETA: 3:52 - loss: 3.5390 - regression_loss: 2.6681 - classification_loss: 0.8709
 487/1000 [=============>................] - ETA: 3:51 - loss: 3.5430 - regression_loss: 2.6711 - classification_loss: 0.8719
 488/1000 [=============>................] - ETA: 3:51 - loss: 3.5358 - regression_loss: 2.6657 - classification_loss: 0.8701
 489/1000 [=============>................] - ETA: 3:51 - loss: 3.5285 - regression_loss: 2.6602 - classification_loss: 0.8683
 490/1000 [=============>................] - ETA: 3:50 - loss: 3.5337 - regression_loss: 2.6653 - classification_loss: 0.8683
 491/1000 [=============>................] - ETA: 3:50 - loss: 3.5265 - regression_loss: 2.6599 - classification_loss: 0.8666
 492/1000 [=============>................] - ETA: 3:49 - loss: 3.5193 - regression_loss: 2.6545 - classification_loss: 0.8648
 493/1000 [=============>................] - ETA: 3:49 - loss: 3.5122 - regression_loss: 2.6491 - classification_loss: 0.8631
 494/1000 [=============>................] - ETA: 3:48 - loss: 3.5129 - regression_loss: 2.6500 - classification_loss: 0.8629
 495/1000 [=============>................] - ETA: 3:48 - loss: 3.5136 - regression_loss: 2.6510 - classification_loss: 0.8626
 496/1000 [=============>................] - ETA: 3:47 - loss: 3.5164 - regression_loss: 2.6532 - classification_loss: 0.8632
 497/1000 [=============>................] - ETA: 3:47 - loss: 3.5093 - regression_loss: 2.6479 - classification_loss: 0.8614
 498/1000 [=============>................] - ETA: 3:46 - loss: 3.5088 - regression_loss: 2.6466 - classification_loss: 0.8622
 499/1000 [=============>................] - ETA: 3:46 - loss: 3.5018 - regression_loss: 2.6413 - classification_loss: 0.8605
 500/1000 [==============>...............] - ETA: 3:46 - loss: 3.4948 - regression_loss: 2.6360 - classification_loss: 0.8588
 501/1000 [==============>...............] - ETA: 3:45 - loss: 3.4926 - regression_loss: 2.6344 - classification_loss: 0.8582
 502/1000 [==============>...............] - ETA: 3:45 - loss: 3.4955 - regression_loss: 2.6375 - classification_loss: 0.8580
 503/1000 [==============>...............] - ETA: 3:44 - loss: 3.4886 - regression_loss: 2.6323 - classification_loss: 0.8563
 504/1000 [==============>...............] - ETA: 3:44 - loss: 3.4923 - regression_loss: 2.6337 - classification_loss: 0.8585
 505/1000 [==============>...............] - ETA: 3:43 - loss: 3.4932 - regression_loss: 2.6344 - classification_loss: 0.8588
 506/1000 [==============>...............] - ETA: 3:43 - loss: 3.4985 - regression_loss: 2.6370 - classification_loss: 0.8615
 507/1000 [==============>...............] - ETA: 3:42 - loss: 3.5019 - regression_loss: 2.6388 - classification_loss: 0.8631
 508/1000 [==============>...............] - ETA: 3:42 - loss: 3.5035 - regression_loss: 2.6408 - classification_loss: 0.8627
 509/1000 [==============>...............] - ETA: 3:42 - loss: 3.5081 - regression_loss: 2.6436 - classification_loss: 0.8645
 510/1000 [==============>...............] - ETA: 3:41 - loss: 3.5116 - regression_loss: 2.6462 - classification_loss: 0.8654
 511/1000 [==============>...............] - ETA: 3:41 - loss: 3.5047 - regression_loss: 2.6410 - classification_loss: 0.8637
 512/1000 [==============>...............] - ETA: 3:40 - loss: 3.5085 - regression_loss: 2.6418 - classification_loss: 0.8666
 513/1000 [==============>...............] - ETA: 3:40 - loss: 3.5146 - regression_loss: 2.6455 - classification_loss: 0.8691
 514/1000 [==============>...............] - ETA: 3:39 - loss: 3.5078 - regression_loss: 2.6404 - classification_loss: 0.8674
 515/1000 [==============>...............] - ETA: 3:39 - loss: 3.5106 - regression_loss: 2.6421 - classification_loss: 0.8685
 516/1000 [==============>...............] - ETA: 3:38 - loss: 3.5112 - regression_loss: 2.6412 - classification_loss: 0.8700
 517/1000 [==============>...............] - ETA: 3:38 - loss: 3.5128 - regression_loss: 2.6432 - classification_loss: 0.8696
 518/1000 [==============>...............] - ETA: 3:37 - loss: 3.5120 - regression_loss: 2.6432 - classification_loss: 0.8688
 519/1000 [==============>...............] - ETA: 3:37 - loss: 3.5152 - regression_loss: 2.6444 - classification_loss: 0.8708
 520/1000 [==============>...............] - ETA: 3:37 - loss: 3.5087 - regression_loss: 2.6393 - classification_loss: 0.8694
 521/1000 [==============>...............] - ETA: 3:36 - loss: 3.5102 - regression_loss: 2.6394 - classification_loss: 0.8708
 522/1000 [==============>...............] - ETA: 3:36 - loss: 3.5113 - regression_loss: 2.6412 - classification_loss: 0.8701
 523/1000 [==============>...............] - ETA: 3:35 - loss: 3.5136 - regression_loss: 2.6434 - classification_loss: 0.8701
 524/1000 [==============>...............] - ETA: 3:35 - loss: 3.5068 - regression_loss: 2.6384 - classification_loss: 0.8685
 525/1000 [==============>...............] - ETA: 3:34 - loss: 3.5077 - regression_loss: 2.6393 - classification_loss: 0.8684
 526/1000 [==============>...............] - ETA: 3:34 - loss: 3.5010 - regression_loss: 2.6342 - classification_loss: 0.8668
 527/1000 [==============>...............] - ETA: 3:33 - loss: 3.5031 - regression_loss: 2.6351 - classification_loss: 0.8680
 528/1000 [==============>...............] - ETA: 3:33 - loss: 3.5059 - regression_loss: 2.6364 - classification_loss: 0.8695
 529/1000 [==============>...............] - ETA: 3:32 - loss: 3.5081 - regression_loss: 2.6380 - classification_loss: 0.8701
 530/1000 [==============>...............] - ETA: 3:32 - loss: 3.5092 - regression_loss: 2.6393 - classification_loss: 0.8699
 531/1000 [==============>...............] - ETA: 3:32 - loss: 3.5026 - regression_loss: 2.6344 - classification_loss: 0.8683
 532/1000 [==============>...............] - ETA: 3:31 - loss: 3.5054 - regression_loss: 2.6365 - classification_loss: 0.8689
 533/1000 [==============>...............] - ETA: 3:31 - loss: 3.5073 - regression_loss: 2.6389 - classification_loss: 0.8684
 534/1000 [===============>..............] - ETA: 3:30 - loss: 3.5106 - regression_loss: 2.6412 - classification_loss: 0.8694
 535/1000 [===============>..............] - ETA: 3:30 - loss: 3.5040 - regression_loss: 2.6363 - classification_loss: 0.8678
 536/1000 [===============>..............] - ETA: 3:29 - loss: 3.5074 - regression_loss: 2.6378 - classification_loss: 0.8696
 537/1000 [===============>..............] - ETA: 3:29 - loss: 3.5078 - regression_loss: 2.6381 - classification_loss: 0.8697
 538/1000 [===============>..............] - ETA: 3:28 - loss: 3.5071 - regression_loss: 2.6379 - classification_loss: 0.8692
 539/1000 [===============>..............] - ETA: 3:28 - loss: 3.5095 - regression_loss: 2.6402 - classification_loss: 0.8693
 540/1000 [===============>..............] - ETA: 3:27 - loss: 3.5110 - regression_loss: 2.6407 - classification_loss: 0.8702
 541/1000 [===============>..............] - ETA: 3:27 - loss: 3.5101 - regression_loss: 2.6405 - classification_loss: 0.8696
 542/1000 [===============>..............] - ETA: 3:27 - loss: 3.5100 - regression_loss: 2.6411 - classification_loss: 0.8689
 543/1000 [===============>..............] - ETA: 3:26 - loss: 3.5171 - regression_loss: 2.6465 - classification_loss: 0.8706
 544/1000 [===============>..............] - ETA: 3:26 - loss: 3.5158 - regression_loss: 2.6458 - classification_loss: 0.8701
 545/1000 [===============>..............] - ETA: 3:25 - loss: 3.5171 - regression_loss: 2.6469 - classification_loss: 0.8702
 546/1000 [===============>..............] - ETA: 3:25 - loss: 3.5106 - regression_loss: 2.6420 - classification_loss: 0.8686
 547/1000 [===============>..............] - ETA: 3:24 - loss: 3.5042 - regression_loss: 2.6372 - classification_loss: 0.8670
 548/1000 [===============>..............] - ETA: 3:24 - loss: 3.5050 - regression_loss: 2.6374 - classification_loss: 0.8676
 549/1000 [===============>..............] - ETA: 3:23 - loss: 3.5053 - regression_loss: 2.6372 - classification_loss: 0.8681
 550/1000 [===============>..............] - ETA: 3:23 - loss: 3.5065 - regression_loss: 2.6390 - classification_loss: 0.8675
 551/1000 [===============>..............] - ETA: 3:23 - loss: 3.5085 - regression_loss: 2.6401 - classification_loss: 0.8684
 552/1000 [===============>..............] - ETA: 3:22 - loss: 3.5021 - regression_loss: 2.6353 - classification_loss: 0.8668
 553/1000 [===============>..............] - ETA: 3:22 - loss: 3.5023 - regression_loss: 2.6351 - classification_loss: 0.8672
 554/1000 [===============>..............] - ETA: 3:21 - loss: 3.4960 - regression_loss: 2.6304 - classification_loss: 0.8656
 555/1000 [===============>..............] - ETA: 3:21 - loss: 3.4956 - regression_loss: 2.6307 - classification_loss: 0.8649
 556/1000 [===============>..............] - ETA: 3:20 - loss: 3.4950 - regression_loss: 2.6308 - classification_loss: 0.8641
 557/1000 [===============>..............] - ETA: 3:20 - loss: 3.4952 - regression_loss: 2.6317 - classification_loss: 0.8635
 558/1000 [===============>..............] - ETA: 3:19 - loss: 3.4968 - regression_loss: 2.6331 - classification_loss: 0.8637
 559/1000 [===============>..............] - ETA: 3:19 - loss: 3.4969 - regression_loss: 2.6338 - classification_loss: 0.8631
 560/1000 [===============>..............] - ETA: 3:18 - loss: 3.5009 - regression_loss: 2.6361 - classification_loss: 0.8648
 561/1000 [===============>..............] - ETA: 3:18 - loss: 3.5020 - regression_loss: 2.6378 - classification_loss: 0.8642
 562/1000 [===============>..............] - ETA: 3:18 - loss: 3.5011 - regression_loss: 2.6371 - classification_loss: 0.8640
 563/1000 [===============>..............] - ETA: 3:17 - loss: 3.5019 - regression_loss: 2.6383 - classification_loss: 0.8636
 564/1000 [===============>..............] - ETA: 3:17 - loss: 3.4957 - regression_loss: 2.6336 - classification_loss: 0.8621
 565/1000 [===============>..............] - ETA: 3:16 - loss: 3.4895 - regression_loss: 2.6289 - classification_loss: 0.8606
 566/1000 [===============>..............] - ETA: 3:16 - loss: 3.4907 - regression_loss: 2.6289 - classification_loss: 0.8618
 567/1000 [================>.............] - ETA: 3:15 - loss: 3.4947 - regression_loss: 2.6328 - classification_loss: 0.8618
 568/1000 [================>.............] - ETA: 3:15 - loss: 3.4938 - regression_loss: 2.6325 - classification_loss: 0.8614
 569/1000 [================>.............] - ETA: 3:14 - loss: 3.4955 - regression_loss: 2.6326 - classification_loss: 0.8630
 570/1000 [================>.............] - ETA: 3:14 - loss: 3.4944 - regression_loss: 2.6322 - classification_loss: 0.8622
 571/1000 [================>.............] - ETA: 3:13 - loss: 3.4963 - regression_loss: 2.6328 - classification_loss: 0.8635
 572/1000 [================>.............] - ETA: 3:13 - loss: 3.5011 - regression_loss: 2.6355 - classification_loss: 0.8656
 573/1000 [================>.............] - ETA: 3:13 - loss: 3.5012 - regression_loss: 2.6353 - classification_loss: 0.8659
 574/1000 [================>.............] - ETA: 3:12 - loss: 3.4951 - regression_loss: 2.6307 - classification_loss: 0.8643
 575/1000 [================>.............] - ETA: 3:12 - loss: 3.4962 - regression_loss: 2.6313 - classification_loss: 0.8649
 576/1000 [================>.............] - ETA: 3:11 - loss: 3.4977 - regression_loss: 2.6326 - classification_loss: 0.8651
 577/1000 [================>.............] - ETA: 3:11 - loss: 3.4917 - regression_loss: 2.6281 - classification_loss: 0.8636
 578/1000 [================>.............] - ETA: 3:10 - loss: 3.4909 - regression_loss: 2.6276 - classification_loss: 0.8633
 579/1000 [================>.............] - ETA: 3:10 - loss: 3.4908 - regression_loss: 2.6283 - classification_loss: 0.8625
 580/1000 [================>.............] - ETA: 3:09 - loss: 3.4918 - regression_loss: 2.6294 - classification_loss: 0.8624
 581/1000 [================>.............] - ETA: 3:09 - loss: 3.4858 - regression_loss: 2.6249 - classification_loss: 0.8609
 582/1000 [================>.............] - ETA: 3:09 - loss: 3.4876 - regression_loss: 2.6265 - classification_loss: 0.8611
 583/1000 [================>.............] - ETA: 3:08 - loss: 3.4880 - regression_loss: 2.6263 - classification_loss: 0.8616
 584/1000 [================>.............] - ETA: 3:08 - loss: 3.4900 - regression_loss: 2.6278 - classification_loss: 0.8622
 585/1000 [================>.............] - ETA: 3:07 - loss: 3.4841 - regression_loss: 2.6233 - classification_loss: 0.8607
 586/1000 [================>.............] - ETA: 3:07 - loss: 3.4861 - regression_loss: 2.6256 - classification_loss: 0.8606
 587/1000 [================>.............] - ETA: 3:06 - loss: 3.4886 - regression_loss: 2.6265 - classification_loss: 0.8621
 588/1000 [================>.............] - ETA: 3:06 - loss: 3.4910 - regression_loss: 2.6279 - classification_loss: 0.8631
 589/1000 [================>.............] - ETA: 3:05 - loss: 3.4851 - regression_loss: 2.6234 - classification_loss: 0.8616
 590/1000 [================>.............] - ETA: 3:05 - loss: 3.4868 - regression_loss: 2.6237 - classification_loss: 0.8631
 591/1000 [================>.............] - ETA: 3:04 - loss: 3.4857 - regression_loss: 2.6233 - classification_loss: 0.8624
 592/1000 [================>.............] - ETA: 3:04 - loss: 3.4798 - regression_loss: 2.6188 - classification_loss: 0.8610
 593/1000 [================>.............] - ETA: 3:04 - loss: 3.4816 - regression_loss: 2.6205 - classification_loss: 0.8611
 594/1000 [================>.............] - ETA: 3:03 - loss: 3.4758 - regression_loss: 2.6161 - classification_loss: 0.8597
 595/1000 [================>.............] - ETA: 3:03 - loss: 3.4699 - regression_loss: 2.6117 - classification_loss: 0.8582
 596/1000 [================>.............] - ETA: 3:02 - loss: 3.4708 - regression_loss: 2.6131 - classification_loss: 0.8577
 597/1000 [================>.............] - ETA: 3:02 - loss: 3.4650 - regression_loss: 2.6087 - classification_loss: 0.8563
 598/1000 [================>.............] - ETA: 3:01 - loss: 3.4671 - regression_loss: 2.6102 - classification_loss: 0.8569
 599/1000 [================>.............] - ETA: 3:01 - loss: 3.4658 - regression_loss: 2.6096 - classification_loss: 0.8562
 600/1000 [=================>............] - ETA: 3:00 - loss: 3.4681 - regression_loss: 2.6118 - classification_loss: 0.8562
 601/1000 [=================>............] - ETA: 3:00 - loss: 3.4711 - regression_loss: 2.6140 - classification_loss: 0.8570
 602/1000 [=================>............] - ETA: 2:59 - loss: 3.4654 - regression_loss: 2.6097 - classification_loss: 0.8557
 603/1000 [=================>............] - ETA: 2:59 - loss: 3.4596 - regression_loss: 2.6054 - classification_loss: 0.8543
 604/1000 [=================>............] - ETA: 2:59 - loss: 3.4639 - regression_loss: 2.6083 - classification_loss: 0.8556
 605/1000 [=================>............] - ETA: 2:58 - loss: 3.4666 - regression_loss: 2.6106 - classification_loss: 0.8560
 606/1000 [=================>............] - ETA: 2:58 - loss: 3.4609 - regression_loss: 2.6063 - classification_loss: 0.8546
 607/1000 [=================>............] - ETA: 2:57 - loss: 3.4650 - regression_loss: 2.6086 - classification_loss: 0.8564
 608/1000 [=================>............] - ETA: 2:57 - loss: 3.4670 - regression_loss: 2.6098 - classification_loss: 0.8572
 609/1000 [=================>............] - ETA: 2:56 - loss: 3.4695 - regression_loss: 2.6119 - classification_loss: 0.8577
 610/1000 [=================>............] - ETA: 2:56 - loss: 3.4716 - regression_loss: 2.6131 - classification_loss: 0.8585
 611/1000 [=================>............] - ETA: 2:55 - loss: 3.4753 - regression_loss: 2.6158 - classification_loss: 0.8594
 612/1000 [=================>............] - ETA: 2:55 - loss: 3.4794 - regression_loss: 2.6181 - classification_loss: 0.8613
 613/1000 [=================>............] - ETA: 2:54 - loss: 3.4823 - regression_loss: 2.6193 - classification_loss: 0.8630
 614/1000 [=================>............] - ETA: 2:54 - loss: 3.4844 - regression_loss: 2.6198 - classification_loss: 0.8646
 615/1000 [=================>............] - ETA: 2:54 - loss: 3.4879 - regression_loss: 2.6217 - classification_loss: 0.8662
 616/1000 [=================>............] - ETA: 2:53 - loss: 3.4879 - regression_loss: 2.6221 - classification_loss: 0.8658
 617/1000 [=================>............] - ETA: 2:53 - loss: 3.4907 - regression_loss: 2.6237 - classification_loss: 0.8670
 618/1000 [=================>............] - ETA: 2:52 - loss: 3.4914 - regression_loss: 2.6249 - classification_loss: 0.8666
 619/1000 [=================>............] - ETA: 2:52 - loss: 3.4931 - regression_loss: 2.6254 - classification_loss: 0.8676
 620/1000 [=================>............] - ETA: 2:51 - loss: 3.4937 - regression_loss: 2.6264 - classification_loss: 0.8674
 621/1000 [=================>............] - ETA: 2:51 - loss: 3.4888 - regression_loss: 2.6221 - classification_loss: 0.8667
 622/1000 [=================>............] - ETA: 2:50 - loss: 3.4893 - regression_loss: 2.6230 - classification_loss: 0.8662
 623/1000 [=================>............] - ETA: 2:50 - loss: 3.4837 - regression_loss: 2.6188 - classification_loss: 0.8649
 624/1000 [=================>............] - ETA: 2:50 - loss: 3.4853 - regression_loss: 2.6195 - classification_loss: 0.8658
 625/1000 [=================>............] - ETA: 2:49 - loss: 3.4873 - regression_loss: 2.6214 - classification_loss: 0.8659
 626/1000 [=================>............] - ETA: 2:49 - loss: 3.4896 - regression_loss: 2.6231 - classification_loss: 0.8665
 627/1000 [=================>............] - ETA: 2:48 - loss: 3.4904 - regression_loss: 2.6241 - classification_loss: 0.8663
 628/1000 [=================>............] - ETA: 2:48 - loss: 3.4918 - regression_loss: 2.6257 - classification_loss: 0.8661
 629/1000 [=================>............] - ETA: 2:47 - loss: 3.4923 - regression_loss: 2.6262 - classification_loss: 0.8661
 630/1000 [=================>............] - ETA: 2:47 - loss: 3.4944 - regression_loss: 2.6285 - classification_loss: 0.8659
 631/1000 [=================>............] - ETA: 2:46 - loss: 3.4941 - regression_loss: 2.6286 - classification_loss: 0.8655
 632/1000 [=================>............] - ETA: 2:46 - loss: 3.4943 - regression_loss: 2.6292 - classification_loss: 0.8651
 633/1000 [=================>............] - ETA: 2:45 - loss: 3.4972 - regression_loss: 2.6316 - classification_loss: 0.8656
 634/1000 [==================>...........] - ETA: 2:45 - loss: 3.5019 - regression_loss: 2.6346 - classification_loss: 0.8674
 635/1000 [==================>...........] - ETA: 2:45 - loss: 3.5016 - regression_loss: 2.6348 - classification_loss: 0.8668
 636/1000 [==================>...........] - ETA: 2:44 - loss: 3.5031 - regression_loss: 2.6362 - classification_loss: 0.8668
 637/1000 [==================>...........] - ETA: 2:44 - loss: 3.5041 - regression_loss: 2.6372 - classification_loss: 0.8669
 638/1000 [==================>...........] - ETA: 2:43 - loss: 3.4989 - regression_loss: 2.6331 - classification_loss: 0.8658
 639/1000 [==================>...........] - ETA: 2:43 - loss: 3.4993 - regression_loss: 2.6336 - classification_loss: 0.8657
 640/1000 [==================>...........] - ETA: 2:42 - loss: 3.5005 - regression_loss: 2.6350 - classification_loss: 0.8655
 641/1000 [==================>...........] - ETA: 2:42 - loss: 3.4954 - regression_loss: 2.6309 - classification_loss: 0.8645
 642/1000 [==================>...........] - ETA: 2:41 - loss: 3.4907 - regression_loss: 2.6268 - classification_loss: 0.8639
 643/1000 [==================>...........] - ETA: 2:41 - loss: 3.4853 - regression_loss: 2.6227 - classification_loss: 0.8626
 644/1000 [==================>...........] - ETA: 2:40 - loss: 3.4800 - regression_loss: 2.6186 - classification_loss: 0.8614
 645/1000 [==================>...........] - ETA: 2:40 - loss: 3.4819 - regression_loss: 2.6202 - classification_loss: 0.8617
 646/1000 [==================>...........] - ETA: 2:40 - loss: 3.4846 - regression_loss: 2.6227 - classification_loss: 0.8618
 647/1000 [==================>...........] - ETA: 2:39 - loss: 3.4857 - regression_loss: 2.6237 - classification_loss: 0.8620
 648/1000 [==================>...........] - ETA: 2:39 - loss: 3.4850 - regression_loss: 2.6233 - classification_loss: 0.8616
 649/1000 [==================>...........] - ETA: 2:38 - loss: 3.4857 - regression_loss: 2.6241 - classification_loss: 0.8616
 650/1000 [==================>...........] - ETA: 2:38 - loss: 3.4803 - regression_loss: 2.6201 - classification_loss: 0.8602
 651/1000 [==================>...........] - ETA: 2:37 - loss: 3.4750 - regression_loss: 2.6161 - classification_loss: 0.8589
 652/1000 [==================>...........] - ETA: 2:37 - loss: 3.4753 - regression_loss: 2.6165 - classification_loss: 0.8588
 653/1000 [==================>...........] - ETA: 2:36 - loss: 3.4745 - regression_loss: 2.6157 - classification_loss: 0.8587
 654/1000 [==================>...........] - ETA: 2:36 - loss: 3.4745 - regression_loss: 2.6152 - classification_loss: 0.8593
 655/1000 [==================>...........] - ETA: 2:36 - loss: 3.4751 - regression_loss: 2.6155 - classification_loss: 0.8596
 656/1000 [==================>...........] - ETA: 2:35 - loss: 3.4752 - regression_loss: 2.6157 - classification_loss: 0.8594
 657/1000 [==================>...........] - ETA: 2:35 - loss: 3.4699 - regression_loss: 2.6118 - classification_loss: 0.8582
 658/1000 [==================>...........] - ETA: 2:34 - loss: 3.4647 - regression_loss: 2.6078 - classification_loss: 0.8569
 659/1000 [==================>...........] - ETA: 2:34 - loss: 3.4693 - regression_loss: 2.6103 - classification_loss: 0.8590
 660/1000 [==================>...........] - ETA: 2:33 - loss: 3.4706 - regression_loss: 2.6116 - classification_loss: 0.8590
 661/1000 [==================>...........] - ETA: 2:33 - loss: 3.4712 - regression_loss: 2.6124 - classification_loss: 0.8588
 662/1000 [==================>...........] - ETA: 2:32 - loss: 3.4660 - regression_loss: 2.6085 - classification_loss: 0.8575
 663/1000 [==================>...........] - ETA: 2:32 - loss: 3.4676 - regression_loss: 2.6095 - classification_loss: 0.8580
 664/1000 [==================>...........] - ETA: 2:31 - loss: 3.4681 - regression_loss: 2.6098 - classification_loss: 0.8582
 665/1000 [==================>...........] - ETA: 2:31 - loss: 3.4709 - regression_loss: 2.6123 - classification_loss: 0.8586
 666/1000 [==================>...........] - ETA: 2:31 - loss: 3.4729 - regression_loss: 2.6132 - classification_loss: 0.8597
 667/1000 [===================>..........] - ETA: 2:30 - loss: 3.4677 - regression_loss: 2.6092 - classification_loss: 0.8585
 668/1000 [===================>..........] - ETA: 2:30 - loss: 3.4625 - regression_loss: 2.6053 - classification_loss: 0.8572
 669/1000 [===================>..........] - ETA: 2:29 - loss: 3.4652 - regression_loss: 2.6066 - classification_loss: 0.8586
 670/1000 [===================>..........] - ETA: 2:29 - loss: 3.4664 - regression_loss: 2.6075 - classification_loss: 0.8589
 671/1000 [===================>..........] - ETA: 2:28 - loss: 3.4669 - regression_loss: 2.6080 - classification_loss: 0.8589
 672/1000 [===================>..........] - ETA: 2:28 - loss: 3.4677 - regression_loss: 2.6087 - classification_loss: 0.8590
 673/1000 [===================>..........] - ETA: 2:27 - loss: 3.4625 - regression_loss: 2.6048 - classification_loss: 0.8577
 674/1000 [===================>..........] - ETA: 2:27 - loss: 3.4635 - regression_loss: 2.6061 - classification_loss: 0.8574
 675/1000 [===================>..........] - ETA: 2:26 - loss: 3.4583 - regression_loss: 2.6022 - classification_loss: 0.8561
 676/1000 [===================>..........] - ETA: 2:26 - loss: 3.4595 - regression_loss: 2.6033 - classification_loss: 0.8563
 677/1000 [===================>..........] - ETA: 2:26 - loss: 3.4589 - regression_loss: 2.6028 - classification_loss: 0.8561
 678/1000 [===================>..........] - ETA: 2:25 - loss: 3.4596 - regression_loss: 2.6027 - classification_loss: 0.8569
 679/1000 [===================>..........] - ETA: 2:25 - loss: 3.4603 - regression_loss: 2.6037 - classification_loss: 0.8566
 680/1000 [===================>..........] - ETA: 2:24 - loss: 3.4552 - regression_loss: 2.5999 - classification_loss: 0.8553
 681/1000 [===================>..........] - ETA: 2:24 - loss: 3.4589 - regression_loss: 2.6017 - classification_loss: 0.8572
 682/1000 [===================>..........] - ETA: 2:23 - loss: 3.4594 - regression_loss: 2.6022 - classification_loss: 0.8571
 683/1000 [===================>..........] - ETA: 2:23 - loss: 3.4543 - regression_loss: 2.5984 - classification_loss: 0.8559
 684/1000 [===================>..........] - ETA: 2:22 - loss: 3.4557 - regression_loss: 2.5987 - classification_loss: 0.8570
 685/1000 [===================>..........] - ETA: 2:22 - loss: 3.4584 - regression_loss: 2.6018 - classification_loss: 0.8566
 686/1000 [===================>..........] - ETA: 2:22 - loss: 3.4597 - regression_loss: 2.6033 - classification_loss: 0.8564
 687/1000 [===================>..........] - ETA: 2:21 - loss: 3.4628 - regression_loss: 2.6064 - classification_loss: 0.8564
 688/1000 [===================>..........] - ETA: 2:21 - loss: 3.4624 - regression_loss: 2.6064 - classification_loss: 0.8560
 689/1000 [===================>..........] - ETA: 2:20 - loss: 3.4623 - regression_loss: 2.6066 - classification_loss: 0.8556
 690/1000 [===================>..........] - ETA: 2:20 - loss: 3.4625 - regression_loss: 2.6029 - classification_loss: 0.8596
 691/1000 [===================>..........] - ETA: 2:19 - loss: 3.4636 - regression_loss: 2.6033 - classification_loss: 0.8603
 692/1000 [===================>..........] - ETA: 2:19 - loss: 3.4633 - regression_loss: 2.6031 - classification_loss: 0.8602
 693/1000 [===================>..........] - ETA: 2:18 - loss: 3.4655 - regression_loss: 2.6047 - classification_loss: 0.8608
 694/1000 [===================>..........] - ETA: 2:18 - loss: 3.4679 - regression_loss: 2.6059 - classification_loss: 0.8621
 695/1000 [===================>..........] - ETA: 2:17 - loss: 3.4693 - regression_loss: 2.6070 - classification_loss: 0.8623
 696/1000 [===================>..........] - ETA: 2:17 - loss: 3.4709 - regression_loss: 2.6078 - classification_loss: 0.8630
 697/1000 [===================>..........] - ETA: 2:17 - loss: 3.4711 - regression_loss: 2.6084 - classification_loss: 0.8628
 698/1000 [===================>..........] - ETA: 2:16 - loss: 3.4662 - regression_loss: 2.6046 - classification_loss: 0.8615
 699/1000 [===================>..........] - ETA: 2:16 - loss: 3.4657 - regression_loss: 2.6042 - classification_loss: 0.8615
 700/1000 [====================>.........] - ETA: 2:15 - loss: 3.4679 - regression_loss: 2.6052 - classification_loss: 0.8626
 701/1000 [====================>.........] - ETA: 2:15 - loss: 3.4679 - regression_loss: 2.6056 - classification_loss: 0.8624
 702/1000 [====================>.........] - ETA: 2:14 - loss: 3.4698 - regression_loss: 2.6066 - classification_loss: 0.8633
 703/1000 [====================>.........] - ETA: 2:14 - loss: 3.4705 - regression_loss: 2.6071 - classification_loss: 0.8634
 704/1000 [====================>.........] - ETA: 2:13 - loss: 3.4726 - regression_loss: 2.6088 - classification_loss: 0.8638
 705/1000 [====================>.........] - ETA: 2:13 - loss: 3.4759 - regression_loss: 2.6112 - classification_loss: 0.8647
 706/1000 [====================>.........] - ETA: 2:12 - loss: 3.4711 - regression_loss: 2.6075 - classification_loss: 0.8635
 707/1000 [====================>.........] - ETA: 2:12 - loss: 3.4662 - regression_loss: 2.6038 - classification_loss: 0.8623
 708/1000 [====================>.........] - ETA: 2:12 - loss: 3.4613 - regression_loss: 2.6002 - classification_loss: 0.8611
 709/1000 [====================>.........] - ETA: 2:11 - loss: 3.4626 - regression_loss: 2.6011 - classification_loss: 0.8616
 710/1000 [====================>.........] - ETA: 2:11 - loss: 3.4635 - regression_loss: 2.6016 - classification_loss: 0.8619
 711/1000 [====================>.........] - ETA: 2:10 - loss: 3.4651 - regression_loss: 2.6030 - classification_loss: 0.8621
 712/1000 [====================>.........] - ETA: 2:10 - loss: 3.4662 - regression_loss: 2.6043 - classification_loss: 0.8618
 713/1000 [====================>.........] - ETA: 2:09 - loss: 3.4685 - regression_loss: 2.6062 - classification_loss: 0.8623
 714/1000 [====================>.........] - ETA: 2:09 - loss: 3.4697 - regression_loss: 2.6063 - classification_loss: 0.8634
 715/1000 [====================>.........] - ETA: 2:08 - loss: 3.4708 - regression_loss: 2.6076 - classification_loss: 0.8632
 716/1000 [====================>.........] - ETA: 2:08 - loss: 3.4718 - regression_loss: 2.6079 - classification_loss: 0.8639
 717/1000 [====================>.........] - ETA: 2:07 - loss: 3.4721 - regression_loss: 2.6079 - classification_loss: 0.8643
 718/1000 [====================>.........] - ETA: 2:07 - loss: 3.4735 - regression_loss: 2.6096 - classification_loss: 0.8639
 719/1000 [====================>.........] - ETA: 2:07 - loss: 3.4763 - regression_loss: 2.6113 - classification_loss: 0.8649
 720/1000 [====================>.........] - ETA: 2:06 - loss: 3.4777 - regression_loss: 2.6128 - classification_loss: 0.8648
 721/1000 [====================>.........] - ETA: 2:06 - loss: 3.4784 - regression_loss: 2.6140 - classification_loss: 0.8644
 722/1000 [====================>.........] - ETA: 2:05 - loss: 3.4736 - regression_loss: 2.6103 - classification_loss: 0.8632
 723/1000 [====================>.........] - ETA: 2:05 - loss: 3.4738 - regression_loss: 2.6109 - classification_loss: 0.8629
 724/1000 [====================>.........] - ETA: 2:04 - loss: 3.4752 - regression_loss: 2.6128 - classification_loss: 0.8624
 725/1000 [====================>.........] - ETA: 2:04 - loss: 3.4704 - regression_loss: 2.6092 - classification_loss: 0.8612
 726/1000 [====================>.........] - ETA: 2:03 - loss: 3.4714 - regression_loss: 2.6107 - classification_loss: 0.8607
 727/1000 [====================>.........] - ETA: 2:03 - loss: 3.4667 - regression_loss: 2.6071 - classification_loss: 0.8596
 728/1000 [====================>.........] - ETA: 2:02 - loss: 3.4691 - regression_loss: 2.6085 - classification_loss: 0.8606
 729/1000 [====================>.........] - ETA: 2:02 - loss: 3.4718 - regression_loss: 2.6110 - classification_loss: 0.8608
 730/1000 [====================>.........] - ETA: 2:02 - loss: 3.4671 - regression_loss: 2.6074 - classification_loss: 0.8596
 731/1000 [====================>.........] - ETA: 2:01 - loss: 3.4623 - regression_loss: 2.6039 - classification_loss: 0.8585
 732/1000 [====================>.........] - ETA: 2:01 - loss: 3.4576 - regression_loss: 2.6003 - classification_loss: 0.8573
 733/1000 [====================>.........] - ETA: 2:00 - loss: 3.4586 - regression_loss: 2.6015 - classification_loss: 0.8571
 734/1000 [=====================>........] - ETA: 2:00 - loss: 3.4596 - regression_loss: 2.6025 - classification_loss: 0.8570
 735/1000 [=====================>........] - ETA: 1:59 - loss: 3.4604 - regression_loss: 2.6037 - classification_loss: 0.8567
 736/1000 [=====================>........] - ETA: 1:59 - loss: 3.4633 - regression_loss: 2.6057 - classification_loss: 0.8576
 737/1000 [=====================>........] - ETA: 1:58 - loss: 3.4586 - regression_loss: 2.6022 - classification_loss: 0.8564
 738/1000 [=====================>........] - ETA: 1:58 - loss: 3.4598 - regression_loss: 2.6023 - classification_loss: 0.8574
 739/1000 [=====================>........] - ETA: 1:58 - loss: 3.4592 - regression_loss: 2.6023 - classification_loss: 0.8569
 740/1000 [=====================>........] - ETA: 1:57 - loss: 3.4546 - regression_loss: 2.5988 - classification_loss: 0.8559
 741/1000 [=====================>........] - ETA: 1:57 - loss: 3.4500 - regression_loss: 2.5953 - classification_loss: 0.8547
 742/1000 [=====================>........] - ETA: 1:56 - loss: 3.4453 - regression_loss: 2.5918 - classification_loss: 0.8535
 743/1000 [=====================>........] - ETA: 1:56 - loss: 3.4479 - regression_loss: 2.5928 - classification_loss: 0.8551
 744/1000 [=====================>........] - ETA: 1:55 - loss: 3.4488 - regression_loss: 2.5932 - classification_loss: 0.8556
 745/1000 [=====================>........] - ETA: 1:55 - loss: 3.4441 - regression_loss: 2.5897 - classification_loss: 0.8544
 746/1000 [=====================>........] - ETA: 1:54 - loss: 3.4458 - regression_loss: 2.5897 - classification_loss: 0.8561
 747/1000 [=====================>........] - ETA: 1:54 - loss: 3.4482 - regression_loss: 2.5906 - classification_loss: 0.8576
 748/1000 [=====================>........] - ETA: 1:53 - loss: 3.4487 - regression_loss: 2.5910 - classification_loss: 0.8577
 749/1000 [=====================>........] - ETA: 1:53 - loss: 3.4499 - regression_loss: 2.5912 - classification_loss: 0.8588
 750/1000 [=====================>........] - ETA: 1:53 - loss: 3.4460 - regression_loss: 2.5877 - classification_loss: 0.8583
 751/1000 [=====================>........] - ETA: 1:52 - loss: 3.4473 - regression_loss: 2.5886 - classification_loss: 0.8587
 752/1000 [=====================>........] - ETA: 1:52 - loss: 3.4489 - regression_loss: 2.5890 - classification_loss: 0.8600
 753/1000 [=====================>........] - ETA: 1:51 - loss: 3.4505 - regression_loss: 2.5892 - classification_loss: 0.8614
 754/1000 [=====================>........] - ETA: 1:51 - loss: 3.4547 - regression_loss: 2.5927 - classification_loss: 0.8620
 755/1000 [=====================>........] - ETA: 1:50 - loss: 3.4555 - regression_loss: 2.5930 - classification_loss: 0.8625
 756/1000 [=====================>........] - ETA: 1:50 - loss: 3.4573 - regression_loss: 2.5937 - classification_loss: 0.8636
 757/1000 [=====================>........] - ETA: 1:49 - loss: 3.4585 - regression_loss: 2.5941 - classification_loss: 0.8644
 758/1000 [=====================>........] - ETA: 1:49 - loss: 3.4605 - regression_loss: 2.5952 - classification_loss: 0.8653
 759/1000 [=====================>........] - ETA: 1:48 - loss: 3.4560 - regression_loss: 2.5918 - classification_loss: 0.8642
 760/1000 [=====================>........] - ETA: 1:48 - loss: 3.4565 - regression_loss: 2.5917 - classification_loss: 0.8649
 761/1000 [=====================>........] - ETA: 1:48 - loss: 3.4585 - regression_loss: 2.5934 - classification_loss: 0.8651
 762/1000 [=====================>........] - ETA: 1:47 - loss: 3.4580 - regression_loss: 2.5933 - classification_loss: 0.8647
 763/1000 [=====================>........] - ETA: 1:47 - loss: 3.4584 - regression_loss: 2.5930 - classification_loss: 0.8654
 764/1000 [=====================>........] - ETA: 1:46 - loss: 3.4540 - regression_loss: 2.5896 - classification_loss: 0.8644
 765/1000 [=====================>........] - ETA: 1:46 - loss: 3.4526 - regression_loss: 2.5886 - classification_loss: 0.8639
 766/1000 [=====================>........] - ETA: 1:45 - loss: 3.4549 - regression_loss: 2.5902 - classification_loss: 0.8648
 767/1000 [======================>.......] - ETA: 1:45 - loss: 3.4566 - regression_loss: 2.5913 - classification_loss: 0.8653
 768/1000 [======================>.......] - ETA: 1:44 - loss: 3.4570 - regression_loss: 2.5915 - classification_loss: 0.8654
 769/1000 [======================>.......] - ETA: 1:44 - loss: 3.4578 - regression_loss: 2.5919 - classification_loss: 0.8659
 770/1000 [======================>.......] - ETA: 1:44 - loss: 3.4533 - regression_loss: 2.5885 - classification_loss: 0.8648
 771/1000 [======================>.......] - ETA: 1:43 - loss: 3.4535 - regression_loss: 2.5892 - classification_loss: 0.8644
 772/1000 [======================>.......] - ETA: 1:43 - loss: 3.4538 - regression_loss: 2.5897 - classification_loss: 0.8641
 773/1000 [======================>.......] - ETA: 1:42 - loss: 3.4549 - regression_loss: 2.5901 - classification_loss: 0.8648
 774/1000 [======================>.......] - ETA: 1:42 - loss: 3.4504 - regression_loss: 2.5868 - classification_loss: 0.8636
 775/1000 [======================>.......] - ETA: 1:41 - loss: 3.4522 - regression_loss: 2.5885 - classification_loss: 0.8636
 776/1000 [======================>.......] - ETA: 1:41 - loss: 3.4544 - regression_loss: 2.5896 - classification_loss: 0.8648
 777/1000 [======================>.......] - ETA: 1:40 - loss: 3.4554 - regression_loss: 2.5910 - classification_loss: 0.8645
 778/1000 [======================>.......] - ETA: 1:40 - loss: 3.4564 - regression_loss: 2.5922 - classification_loss: 0.8642
 779/1000 [======================>.......] - ETA: 1:39 - loss: 3.4570 - regression_loss: 2.5931 - classification_loss: 0.8639
 780/1000 [======================>.......] - ETA: 1:39 - loss: 3.4574 - regression_loss: 2.5939 - classification_loss: 0.8634
 781/1000 [======================>.......] - ETA: 1:39 - loss: 3.4571 - regression_loss: 2.5938 - classification_loss: 0.8633
 782/1000 [======================>.......] - ETA: 1:38 - loss: 3.4584 - regression_loss: 2.5949 - classification_loss: 0.8635
 783/1000 [======================>.......] - ETA: 1:38 - loss: 3.4541 - regression_loss: 2.5915 - classification_loss: 0.8625
 784/1000 [======================>.......] - ETA: 1:37 - loss: 3.4558 - regression_loss: 2.5924 - classification_loss: 0.8634
 785/1000 [======================>.......] - ETA: 1:37 - loss: 3.4561 - regression_loss: 2.5928 - classification_loss: 0.8633
 786/1000 [======================>.......] - ETA: 1:36 - loss: 3.4580 - regression_loss: 2.5938 - classification_loss: 0.8642
 787/1000 [======================>.......] - ETA: 1:36 - loss: 3.4537 - regression_loss: 2.5905 - classification_loss: 0.8631
 788/1000 [======================>.......] - ETA: 1:35 - loss: 3.4539 - regression_loss: 2.5910 - classification_loss: 0.8629
 789/1000 [======================>.......] - ETA: 1:35 - loss: 3.4496 - regression_loss: 2.5877 - classification_loss: 0.8619
 790/1000 [======================>.......] - ETA: 1:34 - loss: 3.4501 - regression_loss: 2.5880 - classification_loss: 0.8621
 791/1000 [======================>.......] - ETA: 1:34 - loss: 3.4521 - regression_loss: 2.5893 - classification_loss: 0.8628
 792/1000 [======================>.......] - ETA: 1:34 - loss: 3.4515 - regression_loss: 2.5890 - classification_loss: 0.8625
 793/1000 [======================>.......] - ETA: 1:33 - loss: 3.4522 - regression_loss: 2.5901 - classification_loss: 0.8621
 794/1000 [======================>.......] - ETA: 1:33 - loss: 3.4536 - regression_loss: 2.5914 - classification_loss: 0.8622
 795/1000 [======================>.......] - ETA: 1:32 - loss: 3.4493 - regression_loss: 2.5881 - classification_loss: 0.8612
 796/1000 [======================>.......] - ETA: 1:32 - loss: 3.4502 - regression_loss: 2.5894 - classification_loss: 0.8608
 797/1000 [======================>.......] - ETA: 1:31 - loss: 3.4460 - regression_loss: 2.5862 - classification_loss: 0.8598
 798/1000 [======================>.......] - ETA: 1:31 - loss: 3.4463 - regression_loss: 2.5867 - classification_loss: 0.8595
 799/1000 [======================>.......] - ETA: 1:30 - loss: 3.4419 - regression_loss: 2.5835 - classification_loss: 0.8585
 800/1000 [=======================>......] - ETA: 1:30 - loss: 3.4376 - regression_loss: 2.5803 - classification_loss: 0.8574
 801/1000 [=======================>......] - ETA: 1:30 - loss: 3.4416 - regression_loss: 2.5821 - classification_loss: 0.8595
 802/1000 [=======================>......] - ETA: 1:29 - loss: 3.4436 - regression_loss: 2.5827 - classification_loss: 0.8609
 803/1000 [=======================>......] - ETA: 1:29 - loss: 3.4431 - regression_loss: 2.5822 - classification_loss: 0.8609
 804/1000 [=======================>......] - ETA: 1:28 - loss: 3.4433 - regression_loss: 2.5810 - classification_loss: 0.8623
 805/1000 [=======================>......] - ETA: 1:28 - loss: 3.4391 - regression_loss: 2.5778 - classification_loss: 0.8613
 806/1000 [=======================>......] - ETA: 1:27 - loss: 3.4407 - regression_loss: 2.5786 - classification_loss: 0.8621
 807/1000 [=======================>......] - ETA: 1:27 - loss: 3.4406 - regression_loss: 2.5788 - classification_loss: 0.8618
 808/1000 [=======================>......] - ETA: 1:26 - loss: 3.4364 - regression_loss: 2.5756 - classification_loss: 0.8607
 809/1000 [=======================>......] - ETA: 1:26 - loss: 3.4378 - regression_loss: 2.5767 - classification_loss: 0.8611
 810/1000 [=======================>......] - ETA: 1:25 - loss: 3.4336 - regression_loss: 2.5735 - classification_loss: 0.8600
 811/1000 [=======================>......] - ETA: 1:25 - loss: 3.4340 - regression_loss: 2.5743 - classification_loss: 0.8597
 812/1000 [=======================>......] - ETA: 1:25 - loss: 3.4340 - regression_loss: 2.5744 - classification_loss: 0.8596
 813/1000 [=======================>......] - ETA: 1:24 - loss: 3.4359 - regression_loss: 2.5751 - classification_loss: 0.8608
 814/1000 [=======================>......] - ETA: 1:24 - loss: 3.4319 - regression_loss: 2.5719 - classification_loss: 0.8600
 815/1000 [=======================>......] - ETA: 1:23 - loss: 3.4309 - regression_loss: 2.5715 - classification_loss: 0.8594
 816/1000 [=======================>......] - ETA: 1:23 - loss: 3.4343 - regression_loss: 2.5735 - classification_loss: 0.8609
 817/1000 [=======================>......] - ETA: 1:22 - loss: 3.4301 - regression_loss: 2.5703 - classification_loss: 0.8598
 818/1000 [=======================>......] - ETA: 1:22 - loss: 3.4260 - regression_loss: 2.5672 - classification_loss: 0.8588
 819/1000 [=======================>......] - ETA: 1:21 - loss: 3.4274 - regression_loss: 2.5679 - classification_loss: 0.8594
 820/1000 [=======================>......] - ETA: 1:21 - loss: 3.4291 - regression_loss: 2.5683 - classification_loss: 0.8608
 821/1000 [=======================>......] - ETA: 1:20 - loss: 3.4304 - regression_loss: 2.5681 - classification_loss: 0.8623
 822/1000 [=======================>......] - ETA: 1:20 - loss: 3.4338 - regression_loss: 2.5700 - classification_loss: 0.8638
 823/1000 [=======================>......] - ETA: 1:20 - loss: 3.4333 - regression_loss: 2.5693 - classification_loss: 0.8640
 824/1000 [=======================>......] - ETA: 1:19 - loss: 3.4346 - regression_loss: 2.5695 - classification_loss: 0.8651
 825/1000 [=======================>......] - ETA: 1:19 - loss: 3.4304 - regression_loss: 2.5664 - classification_loss: 0.8640
 826/1000 [=======================>......] - ETA: 1:18 - loss: 3.4263 - regression_loss: 2.5633 - classification_loss: 0.8630
 827/1000 [=======================>......] - ETA: 1:18 - loss: 3.4278 - regression_loss: 2.5638 - classification_loss: 0.8641
 828/1000 [=======================>......] - ETA: 1:17 - loss: 3.4296 - regression_loss: 2.5646 - classification_loss: 0.8649
 829/1000 [=======================>......] - ETA: 1:17 - loss: 3.4296 - regression_loss: 2.5640 - classification_loss: 0.8655
 830/1000 [=======================>......] - ETA: 1:16 - loss: 3.4317 - regression_loss: 2.5649 - classification_loss: 0.8668
 831/1000 [=======================>......] - ETA: 1:16 - loss: 3.4276 - regression_loss: 2.5618 - classification_loss: 0.8658
 832/1000 [=======================>......] - ETA: 1:15 - loss: 3.4300 - regression_loss: 2.5643 - classification_loss: 0.8657
 833/1000 [=======================>......] - ETA: 1:15 - loss: 3.4259 - regression_loss: 2.5612 - classification_loss: 0.8647
 834/1000 [========================>.....] - ETA: 1:15 - loss: 3.4292 - regression_loss: 2.5629 - classification_loss: 0.8663
 835/1000 [========================>.....] - ETA: 1:14 - loss: 3.4253 - regression_loss: 2.5598 - classification_loss: 0.8654
 836/1000 [========================>.....] - ETA: 1:14 - loss: 3.4272 - regression_loss: 2.5608 - classification_loss: 0.8664
 837/1000 [========================>.....] - ETA: 1:13 - loss: 3.4297 - regression_loss: 2.5624 - classification_loss: 0.8673
 838/1000 [========================>.....] - ETA: 1:13 - loss: 3.4319 - regression_loss: 2.5645 - classification_loss: 0.8673
 839/1000 [========================>.....] - ETA: 1:12 - loss: 3.4331 - regression_loss: 2.5651 - classification_loss: 0.8679
 840/1000 [========================>.....] - ETA: 1:12 - loss: 3.4336 - regression_loss: 2.5659 - classification_loss: 0.8677
 841/1000 [========================>.....] - ETA: 1:11 - loss: 3.4350 - regression_loss: 2.5671 - classification_loss: 0.8679
 842/1000 [========================>.....] - ETA: 1:11 - loss: 3.4360 - regression_loss: 2.5674 - classification_loss: 0.8686
 843/1000 [========================>.....] - ETA: 1:11 - loss: 3.4349 - regression_loss: 2.5666 - classification_loss: 0.8683
 844/1000 [========================>.....] - ETA: 1:10 - loss: 3.4366 - regression_loss: 2.5679 - classification_loss: 0.8687
 845/1000 [========================>.....] - ETA: 1:10 - loss: 3.4325 - regression_loss: 2.5648 - classification_loss: 0.8677
 846/1000 [========================>.....] - ETA: 1:09 - loss: 3.4337 - regression_loss: 2.5660 - classification_loss: 0.8677
 847/1000 [========================>.....] - ETA: 1:09 - loss: 3.4346 - regression_loss: 2.5661 - classification_loss: 0.8684
 848/1000 [========================>.....] - ETA: 1:08 - loss: 3.4341 - regression_loss: 2.5664 - classification_loss: 0.8678
 849/1000 [========================>.....] - ETA: 1:08 - loss: 3.4367 - regression_loss: 2.5685 - classification_loss: 0.8682
 850/1000 [========================>.....] - ETA: 1:07 - loss: 3.4371 - regression_loss: 2.5689 - classification_loss: 0.8683
 851/1000 [========================>.....] - ETA: 1:07 - loss: 3.4402 - regression_loss: 2.5720 - classification_loss: 0.8682
 852/1000 [========================>.....] - ETA: 1:06 - loss: 3.4426 - regression_loss: 2.5734 - classification_loss: 0.8692
 853/1000 [========================>.....] - ETA: 1:06 - loss: 3.4439 - regression_loss: 2.5748 - classification_loss: 0.8692
 854/1000 [========================>.....] - ETA: 1:06 - loss: 3.4399 - regression_loss: 2.5717 - classification_loss: 0.8681
 855/1000 [========================>.....] - ETA: 1:05 - loss: 3.4412 - regression_loss: 2.5687 - classification_loss: 0.8725
 856/1000 [========================>.....] - ETA: 1:05 - loss: 3.4421 - regression_loss: 2.5692 - classification_loss: 0.8729
 857/1000 [========================>.....] - ETA: 1:04 - loss: 3.4421 - regression_loss: 2.5694 - classification_loss: 0.8726
 858/1000 [========================>.....] - ETA: 1:04 - loss: 3.4413 - regression_loss: 2.5693 - classification_loss: 0.8721
 859/1000 [========================>.....] - ETA: 1:03 - loss: 3.4410 - regression_loss: 2.5695 - classification_loss: 0.8715
 860/1000 [========================>.....] - ETA: 1:03 - loss: 3.4370 - regression_loss: 2.5665 - classification_loss: 0.8705
 861/1000 [========================>.....] - ETA: 1:02 - loss: 3.4381 - regression_loss: 2.5678 - classification_loss: 0.8704
 862/1000 [========================>.....] - ETA: 1:02 - loss: 3.4388 - regression_loss: 2.5686 - classification_loss: 0.8702
 863/1000 [========================>.....] - ETA: 1:01 - loss: 3.4388 - regression_loss: 2.5684 - classification_loss: 0.8704
 864/1000 [========================>.....] - ETA: 1:01 - loss: 3.4351 - regression_loss: 2.5655 - classification_loss: 0.8696
 865/1000 [========================>.....] - ETA: 1:01 - loss: 3.4370 - regression_loss: 2.5669 - classification_loss: 0.8701
 866/1000 [========================>.....] - ETA: 1:00 - loss: 3.4377 - regression_loss: 2.5670 - classification_loss: 0.8707
 867/1000 [=========================>....] - ETA: 1:00 - loss: 3.4338 - regression_loss: 2.5641 - classification_loss: 0.8697
 868/1000 [=========================>....] - ETA: 59s - loss: 3.4333 - regression_loss: 2.5638 - classification_loss: 0.8695 
 869/1000 [=========================>....] - ETA: 59s - loss: 3.4350 - regression_loss: 2.5647 - classification_loss: 0.8703
 870/1000 [=========================>....] - ETA: 58s - loss: 3.4357 - regression_loss: 2.5647 - classification_loss: 0.8710
 871/1000 [=========================>....] - ETA: 58s - loss: 3.4353 - regression_loss: 2.5647 - classification_loss: 0.8707
 872/1000 [=========================>....] - ETA: 57s - loss: 3.4383 - regression_loss: 2.5676 - classification_loss: 0.8707
 873/1000 [=========================>....] - ETA: 57s - loss: 3.4392 - regression_loss: 2.5689 - classification_loss: 0.8704
 874/1000 [=========================>....] - ETA: 56s - loss: 3.4398 - regression_loss: 2.5698 - classification_loss: 0.8700
 875/1000 [=========================>....] - ETA: 56s - loss: 3.4359 - regression_loss: 2.5669 - classification_loss: 0.8691
 876/1000 [=========================>....] - ETA: 56s - loss: 3.4366 - regression_loss: 2.5677 - classification_loss: 0.8688
 877/1000 [=========================>....] - ETA: 55s - loss: 3.4374 - regression_loss: 2.5689 - classification_loss: 0.8685
 878/1000 [=========================>....] - ETA: 55s - loss: 3.4389 - regression_loss: 2.5702 - classification_loss: 0.8687
 879/1000 [=========================>....] - ETA: 54s - loss: 3.4402 - regression_loss: 2.5711 - classification_loss: 0.8691
 880/1000 [=========================>....] - ETA: 54s - loss: 3.4363 - regression_loss: 2.5682 - classification_loss: 0.8681
 881/1000 [=========================>....] - ETA: 53s - loss: 3.4375 - regression_loss: 2.5697 - classification_loss: 0.8678
 882/1000 [=========================>....] - ETA: 53s - loss: 3.4338 - regression_loss: 2.5668 - classification_loss: 0.8670
 883/1000 [=========================>....] - ETA: 52s - loss: 3.4299 - regression_loss: 2.5639 - classification_loss: 0.8660
 884/1000 [=========================>....] - ETA: 52s - loss: 3.4315 - regression_loss: 2.5650 - classification_loss: 0.8665
 885/1000 [=========================>....] - ETA: 52s - loss: 3.4329 - regression_loss: 2.5660 - classification_loss: 0.8669
 886/1000 [=========================>....] - ETA: 51s - loss: 3.4290 - regression_loss: 2.5631 - classification_loss: 0.8659
 887/1000 [=========================>....] - ETA: 51s - loss: 3.4251 - regression_loss: 2.5602 - classification_loss: 0.8649
 888/1000 [=========================>....] - ETA: 50s - loss: 3.4255 - regression_loss: 2.5604 - classification_loss: 0.8651
 889/1000 [=========================>....] - ETA: 50s - loss: 3.4261 - regression_loss: 2.5610 - classification_loss: 0.8651
 890/1000 [=========================>....] - ETA: 49s - loss: 3.4276 - regression_loss: 2.5622 - classification_loss: 0.8653
 891/1000 [=========================>....] - ETA: 49s - loss: 3.4278 - regression_loss: 2.5626 - classification_loss: 0.8651
 892/1000 [=========================>....] - ETA: 48s - loss: 3.4293 - regression_loss: 2.5637 - classification_loss: 0.8657
 893/1000 [=========================>....] - ETA: 48s - loss: 3.4255 - regression_loss: 2.5608 - classification_loss: 0.8647
 894/1000 [=========================>....] - ETA: 47s - loss: 3.4311 - regression_loss: 2.5648 - classification_loss: 0.8663
 895/1000 [=========================>....] - ETA: 47s - loss: 3.4325 - regression_loss: 2.5651 - classification_loss: 0.8675
 896/1000 [=========================>....] - ETA: 47s - loss: 3.4331 - regression_loss: 2.5656 - classification_loss: 0.8674
 897/1000 [=========================>....] - ETA: 46s - loss: 3.4334 - regression_loss: 2.5660 - classification_loss: 0.8673
 898/1000 [=========================>....] - ETA: 46s - loss: 3.4295 - regression_loss: 2.5632 - classification_loss: 0.8664
 899/1000 [=========================>....] - ETA: 45s - loss: 3.4258 - regression_loss: 2.5603 - classification_loss: 0.8654
 900/1000 [==========================>...] - ETA: 45s - loss: 3.4269 - regression_loss: 2.5603 - classification_loss: 0.8667
 901/1000 [==========================>...] - ETA: 44s - loss: 3.4286 - regression_loss: 2.5611 - classification_loss: 0.8675
 902/1000 [==========================>...] - ETA: 44s - loss: 3.4289 - regression_loss: 2.5616 - classification_loss: 0.8673
 903/1000 [==========================>...] - ETA: 43s - loss: 3.4251 - regression_loss: 2.5588 - classification_loss: 0.8664
 904/1000 [==========================>...] - ETA: 43s - loss: 3.4255 - regression_loss: 2.5592 - classification_loss: 0.8663
 905/1000 [==========================>...] - ETA: 42s - loss: 3.4217 - regression_loss: 2.5564 - classification_loss: 0.8653
 906/1000 [==========================>...] - ETA: 42s - loss: 3.4230 - regression_loss: 2.5577 - classification_loss: 0.8653
 907/1000 [==========================>...] - ETA: 42s - loss: 3.4263 - regression_loss: 2.5600 - classification_loss: 0.8663
 908/1000 [==========================>...] - ETA: 41s - loss: 3.4263 - regression_loss: 2.5603 - classification_loss: 0.8661
 909/1000 [==========================>...] - ETA: 41s - loss: 3.4226 - regression_loss: 2.5574 - classification_loss: 0.8651
 910/1000 [==========================>...] - ETA: 40s - loss: 3.4267 - regression_loss: 2.5608 - classification_loss: 0.8659
 911/1000 [==========================>...] - ETA: 40s - loss: 3.4292 - regression_loss: 2.5637 - classification_loss: 0.8655
 912/1000 [==========================>...] - ETA: 39s - loss: 3.4316 - regression_loss: 2.5650 - classification_loss: 0.8666
 913/1000 [==========================>...] - ETA: 39s - loss: 3.4333 - regression_loss: 2.5665 - classification_loss: 0.8667
 914/1000 [==========================>...] - ETA: 38s - loss: 3.4347 - regression_loss: 2.5670 - classification_loss: 0.8677
 915/1000 [==========================>...] - ETA: 38s - loss: 3.4310 - regression_loss: 2.5642 - classification_loss: 0.8668
 916/1000 [==========================>...] - ETA: 37s - loss: 3.4272 - regression_loss: 2.5614 - classification_loss: 0.8658
 917/1000 [==========================>...] - ETA: 37s - loss: 3.4288 - regression_loss: 2.5623 - classification_loss: 0.8665
 918/1000 [==========================>...] - ETA: 37s - loss: 3.4293 - regression_loss: 2.5631 - classification_loss: 0.8661
 919/1000 [==========================>...] - ETA: 36s - loss: 3.4297 - regression_loss: 2.5641 - classification_loss: 0.8656
 920/1000 [==========================>...] - ETA: 36s - loss: 3.4308 - regression_loss: 2.5646 - classification_loss: 0.8662
 921/1000 [==========================>...] - ETA: 35s - loss: 3.4311 - regression_loss: 2.5653 - classification_loss: 0.8659
 922/1000 [==========================>...] - ETA: 35s - loss: 3.4339 - regression_loss: 2.5669 - classification_loss: 0.8671
 923/1000 [==========================>...] - ETA: 34s - loss: 3.4335 - regression_loss: 2.5668 - classification_loss: 0.8667
 924/1000 [==========================>...] - ETA: 34s - loss: 3.4344 - regression_loss: 2.5675 - classification_loss: 0.8669
 925/1000 [==========================>...] - ETA: 33s - loss: 3.4345 - regression_loss: 2.5678 - classification_loss: 0.8666
 926/1000 [==========================>...] - ETA: 33s - loss: 3.4358 - regression_loss: 2.5690 - classification_loss: 0.8668
 927/1000 [==========================>...] - ETA: 33s - loss: 3.4371 - regression_loss: 2.5706 - classification_loss: 0.8665
 928/1000 [==========================>...] - ETA: 32s - loss: 3.4376 - regression_loss: 2.5709 - classification_loss: 0.8667
 929/1000 [==========================>...] - ETA: 32s - loss: 3.4372 - regression_loss: 2.5703 - classification_loss: 0.8669
 930/1000 [==========================>...] - ETA: 31s - loss: 3.4335 - regression_loss: 2.5676 - classification_loss: 0.8660
 931/1000 [==========================>...] - ETA: 31s - loss: 3.4335 - regression_loss: 2.5674 - classification_loss: 0.8661
 932/1000 [==========================>...] - ETA: 30s - loss: 3.4333 - regression_loss: 2.5675 - classification_loss: 0.8659
 933/1000 [==========================>...] - ETA: 30s - loss: 3.4338 - regression_loss: 2.5682 - classification_loss: 0.8656
 934/1000 [===========================>..] - ETA: 29s - loss: 3.4330 - regression_loss: 2.5680 - classification_loss: 0.8649
 935/1000 [===========================>..] - ETA: 29s - loss: 3.4342 - regression_loss: 2.5686 - classification_loss: 0.8656
 936/1000 [===========================>..] - ETA: 28s - loss: 3.4361 - regression_loss: 2.5698 - classification_loss: 0.8663
 937/1000 [===========================>..] - ETA: 28s - loss: 3.4373 - regression_loss: 2.5709 - classification_loss: 0.8664
 938/1000 [===========================>..] - ETA: 28s - loss: 3.4389 - regression_loss: 2.5719 - classification_loss: 0.8670
 939/1000 [===========================>..] - ETA: 27s - loss: 3.4352 - regression_loss: 2.5692 - classification_loss: 0.8661
 940/1000 [===========================>..] - ETA: 27s - loss: 3.4361 - regression_loss: 2.5698 - classification_loss: 0.8663
 941/1000 [===========================>..] - ETA: 26s - loss: 3.4362 - regression_loss: 2.5701 - classification_loss: 0.8661
 942/1000 [===========================>..] - ETA: 26s - loss: 3.4363 - regression_loss: 2.5694 - classification_loss: 0.8669
 943/1000 [===========================>..] - ETA: 25s - loss: 3.4379 - regression_loss: 2.5708 - classification_loss: 0.8671
 944/1000 [===========================>..] - ETA: 25s - loss: 3.4343 - regression_loss: 2.5680 - classification_loss: 0.8662
 945/1000 [===========================>..] - ETA: 24s - loss: 3.4357 - regression_loss: 2.5687 - classification_loss: 0.8670
 946/1000 [===========================>..] - ETA: 24s - loss: 3.4363 - regression_loss: 2.5697 - classification_loss: 0.8666
 947/1000 [===========================>..] - ETA: 23s - loss: 3.4376 - regression_loss: 2.5711 - classification_loss: 0.8665
 948/1000 [===========================>..] - ETA: 23s - loss: 3.4339 - regression_loss: 2.5683 - classification_loss: 0.8656
 949/1000 [===========================>..] - ETA: 23s - loss: 3.4346 - regression_loss: 2.5683 - classification_loss: 0.8662
 950/1000 [===========================>..] - ETA: 22s - loss: 3.4350 - regression_loss: 2.5688 - classification_loss: 0.8662
 951/1000 [===========================>..] - ETA: 22s - loss: 3.4349 - regression_loss: 2.5688 - classification_loss: 0.8662
 952/1000 [===========================>..] - ETA: 21s - loss: 3.4340 - regression_loss: 2.5682 - classification_loss: 0.8658
 953/1000 [===========================>..] - ETA: 21s - loss: 3.4379 - regression_loss: 2.5722 - classification_loss: 0.8657
 954/1000 [===========================>..] - ETA: 20s - loss: 3.4343 - regression_loss: 2.5695 - classification_loss: 0.8648
 955/1000 [===========================>..] - ETA: 20s - loss: 3.4360 - regression_loss: 2.5705 - classification_loss: 0.8654
 956/1000 [===========================>..] - ETA: 19s - loss: 3.4368 - regression_loss: 2.5707 - classification_loss: 0.8661
 957/1000 [===========================>..] - ETA: 19s - loss: 3.4365 - regression_loss: 2.5706 - classification_loss: 0.8659
 958/1000 [===========================>..] - ETA: 18s - loss: 3.4370 - regression_loss: 2.5707 - classification_loss: 0.8663
 959/1000 [===========================>..] - ETA: 18s - loss: 3.4378 - regression_loss: 2.5707 - classification_loss: 0.8672
 960/1000 [===========================>..] - ETA: 18s - loss: 3.4343 - regression_loss: 2.5680 - classification_loss: 0.8663
 961/1000 [===========================>..] - ETA: 17s - loss: 3.4357 - regression_loss: 2.5696 - classification_loss: 0.8661
 962/1000 [===========================>..] - ETA: 17s - loss: 3.4321 - regression_loss: 2.5669 - classification_loss: 0.8652
 963/1000 [===========================>..] - ETA: 16s - loss: 3.4329 - regression_loss: 2.5676 - classification_loss: 0.8653
 964/1000 [===========================>..] - ETA: 16s - loss: 3.4330 - regression_loss: 2.5673 - classification_loss: 0.8656
 965/1000 [===========================>..] - ETA: 15s - loss: 3.4294 - regression_loss: 2.5647 - classification_loss: 0.8648
 966/1000 [===========================>..] - ETA: 15s - loss: 3.4259 - regression_loss: 2.5620 - classification_loss: 0.8639
 967/1000 [============================>.] - ETA: 14s - loss: 3.4270 - regression_loss: 2.5631 - classification_loss: 0.8638
 968/1000 [============================>.] - ETA: 14s - loss: 3.4234 - regression_loss: 2.5605 - classification_loss: 0.8630
 969/1000 [============================>.] - ETA: 14s - loss: 3.4199 - regression_loss: 2.5578 - classification_loss: 0.8621
 970/1000 [============================>.] - ETA: 13s - loss: 3.4212 - regression_loss: 2.5582 - classification_loss: 0.8630
 971/1000 [============================>.] - ETA: 13s - loss: 3.4211 - regression_loss: 2.5578 - classification_loss: 0.8633
 972/1000 [============================>.] - ETA: 12s - loss: 3.4212 - regression_loss: 2.5580 - classification_loss: 0.8632
 973/1000 [============================>.] - ETA: 12s - loss: 3.4226 - regression_loss: 2.5592 - classification_loss: 0.8635
 974/1000 [============================>.] - ETA: 11s - loss: 3.4191 - regression_loss: 2.5565 - classification_loss: 0.8626
 975/1000 [============================>.] - ETA: 11s - loss: 3.4156 - regression_loss: 2.5539 - classification_loss: 0.8617
 976/1000 [============================>.] - ETA: 10s - loss: 3.4185 - regression_loss: 2.5552 - classification_loss: 0.8633
 977/1000 [============================>.] - ETA: 10s - loss: 3.4181 - regression_loss: 2.5550 - classification_loss: 0.8631
 978/1000 [============================>.] - ETA: 9s - loss: 3.4177 - regression_loss: 2.5548 - classification_loss: 0.8629 
 979/1000 [============================>.] - ETA: 9s - loss: 3.4191 - regression_loss: 2.5550 - classification_loss: 0.8641
 980/1000 [============================>.] - ETA: 9s - loss: 3.4224 - regression_loss: 2.5572 - classification_loss: 0.8653
 981/1000 [============================>.] - ETA: 8s - loss: 3.4236 - regression_loss: 2.5574 - classification_loss: 0.8662
 982/1000 [============================>.] - ETA: 8s - loss: 3.4201 - regression_loss: 2.5548 - classification_loss: 0.8653
 983/1000 [============================>.] - ETA: 7s - loss: 3.4166 - regression_loss: 2.5522 - classification_loss: 0.8644
 984/1000 [============================>.] - ETA: 7s - loss: 3.4173 - regression_loss: 2.5521 - classification_loss: 0.8652
 985/1000 [============================>.] - ETA: 6s - loss: 3.4182 - regression_loss: 2.5525 - classification_loss: 0.8657
 986/1000 [============================>.] - ETA: 6s - loss: 3.4191 - regression_loss: 2.5528 - classification_loss: 0.8663
 987/1000 [============================>.] - ETA: 5s - loss: 3.4211 - regression_loss: 2.5541 - classification_loss: 0.8670
 988/1000 [============================>.] - ETA: 5s - loss: 3.4223 - regression_loss: 2.5546 - classification_loss: 0.8678
 989/1000 [============================>.] - ETA: 4s - loss: 3.4244 - regression_loss: 2.5557 - classification_loss: 0.8686
 990/1000 [============================>.] - ETA: 4s - loss: 3.4258 - regression_loss: 2.5564 - classification_loss: 0.8694
 991/1000 [============================>.] - ETA: 4s - loss: 3.4282 - regression_loss: 2.5584 - classification_loss: 0.8697
 992/1000 [============================>.] - ETA: 3s - loss: 3.4247 - regression_loss: 2.5558 - classification_loss: 0.8689
 993/1000 [============================>.] - ETA: 3s - loss: 3.4255 - regression_loss: 2.5568 - classification_loss: 0.8687
 994/1000 [============================>.] - ETA: 2s - loss: 3.4253 - regression_loss: 2.5566 - classification_loss: 0.8687
 995/1000 [============================>.] - ETA: 2s - loss: 3.4264 - regression_loss: 2.5570 - classification_loss: 0.8694
 996/1000 [============================>.] - ETA: 1s - loss: 3.4270 - regression_loss: 2.5574 - classification_loss: 0.8696
 997/1000 [============================>.] - ETA: 1s - loss: 3.4236 - regression_loss: 2.5548 - classification_loss: 0.8688
 998/1000 [============================>.] - ETA: 0s - loss: 3.4202 - regression_loss: 2.5523 - classification_loss: 0.8679
 999/1000 [============================>.] - ETA: 0s - loss: 3.4195 - regression_loss: 2.5518 - classification_loss: 0.8676
1000/1000 [==============================] - 452s 452ms/step - loss: 3.4160 - regression_loss: 2.5493 - classification_loss: 0.8668

Epoch 00005: saving model to ./snapshots/resnet50_csv_05.h5
0/2001/2002/2003/2004/2005/2006/2007/2008/2009/20010/20011/20012/20013/20014/20015/20016/20017/20018/20019/20020/20021/20022/20023/20024/20025/20026/20027/20028/20029/20030/20031/20032/20033/20034/20035/20036/20037/20038/20039/20040/20041/20042/20043/20044/20045/20046/20047/20048/20049/20050/20051/20052/20053/20054/20055/20056/20057/20058/20059/20060/20061/20062/20063/20064/20065/20066/20067/20068/20069/20070/20071/20072/20073/20074/20075/20076/20077/20078/20079/20080/20081/20082/20083/20084/20085/20086/20087/20088/20089/20090/20091/20092/20093/20094/20095/20096/20097/20098/20099/200100/200101/200102/200103/200104/200105/200106/200107/200108/200109/200110/200111/200112/200113/200114/200115/200116/200117/200118/200119/200120/200121/200122/200123/200124/200125/200126/200127/200128/200129/200130/200131/200132/200133/200134/200135/200136/200137/200138/200139/200140/200141/200142/200143/200144/200145/200146/200147/200148/200149/200150/200151/200152/200153/200154/200155/200156/200157/200158/200159/200160/200161/200162/200163/200164/200165/200166/200167/200168/200169/200170/200171/200172/200173/200174/200175/200176/200177/200178/200179/200180/200181/200182/200183/200184/200185/200186/200187/200188/200189/200190/200191/200192/200193/200194/200195/200196/200197/200198/200199/2000/2001/2002/2003/2004/2005/2006/2007/2008/2009/20010/20011/20012/20013/20014/20015/20016/20017/20018/20019/20020/20021/20022/20023/20024/20025/20026/20027/20028/20029/20030/20031/20032/20033/20034/20035/20036/20037/20038/20039/20040/20041/20042/20043/20044/20045/20046/20047/20048/20049/20050/20051/20052/20053/20054/20055/20056/20057/20058/20059/20060/20061/20062/20063/20064/20065/20066/20067/20068/20069/20070/20071/20072/20073/20074/20075/20076/20077/20078/20079/20080/20081/20082/20083/20084/20085/20086/20087/20088/20089/20090/20091/20092/20093/20094/20095/20096/20097/20098/20099/200100/200101/200102/200103/200104/200105/200106/200107/200108/200109/200110/200111/200112/200113/200114/200115/200116/200117/200118/200119/200120/200121/200122/200123/200124/200125/200126/200127/200128/200129/200130/200131/200132/200133/200134/200135/200136/200137/200138/200139/200140/200141/200142/200143/200144/200145/200146/200147/200148/200149/200150/200151/200152/200153/200154/200155/200156/200157/200158/200159/200160/200161/200162/200163/200164/200165/200166/200167/200168/200169/200170/200171/200172/200173/200174/200175/200176/200177/200178/200179/200180/200181/200182/200183/200184/200185/200186/200187/200188/200189/200190/200191/200192/200193/200194/200195/200196/200197/200198/200199/200

M 0.0926
N 0.0000
mAP: 0.0463
Epoch 6/30

   1/1000 [..............................] - ETA: 7:15 - loss: 5.2546 - regression_loss: 3.1218 - classification_loss: 2.1328
   2/1000 [..............................] - ETA: 7:23 - loss: 4.8654 - regression_loss: 2.9151 - classification_loss: 1.9503
   3/1000 [..............................] - ETA: 7:23 - loss: 4.2677 - regression_loss: 2.8035 - classification_loss: 1.4642
   4/1000 [..............................] - ETA: 7:25 - loss: 4.0629 - regression_loss: 2.5643 - classification_loss: 1.4986
   5/1000 [..............................] - ETA: 7:26 - loss: 4.0431 - regression_loss: 2.6772 - classification_loss: 1.3659
   6/1000 [..............................] - ETA: 7:26 - loss: 3.9232 - regression_loss: 2.6780 - classification_loss: 1.2452
   7/1000 [..............................] - ETA: 7:27 - loss: 3.9536 - regression_loss: 2.7052 - classification_loss: 1.2484
   8/1000 [..............................] - ETA: 7:26 - loss: 3.4594 - regression_loss: 2.3671 - classification_loss: 1.0923
   9/1000 [..............................] - ETA: 7:22 - loss: 3.5792 - regression_loss: 2.5166 - classification_loss: 1.0626
  10/1000 [..............................] - ETA: 7:20 - loss: 3.6517 - regression_loss: 2.5299 - classification_loss: 1.1218
  11/1000 [..............................] - ETA: 7:20 - loss: 3.3197 - regression_loss: 2.2999 - classification_loss: 1.0198
  12/1000 [..............................] - ETA: 7:21 - loss: 3.4803 - regression_loss: 2.4295 - classification_loss: 1.0509
  13/1000 [..............................] - ETA: 7:21 - loss: 3.5812 - regression_loss: 2.4819 - classification_loss: 1.0993
  14/1000 [..............................] - ETA: 7:21 - loss: 3.3254 - regression_loss: 2.3047 - classification_loss: 1.0208
  15/1000 [..............................] - ETA: 7:22 - loss: 3.3339 - regression_loss: 2.3273 - classification_loss: 1.0067
  16/1000 [..............................] - ETA: 7:21 - loss: 3.1255 - regression_loss: 2.1818 - classification_loss: 0.9437
  17/1000 [..............................] - ETA: 7:21 - loss: 3.2409 - regression_loss: 2.2376 - classification_loss: 1.0033
  18/1000 [..............................] - ETA: 7:21 - loss: 3.0608 - regression_loss: 2.1133 - classification_loss: 0.9476
  19/1000 [..............................] - ETA: 7:20 - loss: 2.8997 - regression_loss: 2.0020 - classification_loss: 0.8977
  20/1000 [..............................] - ETA: 7:20 - loss: 2.9535 - regression_loss: 2.0382 - classification_loss: 0.9153
  21/1000 [..............................] - ETA: 7:20 - loss: 3.0728 - regression_loss: 2.1065 - classification_loss: 0.9664
  22/1000 [..............................] - ETA: 7:20 - loss: 2.9332 - regression_loss: 2.0107 - classification_loss: 0.9224
  23/1000 [..............................] - ETA: 7:20 - loss: 2.8056 - regression_loss: 1.9233 - classification_loss: 0.8823
  24/1000 [..............................] - ETA: 7:20 - loss: 2.8971 - regression_loss: 1.9693 - classification_loss: 0.9278
  25/1000 [..............................] - ETA: 7:19 - loss: 2.9190 - regression_loss: 1.9624 - classification_loss: 0.9566
  26/1000 [..............................] - ETA: 7:19 - loss: 2.9606 - regression_loss: 1.9931 - classification_loss: 0.9675
  27/1000 [..............................] - ETA: 7:18 - loss: 2.9868 - regression_loss: 2.0336 - classification_loss: 0.9531
  28/1000 [..............................] - ETA: 7:18 - loss: 3.0153 - regression_loss: 2.0730 - classification_loss: 0.9423
  29/1000 [..............................] - ETA: 7:17 - loss: 3.0108 - regression_loss: 2.0789 - classification_loss: 0.9319
  30/1000 [..............................] - ETA: 7:16 - loss: 2.9117 - regression_loss: 2.0096 - classification_loss: 0.9022
  31/1000 [..............................] - ETA: 7:16 - loss: 2.9170 - regression_loss: 2.0234 - classification_loss: 0.8936
  32/1000 [..............................] - ETA: 7:16 - loss: 2.9704 - regression_loss: 2.0561 - classification_loss: 0.9143
  33/1000 [..............................] - ETA: 7:15 - loss: 3.0485 - regression_loss: 2.1386 - classification_loss: 0.9099
  34/1000 [>.............................] - ETA: 7:15 - loss: 3.0944 - regression_loss: 2.1658 - classification_loss: 0.9286
  35/1000 [>.............................] - ETA: 7:15 - loss: 3.0060 - regression_loss: 2.1040 - classification_loss: 0.9021
  36/1000 [>.............................] - ETA: 7:14 - loss: 3.0855 - regression_loss: 2.1533 - classification_loss: 0.9322
  37/1000 [>.............................] - ETA: 7:14 - loss: 3.0021 - regression_loss: 2.0951 - classification_loss: 0.9070
  38/1000 [>.............................] - ETA: 7:12 - loss: 3.0057 - regression_loss: 2.1005 - classification_loss: 0.9052
  39/1000 [>.............................] - ETA: 7:12 - loss: 3.0403 - regression_loss: 2.1346 - classification_loss: 0.9057
  40/1000 [>.............................] - ETA: 7:11 - loss: 3.0785 - regression_loss: 2.1740 - classification_loss: 0.9045
  41/1000 [>.............................] - ETA: 7:11 - loss: 3.0989 - regression_loss: 2.1943 - classification_loss: 0.9046
  42/1000 [>.............................] - ETA: 7:11 - loss: 3.0251 - regression_loss: 2.1421 - classification_loss: 0.8830
  43/1000 [>.............................] - ETA: 7:11 - loss: 3.0330 - regression_loss: 2.1566 - classification_loss: 0.8765
  44/1000 [>.............................] - ETA: 7:10 - loss: 3.0758 - regression_loss: 2.1796 - classification_loss: 0.8961
  45/1000 [>.............................] - ETA: 7:10 - loss: 3.1164 - regression_loss: 2.2045 - classification_loss: 0.9119
  46/1000 [>.............................] - ETA: 7:09 - loss: 3.1092 - regression_loss: 2.2085 - classification_loss: 0.9007
  47/1000 [>.............................] - ETA: 7:09 - loss: 3.1289 - regression_loss: 2.2154 - classification_loss: 0.9135
  48/1000 [>.............................] - ETA: 7:09 - loss: 3.1336 - regression_loss: 2.2303 - classification_loss: 0.9033
  49/1000 [>.............................] - ETA: 7:08 - loss: 3.1652 - regression_loss: 2.2624 - classification_loss: 0.9028
  50/1000 [>.............................] - ETA: 7:08 - loss: 3.1928 - regression_loss: 2.2843 - classification_loss: 0.9084
  51/1000 [>.............................] - ETA: 7:07 - loss: 3.1922 - regression_loss: 2.2936 - classification_loss: 0.8986
  52/1000 [>.............................] - ETA: 7:07 - loss: 3.1965 - regression_loss: 2.3050 - classification_loss: 0.8916
  53/1000 [>.............................] - ETA: 7:07 - loss: 3.1362 - regression_loss: 2.2615 - classification_loss: 0.8748
  54/1000 [>.............................] - ETA: 7:06 - loss: 3.1484 - regression_loss: 2.2669 - classification_loss: 0.8816
  55/1000 [>.............................] - ETA: 7:06 - loss: 3.1464 - regression_loss: 2.2709 - classification_loss: 0.8755
  56/1000 [>.............................] - ETA: 7:06 - loss: 3.1699 - regression_loss: 2.2957 - classification_loss: 0.8741
  57/1000 [>.............................] - ETA: 7:05 - loss: 3.1142 - regression_loss: 2.2554 - classification_loss: 0.8588
  58/1000 [>.............................] - ETA: 7:05 - loss: 3.1012 - regression_loss: 2.2483 - classification_loss: 0.8529
  59/1000 [>.............................] - ETA: 7:04 - loss: 3.0486 - regression_loss: 2.2102 - classification_loss: 0.8385
  60/1000 [>.............................] - ETA: 7:04 - loss: 3.0631 - regression_loss: 2.2224 - classification_loss: 0.8407
  61/1000 [>.............................] - ETA: 7:03 - loss: 3.0836 - regression_loss: 2.2314 - classification_loss: 0.8523
  62/1000 [>.............................] - ETA: 7:03 - loss: 3.0793 - regression_loss: 2.2304 - classification_loss: 0.8489
  63/1000 [>.............................] - ETA: 7:02 - loss: 3.0305 - regression_loss: 2.1950 - classification_loss: 0.8355
  64/1000 [>.............................] - ETA: 7:02 - loss: 3.0425 - regression_loss: 2.2112 - classification_loss: 0.8313
  65/1000 [>.............................] - ETA: 7:01 - loss: 3.0597 - regression_loss: 2.2323 - classification_loss: 0.8274
  66/1000 [>.............................] - ETA: 7:01 - loss: 3.0133 - regression_loss: 2.1985 - classification_loss: 0.8149
  67/1000 [=>............................] - ETA: 7:01 - loss: 3.0473 - regression_loss: 2.2194 - classification_loss: 0.8278
  68/1000 [=>............................] - ETA: 7:00 - loss: 3.0637 - regression_loss: 2.2242 - classification_loss: 0.8396
  69/1000 [=>............................] - ETA: 7:00 - loss: 3.0735 - regression_loss: 2.2385 - classification_loss: 0.8351
  70/1000 [=>............................] - ETA: 6:59 - loss: 3.0296 - regression_loss: 2.2065 - classification_loss: 0.8231
  71/1000 [=>............................] - ETA: 6:59 - loss: 3.0375 - regression_loss: 2.2199 - classification_loss: 0.8176
  72/1000 [=>............................] - ETA: 6:58 - loss: 3.0561 - regression_loss: 2.2390 - classification_loss: 0.8171
  73/1000 [=>............................] - ETA: 6:58 - loss: 3.0661 - regression_loss: 2.2372 - classification_loss: 0.8290
  74/1000 [=>............................] - ETA: 6:57 - loss: 3.0247 - regression_loss: 2.2069 - classification_loss: 0.8178
  75/1000 [=>............................] - ETA: 6:57 - loss: 3.0278 - regression_loss: 2.2143 - classification_loss: 0.8135
  76/1000 [=>............................] - ETA: 6:57 - loss: 3.0286 - regression_loss: 2.2210 - classification_loss: 0.8076
  77/1000 [=>............................] - ETA: 6:56 - loss: 3.0387 - regression_loss: 2.2292 - classification_loss: 0.8095
  78/1000 [=>............................] - ETA: 6:56 - loss: 3.0605 - regression_loss: 2.2510 - classification_loss: 0.8095
  79/1000 [=>............................] - ETA: 6:55 - loss: 3.0783 - regression_loss: 2.2659 - classification_loss: 0.8124
  80/1000 [=>............................] - ETA: 6:55 - loss: 3.0399 - regression_loss: 2.2376 - classification_loss: 0.8023
  81/1000 [=>............................] - ETA: 6:55 - loss: 3.0532 - regression_loss: 2.2504 - classification_loss: 0.8028
  82/1000 [=>............................] - ETA: 6:54 - loss: 3.0873 - regression_loss: 2.2746 - classification_loss: 0.8126
  83/1000 [=>............................] - ETA: 6:54 - loss: 3.0898 - regression_loss: 2.2729 - classification_loss: 0.8169
  84/1000 [=>............................] - ETA: 6:53 - loss: 3.1031 - regression_loss: 2.2889 - classification_loss: 0.8142
  85/1000 [=>............................] - ETA: 6:53 - loss: 3.1143 - regression_loss: 2.3020 - classification_loss: 0.8123
  86/1000 [=>............................] - ETA: 6:52 - loss: 3.1137 - regression_loss: 2.3053 - classification_loss: 0.8085
  87/1000 [=>............................] - ETA: 6:52 - loss: 3.1378 - regression_loss: 2.3182 - classification_loss: 0.8196
  88/1000 [=>............................] - ETA: 6:51 - loss: 3.1574 - regression_loss: 2.3277 - classification_loss: 0.8298
  89/1000 [=>............................] - ETA: 6:51 - loss: 3.1710 - regression_loss: 2.3379 - classification_loss: 0.8331
  90/1000 [=>............................] - ETA: 6:50 - loss: 3.1843 - regression_loss: 2.3537 - classification_loss: 0.8306
  91/1000 [=>............................] - ETA: 6:50 - loss: 3.1493 - regression_loss: 2.3278 - classification_loss: 0.8215
  92/1000 [=>............................] - ETA: 6:49 - loss: 3.1151 - regression_loss: 2.3025 - classification_loss: 0.8126
  93/1000 [=>............................] - ETA: 6:49 - loss: 3.1325 - regression_loss: 2.3141 - classification_loss: 0.8184
  94/1000 [=>............................] - ETA: 6:48 - loss: 3.1529 - regression_loss: 2.3249 - classification_loss: 0.8280
  95/1000 [=>............................] - ETA: 6:48 - loss: 3.1724 - regression_loss: 2.3345 - classification_loss: 0.8379
  96/1000 [=>............................] - ETA: 6:48 - loss: 3.1845 - regression_loss: 2.3399 - classification_loss: 0.8446
  97/1000 [=>............................] - ETA: 6:47 - loss: 3.2165 - regression_loss: 2.3606 - classification_loss: 0.8560
  98/1000 [=>............................] - ETA: 6:47 - loss: 3.2405 - regression_loss: 2.3868 - classification_loss: 0.8537
  99/1000 [=>............................] - ETA: 6:46 - loss: 3.2474 - regression_loss: 2.3935 - classification_loss: 0.8539
 100/1000 [==>...........................] - ETA: 6:46 - loss: 3.2477 - regression_loss: 2.3977 - classification_loss: 0.8500
 101/1000 [==>...........................] - ETA: 6:45 - loss: 3.2518 - regression_loss: 2.4030 - classification_loss: 0.8488
 102/1000 [==>...........................] - ETA: 6:45 - loss: 3.2199 - regression_loss: 2.3795 - classification_loss: 0.8405
 103/1000 [==>...........................] - ETA: 6:44 - loss: 3.2322 - regression_loss: 2.3892 - classification_loss: 0.8430
 104/1000 [==>...........................] - ETA: 6:44 - loss: 3.2396 - regression_loss: 2.3994 - classification_loss: 0.8402
 105/1000 [==>...........................] - ETA: 6:43 - loss: 3.2087 - regression_loss: 2.3766 - classification_loss: 0.8321
 106/1000 [==>...........................] - ETA: 6:43 - loss: 3.2182 - regression_loss: 2.3848 - classification_loss: 0.8334
 107/1000 [==>...........................] - ETA: 6:42 - loss: 3.2129 - regression_loss: 2.3829 - classification_loss: 0.8301
 108/1000 [==>...........................] - ETA: 6:42 - loss: 3.1832 - regression_loss: 2.3608 - classification_loss: 0.8224
 109/1000 [==>...........................] - ETA: 6:41 - loss: 3.1975 - regression_loss: 2.3646 - classification_loss: 0.8329
 110/1000 [==>...........................] - ETA: 6:41 - loss: 3.2171 - regression_loss: 2.3748 - classification_loss: 0.8422
 111/1000 [==>...........................] - ETA: 6:41 - loss: 3.2232 - regression_loss: 2.3808 - classification_loss: 0.8424
 112/1000 [==>...........................] - ETA: 6:40 - loss: 3.2295 - regression_loss: 2.3891 - classification_loss: 0.8403
 113/1000 [==>...........................] - ETA: 6:40 - loss: 3.2286 - regression_loss: 2.3897 - classification_loss: 0.8389
 114/1000 [==>...........................] - ETA: 6:39 - loss: 3.2328 - regression_loss: 2.3955 - classification_loss: 0.8374
 115/1000 [==>...........................] - ETA: 6:39 - loss: 3.2053 - regression_loss: 2.3746 - classification_loss: 0.8307
 116/1000 [==>...........................] - ETA: 6:38 - loss: 3.2123 - regression_loss: 2.3813 - classification_loss: 0.8310
 117/1000 [==>...........................] - ETA: 6:38 - loss: 3.1848 - regression_loss: 2.3609 - classification_loss: 0.8239
 118/1000 [==>...........................] - ETA: 6:38 - loss: 3.1578 - regression_loss: 2.3409 - classification_loss: 0.8169
 119/1000 [==>...........................] - ETA: 6:37 - loss: 3.1313 - regression_loss: 2.3212 - classification_loss: 0.8101
 120/1000 [==>...........................] - ETA: 6:37 - loss: 3.1459 - regression_loss: 2.3365 - classification_loss: 0.8093
 121/1000 [==>...........................] - ETA: 6:36 - loss: 3.1549 - regression_loss: 2.3485 - classification_loss: 0.8065
 122/1000 [==>...........................] - ETA: 6:36 - loss: 3.1661 - regression_loss: 2.3607 - classification_loss: 0.8054
 123/1000 [==>...........................] - ETA: 6:35 - loss: 3.1404 - regression_loss: 2.3415 - classification_loss: 0.7989
 124/1000 [==>...........................] - ETA: 6:35 - loss: 3.1150 - regression_loss: 2.3226 - classification_loss: 0.7924
 125/1000 [==>...........................] - ETA: 6:34 - loss: 3.1290 - regression_loss: 2.3345 - classification_loss: 0.7945
 126/1000 [==>...........................] - ETA: 6:34 - loss: 3.1042 - regression_loss: 2.3160 - classification_loss: 0.7882
 127/1000 [==>...........................] - ETA: 6:34 - loss: 3.1058 - regression_loss: 2.3202 - classification_loss: 0.7856
 128/1000 [==>...........................] - ETA: 6:33 - loss: 3.1291 - regression_loss: 2.3337 - classification_loss: 0.7955
 129/1000 [==>...........................] - ETA: 6:33 - loss: 3.1426 - regression_loss: 2.3460 - classification_loss: 0.7965
 130/1000 [==>...........................] - ETA: 6:32 - loss: 3.1615 - regression_loss: 2.3538 - classification_loss: 0.8076
 131/1000 [==>...........................] - ETA: 6:32 - loss: 3.1710 - regression_loss: 2.3528 - classification_loss: 0.8181
 132/1000 [==>...........................] - ETA: 6:31 - loss: 3.1821 - regression_loss: 2.3647 - classification_loss: 0.8174
 133/1000 [==>...........................] - ETA: 6:31 - loss: 3.1910 - regression_loss: 2.3701 - classification_loss: 0.8209
 134/1000 [===>..........................] - ETA: 6:30 - loss: 3.1905 - regression_loss: 2.3717 - classification_loss: 0.8188
 135/1000 [===>..........................] - ETA: 6:30 - loss: 3.1921 - regression_loss: 2.3761 - classification_loss: 0.8160
 136/1000 [===>..........................] - ETA: 6:29 - loss: 3.1688 - regression_loss: 2.3586 - classification_loss: 0.8102
 137/1000 [===>..........................] - ETA: 6:29 - loss: 3.1466 - regression_loss: 2.3414 - classification_loss: 0.8052
 138/1000 [===>..........................] - ETA: 6:29 - loss: 3.1556 - regression_loss: 2.3436 - classification_loss: 0.8120
 139/1000 [===>..........................] - ETA: 6:28 - loss: 3.1329 - regression_loss: 2.3268 - classification_loss: 0.8062
 140/1000 [===>..........................] - ETA: 6:28 - loss: 3.1414 - regression_loss: 2.3347 - classification_loss: 0.8067
 141/1000 [===>..........................] - ETA: 6:27 - loss: 3.1431 - regression_loss: 2.3394 - classification_loss: 0.8037
 142/1000 [===>..........................] - ETA: 6:27 - loss: 3.1590 - regression_loss: 2.3469 - classification_loss: 0.8121
 143/1000 [===>..........................] - ETA: 6:26 - loss: 3.1642 - regression_loss: 2.3486 - classification_loss: 0.8156
 144/1000 [===>..........................] - ETA: 6:26 - loss: 3.1725 - regression_loss: 2.3537 - classification_loss: 0.8188
 145/1000 [===>..........................] - ETA: 6:25 - loss: 3.1825 - regression_loss: 2.3572 - classification_loss: 0.8252
 146/1000 [===>..........................] - ETA: 6:25 - loss: 3.2021 - regression_loss: 2.3717 - classification_loss: 0.8304
 147/1000 [===>..........................] - ETA: 6:24 - loss: 3.2207 - regression_loss: 2.3836 - classification_loss: 0.8371
 148/1000 [===>..........................] - ETA: 6:24 - loss: 3.2303 - regression_loss: 2.3889 - classification_loss: 0.8413
 149/1000 [===>..........................] - ETA: 6:24 - loss: 3.2336 - regression_loss: 2.3923 - classification_loss: 0.8413
 150/1000 [===>..........................] - ETA: 6:23 - loss: 3.2396 - regression_loss: 2.3963 - classification_loss: 0.8433
 151/1000 [===>..........................] - ETA: 6:23 - loss: 3.2525 - regression_loss: 2.4047 - classification_loss: 0.8478
 152/1000 [===>..........................] - ETA: 6:22 - loss: 3.2512 - regression_loss: 2.4056 - classification_loss: 0.8456
 153/1000 [===>..........................] - ETA: 6:22 - loss: 3.2567 - regression_loss: 2.4120 - classification_loss: 0.8447
 154/1000 [===>..........................] - ETA: 6:21 - loss: 3.2616 - regression_loss: 2.4139 - classification_loss: 0.8477
 155/1000 [===>..........................] - ETA: 6:21 - loss: 3.2662 - regression_loss: 2.4156 - classification_loss: 0.8506
 156/1000 [===>..........................] - ETA: 6:20 - loss: 3.2704 - regression_loss: 2.4190 - classification_loss: 0.8514
 157/1000 [===>..........................] - ETA: 6:20 - loss: 3.2723 - regression_loss: 2.4222 - classification_loss: 0.8501
 158/1000 [===>..........................] - ETA: 6:20 - loss: 3.2787 - regression_loss: 2.4275 - classification_loss: 0.8512
 159/1000 [===>..........................] - ETA: 6:19 - loss: 3.2915 - regression_loss: 2.4388 - classification_loss: 0.8526
 160/1000 [===>..........................] - ETA: 6:19 - loss: 3.2992 - regression_loss: 2.4458 - classification_loss: 0.8535
 161/1000 [===>..........................] - ETA: 6:18 - loss: 3.3023 - regression_loss: 2.4508 - classification_loss: 0.8515
 162/1000 [===>..........................] - ETA: 6:18 - loss: 3.3164 - regression_loss: 2.4619 - classification_loss: 0.8545
 163/1000 [===>..........................] - ETA: 6:17 - loss: 3.2961 - regression_loss: 2.4468 - classification_loss: 0.8493
 164/1000 [===>..........................] - ETA: 6:17 - loss: 3.2762 - regression_loss: 2.4319 - classification_loss: 0.8443
 165/1000 [===>..........................] - ETA: 6:16 - loss: 3.2836 - regression_loss: 2.4387 - classification_loss: 0.8449
 166/1000 [===>..........................] - ETA: 6:16 - loss: 3.2829 - regression_loss: 2.4388 - classification_loss: 0.8440
 167/1000 [====>.........................] - ETA: 6:16 - loss: 3.2889 - regression_loss: 2.4463 - classification_loss: 0.8425
 168/1000 [====>.........................] - ETA: 6:15 - loss: 3.2900 - regression_loss: 2.4489 - classification_loss: 0.8411
 169/1000 [====>.........................] - ETA: 6:15 - loss: 3.3082 - regression_loss: 2.4679 - classification_loss: 0.8402
 170/1000 [====>.........................] - ETA: 6:14 - loss: 3.3168 - regression_loss: 2.4751 - classification_loss: 0.8417
 171/1000 [====>.........................] - ETA: 6:14 - loss: 3.3186 - regression_loss: 2.4767 - classification_loss: 0.8419
 172/1000 [====>.........................] - ETA: 6:13 - loss: 3.3192 - regression_loss: 2.4789 - classification_loss: 0.8403
 173/1000 [====>.........................] - ETA: 6:13 - loss: 3.3223 - regression_loss: 2.4829 - classification_loss: 0.8394
 174/1000 [====>.........................] - ETA: 6:12 - loss: 3.3237 - regression_loss: 2.4859 - classification_loss: 0.8377
 175/1000 [====>.........................] - ETA: 6:12 - loss: 3.3047 - regression_loss: 2.4717 - classification_loss: 0.8329
 176/1000 [====>.........................] - ETA: 6:12 - loss: 3.3168 - regression_loss: 2.4783 - classification_loss: 0.8386
 177/1000 [====>.........................] - ETA: 6:11 - loss: 3.3215 - regression_loss: 2.4834 - classification_loss: 0.8380
 178/1000 [====>.........................] - ETA: 6:11 - loss: 3.3233 - regression_loss: 2.4856 - classification_loss: 0.8377
 179/1000 [====>.........................] - ETA: 6:10 - loss: 3.3268 - regression_loss: 2.4885 - classification_loss: 0.8383
 180/1000 [====>.........................] - ETA: 6:10 - loss: 3.3410 - regression_loss: 2.4983 - classification_loss: 0.8428
 181/1000 [====>.........................] - ETA: 6:09 - loss: 3.3454 - regression_loss: 2.5044 - classification_loss: 0.8410
 182/1000 [====>.........................] - ETA: 6:09 - loss: 3.3557 - regression_loss: 2.5138 - classification_loss: 0.8418
 183/1000 [====>.........................] - ETA: 6:08 - loss: 3.3633 - regression_loss: 2.5217 - classification_loss: 0.8416
 184/1000 [====>.........................] - ETA: 6:08 - loss: 3.3661 - regression_loss: 2.5232 - classification_loss: 0.8429
 185/1000 [====>.........................] - ETA: 6:08 - loss: 3.3666 - regression_loss: 2.5251 - classification_loss: 0.8416
 186/1000 [====>.........................] - ETA: 6:07 - loss: 3.3709 - regression_loss: 2.5297 - classification_loss: 0.8411
 187/1000 [====>.........................] - ETA: 6:07 - loss: 3.3799 - regression_loss: 2.5370 - classification_loss: 0.8429
 188/1000 [====>.........................] - ETA: 6:06 - loss: 3.3806 - regression_loss: 2.5401 - classification_loss: 0.8404
 189/1000 [====>.........................] - ETA: 6:06 - loss: 3.3627 - regression_loss: 2.5267 - classification_loss: 0.8360
 190/1000 [====>.........................] - ETA: 6:06 - loss: 3.3450 - regression_loss: 2.5134 - classification_loss: 0.8316
 191/1000 [====>.........................] - ETA: 6:05 - loss: 3.3465 - regression_loss: 2.5166 - classification_loss: 0.8298
 192/1000 [====>.........................] - ETA: 6:04 - loss: 3.3325 - regression_loss: 2.5035 - classification_loss: 0.8289
 193/1000 [====>.........................] - ETA: 6:04 - loss: 3.3152 - regression_loss: 2.4906 - classification_loss: 0.8246
 194/1000 [====>.........................] - ETA: 6:03 - loss: 3.3199 - regression_loss: 2.4925 - classification_loss: 0.8274
 195/1000 [====>.........................] - ETA: 6:03 - loss: 3.3325 - regression_loss: 2.5000 - classification_loss: 0.8325
 196/1000 [====>.........................] - ETA: 6:03 - loss: 3.3423 - regression_loss: 2.5052 - classification_loss: 0.8372
 197/1000 [====>.........................] - ETA: 6:02 - loss: 3.3485 - regression_loss: 2.5081 - classification_loss: 0.8404
 198/1000 [====>.........................] - ETA: 6:02 - loss: 3.3621 - regression_loss: 2.5170 - classification_loss: 0.8452
 199/1000 [====>.........................] - ETA: 6:01 - loss: 3.3741 - regression_loss: 2.5237 - classification_loss: 0.8504
 200/1000 [=====>........................] - ETA: 6:01 - loss: 3.3773 - regression_loss: 2.5269 - classification_loss: 0.8504
 201/1000 [=====>........................] - ETA: 6:00 - loss: 3.3945 - regression_loss: 2.5448 - classification_loss: 0.8497
 202/1000 [=====>........................] - ETA: 6:00 - loss: 3.3970 - regression_loss: 2.5492 - classification_loss: 0.8478
 203/1000 [=====>........................] - ETA: 6:00 - loss: 3.3803 - regression_loss: 2.5366 - classification_loss: 0.8436
 204/1000 [=====>........................] - ETA: 5:59 - loss: 3.3837 - regression_loss: 2.5384 - classification_loss: 0.8453
 205/1000 [=====>........................] - ETA: 5:59 - loss: 3.3672 - regression_loss: 2.5260 - classification_loss: 0.8412
 206/1000 [=====>........................] - ETA: 5:58 - loss: 3.3704 - regression_loss: 2.5275 - classification_loss: 0.8430
 207/1000 [=====>........................] - ETA: 5:58 - loss: 3.3777 - regression_loss: 2.5298 - classification_loss: 0.8479
 208/1000 [=====>........................] - ETA: 5:57 - loss: 3.3741 - regression_loss: 2.5287 - classification_loss: 0.8455
 209/1000 [=====>........................] - ETA: 5:57 - loss: 3.3720 - regression_loss: 2.5279 - classification_loss: 0.8441
 210/1000 [=====>........................] - ETA: 5:56 - loss: 3.3559 - regression_loss: 2.5159 - classification_loss: 0.8401
 211/1000 [=====>........................] - ETA: 5:56 - loss: 3.3574 - regression_loss: 2.5183 - classification_loss: 0.8391
 212/1000 [=====>........................] - ETA: 5:56 - loss: 3.3415 - regression_loss: 2.5064 - classification_loss: 0.8351
 213/1000 [=====>........................] - ETA: 5:55 - loss: 3.3258 - regression_loss: 2.4947 - classification_loss: 0.8312
 214/1000 [=====>........................] - ETA: 5:55 - loss: 3.3103 - regression_loss: 2.4830 - classification_loss: 0.8273
 215/1000 [=====>........................] - ETA: 5:54 - loss: 3.3084 - regression_loss: 2.4802 - classification_loss: 0.8282
 216/1000 [=====>........................] - ETA: 5:54 - loss: 3.3106 - regression_loss: 2.4828 - classification_loss: 0.8278
 217/1000 [=====>........................] - ETA: 5:53 - loss: 3.3147 - regression_loss: 2.4878 - classification_loss: 0.8269
 218/1000 [=====>........................] - ETA: 5:53 - loss: 3.3111 - regression_loss: 2.4858 - classification_loss: 0.8253
 219/1000 [=====>........................] - ETA: 5:52 - loss: 3.3110 - regression_loss: 2.4858 - classification_loss: 0.8252
 220/1000 [=====>........................] - ETA: 5:52 - loss: 3.3165 - regression_loss: 2.4920 - classification_loss: 0.8246
 221/1000 [=====>........................] - ETA: 5:52 - loss: 3.3158 - regression_loss: 2.4931 - classification_loss: 0.8228
 222/1000 [=====>........................] - ETA: 5:51 - loss: 3.3248 - regression_loss: 2.4970 - classification_loss: 0.8278
 223/1000 [=====>........................] - ETA: 5:51 - loss: 3.3099 - regression_loss: 2.4858 - classification_loss: 0.8241
 224/1000 [=====>........................] - ETA: 5:50 - loss: 3.3168 - regression_loss: 2.4882 - classification_loss: 0.8286
 225/1000 [=====>........................] - ETA: 5:50 - loss: 3.3134 - regression_loss: 2.4865 - classification_loss: 0.8269
 226/1000 [=====>........................] - ETA: 5:49 - loss: 3.3162 - regression_loss: 2.4883 - classification_loss: 0.8279
 227/1000 [=====>........................] - ETA: 5:49 - loss: 3.3187 - regression_loss: 2.4891 - classification_loss: 0.8295
 228/1000 [=====>........................] - ETA: 5:48 - loss: 3.3255 - regression_loss: 2.4931 - classification_loss: 0.8324
 229/1000 [=====>........................] - ETA: 5:48 - loss: 3.3109 - regression_loss: 2.4822 - classification_loss: 0.8288
 230/1000 [=====>........................] - ETA: 5:47 - loss: 3.3148 - regression_loss: 2.4860 - classification_loss: 0.8288
 231/1000 [=====>........................] - ETA: 5:47 - loss: 3.3204 - regression_loss: 2.4922 - classification_loss: 0.8282
 232/1000 [=====>........................] - ETA: 5:47 - loss: 3.3284 - regression_loss: 2.4963 - classification_loss: 0.8322
 233/1000 [=====>........................] - ETA: 5:46 - loss: 3.3142 - regression_loss: 2.4856 - classification_loss: 0.8286
 234/1000 [======>.......................] - ETA: 5:46 - loss: 3.3144 - regression_loss: 2.4875 - classification_loss: 0.8269
 235/1000 [======>.......................] - ETA: 5:45 - loss: 3.3177 - regression_loss: 2.4914 - classification_loss: 0.8262
 236/1000 [======>.......................] - ETA: 5:45 - loss: 3.3215 - regression_loss: 2.4942 - classification_loss: 0.8273
 237/1000 [======>.......................] - ETA: 5:44 - loss: 3.3317 - regression_loss: 2.5038 - classification_loss: 0.8279
 238/1000 [======>.......................] - ETA: 5:44 - loss: 3.3313 - regression_loss: 2.5050 - classification_loss: 0.8262
 239/1000 [======>.......................] - ETA: 5:44 - loss: 3.3368 - regression_loss: 2.5063 - classification_loss: 0.8306
 240/1000 [======>.......................] - ETA: 5:43 - loss: 3.3229 - regression_loss: 2.4958 - classification_loss: 0.8271
 241/1000 [======>.......................] - ETA: 5:43 - loss: 3.3091 - regression_loss: 2.4855 - classification_loss: 0.8237
 242/1000 [======>.......................] - ETA: 5:42 - loss: 3.2955 - regression_loss: 2.4752 - classification_loss: 0.8203
 243/1000 [======>.......................] - ETA: 5:42 - loss: 3.3057 - regression_loss: 2.4783 - classification_loss: 0.8274
 244/1000 [======>.......................] - ETA: 5:41 - loss: 3.3122 - regression_loss: 2.4808 - classification_loss: 0.8314
 245/1000 [======>.......................] - ETA: 5:41 - loss: 3.3167 - regression_loss: 2.4822 - classification_loss: 0.8345
 246/1000 [======>.......................] - ETA: 5:40 - loss: 3.3219 - regression_loss: 2.4858 - classification_loss: 0.8361
 247/1000 [======>.......................] - ETA: 5:40 - loss: 3.3085 - regression_loss: 2.4758 - classification_loss: 0.8327
 248/1000 [======>.......................] - ETA: 5:39 - loss: 3.3123 - regression_loss: 2.4808 - classification_loss: 0.8315
 249/1000 [======>.......................] - ETA: 5:39 - loss: 3.3187 - regression_loss: 2.4852 - classification_loss: 0.8335
 250/1000 [======>.......................] - ETA: 5:39 - loss: 3.3183 - regression_loss: 2.4855 - classification_loss: 0.8328
 251/1000 [======>.......................] - ETA: 5:38 - loss: 3.3190 - regression_loss: 2.4860 - classification_loss: 0.8330
 252/1000 [======>.......................] - ETA: 5:38 - loss: 3.3202 - regression_loss: 2.4849 - classification_loss: 0.8353
 253/1000 [======>.......................] - ETA: 5:37 - loss: 3.3206 - regression_loss: 2.4861 - classification_loss: 0.8345
 254/1000 [======>.......................] - ETA: 5:37 - loss: 3.3186 - regression_loss: 2.4850 - classification_loss: 0.8335
 255/1000 [======>.......................] - ETA: 5:36 - loss: 3.3270 - regression_loss: 2.4908 - classification_loss: 0.8363
 256/1000 [======>.......................] - ETA: 5:36 - loss: 3.3305 - regression_loss: 2.4933 - classification_loss: 0.8372
 257/1000 [======>.......................] - ETA: 5:35 - loss: 3.3375 - regression_loss: 2.4996 - classification_loss: 0.8380
 258/1000 [======>.......................] - ETA: 5:35 - loss: 3.3478 - regression_loss: 2.5053 - classification_loss: 0.8425
 259/1000 [======>.......................] - ETA: 5:35 - loss: 3.3537 - regression_loss: 2.5082 - classification_loss: 0.8455
 260/1000 [======>.......................] - ETA: 5:34 - loss: 3.3578 - regression_loss: 2.5110 - classification_loss: 0.8468
 261/1000 [======>.......................] - ETA: 5:34 - loss: 3.3562 - regression_loss: 2.5109 - classification_loss: 0.8452
 262/1000 [======>.......................] - ETA: 5:33 - loss: 3.3587 - regression_loss: 2.5146 - classification_loss: 0.8441
 263/1000 [======>.......................] - ETA: 5:33 - loss: 3.3654 - regression_loss: 2.5216 - classification_loss: 0.8438
 264/1000 [======>.......................] - ETA: 5:32 - loss: 3.3674 - regression_loss: 2.5250 - classification_loss: 0.8424
 265/1000 [======>.......................] - ETA: 5:32 - loss: 3.3733 - regression_loss: 2.5269 - classification_loss: 0.8464
 266/1000 [======>.......................] - ETA: 5:31 - loss: 3.3813 - regression_loss: 2.5333 - classification_loss: 0.8480
 267/1000 [=======>......................] - ETA: 5:31 - loss: 3.3779 - regression_loss: 2.5321 - classification_loss: 0.8458
 268/1000 [=======>......................] - ETA: 5:31 - loss: 3.3653 - regression_loss: 2.5227 - classification_loss: 0.8426
 269/1000 [=======>......................] - ETA: 5:30 - loss: 3.3653 - regression_loss: 2.5231 - classification_loss: 0.8422
 270/1000 [=======>......................] - ETA: 5:30 - loss: 3.3632 - regression_loss: 2.5215 - classification_loss: 0.8417
 271/1000 [=======>......................] - ETA: 5:29 - loss: 3.3508 - regression_loss: 2.5122 - classification_loss: 0.8387
 272/1000 [=======>......................] - ETA: 5:29 - loss: 3.3523 - regression_loss: 2.5136 - classification_loss: 0.8387
 273/1000 [=======>......................] - ETA: 5:28 - loss: 3.3507 - regression_loss: 2.5133 - classification_loss: 0.8375
 274/1000 [=======>......................] - ETA: 5:28 - loss: 3.3530 - regression_loss: 2.5141 - classification_loss: 0.8389
 275/1000 [=======>......................] - ETA: 5:27 - loss: 3.3594 - regression_loss: 2.5179 - classification_loss: 0.8415
 276/1000 [=======>......................] - ETA: 5:27 - loss: 3.3602 - regression_loss: 2.5181 - classification_loss: 0.8421
 277/1000 [=======>......................] - ETA: 5:27 - loss: 3.3638 - regression_loss: 2.5200 - classification_loss: 0.8437
 278/1000 [=======>......................] - ETA: 5:26 - loss: 3.3721 - regression_loss: 2.5269 - classification_loss: 0.8453
 279/1000 [=======>......................] - ETA: 5:26 - loss: 3.3735 - regression_loss: 2.5282 - classification_loss: 0.8454
 280/1000 [=======>......................] - ETA: 5:25 - loss: 3.3734 - regression_loss: 2.5278 - classification_loss: 0.8456
 281/1000 [=======>......................] - ETA: 5:25 - loss: 3.3743 - regression_loss: 2.5296 - classification_loss: 0.8447
 282/1000 [=======>......................] - ETA: 5:24 - loss: 3.3773 - regression_loss: 2.5316 - classification_loss: 0.8457
 283/1000 [=======>......................] - ETA: 5:24 - loss: 3.3758 - regression_loss: 2.5309 - classification_loss: 0.8449
 284/1000 [=======>......................] - ETA: 5:23 - loss: 3.3782 - regression_loss: 2.5334 - classification_loss: 0.8448
 285/1000 [=======>......................] - ETA: 5:23 - loss: 3.3819 - regression_loss: 2.5372 - classification_loss: 0.8446
 286/1000 [=======>......................] - ETA: 5:22 - loss: 3.3840 - regression_loss: 2.5402 - classification_loss: 0.8438
 287/1000 [=======>......................] - ETA: 5:22 - loss: 3.3859 - regression_loss: 2.5421 - classification_loss: 0.8437
 288/1000 [=======>......................] - ETA: 5:22 - loss: 3.3879 - regression_loss: 2.5449 - classification_loss: 0.8430
 289/1000 [=======>......................] - ETA: 5:21 - loss: 3.3898 - regression_loss: 2.5474 - classification_loss: 0.8425
 290/1000 [=======>......................] - ETA: 5:21 - loss: 3.3916 - regression_loss: 2.5502 - classification_loss: 0.8414
 291/1000 [=======>......................] - ETA: 5:20 - loss: 3.3975 - regression_loss: 2.5552 - classification_loss: 0.8423
 292/1000 [=======>......................] - ETA: 5:20 - loss: 3.3996 - regression_loss: 2.5465 - classification_loss: 0.8532
 293/1000 [=======>......................] - ETA: 5:19 - loss: 3.4009 - regression_loss: 2.5487 - classification_loss: 0.8522
 294/1000 [=======>......................] - ETA: 5:19 - loss: 3.4029 - regression_loss: 2.5514 - classification_loss: 0.8514
 295/1000 [=======>......................] - ETA: 5:18 - loss: 3.4065 - regression_loss: 2.5546 - classification_loss: 0.8519
 296/1000 [=======>......................] - ETA: 5:18 - loss: 3.4010 - regression_loss: 2.5459 - classification_loss: 0.8550
 297/1000 [=======>......................] - ETA: 5:17 - loss: 3.3901 - regression_loss: 2.5374 - classification_loss: 0.8528
 298/1000 [=======>......................] - ETA: 5:17 - loss: 3.3923 - regression_loss: 2.5400 - classification_loss: 0.8524
 299/1000 [=======>......................] - ETA: 5:17 - loss: 3.3938 - regression_loss: 2.5416 - classification_loss: 0.8522
 300/1000 [========>.....................] - ETA: 5:16 - loss: 3.3825 - regression_loss: 2.5331 - classification_loss: 0.8494
 301/1000 [========>.....................] - ETA: 5:16 - loss: 3.3861 - regression_loss: 2.5373 - classification_loss: 0.8488
 302/1000 [========>.....................] - ETA: 5:15 - loss: 3.3914 - regression_loss: 2.5421 - classification_loss: 0.8494
 303/1000 [========>.....................] - ETA: 5:15 - loss: 3.3932 - regression_loss: 2.5443 - classification_loss: 0.8490
 304/1000 [========>.....................] - ETA: 5:14 - loss: 3.3940 - regression_loss: 2.5448 - classification_loss: 0.8492
 305/1000 [========>.....................] - ETA: 5:14 - loss: 3.3961 - regression_loss: 2.5457 - classification_loss: 0.8504
 306/1000 [========>.....................] - ETA: 5:13 - loss: 3.3993 - regression_loss: 2.5496 - classification_loss: 0.8498
 307/1000 [========>.....................] - ETA: 5:13 - loss: 3.4036 - regression_loss: 2.5532 - classification_loss: 0.8504
 308/1000 [========>.....................] - ETA: 5:12 - loss: 3.3925 - regression_loss: 2.5449 - classification_loss: 0.8476
 309/1000 [========>.....................] - ETA: 5:12 - loss: 3.3990 - regression_loss: 2.5511 - classification_loss: 0.8478
 310/1000 [========>.....................] - ETA: 5:12 - loss: 3.4012 - regression_loss: 2.5532 - classification_loss: 0.8480
 311/1000 [========>.....................] - ETA: 5:11 - loss: 3.4059 - regression_loss: 2.5562 - classification_loss: 0.8497
 312/1000 [========>.....................] - ETA: 5:11 - loss: 3.3954 - regression_loss: 2.5480 - classification_loss: 0.8474
 313/1000 [========>.....................] - ETA: 5:10 - loss: 3.3979 - regression_loss: 2.5504 - classification_loss: 0.8475
 314/1000 [========>.....................] - ETA: 5:10 - loss: 3.3997 - regression_loss: 2.5518 - classification_loss: 0.8480
 315/1000 [========>.....................] - ETA: 5:09 - loss: 3.4017 - regression_loss: 2.5542 - classification_loss: 0.8475
 316/1000 [========>.....................] - ETA: 5:09 - loss: 3.3910 - regression_loss: 2.5461 - classification_loss: 0.8449
 317/1000 [========>.....................] - ETA: 5:08 - loss: 3.3904 - regression_loss: 2.5461 - classification_loss: 0.8443
 318/1000 [========>.....................] - ETA: 5:08 - loss: 3.3865 - regression_loss: 2.5435 - classification_loss: 0.8430
 319/1000 [========>.....................] - ETA: 5:07 - loss: 3.3915 - regression_loss: 2.5482 - classification_loss: 0.8433
 320/1000 [========>.....................] - ETA: 5:07 - loss: 3.3913 - regression_loss: 2.5485 - classification_loss: 0.8428
 321/1000 [========>.....................] - ETA: 5:07 - loss: 3.3935 - regression_loss: 2.5488 - classification_loss: 0.8448
 322/1000 [========>.....................] - ETA: 5:06 - loss: 3.3842 - regression_loss: 2.5408 - classification_loss: 0.8434
 323/1000 [========>.....................] - ETA: 5:06 - loss: 3.3737 - regression_loss: 2.5330 - classification_loss: 0.8408
 324/1000 [========>.....................] - ETA: 5:05 - loss: 3.3734 - regression_loss: 2.5324 - classification_loss: 0.8409
 325/1000 [========>.....................] - ETA: 5:05 - loss: 3.3774 - regression_loss: 2.5368 - classification_loss: 0.8406
 326/1000 [========>.....................] - ETA: 5:04 - loss: 3.3741 - regression_loss: 2.5342 - classification_loss: 0.8399
 327/1000 [========>.....................] - ETA: 5:04 - loss: 3.3765 - regression_loss: 2.5359 - classification_loss: 0.8405
 328/1000 [========>.....................] - ETA: 5:03 - loss: 3.3799 - regression_loss: 2.5378 - classification_loss: 0.8421
 329/1000 [========>.....................] - ETA: 5:03 - loss: 3.3696 - regression_loss: 2.5301 - classification_loss: 0.8395
 330/1000 [========>.....................] - ETA: 5:02 - loss: 3.3722 - regression_loss: 2.5312 - classification_loss: 0.8410
 331/1000 [========>.....................] - ETA: 5:02 - loss: 3.3621 - regression_loss: 2.5236 - classification_loss: 0.8385
 332/1000 [========>.....................] - ETA: 5:02 - loss: 3.3520 - regression_loss: 2.5160 - classification_loss: 0.8360
 333/1000 [========>.....................] - ETA: 5:01 - loss: 3.3543 - regression_loss: 2.5184 - classification_loss: 0.8359
 334/1000 [=========>....................] - ETA: 5:01 - loss: 3.3596 - regression_loss: 2.5217 - classification_loss: 0.8380
 335/1000 [=========>....................] - ETA: 5:00 - loss: 3.3645 - regression_loss: 2.5267 - classification_loss: 0.8378
 336/1000 [=========>....................] - ETA: 5:00 - loss: 3.3716 - regression_loss: 2.5312 - classification_loss: 0.8404
 337/1000 [=========>....................] - ETA: 4:59 - loss: 3.3735 - regression_loss: 2.5311 - classification_loss: 0.8425
 338/1000 [=========>....................] - ETA: 4:59 - loss: 3.3778 - regression_loss: 2.5347 - classification_loss: 0.8431
 339/1000 [=========>....................] - ETA: 4:58 - loss: 3.3786 - regression_loss: 2.5353 - classification_loss: 0.8434
 340/1000 [=========>....................] - ETA: 4:58 - loss: 3.3815 - regression_loss: 2.5378 - classification_loss: 0.8437
 341/1000 [=========>....................] - ETA: 4:57 - loss: 3.3815 - regression_loss: 2.5382 - classification_loss: 0.8433
 342/1000 [=========>....................] - ETA: 4:57 - loss: 3.3822 - regression_loss: 2.5389 - classification_loss: 0.8433
 343/1000 [=========>....................] - ETA: 4:57 - loss: 3.3813 - regression_loss: 2.5371 - classification_loss: 0.8442
 344/1000 [=========>....................] - ETA: 4:56 - loss: 3.3715 - regression_loss: 2.5297 - classification_loss: 0.8417
 345/1000 [=========>....................] - ETA: 4:56 - loss: 3.3778 - regression_loss: 2.5332 - classification_loss: 0.8447
 346/1000 [=========>....................] - ETA: 4:55 - loss: 3.3691 - regression_loss: 2.5258 - classification_loss: 0.8432
 347/1000 [=========>....................] - ETA: 4:55 - loss: 3.3749 - regression_loss: 2.5288 - classification_loss: 0.8461
 348/1000 [=========>....................] - ETA: 4:54 - loss: 3.3768 - regression_loss: 2.5309 - classification_loss: 0.8459
 349/1000 [=========>....................] - ETA: 4:54 - loss: 3.3771 - regression_loss: 2.5300 - classification_loss: 0.8470
 350/1000 [=========>....................] - ETA: 4:53 - loss: 3.3810 - regression_loss: 2.5337 - classification_loss: 0.8473
 351/1000 [=========>....................] - ETA: 4:53 - loss: 3.3829 - regression_loss: 2.5359 - classification_loss: 0.8470
 352/1000 [=========>....................] - ETA: 4:52 - loss: 3.3848 - regression_loss: 2.5373 - classification_loss: 0.8476
 353/1000 [=========>....................] - ETA: 4:52 - loss: 3.3875 - regression_loss: 2.5374 - classification_loss: 0.8501
 354/1000 [=========>....................] - ETA: 4:52 - loss: 3.3870 - regression_loss: 2.5377 - classification_loss: 0.8493
 355/1000 [=========>....................] - ETA: 4:51 - loss: 3.3923 - regression_loss: 2.5408 - classification_loss: 0.8514
 356/1000 [=========>....................] - ETA: 4:51 - loss: 3.3972 - regression_loss: 2.5458 - classification_loss: 0.8514
 357/1000 [=========>....................] - ETA: 4:50 - loss: 3.4003 - regression_loss: 2.5491 - classification_loss: 0.8512
 358/1000 [=========>....................] - ETA: 4:50 - loss: 3.4020 - regression_loss: 2.5500 - classification_loss: 0.8519
 359/1000 [=========>....................] - ETA: 4:49 - loss: 3.4067 - regression_loss: 2.5532 - classification_loss: 0.8536
 360/1000 [=========>....................] - ETA: 4:49 - loss: 3.4090 - regression_loss: 2.5541 - classification_loss: 0.8549
 361/1000 [=========>....................] - ETA: 4:48 - loss: 3.3996 - regression_loss: 2.5471 - classification_loss: 0.8525
 362/1000 [=========>....................] - ETA: 4:48 - loss: 3.4043 - regression_loss: 2.5496 - classification_loss: 0.8548
 363/1000 [=========>....................] - ETA: 4:47 - loss: 3.4077 - regression_loss: 2.5516 - classification_loss: 0.8561
 364/1000 [=========>....................] - ETA: 4:47 - loss: 3.3983 - regression_loss: 2.5446 - classification_loss: 0.8537
 365/1000 [=========>....................] - ETA: 4:47 - loss: 3.3890 - regression_loss: 2.5376 - classification_loss: 0.8514
 366/1000 [=========>....................] - ETA: 4:46 - loss: 3.3963 - regression_loss: 2.5441 - classification_loss: 0.8523
 367/1000 [==========>...................] - ETA: 4:46 - loss: 3.4017 - regression_loss: 2.5479 - classification_loss: 0.8538
 368/1000 [==========>...................] - ETA: 4:45 - loss: 3.4015 - regression_loss: 2.5482 - classification_loss: 0.8533
 369/1000 [==========>...................] - ETA: 4:45 - loss: 3.4074 - regression_loss: 2.5514 - classification_loss: 0.8560
 370/1000 [==========>...................] - ETA: 4:44 - loss: 3.4088 - regression_loss: 2.5531 - classification_loss: 0.8558
 371/1000 [==========>...................] - ETA: 4:44 - loss: 3.4082 - regression_loss: 2.5529 - classification_loss: 0.8553
 372/1000 [==========>...................] - ETA: 4:43 - loss: 3.4081 - regression_loss: 2.5524 - classification_loss: 0.8557
 373/1000 [==========>...................] - ETA: 4:43 - loss: 3.4114 - regression_loss: 2.5546 - classification_loss: 0.8568
 374/1000 [==========>...................] - ETA: 4:43 - loss: 3.4181 - regression_loss: 2.5611 - classification_loss: 0.8569
 375/1000 [==========>...................] - ETA: 4:42 - loss: 3.4177 - regression_loss: 2.5611 - classification_loss: 0.8566
 376/1000 [==========>...................] - ETA: 4:42 - loss: 3.4225 - regression_loss: 2.5654 - classification_loss: 0.8571
 377/1000 [==========>...................] - ETA: 4:41 - loss: 3.4135 - regression_loss: 2.5586 - classification_loss: 0.8549
 378/1000 [==========>...................] - ETA: 4:41 - loss: 3.4144 - regression_loss: 2.5601 - classification_loss: 0.8543
 379/1000 [==========>...................] - ETA: 4:40 - loss: 3.4151 - regression_loss: 2.5620 - classification_loss: 0.8531
 380/1000 [==========>...................] - ETA: 4:40 - loss: 3.4201 - regression_loss: 2.5657 - classification_loss: 0.8544
 381/1000 [==========>...................] - ETA: 4:39 - loss: 3.4112 - regression_loss: 2.5590 - classification_loss: 0.8522
 382/1000 [==========>...................] - ETA: 4:39 - loss: 3.4023 - regression_loss: 2.5523 - classification_loss: 0.8500
 383/1000 [==========>...................] - ETA: 4:39 - loss: 3.4016 - regression_loss: 2.5526 - classification_loss: 0.8490
 384/1000 [==========>...................] - ETA: 4:38 - loss: 3.4050 - regression_loss: 2.5551 - classification_loss: 0.8499
 385/1000 [==========>...................] - ETA: 4:38 - loss: 3.3962 - regression_loss: 2.5485 - classification_loss: 0.8477
 386/1000 [==========>...................] - ETA: 4:37 - loss: 3.4012 - regression_loss: 2.5512 - classification_loss: 0.8500
 387/1000 [==========>...................] - ETA: 4:37 - loss: 3.3924 - regression_loss: 2.5446 - classification_loss: 0.8478
 388/1000 [==========>...................] - ETA: 4:36 - loss: 3.3948 - regression_loss: 2.5474 - classification_loss: 0.8475
 389/1000 [==========>...................] - ETA: 4:36 - loss: 3.3964 - regression_loss: 2.5481 - classification_loss: 0.8483
 390/1000 [==========>...................] - ETA: 4:35 - loss: 3.3992 - regression_loss: 2.5504 - classification_loss: 0.8488
 391/1000 [==========>...................] - ETA: 4:35 - loss: 3.4025 - regression_loss: 2.5528 - classification_loss: 0.8497
 392/1000 [==========>...................] - ETA: 4:34 - loss: 3.4036 - regression_loss: 2.5536 - classification_loss: 0.8500
 393/1000 [==========>...................] - ETA: 4:34 - loss: 3.3950 - regression_loss: 2.5471 - classification_loss: 0.8478
 394/1000 [==========>...................] - ETA: 4:34 - loss: 3.4005 - regression_loss: 2.5489 - classification_loss: 0.8516
 395/1000 [==========>...................] - ETA: 4:33 - loss: 3.4013 - regression_loss: 2.5482 - classification_loss: 0.8531
 396/1000 [==========>...................] - ETA: 4:33 - loss: 3.4050 - regression_loss: 2.5502 - classification_loss: 0.8547
 397/1000 [==========>...................] - ETA: 4:32 - loss: 3.4062 - regression_loss: 2.5515 - classification_loss: 0.8547
 398/1000 [==========>...................] - ETA: 4:32 - loss: 3.4080 - regression_loss: 2.5524 - classification_loss: 0.8556
 399/1000 [==========>...................] - ETA: 4:31 - loss: 3.4101 - regression_loss: 2.5533 - classification_loss: 0.8568
 400/1000 [===========>..................] - ETA: 4:31 - loss: 3.4085 - regression_loss: 2.5524 - classification_loss: 0.8561
 401/1000 [===========>..................] - ETA: 4:30 - loss: 3.4086 - regression_loss: 2.5521 - classification_loss: 0.8564
 402/1000 [===========>..................] - ETA: 4:30 - loss: 3.4138 - regression_loss: 2.5561 - classification_loss: 0.8577
 403/1000 [===========>..................] - ETA: 4:30 - loss: 3.4173 - regression_loss: 2.5589 - classification_loss: 0.8584
 404/1000 [===========>..................] - ETA: 4:29 - loss: 3.4200 - regression_loss: 2.5605 - classification_loss: 0.8595
 405/1000 [===========>..................] - ETA: 4:29 - loss: 3.4201 - regression_loss: 2.5608 - classification_loss: 0.8593
 406/1000 [===========>..................] - ETA: 4:28 - loss: 3.4180 - regression_loss: 2.5594 - classification_loss: 0.8587
 407/1000 [===========>..................] - ETA: 4:28 - loss: 3.4193 - regression_loss: 2.5610 - classification_loss: 0.8583
 408/1000 [===========>..................] - ETA: 4:27 - loss: 3.4181 - regression_loss: 2.5604 - classification_loss: 0.8578
 409/1000 [===========>..................] - ETA: 4:27 - loss: 3.4206 - regression_loss: 2.5612 - classification_loss: 0.8593
 410/1000 [===========>..................] - ETA: 4:26 - loss: 3.4122 - regression_loss: 2.5550 - classification_loss: 0.8572
 411/1000 [===========>..................] - ETA: 4:26 - loss: 3.4163 - regression_loss: 2.5578 - classification_loss: 0.8585
 412/1000 [===========>..................] - ETA: 4:25 - loss: 3.4174 - regression_loss: 2.5596 - classification_loss: 0.8578
 413/1000 [===========>..................] - ETA: 4:25 - loss: 3.4181 - regression_loss: 2.5602 - classification_loss: 0.8579
 414/1000 [===========>..................] - ETA: 4:25 - loss: 3.4185 - regression_loss: 2.5612 - classification_loss: 0.8572
 415/1000 [===========>..................] - ETA: 4:24 - loss: 3.4230 - regression_loss: 2.5652 - classification_loss: 0.8578
 416/1000 [===========>..................] - ETA: 4:24 - loss: 3.4147 - regression_loss: 2.5590 - classification_loss: 0.8557
 417/1000 [===========>..................] - ETA: 4:23 - loss: 3.4168 - regression_loss: 2.5608 - classification_loss: 0.8560
 418/1000 [===========>..................] - ETA: 4:23 - loss: 3.4183 - regression_loss: 2.5626 - classification_loss: 0.8557
 419/1000 [===========>..................] - ETA: 4:22 - loss: 3.4101 - regression_loss: 2.5564 - classification_loss: 0.8537
 420/1000 [===========>..................] - ETA: 4:22 - loss: 3.4133 - regression_loss: 2.5585 - classification_loss: 0.8548
 421/1000 [===========>..................] - ETA: 4:21 - loss: 3.4135 - regression_loss: 2.5586 - classification_loss: 0.8548
 422/1000 [===========>..................] - ETA: 4:21 - loss: 3.4156 - regression_loss: 2.5592 - classification_loss: 0.8563
 423/1000 [===========>..................] - ETA: 4:21 - loss: 3.4176 - regression_loss: 2.5607 - classification_loss: 0.8569
 424/1000 [===========>..................] - ETA: 4:20 - loss: 3.4096 - regression_loss: 2.5547 - classification_loss: 0.8549
 425/1000 [===========>..................] - ETA: 4:20 - loss: 3.4120 - regression_loss: 2.5560 - classification_loss: 0.8560
 426/1000 [===========>..................] - ETA: 4:19 - loss: 3.4133 - regression_loss: 2.5571 - classification_loss: 0.8562
 427/1000 [===========>..................] - ETA: 4:19 - loss: 3.4094 - regression_loss: 2.5512 - classification_loss: 0.8583
 428/1000 [===========>..................] - ETA: 4:18 - loss: 3.4109 - regression_loss: 2.5522 - classification_loss: 0.8587
 429/1000 [===========>..................] - ETA: 4:18 - loss: 3.4030 - regression_loss: 2.5463 - classification_loss: 0.8567
 430/1000 [===========>..................] - ETA: 4:17 - loss: 3.4057 - regression_loss: 2.5476 - classification_loss: 0.8581
 431/1000 [===========>..................] - ETA: 4:17 - loss: 3.4072 - regression_loss: 2.5494 - classification_loss: 0.8578
 432/1000 [===========>..................] - ETA: 4:16 - loss: 3.4082 - regression_loss: 2.5509 - classification_loss: 0.8573
 433/1000 [===========>..................] - ETA: 4:16 - loss: 3.4084 - regression_loss: 2.5510 - classification_loss: 0.8574
 434/1000 [============>.................] - ETA: 4:16 - loss: 3.4055 - regression_loss: 2.5493 - classification_loss: 0.8561
 435/1000 [============>.................] - ETA: 4:15 - loss: 3.4060 - regression_loss: 2.5505 - classification_loss: 0.8555
 436/1000 [============>.................] - ETA: 4:15 - loss: 3.3989 - regression_loss: 2.5447 - classification_loss: 0.8542
 437/1000 [============>.................] - ETA: 4:14 - loss: 3.3911 - regression_loss: 2.5389 - classification_loss: 0.8523
 438/1000 [============>.................] - ETA: 4:14 - loss: 3.3920 - regression_loss: 2.5402 - classification_loss: 0.8518
 439/1000 [============>.................] - ETA: 4:13 - loss: 3.3972 - regression_loss: 2.5453 - classification_loss: 0.8518
 440/1000 [============>.................] - ETA: 4:13 - loss: 3.3895 - regression_loss: 2.5395 - classification_loss: 0.8499
 441/1000 [============>.................] - ETA: 4:12 - loss: 3.3922 - regression_loss: 2.5408 - classification_loss: 0.8514
 442/1000 [============>.................] - ETA: 4:12 - loss: 3.3966 - regression_loss: 2.5434 - classification_loss: 0.8532
 443/1000 [============>.................] - ETA: 4:11 - loss: 3.3975 - regression_loss: 2.5449 - classification_loss: 0.8526
 444/1000 [============>.................] - ETA: 4:11 - loss: 3.4007 - regression_loss: 2.5487 - classification_loss: 0.8520
 445/1000 [============>.................] - ETA: 4:11 - loss: 3.4043 - regression_loss: 2.5511 - classification_loss: 0.8532
 446/1000 [============>.................] - ETA: 4:10 - loss: 3.4060 - regression_loss: 2.5519 - classification_loss: 0.8541
 447/1000 [============>.................] - ETA: 4:10 - loss: 3.3983 - regression_loss: 2.5462 - classification_loss: 0.8522
 448/1000 [============>.................] - ETA: 4:09 - loss: 3.3981 - regression_loss: 2.5468 - classification_loss: 0.8513
 449/1000 [============>.................] - ETA: 4:09 - loss: 3.4015 - regression_loss: 2.5489 - classification_loss: 0.8526
 450/1000 [============>.................] - ETA: 4:08 - loss: 3.4028 - regression_loss: 2.5488 - classification_loss: 0.8540
 451/1000 [============>.................] - ETA: 4:08 - loss: 3.4042 - regression_loss: 2.5510 - classification_loss: 0.8532
 452/1000 [============>.................] - ETA: 4:07 - loss: 3.4057 - regression_loss: 2.5521 - classification_loss: 0.8537
 453/1000 [============>.................] - ETA: 4:07 - loss: 3.4041 - regression_loss: 2.5514 - classification_loss: 0.8527
 454/1000 [============>.................] - ETA: 4:06 - loss: 3.4097 - regression_loss: 2.5547 - classification_loss: 0.8550
 455/1000 [============>.................] - ETA: 4:06 - loss: 3.4139 - regression_loss: 2.5584 - classification_loss: 0.8555
 456/1000 [============>.................] - ETA: 4:06 - loss: 3.4156 - regression_loss: 2.5599 - classification_loss: 0.8557
 457/1000 [============>.................] - ETA: 4:05 - loss: 3.4145 - regression_loss: 2.5595 - classification_loss: 0.8550
 458/1000 [============>.................] - ETA: 4:05 - loss: 3.4071 - regression_loss: 2.5539 - classification_loss: 0.8532
 459/1000 [============>.................] - ETA: 4:04 - loss: 3.3998 - regression_loss: 2.5484 - classification_loss: 0.8515
 460/1000 [============>.................] - ETA: 4:04 - loss: 3.4013 - regression_loss: 2.5505 - classification_loss: 0.8508
 461/1000 [============>.................] - ETA: 4:03 - loss: 3.3964 - regression_loss: 2.5450 - classification_loss: 0.8515
 462/1000 [============>.................] - ETA: 4:03 - loss: 3.4006 - regression_loss: 2.5481 - classification_loss: 0.8526
 463/1000 [============>.................] - ETA: 4:02 - loss: 3.4017 - regression_loss: 2.5497 - classification_loss: 0.8520
 464/1000 [============>.................] - ETA: 4:02 - loss: 3.4040 - regression_loss: 2.5507 - classification_loss: 0.8533
 465/1000 [============>.................] - ETA: 4:02 - loss: 3.4061 - regression_loss: 2.5521 - classification_loss: 0.8540
 466/1000 [============>.................] - ETA: 4:01 - loss: 3.4064 - regression_loss: 2.5533 - classification_loss: 0.8531
 467/1000 [=============>................] - ETA: 4:01 - loss: 3.4063 - regression_loss: 2.5540 - classification_loss: 0.8523
 468/1000 [=============>................] - ETA: 4:00 - loss: 3.4119 - regression_loss: 2.5596 - classification_loss: 0.8523
 469/1000 [=============>................] - ETA: 4:00 - loss: 3.4048 - regression_loss: 2.5541 - classification_loss: 0.8506
 470/1000 [=============>................] - ETA: 3:59 - loss: 3.3975 - regression_loss: 2.5487 - classification_loss: 0.8488
 471/1000 [=============>................] - ETA: 3:59 - loss: 3.3998 - regression_loss: 2.5506 - classification_loss: 0.8492
 472/1000 [=============>................] - ETA: 3:58 - loss: 3.3926 - regression_loss: 2.5452 - classification_loss: 0.8474
 473/1000 [=============>................] - ETA: 3:58 - loss: 3.3933 - regression_loss: 2.5464 - classification_loss: 0.8469
 474/1000 [=============>................] - ETA: 3:57 - loss: 3.3962 - regression_loss: 2.5483 - classification_loss: 0.8479
 475/1000 [=============>................] - ETA: 3:57 - loss: 3.3891 - regression_loss: 2.5429 - classification_loss: 0.8462
 476/1000 [=============>................] - ETA: 3:57 - loss: 3.3928 - regression_loss: 2.5448 - classification_loss: 0.8480
 477/1000 [=============>................] - ETA: 3:56 - loss: 3.3946 - regression_loss: 2.5459 - classification_loss: 0.8487
 478/1000 [=============>................] - ETA: 3:56 - loss: 3.3981 - regression_loss: 2.5479 - classification_loss: 0.8501
 479/1000 [=============>................] - ETA: 3:55 - loss: 3.4007 - regression_loss: 2.5502 - classification_loss: 0.8505
 480/1000 [=============>................] - ETA: 3:55 - loss: 3.4027 - regression_loss: 2.5526 - classification_loss: 0.8501
 481/1000 [=============>................] - ETA: 3:54 - loss: 3.4024 - regression_loss: 2.5527 - classification_loss: 0.8497
 482/1000 [=============>................] - ETA: 3:54 - loss: 3.4040 - regression_loss: 2.5540 - classification_loss: 0.8501
 483/1000 [=============>................] - ETA: 3:53 - loss: 3.3970 - regression_loss: 2.5487 - classification_loss: 0.8483
 484/1000 [=============>................] - ETA: 3:53 - loss: 3.3958 - regression_loss: 2.5481 - classification_loss: 0.8477
 485/1000 [=============>................] - ETA: 3:52 - loss: 3.4003 - regression_loss: 2.5520 - classification_loss: 0.8483
 486/1000 [=============>................] - ETA: 3:52 - loss: 3.4031 - regression_loss: 2.5517 - classification_loss: 0.8514
 487/1000 [=============>................] - ETA: 3:52 - loss: 3.4048 - regression_loss: 2.5536 - classification_loss: 0.8512
 488/1000 [=============>................] - ETA: 3:51 - loss: 3.4045 - regression_loss: 2.5538 - classification_loss: 0.8507
 489/1000 [=============>................] - ETA: 3:51 - loss: 3.4036 - regression_loss: 2.5536 - classification_loss: 0.8500
 490/1000 [=============>................] - ETA: 3:50 - loss: 3.4057 - regression_loss: 2.5543 - classification_loss: 0.8514
 491/1000 [=============>................] - ETA: 3:50 - loss: 3.4054 - regression_loss: 2.5545 - classification_loss: 0.8509
 492/1000 [=============>................] - ETA: 3:49 - loss: 3.4113 - regression_loss: 2.5582 - classification_loss: 0.8530
 493/1000 [=============>................] - ETA: 3:49 - loss: 3.4141 - regression_loss: 2.5602 - classification_loss: 0.8539
 494/1000 [=============>................] - ETA: 3:48 - loss: 3.4071 - regression_loss: 2.5550 - classification_loss: 0.8522
 495/1000 [=============>................] - ETA: 3:48 - loss: 3.4135 - regression_loss: 2.5595 - classification_loss: 0.8540
 496/1000 [=============>................] - ETA: 3:48 - loss: 3.4067 - regression_loss: 2.5543 - classification_loss: 0.8523
 497/1000 [=============>................] - ETA: 3:47 - loss: 3.4102 - regression_loss: 2.5572 - classification_loss: 0.8530
 498/1000 [=============>................] - ETA: 3:47 - loss: 3.4033 - regression_loss: 2.5521 - classification_loss: 0.8513
 499/1000 [=============>................] - ETA: 3:46 - loss: 3.4025 - regression_loss: 2.5518 - classification_loss: 0.8506
 500/1000 [==============>...............] - ETA: 3:46 - loss: 3.3957 - regression_loss: 2.5467 - classification_loss: 0.8489
 501/1000 [==============>...............] - ETA: 3:45 - loss: 3.3970 - regression_loss: 2.5462 - classification_loss: 0.8507
 502/1000 [==============>...............] - ETA: 3:45 - loss: 3.3952 - regression_loss: 2.5451 - classification_loss: 0.8501
 503/1000 [==============>...............] - ETA: 3:44 - loss: 3.3885 - regression_loss: 2.5401 - classification_loss: 0.8484
 504/1000 [==============>...............] - ETA: 3:44 - loss: 3.3940 - regression_loss: 2.5430 - classification_loss: 0.8509
 505/1000 [==============>...............] - ETA: 3:43 - loss: 3.3955 - regression_loss: 2.5430 - classification_loss: 0.8525
 506/1000 [==============>...............] - ETA: 3:43 - loss: 3.3962 - regression_loss: 2.5437 - classification_loss: 0.8525
 507/1000 [==============>...............] - ETA: 3:43 - loss: 3.3895 - regression_loss: 2.5386 - classification_loss: 0.8508
 508/1000 [==============>...............] - ETA: 3:42 - loss: 3.3917 - regression_loss: 2.5408 - classification_loss: 0.8509
 509/1000 [==============>...............] - ETA: 3:42 - loss: 3.3945 - regression_loss: 2.5433 - classification_loss: 0.8512
 510/1000 [==============>...............] - ETA: 3:41 - loss: 3.3954 - regression_loss: 2.5433 - classification_loss: 0.8521
 511/1000 [==============>...............] - ETA: 3:41 - loss: 3.3972 - regression_loss: 2.5455 - classification_loss: 0.8517
 512/1000 [==============>...............] - ETA: 3:40 - loss: 3.3968 - regression_loss: 2.5462 - classification_loss: 0.8506
 513/1000 [==============>...............] - ETA: 3:40 - loss: 3.3983 - regression_loss: 2.5470 - classification_loss: 0.8513
 514/1000 [==============>...............] - ETA: 3:39 - loss: 3.3998 - regression_loss: 2.5493 - classification_loss: 0.8505
 515/1000 [==============>...............] - ETA: 3:39 - loss: 3.3932 - regression_loss: 2.5444 - classification_loss: 0.8489
 516/1000 [==============>...............] - ETA: 3:38 - loss: 3.3937 - regression_loss: 2.5449 - classification_loss: 0.8489
 517/1000 [==============>...............] - ETA: 3:38 - loss: 3.3946 - regression_loss: 2.5448 - classification_loss: 0.8498
 518/1000 [==============>...............] - ETA: 3:38 - loss: 3.3971 - regression_loss: 2.5468 - classification_loss: 0.8503
 519/1000 [==============>...............] - ETA: 3:37 - loss: 3.4000 - regression_loss: 2.5485 - classification_loss: 0.8514
 520/1000 [==============>...............] - ETA: 3:37 - loss: 3.4010 - regression_loss: 2.5495 - classification_loss: 0.8515
 521/1000 [==============>...............] - ETA: 3:36 - loss: 3.4051 - regression_loss: 2.5522 - classification_loss: 0.8529
 522/1000 [==============>...............] - ETA: 3:36 - loss: 3.4045 - regression_loss: 2.5526 - classification_loss: 0.8518
 523/1000 [==============>...............] - ETA: 3:35 - loss: 3.4075 - regression_loss: 2.5545 - classification_loss: 0.8530
 524/1000 [==============>...............] - ETA: 3:35 - loss: 3.4010 - regression_loss: 2.5496 - classification_loss: 0.8513
 525/1000 [==============>...............] - ETA: 3:34 - loss: 3.4010 - regression_loss: 2.5490 - classification_loss: 0.8520
 526/1000 [==============>...............] - ETA: 3:34 - loss: 3.4027 - regression_loss: 2.5507 - classification_loss: 0.8520
 527/1000 [==============>...............] - ETA: 3:33 - loss: 3.4033 - regression_loss: 2.5506 - classification_loss: 0.8527
 528/1000 [==============>...............] - ETA: 3:33 - loss: 3.4075 - regression_loss: 2.5541 - classification_loss: 0.8535
 529/1000 [==============>...............] - ETA: 3:33 - loss: 3.4011 - regression_loss: 2.5492 - classification_loss: 0.8518
 530/1000 [==============>...............] - ETA: 3:32 - loss: 3.3950 - regression_loss: 2.5444 - classification_loss: 0.8505
 531/1000 [==============>...............] - ETA: 3:32 - loss: 3.3886 - regression_loss: 2.5396 - classification_loss: 0.8489
 532/1000 [==============>...............] - ETA: 3:31 - loss: 3.3889 - regression_loss: 2.5394 - classification_loss: 0.8495
 533/1000 [==============>...............] - ETA: 3:31 - loss: 3.3906 - regression_loss: 2.5406 - classification_loss: 0.8500
 534/1000 [===============>..............] - ETA: 3:30 - loss: 3.3892 - regression_loss: 2.5398 - classification_loss: 0.8494
 535/1000 [===============>..............] - ETA: 3:30 - loss: 3.3927 - regression_loss: 2.5412 - classification_loss: 0.8515
 536/1000 [===============>..............] - ETA: 3:29 - loss: 3.3949 - regression_loss: 2.5426 - classification_loss: 0.8524
 537/1000 [===============>..............] - ETA: 3:29 - loss: 3.3959 - regression_loss: 2.5439 - classification_loss: 0.8520
 538/1000 [===============>..............] - ETA: 3:28 - loss: 3.3978 - regression_loss: 2.5448 - classification_loss: 0.8531
 539/1000 [===============>..............] - ETA: 3:28 - loss: 3.3989 - regression_loss: 2.5456 - classification_loss: 0.8533
 540/1000 [===============>..............] - ETA: 3:28 - loss: 3.4000 - regression_loss: 2.5464 - classification_loss: 0.8536
 541/1000 [===============>..............] - ETA: 3:27 - loss: 3.3996 - regression_loss: 2.5464 - classification_loss: 0.8532
 542/1000 [===============>..............] - ETA: 3:27 - loss: 3.3980 - regression_loss: 2.5455 - classification_loss: 0.8526
 543/1000 [===============>..............] - ETA: 3:26 - loss: 3.3987 - regression_loss: 2.5464 - classification_loss: 0.8523
 544/1000 [===============>..............] - ETA: 3:26 - loss: 3.3993 - regression_loss: 2.5475 - classification_loss: 0.8518
 545/1000 [===============>..............] - ETA: 3:25 - loss: 3.3931 - regression_loss: 2.5428 - classification_loss: 0.8503
 546/1000 [===============>..............] - ETA: 3:25 - loss: 3.3934 - regression_loss: 2.5435 - classification_loss: 0.8500
 547/1000 [===============>..............] - ETA: 3:24 - loss: 3.3910 - regression_loss: 2.5418 - classification_loss: 0.8492
 548/1000 [===============>..............] - ETA: 3:24 - loss: 3.3914 - regression_loss: 2.5428 - classification_loss: 0.8486
 549/1000 [===============>..............] - ETA: 3:24 - loss: 3.3909 - regression_loss: 2.5421 - classification_loss: 0.8489
 550/1000 [===============>..............] - ETA: 3:23 - loss: 3.3956 - regression_loss: 2.5455 - classification_loss: 0.8501
 551/1000 [===============>..............] - ETA: 3:23 - loss: 3.3979 - regression_loss: 2.5472 - classification_loss: 0.8507
 552/1000 [===============>..............] - ETA: 3:22 - loss: 3.3995 - regression_loss: 2.5480 - classification_loss: 0.8515
 553/1000 [===============>..............] - ETA: 3:22 - loss: 3.3934 - regression_loss: 2.5433 - classification_loss: 0.8500
 554/1000 [===============>..............] - ETA: 3:21 - loss: 3.3952 - regression_loss: 2.5441 - classification_loss: 0.8511
 555/1000 [===============>..............] - ETA: 3:21 - loss: 3.3968 - regression_loss: 2.5445 - classification_loss: 0.8523
 556/1000 [===============>..............] - ETA: 3:20 - loss: 3.3909 - regression_loss: 2.5399 - classification_loss: 0.8510
 557/1000 [===============>..............] - ETA: 3:20 - loss: 3.3926 - regression_loss: 2.5408 - classification_loss: 0.8519
 558/1000 [===============>..............] - ETA: 3:19 - loss: 3.3929 - regression_loss: 2.5415 - classification_loss: 0.8514
 559/1000 [===============>..............] - ETA: 3:19 - loss: 3.3962 - regression_loss: 2.5431 - classification_loss: 0.8532
 560/1000 [===============>..............] - ETA: 3:19 - loss: 3.3960 - regression_loss: 2.5431 - classification_loss: 0.8529
 561/1000 [===============>..............] - ETA: 3:18 - loss: 3.3962 - regression_loss: 2.5438 - classification_loss: 0.8524
 562/1000 [===============>..............] - ETA: 3:18 - loss: 3.3979 - regression_loss: 2.5441 - classification_loss: 0.8538
 563/1000 [===============>..............] - ETA: 3:17 - loss: 3.4002 - regression_loss: 2.5456 - classification_loss: 0.8546
 564/1000 [===============>..............] - ETA: 3:17 - loss: 3.4004 - regression_loss: 2.5467 - classification_loss: 0.8537
 565/1000 [===============>..............] - ETA: 3:16 - loss: 3.4013 - regression_loss: 2.5477 - classification_loss: 0.8536
 566/1000 [===============>..............] - ETA: 3:16 - loss: 3.3953 - regression_loss: 2.5432 - classification_loss: 0.8521
 567/1000 [================>.............] - ETA: 3:15 - loss: 3.3894 - regression_loss: 2.5387 - classification_loss: 0.8507
 568/1000 [================>.............] - ETA: 3:15 - loss: 3.3891 - regression_loss: 2.5386 - classification_loss: 0.8505
 569/1000 [================>.............] - ETA: 3:14 - loss: 3.3905 - regression_loss: 2.5400 - classification_loss: 0.8505
 570/1000 [================>.............] - ETA: 3:14 - loss: 3.3914 - regression_loss: 2.5413 - classification_loss: 0.8501
 571/1000 [================>.............] - ETA: 3:14 - loss: 3.3856 - regression_loss: 2.5369 - classification_loss: 0.8487
 572/1000 [================>.............] - ETA: 3:13 - loss: 3.3858 - regression_loss: 2.5375 - classification_loss: 0.8483
 573/1000 [================>.............] - ETA: 3:13 - loss: 3.3882 - regression_loss: 2.5385 - classification_loss: 0.8497
 574/1000 [================>.............] - ETA: 3:12 - loss: 3.3921 - regression_loss: 2.5416 - classification_loss: 0.8505
 575/1000 [================>.............] - ETA: 3:12 - loss: 3.3940 - regression_loss: 2.5436 - classification_loss: 0.8505
 576/1000 [================>.............] - ETA: 3:11 - loss: 3.3956 - regression_loss: 2.5450 - classification_loss: 0.8506
 577/1000 [================>.............] - ETA: 3:11 - loss: 3.3978 - regression_loss: 2.5477 - classification_loss: 0.8502
 578/1000 [================>.............] - ETA: 3:10 - loss: 3.3983 - regression_loss: 2.5484 - classification_loss: 0.8499
 579/1000 [================>.............] - ETA: 3:10 - loss: 3.3925 - regression_loss: 2.5440 - classification_loss: 0.8484
 580/1000 [================>.............] - ETA: 3:10 - loss: 3.3923 - regression_loss: 2.5439 - classification_loss: 0.8484
 581/1000 [================>.............] - ETA: 3:09 - loss: 3.3923 - regression_loss: 2.5442 - classification_loss: 0.8481
 582/1000 [================>.............] - ETA: 3:09 - loss: 3.3949 - regression_loss: 2.5462 - classification_loss: 0.8487
 583/1000 [================>.............] - ETA: 3:08 - loss: 3.3891 - regression_loss: 2.5419 - classification_loss: 0.8472
 584/1000 [================>.............] - ETA: 3:08 - loss: 3.3894 - regression_loss: 2.5419 - classification_loss: 0.8475
 585/1000 [================>.............] - ETA: 3:07 - loss: 3.3906 - regression_loss: 2.5433 - classification_loss: 0.8473
 586/1000 [================>.............] - ETA: 3:07 - loss: 3.3938 - regression_loss: 2.5456 - classification_loss: 0.8482
 587/1000 [================>.............] - ETA: 3:06 - loss: 3.3888 - regression_loss: 2.5413 - classification_loss: 0.8475
 588/1000 [================>.............] - ETA: 3:06 - loss: 3.3900 - regression_loss: 2.5426 - classification_loss: 0.8474
 589/1000 [================>.............] - ETA: 3:05 - loss: 3.3843 - regression_loss: 2.5383 - classification_loss: 0.8460
 590/1000 [================>.............] - ETA: 3:05 - loss: 3.3785 - regression_loss: 2.5340 - classification_loss: 0.8445
 591/1000 [================>.............] - ETA: 3:05 - loss: 3.3793 - regression_loss: 2.5348 - classification_loss: 0.8445
 592/1000 [================>.............] - ETA: 3:04 - loss: 3.3736 - regression_loss: 2.5305 - classification_loss: 0.8431
 593/1000 [================>.............] - ETA: 3:04 - loss: 3.3727 - regression_loss: 2.5299 - classification_loss: 0.8428
 594/1000 [================>.............] - ETA: 3:03 - loss: 3.3725 - regression_loss: 2.5304 - classification_loss: 0.8422
 595/1000 [================>.............] - ETA: 3:03 - loss: 3.3749 - regression_loss: 2.5321 - classification_loss: 0.8428
 596/1000 [================>.............] - ETA: 3:02 - loss: 3.3757 - regression_loss: 2.5334 - classification_loss: 0.8424
 597/1000 [================>.............] - ETA: 3:02 - loss: 3.3761 - regression_loss: 2.5326 - classification_loss: 0.8435
 598/1000 [================>.............] - ETA: 3:01 - loss: 3.3765 - regression_loss: 2.5332 - classification_loss: 0.8433
 599/1000 [================>.............] - ETA: 3:01 - loss: 3.3766 - regression_loss: 2.5337 - classification_loss: 0.8429
 600/1000 [=================>............] - ETA: 3:01 - loss: 3.3773 - regression_loss: 2.5336 - classification_loss: 0.8437
 601/1000 [=================>............] - ETA: 3:00 - loss: 3.3717 - regression_loss: 2.5294 - classification_loss: 0.8423
 602/1000 [=================>............] - ETA: 3:00 - loss: 3.3741 - regression_loss: 2.5318 - classification_loss: 0.8423
 603/1000 [=================>............] - ETA: 2:59 - loss: 3.3685 - regression_loss: 2.5276 - classification_loss: 0.8409
 604/1000 [=================>............] - ETA: 2:59 - loss: 3.3685 - regression_loss: 2.5279 - classification_loss: 0.8406
 605/1000 [=================>............] - ETA: 2:58 - loss: 3.3754 - regression_loss: 2.5328 - classification_loss: 0.8426
 606/1000 [=================>............] - ETA: 2:58 - loss: 3.3753 - regression_loss: 2.5333 - classification_loss: 0.8420
 607/1000 [=================>............] - ETA: 2:57 - loss: 3.3757 - regression_loss: 2.5338 - classification_loss: 0.8419
 608/1000 [=================>............] - ETA: 2:57 - loss: 3.3778 - regression_loss: 2.5347 - classification_loss: 0.8432
 609/1000 [=================>............] - ETA: 2:56 - loss: 3.3791 - regression_loss: 2.5359 - classification_loss: 0.8432
 610/1000 [=================>............] - ETA: 2:56 - loss: 3.3842 - regression_loss: 2.5392 - classification_loss: 0.8450
 611/1000 [=================>............] - ETA: 2:56 - loss: 3.3885 - regression_loss: 2.5432 - classification_loss: 0.8454
 612/1000 [=================>............] - ETA: 2:55 - loss: 3.3891 - regression_loss: 2.5430 - classification_loss: 0.8462
 613/1000 [=================>............] - ETA: 2:55 - loss: 3.3904 - regression_loss: 2.5432 - classification_loss: 0.8473
 614/1000 [=================>............] - ETA: 2:54 - loss: 3.3849 - regression_loss: 2.5390 - classification_loss: 0.8459
 615/1000 [=================>............] - ETA: 2:54 - loss: 3.3860 - regression_loss: 2.5400 - classification_loss: 0.8460
 616/1000 [=================>............] - ETA: 2:53 - loss: 3.3886 - regression_loss: 2.5412 - classification_loss: 0.8474
 617/1000 [=================>............] - ETA: 2:53 - loss: 3.3831 - regression_loss: 2.5371 - classification_loss: 0.8460
 618/1000 [=================>............] - ETA: 2:52 - loss: 3.3820 - regression_loss: 2.5361 - classification_loss: 0.8459
 619/1000 [=================>............] - ETA: 2:52 - loss: 3.3854 - regression_loss: 2.5387 - classification_loss: 0.8467
 620/1000 [=================>............] - ETA: 2:51 - loss: 3.3852 - regression_loss: 2.5385 - classification_loss: 0.8468
 621/1000 [=================>............] - ETA: 2:51 - loss: 3.3869 - regression_loss: 2.5403 - classification_loss: 0.8466
 622/1000 [=================>............] - ETA: 2:51 - loss: 3.3814 - regression_loss: 2.5362 - classification_loss: 0.8452
 623/1000 [=================>............] - ETA: 2:50 - loss: 3.3760 - regression_loss: 2.5321 - classification_loss: 0.8439
 624/1000 [=================>............] - ETA: 2:50 - loss: 3.3706 - regression_loss: 2.5281 - classification_loss: 0.8425
 625/1000 [=================>............] - ETA: 2:49 - loss: 3.3728 - regression_loss: 2.5292 - classification_loss: 0.8436
 626/1000 [=================>............] - ETA: 2:49 - loss: 3.3728 - regression_loss: 2.5296 - classification_loss: 0.8432
 627/1000 [=================>............] - ETA: 2:48 - loss: 3.3675 - regression_loss: 2.5256 - classification_loss: 0.8420
 628/1000 [=================>............] - ETA: 2:48 - loss: 3.3678 - regression_loss: 2.5265 - classification_loss: 0.8413
 629/1000 [=================>............] - ETA: 2:47 - loss: 3.3708 - regression_loss: 2.5290 - classification_loss: 0.8418
 630/1000 [=================>............] - ETA: 2:47 - loss: 3.3721 - regression_loss: 2.5304 - classification_loss: 0.8417
 631/1000 [=================>............] - ETA: 2:46 - loss: 3.3742 - regression_loss: 2.5315 - classification_loss: 0.8427
 632/1000 [=================>............] - ETA: 2:46 - loss: 3.3742 - regression_loss: 2.5319 - classification_loss: 0.8423
 633/1000 [=================>............] - ETA: 2:46 - loss: 3.3728 - regression_loss: 2.5313 - classification_loss: 0.8415
 634/1000 [==================>...........] - ETA: 2:45 - loss: 3.3734 - regression_loss: 2.5307 - classification_loss: 0.8427
 635/1000 [==================>...........] - ETA: 2:45 - loss: 3.3769 - regression_loss: 2.5324 - classification_loss: 0.8445
 636/1000 [==================>...........] - ETA: 2:44 - loss: 3.3790 - regression_loss: 2.5335 - classification_loss: 0.8455
 637/1000 [==================>...........] - ETA: 2:44 - loss: 3.3793 - regression_loss: 2.5343 - classification_loss: 0.8450
 638/1000 [==================>...........] - ETA: 2:43 - loss: 3.3740 - regression_loss: 2.5303 - classification_loss: 0.8437
 639/1000 [==================>...........] - ETA: 2:43 - loss: 3.3746 - regression_loss: 2.5314 - classification_loss: 0.8432
 640/1000 [==================>...........] - ETA: 2:42 - loss: 3.3693 - regression_loss: 2.5274 - classification_loss: 0.8419
 641/1000 [==================>...........] - ETA: 2:42 - loss: 3.3706 - regression_loss: 2.5277 - classification_loss: 0.8429
 642/1000 [==================>...........] - ETA: 2:42 - loss: 3.3653 - regression_loss: 2.5238 - classification_loss: 0.8416
 643/1000 [==================>...........] - ETA: 2:41 - loss: 3.3687 - regression_loss: 2.5256 - classification_loss: 0.8431
 644/1000 [==================>...........] - ETA: 2:41 - loss: 3.3690 - regression_loss: 2.5259 - classification_loss: 0.8431
 645/1000 [==================>...........] - ETA: 2:40 - loss: 3.3638 - regression_loss: 2.5220 - classification_loss: 0.8418
 646/1000 [==================>...........] - ETA: 2:40 - loss: 3.3586 - regression_loss: 2.5181 - classification_loss: 0.8405
 647/1000 [==================>...........] - ETA: 2:39 - loss: 3.3608 - regression_loss: 2.5187 - classification_loss: 0.8422
 648/1000 [==================>...........] - ETA: 2:39 - loss: 3.3634 - regression_loss: 2.5196 - classification_loss: 0.8438
 649/1000 [==================>...........] - ETA: 2:38 - loss: 3.3620 - regression_loss: 2.5192 - classification_loss: 0.8428
 650/1000 [==================>...........] - ETA: 2:38 - loss: 3.3568 - regression_loss: 2.5153 - classification_loss: 0.8415
 651/1000 [==================>...........] - ETA: 2:37 - loss: 3.3517 - regression_loss: 2.5115 - classification_loss: 0.8403
 652/1000 [==================>...........] - ETA: 2:37 - loss: 3.3502 - regression_loss: 2.5107 - classification_loss: 0.8395
 653/1000 [==================>...........] - ETA: 2:37 - loss: 3.3513 - regression_loss: 2.5121 - classification_loss: 0.8393
 654/1000 [==================>...........] - ETA: 2:36 - loss: 3.3514 - regression_loss: 2.5125 - classification_loss: 0.8389
 655/1000 [==================>...........] - ETA: 2:36 - loss: 3.3464 - regression_loss: 2.5087 - classification_loss: 0.8377
 656/1000 [==================>...........] - ETA: 2:35 - loss: 3.3460 - regression_loss: 2.5090 - classification_loss: 0.8370
 657/1000 [==================>...........] - ETA: 2:35 - loss: 3.3468 - regression_loss: 2.5086 - classification_loss: 0.8383
 658/1000 [==================>...........] - ETA: 2:34 - loss: 3.3501 - regression_loss: 2.5106 - classification_loss: 0.8395
 659/1000 [==================>...........] - ETA: 2:34 - loss: 3.3450 - regression_loss: 2.5068 - classification_loss: 0.8382
 660/1000 [==================>...........] - ETA: 2:33 - loss: 3.3493 - regression_loss: 2.5094 - classification_loss: 0.8399
 661/1000 [==================>...........] - ETA: 2:33 - loss: 3.3497 - regression_loss: 2.5093 - classification_loss: 0.8404
 662/1000 [==================>...........] - ETA: 2:32 - loss: 3.3513 - regression_loss: 2.5092 - classification_loss: 0.8421
 663/1000 [==================>...........] - ETA: 2:32 - loss: 3.3508 - regression_loss: 2.5089 - classification_loss: 0.8419
 664/1000 [==================>...........] - ETA: 2:32 - loss: 3.3504 - regression_loss: 2.5089 - classification_loss: 0.8415
 665/1000 [==================>...........] - ETA: 2:31 - loss: 3.3534 - regression_loss: 2.5109 - classification_loss: 0.8425
 666/1000 [==================>...........] - ETA: 2:31 - loss: 3.3555 - regression_loss: 2.5126 - classification_loss: 0.8429
 667/1000 [===================>..........] - ETA: 2:30 - loss: 3.3542 - regression_loss: 2.5120 - classification_loss: 0.8421
 668/1000 [===================>..........] - ETA: 2:30 - loss: 3.3568 - regression_loss: 2.5130 - classification_loss: 0.8438
 669/1000 [===================>..........] - ETA: 2:29 - loss: 3.3518 - regression_loss: 2.5092 - classification_loss: 0.8425
 670/1000 [===================>..........] - ETA: 2:29 - loss: 3.3513 - regression_loss: 2.5092 - classification_loss: 0.8421
 671/1000 [===================>..........] - ETA: 2:28 - loss: 3.3464 - regression_loss: 2.5055 - classification_loss: 0.8409
 672/1000 [===================>..........] - ETA: 2:28 - loss: 3.3470 - regression_loss: 2.5064 - classification_loss: 0.8406
 673/1000 [===================>..........] - ETA: 2:27 - loss: 3.3508 - regression_loss: 2.5103 - classification_loss: 0.8405
 674/1000 [===================>..........] - ETA: 2:27 - loss: 3.3520 - regression_loss: 2.5103 - classification_loss: 0.8417
 675/1000 [===================>..........] - ETA: 2:27 - loss: 3.3533 - regression_loss: 2.5115 - classification_loss: 0.8418
 676/1000 [===================>..........] - ETA: 2:26 - loss: 3.3543 - regression_loss: 2.5117 - classification_loss: 0.8426
 677/1000 [===================>..........] - ETA: 2:26 - loss: 3.3544 - regression_loss: 2.5118 - classification_loss: 0.8426
 678/1000 [===================>..........] - ETA: 2:25 - loss: 3.3549 - regression_loss: 2.5123 - classification_loss: 0.8427
 679/1000 [===================>..........] - ETA: 2:25 - loss: 3.3566 - regression_loss: 2.5135 - classification_loss: 0.8431
 680/1000 [===================>..........] - ETA: 2:24 - loss: 3.3518 - regression_loss: 2.5098 - classification_loss: 0.8420
 681/1000 [===================>..........] - ETA: 2:24 - loss: 3.3544 - regression_loss: 2.5116 - classification_loss: 0.8428
 682/1000 [===================>..........] - ETA: 2:23 - loss: 3.3553 - regression_loss: 2.5120 - classification_loss: 0.8433
 683/1000 [===================>..........] - ETA: 2:23 - loss: 3.3504 - regression_loss: 2.5083 - classification_loss: 0.8421
 684/1000 [===================>..........] - ETA: 2:22 - loss: 3.3539 - regression_loss: 2.5093 - classification_loss: 0.8445
 685/1000 [===================>..........] - ETA: 2:22 - loss: 3.3558 - regression_loss: 2.5106 - classification_loss: 0.8452
 686/1000 [===================>..........] - ETA: 2:22 - loss: 3.3567 - regression_loss: 2.5120 - classification_loss: 0.8448
 687/1000 [===================>..........] - ETA: 2:21 - loss: 3.3519 - regression_loss: 2.5083 - classification_loss: 0.8436
 688/1000 [===================>..........] - ETA: 2:21 - loss: 3.3537 - regression_loss: 2.5087 - classification_loss: 0.8449
 689/1000 [===================>..........] - ETA: 2:20 - loss: 3.3490 - regression_loss: 2.5051 - classification_loss: 0.8439
 690/1000 [===================>..........] - ETA: 2:20 - loss: 3.3523 - regression_loss: 2.5073 - classification_loss: 0.8450
 691/1000 [===================>..........] - ETA: 2:19 - loss: 3.3475 - regression_loss: 2.5037 - classification_loss: 0.8438
 692/1000 [===================>..........] - ETA: 2:19 - loss: 3.3485 - regression_loss: 2.5034 - classification_loss: 0.8452
 693/1000 [===================>..........] - ETA: 2:18 - loss: 3.3479 - regression_loss: 2.5021 - classification_loss: 0.8457
 694/1000 [===================>..........] - ETA: 2:18 - loss: 3.3476 - regression_loss: 2.5022 - classification_loss: 0.8455
 695/1000 [===================>..........] - ETA: 2:17 - loss: 3.3428 - regression_loss: 2.4986 - classification_loss: 0.8442
 696/1000 [===================>..........] - ETA: 2:17 - loss: 3.3380 - regression_loss: 2.4950 - classification_loss: 0.8430
 697/1000 [===================>..........] - ETA: 2:17 - loss: 3.3332 - regression_loss: 2.4914 - classification_loss: 0.8418
 698/1000 [===================>..........] - ETA: 2:16 - loss: 3.3288 - regression_loss: 2.4878 - classification_loss: 0.8410
 699/1000 [===================>..........] - ETA: 2:16 - loss: 3.3241 - regression_loss: 2.4843 - classification_loss: 0.8398
 700/1000 [====================>.........] - ETA: 2:15 - loss: 3.3275 - regression_loss: 2.4861 - classification_loss: 0.8414
 701/1000 [====================>.........] - ETA: 2:15 - loss: 3.3285 - regression_loss: 2.4870 - classification_loss: 0.8415
 702/1000 [====================>.........] - ETA: 2:14 - loss: 3.3301 - regression_loss: 2.4886 - classification_loss: 0.8415
 703/1000 [====================>.........] - ETA: 2:14 - loss: 3.3254 - regression_loss: 2.4851 - classification_loss: 0.8403
 704/1000 [====================>.........] - ETA: 2:13 - loss: 3.3256 - regression_loss: 2.4848 - classification_loss: 0.8407
 705/1000 [====================>.........] - ETA: 2:13 - loss: 3.3209 - regression_loss: 2.4813 - classification_loss: 0.8395
 706/1000 [====================>.........] - ETA: 2:13 - loss: 3.3214 - regression_loss: 2.4817 - classification_loss: 0.8397
 707/1000 [====================>.........] - ETA: 2:12 - loss: 3.3272 - regression_loss: 2.4855 - classification_loss: 0.8417
 708/1000 [====================>.........] - ETA: 2:12 - loss: 3.3289 - regression_loss: 2.4867 - classification_loss: 0.8422
 709/1000 [====================>.........] - ETA: 2:11 - loss: 3.3308 - regression_loss: 2.4873 - classification_loss: 0.8435
 710/1000 [====================>.........] - ETA: 2:11 - loss: 3.3335 - regression_loss: 2.4890 - classification_loss: 0.8445
 711/1000 [====================>.........] - ETA: 2:10 - loss: 3.3340 - regression_loss: 2.4893 - classification_loss: 0.8447
 712/1000 [====================>.........] - ETA: 2:10 - loss: 3.3368 - regression_loss: 2.4912 - classification_loss: 0.8456
 713/1000 [====================>.........] - ETA: 2:09 - loss: 3.3378 - regression_loss: 2.4919 - classification_loss: 0.8459
 714/1000 [====================>.........] - ETA: 2:09 - loss: 3.3389 - regression_loss: 2.4922 - classification_loss: 0.8467
 715/1000 [====================>.........] - ETA: 2:08 - loss: 3.3393 - regression_loss: 2.4912 - classification_loss: 0.8481
 716/1000 [====================>.........] - ETA: 2:08 - loss: 3.3415 - regression_loss: 2.4921 - classification_loss: 0.8494
 717/1000 [====================>.........] - ETA: 2:08 - loss: 3.3420 - regression_loss: 2.4928 - classification_loss: 0.8492
 718/1000 [====================>.........] - ETA: 2:07 - loss: 3.3442 - regression_loss: 2.4939 - classification_loss: 0.8503
 719/1000 [====================>.........] - ETA: 2:07 - loss: 3.3479 - regression_loss: 2.4961 - classification_loss: 0.8518
 720/1000 [====================>.........] - ETA: 2:06 - loss: 3.3433 - regression_loss: 2.4927 - classification_loss: 0.8506
 721/1000 [====================>.........] - ETA: 2:06 - loss: 3.3460 - regression_loss: 2.4945 - classification_loss: 0.8515
 722/1000 [====================>.........] - ETA: 2:05 - loss: 3.3500 - regression_loss: 2.4971 - classification_loss: 0.8529
 723/1000 [====================>.........] - ETA: 2:05 - loss: 3.3516 - regression_loss: 2.4988 - classification_loss: 0.8528
 724/1000 [====================>.........] - ETA: 2:04 - loss: 3.3539 - regression_loss: 2.5007 - classification_loss: 0.8531
 725/1000 [====================>.........] - ETA: 2:04 - loss: 3.3493 - regression_loss: 2.4973 - classification_loss: 0.8520
 726/1000 [====================>.........] - ETA: 2:03 - loss: 3.3527 - regression_loss: 2.4997 - classification_loss: 0.8530
 727/1000 [====================>.........] - ETA: 2:03 - loss: 3.3549 - regression_loss: 2.5013 - classification_loss: 0.8536
 728/1000 [====================>.........] - ETA: 2:03 - loss: 3.3503 - regression_loss: 2.4979 - classification_loss: 0.8524
 729/1000 [====================>.........] - ETA: 2:02 - loss: 3.3512 - regression_loss: 2.4991 - classification_loss: 0.8521
 730/1000 [====================>.........] - ETA: 2:02 - loss: 3.3466 - regression_loss: 2.4957 - classification_loss: 0.8509
 731/1000 [====================>.........] - ETA: 2:01 - loss: 3.3490 - regression_loss: 2.4974 - classification_loss: 0.8516
 732/1000 [====================>.........] - ETA: 2:01 - loss: 3.3444 - regression_loss: 2.4940 - classification_loss: 0.8505
 733/1000 [====================>.........] - ETA: 2:00 - loss: 3.3399 - regression_loss: 2.4906 - classification_loss: 0.8493
 734/1000 [=====================>........] - ETA: 2:00 - loss: 3.3430 - regression_loss: 2.4924 - classification_loss: 0.8506
 735/1000 [=====================>........] - ETA: 1:59 - loss: 3.3428 - regression_loss: 2.4922 - classification_loss: 0.8506
 736/1000 [=====================>........] - ETA: 1:59 - loss: 3.3432 - regression_loss: 2.4926 - classification_loss: 0.8506
 737/1000 [=====================>........] - ETA: 1:59 - loss: 3.3448 - regression_loss: 2.4934 - classification_loss: 0.8514
 738/1000 [=====================>........] - ETA: 1:58 - loss: 3.3447 - regression_loss: 2.4926 - classification_loss: 0.8521
 739/1000 [=====================>........] - ETA: 1:58 - loss: 3.3402 - regression_loss: 2.4893 - classification_loss: 0.8509
 740/1000 [=====================>........] - ETA: 1:57 - loss: 3.3428 - regression_loss: 2.4907 - classification_loss: 0.8521
 741/1000 [=====================>........] - ETA: 1:57 - loss: 3.3438 - regression_loss: 2.4910 - classification_loss: 0.8528
 742/1000 [=====================>........] - ETA: 1:56 - loss: 3.3461 - regression_loss: 2.4919 - classification_loss: 0.8541
 743/1000 [=====================>........] - ETA: 1:56 - loss: 3.3510 - regression_loss: 2.4956 - classification_loss: 0.8554
 744/1000 [=====================>........] - ETA: 1:55 - loss: 3.3509 - regression_loss: 2.4956 - classification_loss: 0.8553
 745/1000 [=====================>........] - ETA: 1:55 - loss: 3.3511 - regression_loss: 2.4946 - classification_loss: 0.8565
 746/1000 [=====================>........] - ETA: 1:54 - loss: 3.3536 - regression_loss: 2.4954 - classification_loss: 0.8581
 747/1000 [=====================>........] - ETA: 1:54 - loss: 3.3560 - regression_loss: 2.4969 - classification_loss: 0.8591
 748/1000 [=====================>........] - ETA: 1:54 - loss: 3.3515 - regression_loss: 2.4936 - classification_loss: 0.8579
 749/1000 [=====================>........] - ETA: 1:53 - loss: 3.3528 - regression_loss: 2.4938 - classification_loss: 0.8590
 750/1000 [=====================>........] - ETA: 1:53 - loss: 3.3547 - regression_loss: 2.4948 - classification_loss: 0.8599
 751/1000 [=====================>........] - ETA: 1:52 - loss: 3.3502 - regression_loss: 2.4915 - classification_loss: 0.8588
 752/1000 [=====================>........] - ETA: 1:52 - loss: 3.3458 - regression_loss: 2.4882 - classification_loss: 0.8576
 753/1000 [=====================>........] - ETA: 1:51 - loss: 3.3489 - regression_loss: 2.4910 - classification_loss: 0.8579
 754/1000 [=====================>........] - ETA: 1:51 - loss: 3.3495 - regression_loss: 2.4915 - classification_loss: 0.8580
 755/1000 [=====================>........] - ETA: 1:50 - loss: 3.3528 - regression_loss: 2.4931 - classification_loss: 0.8597
 756/1000 [=====================>........] - ETA: 1:50 - loss: 3.3525 - regression_loss: 2.4930 - classification_loss: 0.8595
 757/1000 [=====================>........] - ETA: 1:49 - loss: 3.3546 - regression_loss: 2.4949 - classification_loss: 0.8597
 758/1000 [=====================>........] - ETA: 1:49 - loss: 3.3562 - regression_loss: 2.4961 - classification_loss: 0.8600
 759/1000 [=====================>........] - ETA: 1:49 - loss: 3.3570 - regression_loss: 2.4972 - classification_loss: 0.8597
 760/1000 [=====================>........] - ETA: 1:48 - loss: 3.3526 - regression_loss: 2.4940 - classification_loss: 0.8586
 761/1000 [=====================>........] - ETA: 1:48 - loss: 3.3553 - regression_loss: 2.4961 - classification_loss: 0.8593
 762/1000 [=====================>........] - ETA: 1:47 - loss: 3.3565 - regression_loss: 2.4962 - classification_loss: 0.8603
 763/1000 [=====================>........] - ETA: 1:47 - loss: 3.3573 - regression_loss: 2.4963 - classification_loss: 0.8610
 764/1000 [=====================>........] - ETA: 1:46 - loss: 3.3596 - regression_loss: 2.4979 - classification_loss: 0.8616
 765/1000 [=====================>........] - ETA: 1:46 - loss: 3.3626 - regression_loss: 2.5014 - classification_loss: 0.8612
 766/1000 [=====================>........] - ETA: 1:45 - loss: 3.3634 - regression_loss: 2.5015 - classification_loss: 0.8620
 767/1000 [======================>.......] - ETA: 1:45 - loss: 3.3662 - regression_loss: 2.5033 - classification_loss: 0.8629
 768/1000 [======================>.......] - ETA: 1:44 - loss: 3.3676 - regression_loss: 2.5045 - classification_loss: 0.8631
 769/1000 [======================>.......] - ETA: 1:44 - loss: 3.3698 - regression_loss: 2.5070 - classification_loss: 0.8627
 770/1000 [======================>.......] - ETA: 1:44 - loss: 3.3704 - regression_loss: 2.5078 - classification_loss: 0.8626
 771/1000 [======================>.......] - ETA: 1:43 - loss: 3.3719 - regression_loss: 2.5086 - classification_loss: 0.8632
 772/1000 [======================>.......] - ETA: 1:43 - loss: 3.3722 - regression_loss: 2.5094 - classification_loss: 0.8628
 773/1000 [======================>.......] - ETA: 1:42 - loss: 3.3678 - regression_loss: 2.5062 - classification_loss: 0.8616
 774/1000 [======================>.......] - ETA: 1:42 - loss: 3.3687 - regression_loss: 2.5070 - classification_loss: 0.8618
 775/1000 [======================>.......] - ETA: 1:41 - loss: 3.3697 - regression_loss: 2.5081 - classification_loss: 0.8616
 776/1000 [======================>.......] - ETA: 1:41 - loss: 3.3719 - regression_loss: 2.5105 - classification_loss: 0.8614
 777/1000 [======================>.......] - ETA: 1:40 - loss: 3.3675 - regression_loss: 2.5072 - classification_loss: 0.8603
 778/1000 [======================>.......] - ETA: 1:40 - loss: 3.3632 - regression_loss: 2.5040 - classification_loss: 0.8592
 779/1000 [======================>.......] - ETA: 1:40 - loss: 3.3593 - regression_loss: 2.5008 - classification_loss: 0.8585
 780/1000 [======================>.......] - ETA: 1:39 - loss: 3.3592 - regression_loss: 2.5012 - classification_loss: 0.8580
 781/1000 [======================>.......] - ETA: 1:39 - loss: 3.3594 - regression_loss: 2.5014 - classification_loss: 0.8580
 782/1000 [======================>.......] - ETA: 1:38 - loss: 3.3589 - regression_loss: 2.5015 - classification_loss: 0.8574
 783/1000 [======================>.......] - ETA: 1:38 - loss: 3.3591 - regression_loss: 2.5021 - classification_loss: 0.8570
 784/1000 [======================>.......] - ETA: 1:37 - loss: 3.3598 - regression_loss: 2.5028 - classification_loss: 0.8571
 785/1000 [======================>.......] - ETA: 1:37 - loss: 3.3610 - regression_loss: 2.5040 - classification_loss: 0.8571
 786/1000 [======================>.......] - ETA: 1:36 - loss: 3.3663 - regression_loss: 2.5008 - classification_loss: 0.8655
 787/1000 [======================>.......] - ETA: 1:36 - loss: 3.3620 - regression_loss: 2.4976 - classification_loss: 0.8644
 788/1000 [======================>.......] - ETA: 1:35 - loss: 3.3621 - regression_loss: 2.4982 - classification_loss: 0.8639
 789/1000 [======================>.......] - ETA: 1:35 - loss: 3.3619 - regression_loss: 2.4984 - classification_loss: 0.8635
 790/1000 [======================>.......] - ETA: 1:35 - loss: 3.3624 - regression_loss: 2.4991 - classification_loss: 0.8634
 791/1000 [======================>.......] - ETA: 1:34 - loss: 3.3612 - regression_loss: 2.4984 - classification_loss: 0.8628
 792/1000 [======================>.......] - ETA: 1:34 - loss: 3.3569 - regression_loss: 2.4952 - classification_loss: 0.8617
 793/1000 [======================>.......] - ETA: 1:33 - loss: 3.3557 - regression_loss: 2.4946 - classification_loss: 0.8611
 794/1000 [======================>.......] - ETA: 1:33 - loss: 3.3569 - regression_loss: 2.4956 - classification_loss: 0.8613
 795/1000 [======================>.......] - ETA: 1:32 - loss: 3.3577 - regression_loss: 2.4962 - classification_loss: 0.8614
 796/1000 [======================>.......] - ETA: 1:32 - loss: 3.3583 - regression_loss: 2.4963 - classification_loss: 0.8620
 797/1000 [======================>.......] - ETA: 1:31 - loss: 3.3589 - regression_loss: 2.4971 - classification_loss: 0.8618
 798/1000 [======================>.......] - ETA: 1:31 - loss: 3.3604 - regression_loss: 2.4979 - classification_loss: 0.8625
 799/1000 [======================>.......] - ETA: 1:30 - loss: 3.3611 - regression_loss: 2.4990 - classification_loss: 0.8622
 800/1000 [=======================>......] - ETA: 1:30 - loss: 3.3618 - regression_loss: 2.4986 - classification_loss: 0.8632
 801/1000 [=======================>......] - ETA: 1:30 - loss: 3.3648 - regression_loss: 2.4996 - classification_loss: 0.8652
 802/1000 [=======================>......] - ETA: 1:29 - loss: 3.3645 - regression_loss: 2.4999 - classification_loss: 0.8646
 803/1000 [=======================>......] - ETA: 1:29 - loss: 3.3731 - regression_loss: 2.5048 - classification_loss: 0.8683
 804/1000 [=======================>......] - ETA: 1:28 - loss: 3.3689 - regression_loss: 2.5017 - classification_loss: 0.8672
 805/1000 [=======================>......] - ETA: 1:28 - loss: 3.3683 - regression_loss: 2.5012 - classification_loss: 0.8671
 806/1000 [=======================>......] - ETA: 1:27 - loss: 3.3646 - regression_loss: 2.4981 - classification_loss: 0.8665
 807/1000 [=======================>......] - ETA: 1:27 - loss: 3.3645 - regression_loss: 2.4984 - classification_loss: 0.8661
 808/1000 [=======================>......] - ETA: 1:26 - loss: 3.3646 - regression_loss: 2.4985 - classification_loss: 0.8662
 809/1000 [=======================>......] - ETA: 1:26 - loss: 3.3661 - regression_loss: 2.4990 - classification_loss: 0.8671
 810/1000 [=======================>......] - ETA: 1:25 - loss: 3.3678 - regression_loss: 2.5005 - classification_loss: 0.8673
 811/1000 [=======================>......] - ETA: 1:25 - loss: 3.3678 - regression_loss: 2.5006 - classification_loss: 0.8672
 812/1000 [=======================>......] - ETA: 1:25 - loss: 3.3637 - regression_loss: 2.4976 - classification_loss: 0.8661
 813/1000 [=======================>......] - ETA: 1:24 - loss: 3.3657 - regression_loss: 2.4991 - classification_loss: 0.8667
 814/1000 [=======================>......] - ETA: 1:24 - loss: 3.3617 - regression_loss: 2.4960 - classification_loss: 0.8657
 815/1000 [=======================>......] - ETA: 1:23 - loss: 3.3576 - regression_loss: 2.4929 - classification_loss: 0.8646
 816/1000 [=======================>......] - ETA: 1:23 - loss: 3.3535 - regression_loss: 2.4899 - classification_loss: 0.8636
 817/1000 [=======================>......] - ETA: 1:22 - loss: 3.3564 - regression_loss: 2.4917 - classification_loss: 0.8648
 818/1000 [=======================>......] - ETA: 1:22 - loss: 3.3597 - regression_loss: 2.4943 - classification_loss: 0.8653
 819/1000 [=======================>......] - ETA: 1:21 - loss: 3.3621 - regression_loss: 2.4957 - classification_loss: 0.8664
 820/1000 [=======================>......] - ETA: 1:21 - loss: 3.3617 - regression_loss: 2.4955 - classification_loss: 0.8662
 821/1000 [=======================>......] - ETA: 1:20 - loss: 3.3627 - regression_loss: 2.4962 - classification_loss: 0.8664
 822/1000 [=======================>......] - ETA: 1:20 - loss: 3.3626 - regression_loss: 2.4965 - classification_loss: 0.8661
 823/1000 [=======================>......] - ETA: 1:20 - loss: 3.3633 - regression_loss: 2.4975 - classification_loss: 0.8657
 824/1000 [=======================>......] - ETA: 1:19 - loss: 3.3637 - regression_loss: 2.4982 - classification_loss: 0.8655
 825/1000 [=======================>......] - ETA: 1:19 - loss: 3.3636 - regression_loss: 2.4984 - classification_loss: 0.8652
 826/1000 [=======================>......] - ETA: 1:18 - loss: 3.3653 - regression_loss: 2.4991 - classification_loss: 0.8661
 827/1000 [=======================>......] - ETA: 1:18 - loss: 3.3652 - regression_loss: 2.4991 - classification_loss: 0.8661
 828/1000 [=======================>......] - ETA: 1:17 - loss: 3.3676 - regression_loss: 2.5004 - classification_loss: 0.8671
 829/1000 [=======================>......] - ETA: 1:17 - loss: 3.3685 - regression_loss: 2.5017 - classification_loss: 0.8668
 830/1000 [=======================>......] - ETA: 1:16 - loss: 3.3696 - regression_loss: 2.5019 - classification_loss: 0.8677
 831/1000 [=======================>......] - ETA: 1:16 - loss: 3.3655 - regression_loss: 2.4989 - classification_loss: 0.8667
 832/1000 [=======================>......] - ETA: 1:16 - loss: 3.3659 - regression_loss: 2.4990 - classification_loss: 0.8669
 833/1000 [=======================>......] - ETA: 1:15 - loss: 3.3678 - regression_loss: 2.4996 - classification_loss: 0.8682
 834/1000 [========================>.....] - ETA: 1:15 - loss: 3.3691 - regression_loss: 2.5005 - classification_loss: 0.8686
 835/1000 [========================>.....] - ETA: 1:14 - loss: 3.3651 - regression_loss: 2.4975 - classification_loss: 0.8675
 836/1000 [========================>.....] - ETA: 1:14 - loss: 3.3610 - regression_loss: 2.4946 - classification_loss: 0.8665
 837/1000 [========================>.....] - ETA: 1:13 - loss: 3.3630 - regression_loss: 2.4960 - classification_loss: 0.8670
 838/1000 [========================>.....] - ETA: 1:13 - loss: 3.3652 - regression_loss: 2.4972 - classification_loss: 0.8680
 839/1000 [========================>.....] - ETA: 1:12 - loss: 3.3659 - regression_loss: 2.4977 - classification_loss: 0.8681
 840/1000 [========================>.....] - ETA: 1:12 - loss: 3.3676 - regression_loss: 2.4988 - classification_loss: 0.8688
 841/1000 [========================>.....] - ETA: 1:11 - loss: 3.3689 - regression_loss: 2.4995 - classification_loss: 0.8694
 842/1000 [========================>.....] - ETA: 1:11 - loss: 3.3695 - regression_loss: 2.5000 - classification_loss: 0.8694
 843/1000 [========================>.....] - ETA: 1:11 - loss: 3.3695 - regression_loss: 2.5006 - classification_loss: 0.8689
 844/1000 [========================>.....] - ETA: 1:10 - loss: 3.3686 - regression_loss: 2.5002 - classification_loss: 0.8684
 845/1000 [========================>.....] - ETA: 1:10 - loss: 3.3713 - regression_loss: 2.5021 - classification_loss: 0.8692
 846/1000 [========================>.....] - ETA: 1:09 - loss: 3.3732 - regression_loss: 2.5038 - classification_loss: 0.8694
 847/1000 [========================>.....] - ETA: 1:09 - loss: 3.3715 - regression_loss: 2.5028 - classification_loss: 0.8687
 848/1000 [========================>.....] - ETA: 1:08 - loss: 3.3717 - regression_loss: 2.5021 - classification_loss: 0.8696
 849/1000 [========================>.....] - ETA: 1:08 - loss: 3.3726 - regression_loss: 2.5032 - classification_loss: 0.8694
 850/1000 [========================>.....] - ETA: 1:07 - loss: 3.3744 - regression_loss: 2.5045 - classification_loss: 0.8699
 851/1000 [========================>.....] - ETA: 1:07 - loss: 3.3743 - regression_loss: 2.5046 - classification_loss: 0.8698
 852/1000 [========================>.....] - ETA: 1:06 - loss: 3.3745 - regression_loss: 2.5053 - classification_loss: 0.8692
 853/1000 [========================>.....] - ETA: 1:06 - loss: 3.3745 - regression_loss: 2.5050 - classification_loss: 0.8695
 854/1000 [========================>.....] - ETA: 1:06 - loss: 3.3758 - regression_loss: 2.5062 - classification_loss: 0.8696
 855/1000 [========================>.....] - ETA: 1:05 - loss: 3.3772 - regression_loss: 2.5075 - classification_loss: 0.8697
 856/1000 [========================>.....] - ETA: 1:05 - loss: 3.3800 - regression_loss: 2.5096 - classification_loss: 0.8703
 857/1000 [========================>.....] - ETA: 1:04 - loss: 3.3809 - regression_loss: 2.5103 - classification_loss: 0.8706
 858/1000 [========================>.....] - ETA: 1:04 - loss: 3.3827 - regression_loss: 2.5117 - classification_loss: 0.8710
 859/1000 [========================>.....] - ETA: 1:03 - loss: 3.3792 - regression_loss: 2.5088 - classification_loss: 0.8704
 860/1000 [========================>.....] - ETA: 1:03 - loss: 3.3805 - regression_loss: 2.5098 - classification_loss: 0.8708
 861/1000 [========================>.....] - ETA: 1:02 - loss: 3.3829 - regression_loss: 2.5114 - classification_loss: 0.8715
 862/1000 [========================>.....] - ETA: 1:02 - loss: 3.3828 - regression_loss: 2.5116 - classification_loss: 0.8712
 863/1000 [========================>.....] - ETA: 1:01 - loss: 3.3825 - regression_loss: 2.5116 - classification_loss: 0.8708
 864/1000 [========================>.....] - ETA: 1:01 - loss: 3.3828 - regression_loss: 2.5124 - classification_loss: 0.8704
 865/1000 [========================>.....] - ETA: 1:01 - loss: 3.3789 - regression_loss: 2.5095 - classification_loss: 0.8694
 866/1000 [========================>.....] - ETA: 1:00 - loss: 3.3781 - regression_loss: 2.5091 - classification_loss: 0.8690
 867/1000 [=========================>....] - ETA: 1:00 - loss: 3.3792 - regression_loss: 2.5099 - classification_loss: 0.8693
 868/1000 [=========================>....] - ETA: 59s - loss: 3.3807 - regression_loss: 2.5112 - classification_loss: 0.8694 
 869/1000 [=========================>....] - ETA: 59s - loss: 3.3810 - regression_loss: 2.5118 - classification_loss: 0.8692
 870/1000 [=========================>....] - ETA: 58s - loss: 3.3807 - regression_loss: 2.5118 - classification_loss: 0.8689
 871/1000 [=========================>....] - ETA: 58s - loss: 3.3806 - regression_loss: 2.5114 - classification_loss: 0.8692
 872/1000 [=========================>....] - ETA: 57s - loss: 3.3813 - regression_loss: 2.5118 - classification_loss: 0.8695
 873/1000 [=========================>....] - ETA: 57s - loss: 3.3825 - regression_loss: 2.5128 - classification_loss: 0.8697
 874/1000 [=========================>....] - ETA: 57s - loss: 3.3787 - regression_loss: 2.5099 - classification_loss: 0.8687
 875/1000 [=========================>....] - ETA: 56s - loss: 3.3802 - regression_loss: 2.5116 - classification_loss: 0.8686
 876/1000 [=========================>....] - ETA: 56s - loss: 3.3825 - regression_loss: 2.5126 - classification_loss: 0.8699
 877/1000 [=========================>....] - ETA: 55s - loss: 3.3837 - regression_loss: 2.5137 - classification_loss: 0.8700
 878/1000 [=========================>....] - ETA: 55s - loss: 3.3831 - regression_loss: 2.5132 - classification_loss: 0.8699
 879/1000 [=========================>....] - ETA: 54s - loss: 3.3831 - regression_loss: 2.5135 - classification_loss: 0.8696
 880/1000 [=========================>....] - ETA: 54s - loss: 3.3845 - regression_loss: 2.5149 - classification_loss: 0.8696
 881/1000 [=========================>....] - ETA: 53s - loss: 3.3842 - regression_loss: 2.5151 - classification_loss: 0.8691
 882/1000 [=========================>....] - ETA: 53s - loss: 3.3857 - regression_loss: 2.5167 - classification_loss: 0.8689
 883/1000 [=========================>....] - ETA: 52s - loss: 3.3866 - regression_loss: 2.5175 - classification_loss: 0.8691
 884/1000 [=========================>....] - ETA: 52s - loss: 3.3828 - regression_loss: 2.5146 - classification_loss: 0.8681
 885/1000 [=========================>....] - ETA: 52s - loss: 3.3840 - regression_loss: 2.5160 - classification_loss: 0.8681
 886/1000 [=========================>....] - ETA: 51s - loss: 3.3857 - regression_loss: 2.5169 - classification_loss: 0.8688
 887/1000 [=========================>....] - ETA: 51s - loss: 3.3878 - regression_loss: 2.5188 - classification_loss: 0.8691
 888/1000 [=========================>....] - ETA: 50s - loss: 3.3875 - regression_loss: 2.5186 - classification_loss: 0.8689
 889/1000 [=========================>....] - ETA: 50s - loss: 3.3840 - regression_loss: 2.5158 - classification_loss: 0.8682
 890/1000 [=========================>....] - ETA: 49s - loss: 3.3862 - regression_loss: 2.5173 - classification_loss: 0.8689
 891/1000 [=========================>....] - ETA: 49s - loss: 3.3835 - regression_loss: 2.5145 - classification_loss: 0.8690
 892/1000 [=========================>....] - ETA: 48s - loss: 3.3843 - regression_loss: 2.5154 - classification_loss: 0.8690
 893/1000 [=========================>....] - ETA: 48s - loss: 3.3860 - regression_loss: 2.5171 - classification_loss: 0.8689
 894/1000 [=========================>....] - ETA: 47s - loss: 3.3871 - regression_loss: 2.5180 - classification_loss: 0.8691
 895/1000 [=========================>....] - ETA: 47s - loss: 3.3874 - regression_loss: 2.5182 - classification_loss: 0.8692
 896/1000 [=========================>....] - ETA: 47s - loss: 3.3876 - regression_loss: 2.5187 - classification_loss: 0.8689
 897/1000 [=========================>....] - ETA: 46s - loss: 3.3906 - regression_loss: 2.5210 - classification_loss: 0.8695
 898/1000 [=========================>....] - ETA: 46s - loss: 3.3917 - regression_loss: 2.5216 - classification_loss: 0.8700
 899/1000 [=========================>....] - ETA: 45s - loss: 3.3917 - regression_loss: 2.5216 - classification_loss: 0.8701
 900/1000 [==========================>...] - ETA: 45s - loss: 3.3922 - regression_loss: 2.5220 - classification_loss: 0.8702
 901/1000 [==========================>...] - ETA: 44s - loss: 3.3884 - regression_loss: 2.5192 - classification_loss: 0.8692
 902/1000 [==========================>...] - ETA: 44s - loss: 3.3847 - regression_loss: 2.5164 - classification_loss: 0.8682
 903/1000 [==========================>...] - ETA: 43s - loss: 3.3843 - regression_loss: 2.5161 - classification_loss: 0.8682
 904/1000 [==========================>...] - ETA: 43s - loss: 3.3806 - regression_loss: 2.5133 - classification_loss: 0.8673
 905/1000 [==========================>...] - ETA: 42s - loss: 3.3768 - regression_loss: 2.5105 - classification_loss: 0.8663
 906/1000 [==========================>...] - ETA: 42s - loss: 3.3767 - regression_loss: 2.5106 - classification_loss: 0.8661
 907/1000 [==========================>...] - ETA: 42s - loss: 3.3795 - regression_loss: 2.5131 - classification_loss: 0.8664
 908/1000 [==========================>...] - ETA: 41s - loss: 3.3794 - regression_loss: 2.5129 - classification_loss: 0.8665
 909/1000 [==========================>...] - ETA: 41s - loss: 3.3757 - regression_loss: 2.5101 - classification_loss: 0.8655
 910/1000 [==========================>...] - ETA: 40s - loss: 3.3747 - regression_loss: 2.5095 - classification_loss: 0.8652
 911/1000 [==========================>...] - ETA: 40s - loss: 3.3750 - regression_loss: 2.5099 - classification_loss: 0.8651
 912/1000 [==========================>...] - ETA: 39s - loss: 3.3713 - regression_loss: 2.5071 - classification_loss: 0.8642
 913/1000 [==========================>...] - ETA: 39s - loss: 3.3715 - regression_loss: 2.5075 - classification_loss: 0.8640
 914/1000 [==========================>...] - ETA: 38s - loss: 3.3730 - regression_loss: 2.5090 - classification_loss: 0.8640
 915/1000 [==========================>...] - ETA: 38s - loss: 3.3731 - regression_loss: 2.5092 - classification_loss: 0.8639
 916/1000 [==========================>...] - ETA: 38s - loss: 3.3735 - regression_loss: 2.5089 - classification_loss: 0.8646
 917/1000 [==========================>...] - ETA: 37s - loss: 3.3698 - regression_loss: 2.5062 - classification_loss: 0.8636
 918/1000 [==========================>...] - ETA: 37s - loss: 3.3722 - regression_loss: 2.5076 - classification_loss: 0.8647
 919/1000 [==========================>...] - ETA: 36s - loss: 3.3732 - regression_loss: 2.5078 - classification_loss: 0.8654
 920/1000 [==========================>...] - ETA: 36s - loss: 3.3740 - regression_loss: 2.5083 - classification_loss: 0.8657
 921/1000 [==========================>...] - ETA: 35s - loss: 3.3726 - regression_loss: 2.5074 - classification_loss: 0.8652
 922/1000 [==========================>...] - ETA: 35s - loss: 3.3690 - regression_loss: 2.5047 - classification_loss: 0.8643
 923/1000 [==========================>...] - ETA: 34s - loss: 3.3653 - regression_loss: 2.5020 - classification_loss: 0.8634
 924/1000 [==========================>...] - ETA: 34s - loss: 3.3617 - regression_loss: 2.4993 - classification_loss: 0.8624
 925/1000 [==========================>...] - ETA: 33s - loss: 3.3582 - regression_loss: 2.4966 - classification_loss: 0.8616
 926/1000 [==========================>...] - ETA: 33s - loss: 3.3546 - regression_loss: 2.4939 - classification_loss: 0.8607
 927/1000 [==========================>...] - ETA: 33s - loss: 3.3509 - regression_loss: 2.4912 - classification_loss: 0.8598
 928/1000 [==========================>...] - ETA: 32s - loss: 3.3525 - regression_loss: 2.4918 - classification_loss: 0.8608
 929/1000 [==========================>...] - ETA: 32s - loss: 3.3520 - regression_loss: 2.4910 - classification_loss: 0.8610
 930/1000 [==========================>...] - ETA: 31s - loss: 3.3537 - regression_loss: 2.4921 - classification_loss: 0.8616
 931/1000 [==========================>...] - ETA: 31s - loss: 3.3557 - regression_loss: 2.4938 - classification_loss: 0.8619
 932/1000 [==========================>...] - ETA: 30s - loss: 3.3579 - regression_loss: 2.4956 - classification_loss: 0.8623
 933/1000 [==========================>...] - ETA: 30s - loss: 3.3609 - regression_loss: 2.4983 - classification_loss: 0.8626
 934/1000 [===========================>..] - ETA: 29s - loss: 3.3624 - regression_loss: 2.4991 - classification_loss: 0.8633
 935/1000 [===========================>..] - ETA: 29s - loss: 3.3588 - regression_loss: 2.4964 - classification_loss: 0.8624
 936/1000 [===========================>..] - ETA: 28s - loss: 3.3595 - regression_loss: 2.4972 - classification_loss: 0.8624
 937/1000 [===========================>..] - ETA: 28s - loss: 3.3590 - regression_loss: 2.4968 - classification_loss: 0.8622
 938/1000 [===========================>..] - ETA: 28s - loss: 3.3585 - regression_loss: 2.4966 - classification_loss: 0.8619
 939/1000 [===========================>..] - ETA: 27s - loss: 3.3607 - regression_loss: 2.4984 - classification_loss: 0.8622
 940/1000 [===========================>..] - ETA: 27s - loss: 3.3618 - regression_loss: 2.4995 - classification_loss: 0.8623
 941/1000 [===========================>..] - ETA: 26s - loss: 3.3582 - regression_loss: 2.4969 - classification_loss: 0.8614
 942/1000 [===========================>..] - ETA: 26s - loss: 3.3599 - regression_loss: 2.4976 - classification_loss: 0.8623
 943/1000 [===========================>..] - ETA: 25s - loss: 3.3563 - regression_loss: 2.4949 - classification_loss: 0.8614
 944/1000 [===========================>..] - ETA: 25s - loss: 3.3571 - regression_loss: 2.4946 - classification_loss: 0.8625
 945/1000 [===========================>..] - ETA: 24s - loss: 3.3582 - regression_loss: 2.4957 - classification_loss: 0.8625
 946/1000 [===========================>..] - ETA: 24s - loss: 3.3547 - regression_loss: 2.4931 - classification_loss: 0.8616
 947/1000 [===========================>..] - ETA: 23s - loss: 3.3558 - regression_loss: 2.4937 - classification_loss: 0.8621
 948/1000 [===========================>..] - ETA: 23s - loss: 3.3578 - regression_loss: 2.4948 - classification_loss: 0.8630
 949/1000 [===========================>..] - ETA: 23s - loss: 3.3542 - regression_loss: 2.4922 - classification_loss: 0.8621
 950/1000 [===========================>..] - ETA: 22s - loss: 3.3547 - regression_loss: 2.4925 - classification_loss: 0.8622
 951/1000 [===========================>..] - ETA: 22s - loss: 3.3565 - regression_loss: 2.4934 - classification_loss: 0.8632
 952/1000 [===========================>..] - ETA: 21s - loss: 3.3555 - regression_loss: 2.4929 - classification_loss: 0.8626
 953/1000 [===========================>..] - ETA: 21s - loss: 3.3520 - regression_loss: 2.4903 - classification_loss: 0.8617
 954/1000 [===========================>..] - ETA: 20s - loss: 3.3531 - regression_loss: 2.4908 - classification_loss: 0.8624
 955/1000 [===========================>..] - ETA: 20s - loss: 3.3531 - regression_loss: 2.4909 - classification_loss: 0.8622
 956/1000 [===========================>..] - ETA: 19s - loss: 3.3542 - regression_loss: 2.4920 - classification_loss: 0.8622
 957/1000 [===========================>..] - ETA: 19s - loss: 3.3507 - regression_loss: 2.4894 - classification_loss: 0.8613
 958/1000 [===========================>..] - ETA: 19s - loss: 3.3505 - regression_loss: 2.4893 - classification_loss: 0.8612
 959/1000 [===========================>..] - ETA: 18s - loss: 3.3473 - regression_loss: 2.4867 - classification_loss: 0.8606
 960/1000 [===========================>..] - ETA: 18s - loss: 3.3484 - regression_loss: 2.4879 - classification_loss: 0.8605
 961/1000 [===========================>..] - ETA: 17s - loss: 3.3482 - regression_loss: 2.4878 - classification_loss: 0.8604
 962/1000 [===========================>..] - ETA: 17s - loss: 3.3484 - regression_loss: 2.4880 - classification_loss: 0.8604
 963/1000 [===========================>..] - ETA: 16s - loss: 3.3451 - regression_loss: 2.4854 - classification_loss: 0.8597
 964/1000 [===========================>..] - ETA: 16s - loss: 3.3417 - regression_loss: 2.4828 - classification_loss: 0.8588
 965/1000 [===========================>..] - ETA: 15s - loss: 3.3382 - regression_loss: 2.4803 - classification_loss: 0.8579
 966/1000 [===========================>..] - ETA: 15s - loss: 3.3379 - regression_loss: 2.4801 - classification_loss: 0.8578
 967/1000 [============================>.] - ETA: 14s - loss: 3.3376 - regression_loss: 2.4800 - classification_loss: 0.8576
 968/1000 [============================>.] - ETA: 14s - loss: 3.3389 - regression_loss: 2.4812 - classification_loss: 0.8577
 969/1000 [============================>.] - ETA: 14s - loss: 3.3398 - regression_loss: 2.4825 - classification_loss: 0.8573
 970/1000 [============================>.] - ETA: 13s - loss: 3.3398 - regression_loss: 2.4828 - classification_loss: 0.8569
 971/1000 [============================>.] - ETA: 13s - loss: 3.3394 - regression_loss: 2.4827 - classification_loss: 0.8568
 972/1000 [============================>.] - ETA: 12s - loss: 3.3408 - regression_loss: 2.4833 - classification_loss: 0.8574
 973/1000 [============================>.] - ETA: 12s - loss: 3.3410 - regression_loss: 2.4837 - classification_loss: 0.8573
 974/1000 [============================>.] - ETA: 11s - loss: 3.3376 - regression_loss: 2.4811 - classification_loss: 0.8564
 975/1000 [============================>.] - ETA: 11s - loss: 3.3399 - regression_loss: 2.4832 - classification_loss: 0.8567
 976/1000 [============================>.] - ETA: 10s - loss: 3.3413 - regression_loss: 2.4835 - classification_loss: 0.8577
 977/1000 [============================>.] - ETA: 10s - loss: 3.3424 - regression_loss: 2.4845 - classification_loss: 0.8579
 978/1000 [============================>.] - ETA: 9s - loss: 3.3433 - regression_loss: 2.4843 - classification_loss: 0.8590 
 979/1000 [============================>.] - ETA: 9s - loss: 3.3437 - regression_loss: 2.4852 - classification_loss: 0.8585
 980/1000 [============================>.] - ETA: 9s - loss: 3.3466 - regression_loss: 2.4865 - classification_loss: 0.8601
 981/1000 [============================>.] - ETA: 8s - loss: 3.3475 - regression_loss: 2.4876 - classification_loss: 0.8599
 982/1000 [============================>.] - ETA: 8s - loss: 3.3511 - regression_loss: 2.4900 - classification_loss: 0.8611
 983/1000 [============================>.] - ETA: 7s - loss: 3.3518 - regression_loss: 2.4898 - classification_loss: 0.8620
 984/1000 [============================>.] - ETA: 7s - loss: 3.3517 - regression_loss: 2.4903 - classification_loss: 0.8614
 985/1000 [============================>.] - ETA: 6s - loss: 3.3511 - regression_loss: 2.4901 - classification_loss: 0.8611
 986/1000 [============================>.] - ETA: 6s - loss: 3.3478 - regression_loss: 2.4876 - classification_loss: 0.8603
 987/1000 [============================>.] - ETA: 5s - loss: 3.3480 - regression_loss: 2.4872 - classification_loss: 0.8608
 988/1000 [============================>.] - ETA: 5s - loss: 3.3446 - regression_loss: 2.4847 - classification_loss: 0.8599
 989/1000 [============================>.] - ETA: 4s - loss: 3.3462 - regression_loss: 2.4856 - classification_loss: 0.8606
 990/1000 [============================>.] - ETA: 4s - loss: 3.3466 - regression_loss: 2.4862 - classification_loss: 0.8604
 991/1000 [============================>.] - ETA: 4s - loss: 3.3462 - regression_loss: 2.4862 - classification_loss: 0.8600
 992/1000 [============================>.] - ETA: 3s - loss: 3.3478 - regression_loss: 2.4870 - classification_loss: 0.8608
 993/1000 [============================>.] - ETA: 3s - loss: 3.3477 - regression_loss: 2.4874 - classification_loss: 0.8603
 994/1000 [============================>.] - ETA: 2s - loss: 3.3478 - regression_loss: 2.4876 - classification_loss: 0.8602
 995/1000 [============================>.] - ETA: 2s - loss: 3.3497 - regression_loss: 2.4889 - classification_loss: 0.8608
 996/1000 [============================>.] - ETA: 1s - loss: 3.3520 - regression_loss: 2.4905 - classification_loss: 0.8615
 997/1000 [============================>.] - ETA: 1s - loss: 3.3526 - regression_loss: 2.4904 - classification_loss: 0.8622
 998/1000 [============================>.] - ETA: 0s - loss: 3.3544 - regression_loss: 2.4917 - classification_loss: 0.8628
 999/1000 [============================>.] - ETA: 0s - loss: 3.3557 - regression_loss: 2.4925 - classification_loss: 0.8632
1000/1000 [==============================] - 452s 452ms/step - loss: 3.3550 - regression_loss: 2.4923 - classification_loss: 0.8627

Epoch 00006: saving model to ./snapshots/resnet50_csv_06.h5
0/2001/2002/2003/2004/2005/2006/2007/2008/2009/20010/20011/20012/20013/20014/20015/20016/20017/20018/20019/20020/20021/20022/20023/20024/20025/20026/20027/20028/20029/20030/20031/20032/20033/20034/20035/20036/20037/20038/20039/20040/20041/20042/20043/20044/20045/20046/20047/20048/20049/20050/20051/20052/20053/20054/20055/20056/20057/20058/20059/20060/20061/20062/20063/20064/20065/20066/20067/20068/20069/20070/20071/20072/20073/20074/20075/20076/20077/20078/20079/20080/20081/20082/20083/20084/20085/20086/20087/20088/20089/20090/20091/20092/20093/20094/20095/20096/20097/20098/20099/200100/200101/200102/200103/200104/200105/200106/200107/200108/200109/200110/200111/200112/200113/200114/200115/200116/200117/200118/200119/200120/200121/200122/200123/200124/200125/200126/200127/200128/200129/200130/200131/200132/200133/200134/200135/200136/200137/200138/200139/200140/200141/200142/200143/200144/200145/200146/200147/200148/200149/200150/200151/200152/200153/200154/200155/200156/200157/200158/200159/200160/200161/200162/200163/200164/200165/200166/200167/200168/200169/200170/200171/200172/200173/200174/200175/200176/200177/200178/200179/200180/200181/200182/200183/200184/200185/200186/200187/200188/200189/200190/200191/200192/200193/200194/200195/200196/200197/200198/200199/2000/2001/2002/2003/2004/2005/2006/2007/2008/2009/20010/20011/20012/20013/20014/20015/20016/20017/20018/20019/20020/20021/20022/20023/20024/20025/20026/20027/20028/20029/20030/20031/20032/20033/20034/20035/20036/20037/20038/20039/20040/20041/20042/20043/20044/20045/20046/20047/20048/20049/20050/20051/20052/20053/20054/20055/20056/20057/20058/20059/20060/20061/20062/20063/20064/20065/20066/20067/20068/20069/20070/20071/20072/20073/20074/20075/20076/20077/20078/20079/20080/20081/20082/20083/20084/20085/20086/20087/20088/20089/20090/20091/20092/20093/20094/20095/20096/20097/20098/20099/200100/200101/200102/200103/200104/200105/200106/200107/200108/200109/200110/200111/200112/200113/200114/200115/200116/200117/200118/200119/200120/200121/200122/200123/200124/200125/200126/200127/200128/200129/200130/200131/200132/200133/200134/200135/200136/200137/200138/200139/200140/200141/200142/200143/200144/200145/200146/200147/200148/200149/200150/200151/200152/200153/200154/200155/200156/200157/200158/200159/200160/200161/200162/200163/200164/200165/200166/200167/200168/200169/200170/200171/200172/200173/200174/200175/200176/200177/200178/200179/200180/200181/200182/200183/200184/200185/200186/200187/200188/200189/200190/200191/200192/200193/200194/200195/200196/200197/200198/200199/200

M 0.0932
N 0.0029
mAP: 0.0481
Epoch 7/30

   1/1000 [..............................] - ETA: 7:10 - loss: 5.1585 - regression_loss: 3.6772 - classification_loss: 1.4814
   2/1000 [..............................] - ETA: 7:23 - loss: 2.6014 - regression_loss: 1.8386 - classification_loss: 0.7628
   3/1000 [..............................] - ETA: 7:27 - loss: 2.8720 - regression_loss: 2.0243 - classification_loss: 0.8477
   4/1000 [..............................] - ETA: 7:28 - loss: 2.9133 - regression_loss: 2.0883 - classification_loss: 0.8250
   5/1000 [..............................] - ETA: 7:27 - loss: 2.9886 - regression_loss: 2.1271 - classification_loss: 0.8615
   6/1000 [..............................] - ETA: 7:27 - loss: 3.1370 - regression_loss: 2.3263 - classification_loss: 0.8108
   7/1000 [..............................] - ETA: 7:26 - loss: 2.6890 - regression_loss: 1.9940 - classification_loss: 0.6951
   8/1000 [..............................] - ETA: 7:25 - loss: 2.8801 - regression_loss: 2.2062 - classification_loss: 0.6739
   9/1000 [..............................] - ETA: 7:25 - loss: 2.9155 - regression_loss: 2.2897 - classification_loss: 0.6258
  10/1000 [..............................] - ETA: 7:25 - loss: 3.0608 - regression_loss: 2.4209 - classification_loss: 0.6399
  11/1000 [..............................] - ETA: 7:25 - loss: 3.2245 - regression_loss: 2.5425 - classification_loss: 0.6820
  12/1000 [..............................] - ETA: 7:25 - loss: 2.9558 - regression_loss: 2.3306 - classification_loss: 0.6252
  13/1000 [..............................] - ETA: 7:25 - loss: 2.9997 - regression_loss: 2.3768 - classification_loss: 0.6229
  14/1000 [..............................] - ETA: 7:25 - loss: 2.8168 - regression_loss: 2.2070 - classification_loss: 0.6097
  15/1000 [..............................] - ETA: 7:24 - loss: 2.9506 - regression_loss: 2.2808 - classification_loss: 0.6698
  16/1000 [..............................] - ETA: 7:24 - loss: 2.9906 - regression_loss: 2.3410 - classification_loss: 0.6497
  17/1000 [..............................] - ETA: 7:24 - loss: 3.0501 - regression_loss: 2.3898 - classification_loss: 0.6603
  18/1000 [..............................] - ETA: 7:24 - loss: 2.8811 - regression_loss: 2.2571 - classification_loss: 0.6241
  19/1000 [..............................] - ETA: 7:23 - loss: 2.8593 - regression_loss: 2.1383 - classification_loss: 0.7211
  20/1000 [..............................] - ETA: 7:23 - loss: 2.8360 - regression_loss: 2.1107 - classification_loss: 0.7253
  21/1000 [..............................] - ETA: 7:23 - loss: 2.8816 - regression_loss: 2.1317 - classification_loss: 0.7499
  22/1000 [..............................] - ETA: 7:22 - loss: 2.9088 - regression_loss: 2.1386 - classification_loss: 0.7701
  23/1000 [..............................] - ETA: 7:21 - loss: 3.0238 - regression_loss: 2.2269 - classification_loss: 0.7969
  24/1000 [..............................] - ETA: 7:21 - loss: 3.0401 - regression_loss: 2.2472 - classification_loss: 0.7929
  25/1000 [..............................] - ETA: 7:21 - loss: 3.0841 - regression_loss: 2.2890 - classification_loss: 0.7951
  26/1000 [..............................] - ETA: 7:20 - loss: 3.0786 - regression_loss: 2.2911 - classification_loss: 0.7875
  27/1000 [..............................] - ETA: 7:20 - loss: 2.9646 - regression_loss: 2.2063 - classification_loss: 0.7584
  28/1000 [..............................] - ETA: 7:19 - loss: 3.0387 - regression_loss: 2.2633 - classification_loss: 0.7754
  29/1000 [..............................] - ETA: 7:19 - loss: 3.0116 - regression_loss: 2.2515 - classification_loss: 0.7601
  30/1000 [..............................] - ETA: 7:18 - loss: 3.0747 - regression_loss: 2.2989 - classification_loss: 0.7758
  31/1000 [..............................] - ETA: 7:18 - loss: 3.0992 - regression_loss: 2.3298 - classification_loss: 0.7693
  32/1000 [..............................] - ETA: 7:18 - loss: 3.0023 - regression_loss: 2.2570 - classification_loss: 0.7453
  33/1000 [..............................] - ETA: 7:17 - loss: 3.0642 - regression_loss: 2.3033 - classification_loss: 0.7609
  34/1000 [>.............................] - ETA: 7:17 - loss: 3.0910 - regression_loss: 2.3275 - classification_loss: 0.7636
  35/1000 [>.............................] - ETA: 7:17 - loss: 3.1069 - regression_loss: 2.3490 - classification_loss: 0.7578
  36/1000 [>.............................] - ETA: 7:17 - loss: 3.1275 - regression_loss: 2.3724 - classification_loss: 0.7550
  37/1000 [>.............................] - ETA: 7:16 - loss: 3.1768 - regression_loss: 2.4153 - classification_loss: 0.7615
  38/1000 [>.............................] - ETA: 7:16 - loss: 3.2070 - regression_loss: 2.4323 - classification_loss: 0.7746
  39/1000 [>.............................] - ETA: 7:15 - loss: 3.1250 - regression_loss: 2.3700 - classification_loss: 0.7550
  40/1000 [>.............................] - ETA: 7:15 - loss: 3.1367 - regression_loss: 2.3812 - classification_loss: 0.7555
  41/1000 [>.............................] - ETA: 7:14 - loss: 3.1502 - regression_loss: 2.3897 - classification_loss: 0.7605
  42/1000 [>.............................] - ETA: 7:14 - loss: 3.1467 - regression_loss: 2.3869 - classification_loss: 0.7598
  43/1000 [>.............................] - ETA: 7:14 - loss: 3.1682 - regression_loss: 2.3975 - classification_loss: 0.7708
  44/1000 [>.............................] - ETA: 7:13 - loss: 3.0963 - regression_loss: 2.3430 - classification_loss: 0.7533
  45/1000 [>.............................] - ETA: 7:13 - loss: 3.1405 - regression_loss: 2.3777 - classification_loss: 0.7627
  46/1000 [>.............................] - ETA: 7:12 - loss: 3.1583 - regression_loss: 2.4018 - classification_loss: 0.7566
  47/1000 [>.............................] - ETA: 7:11 - loss: 3.0911 - regression_loss: 2.3507 - classification_loss: 0.7405
  48/1000 [>.............................] - ETA: 7:11 - loss: 3.0578 - regression_loss: 2.3017 - classification_loss: 0.7562
  49/1000 [>.............................] - ETA: 7:11 - loss: 3.0025 - regression_loss: 2.2547 - classification_loss: 0.7478
  50/1000 [>.............................] - ETA: 7:10 - loss: 3.0215 - regression_loss: 2.2631 - classification_loss: 0.7584
  51/1000 [>.............................] - ETA: 7:10 - loss: 3.0744 - regression_loss: 2.2946 - classification_loss: 0.7798
  52/1000 [>.............................] - ETA: 7:09 - loss: 3.0925 - regression_loss: 2.3125 - classification_loss: 0.7800
  53/1000 [>.............................] - ETA: 7:09 - loss: 3.1108 - regression_loss: 2.3221 - classification_loss: 0.7887
  54/1000 [>.............................] - ETA: 7:08 - loss: 3.1372 - regression_loss: 2.3320 - classification_loss: 0.8051
  55/1000 [>.............................] - ETA: 7:08 - loss: 3.1499 - regression_loss: 2.3395 - classification_loss: 0.8104
  56/1000 [>.............................] - ETA: 7:07 - loss: 3.1803 - regression_loss: 2.3708 - classification_loss: 0.8096
  57/1000 [>.............................] - ETA: 7:07 - loss: 3.1898 - regression_loss: 2.3811 - classification_loss: 0.8088
  58/1000 [>.............................] - ETA: 7:06 - loss: 3.1348 - regression_loss: 2.3400 - classification_loss: 0.7948
  59/1000 [>.............................] - ETA: 7:06 - loss: 3.1422 - regression_loss: 2.3484 - classification_loss: 0.7938
  60/1000 [>.............................] - ETA: 7:06 - loss: 3.1777 - regression_loss: 2.3725 - classification_loss: 0.8052
  61/1000 [>.............................] - ETA: 7:05 - loss: 3.2055 - regression_loss: 2.4014 - classification_loss: 0.8041
  62/1000 [>.............................] - ETA: 7:05 - loss: 3.1538 - regression_loss: 2.3627 - classification_loss: 0.7911
  63/1000 [>.............................] - ETA: 7:04 - loss: 3.1938 - regression_loss: 2.3887 - classification_loss: 0.8050
  64/1000 [>.............................] - ETA: 7:04 - loss: 3.2088 - regression_loss: 2.4036 - classification_loss: 0.8052
  65/1000 [>.............................] - ETA: 7:03 - loss: 3.2287 - regression_loss: 2.4233 - classification_loss: 0.8054
  66/1000 [>.............................] - ETA: 7:03 - loss: 3.2788 - regression_loss: 2.4604 - classification_loss: 0.8184
  67/1000 [=>............................] - ETA: 7:02 - loss: 3.2299 - regression_loss: 2.4237 - classification_loss: 0.8062
  68/1000 [=>............................] - ETA: 7:02 - loss: 3.1825 - regression_loss: 2.3881 - classification_loss: 0.7944
  69/1000 [=>............................] - ETA: 7:02 - loss: 3.1991 - regression_loss: 2.4017 - classification_loss: 0.7974
  70/1000 [=>............................] - ETA: 7:01 - loss: 3.1534 - regression_loss: 2.3674 - classification_loss: 0.7860
  71/1000 [=>............................] - ETA: 7:01 - loss: 3.1699 - regression_loss: 2.3731 - classification_loss: 0.7968
  72/1000 [=>............................] - ETA: 7:00 - loss: 3.1277 - regression_loss: 2.3401 - classification_loss: 0.7875
  73/1000 [=>............................] - ETA: 6:59 - loss: 3.0848 - regression_loss: 2.3081 - classification_loss: 0.7767
  74/1000 [=>............................] - ETA: 6:59 - loss: 3.0435 - regression_loss: 2.2769 - classification_loss: 0.7666
  75/1000 [=>............................] - ETA: 6:58 - loss: 3.0620 - regression_loss: 2.2878 - classification_loss: 0.7742
  76/1000 [=>............................] - ETA: 6:58 - loss: 3.0724 - regression_loss: 2.2993 - classification_loss: 0.7731
  77/1000 [=>............................] - ETA: 6:57 - loss: 3.1004 - regression_loss: 2.3143 - classification_loss: 0.7861
  78/1000 [=>............................] - ETA: 6:57 - loss: 3.1074 - regression_loss: 2.3184 - classification_loss: 0.7890
  79/1000 [=>............................] - ETA: 6:57 - loss: 3.1267 - regression_loss: 2.3296 - classification_loss: 0.7970
  80/1000 [=>............................] - ETA: 6:56 - loss: 3.0876 - regression_loss: 2.3005 - classification_loss: 0.7871
  81/1000 [=>............................] - ETA: 6:56 - loss: 3.1191 - regression_loss: 2.3287 - classification_loss: 0.7904
  82/1000 [=>............................] - ETA: 6:55 - loss: 3.1281 - regression_loss: 2.3371 - classification_loss: 0.7910
  83/1000 [=>............................] - ETA: 6:55 - loss: 3.1562 - regression_loss: 2.3520 - classification_loss: 0.8042
  84/1000 [=>............................] - ETA: 6:54 - loss: 3.1782 - regression_loss: 2.3603 - classification_loss: 0.8179
  85/1000 [=>............................] - ETA: 6:54 - loss: 3.1923 - regression_loss: 2.3737 - classification_loss: 0.8186
  86/1000 [=>............................] - ETA: 6:53 - loss: 3.2186 - regression_loss: 2.3910 - classification_loss: 0.8276
  87/1000 [=>............................] - ETA: 6:53 - loss: 3.2372 - regression_loss: 2.4112 - classification_loss: 0.8260
  88/1000 [=>............................] - ETA: 6:52 - loss: 3.2588 - regression_loss: 2.4264 - classification_loss: 0.8324
  89/1000 [=>............................] - ETA: 6:52 - loss: 3.2222 - regression_loss: 2.3991 - classification_loss: 0.8231
  90/1000 [=>............................] - ETA: 6:51 - loss: 3.1864 - regression_loss: 2.3725 - classification_loss: 0.8139
  91/1000 [=>............................] - ETA: 6:51 - loss: 3.1924 - regression_loss: 2.3755 - classification_loss: 0.8169
  92/1000 [=>............................] - ETA: 6:51 - loss: 3.1577 - regression_loss: 2.3497 - classification_loss: 0.8080
  93/1000 [=>............................] - ETA: 6:50 - loss: 3.1238 - regression_loss: 2.3244 - classification_loss: 0.7993
  94/1000 [=>............................] - ETA: 6:49 - loss: 3.1464 - regression_loss: 2.3382 - classification_loss: 0.8082
  95/1000 [=>............................] - ETA: 6:49 - loss: 3.1685 - regression_loss: 2.3489 - classification_loss: 0.8195
  96/1000 [=>............................] - ETA: 6:48 - loss: 3.1865 - regression_loss: 2.3577 - classification_loss: 0.8288
  97/1000 [=>............................] - ETA: 6:48 - loss: 3.1898 - regression_loss: 2.3544 - classification_loss: 0.8354
  98/1000 [=>............................] - ETA: 6:48 - loss: 3.2023 - regression_loss: 2.3585 - classification_loss: 0.8439
  99/1000 [=>............................] - ETA: 6:47 - loss: 3.2086 - regression_loss: 2.3657 - classification_loss: 0.8429
 100/1000 [==>...........................] - ETA: 6:47 - loss: 3.2105 - regression_loss: 2.3682 - classification_loss: 0.8422
 101/1000 [==>...........................] - ETA: 6:46 - loss: 3.2215 - regression_loss: 2.3731 - classification_loss: 0.8484
 102/1000 [==>...........................] - ETA: 6:46 - loss: 3.2429 - regression_loss: 2.3899 - classification_loss: 0.8530
 103/1000 [==>...........................] - ETA: 6:45 - loss: 3.2554 - regression_loss: 2.4030 - classification_loss: 0.8524
 104/1000 [==>...........................] - ETA: 6:45 - loss: 3.2532 - regression_loss: 2.4019 - classification_loss: 0.8513
 105/1000 [==>...........................] - ETA: 6:45 - loss: 3.2456 - regression_loss: 2.3984 - classification_loss: 0.8472
 106/1000 [==>...........................] - ETA: 6:44 - loss: 3.2150 - regression_loss: 2.3758 - classification_loss: 0.8392
 107/1000 [==>...........................] - ETA: 6:44 - loss: 3.2196 - regression_loss: 2.3817 - classification_loss: 0.8378
 108/1000 [==>...........................] - ETA: 6:43 - loss: 3.2223 - regression_loss: 2.3869 - classification_loss: 0.8353
 109/1000 [==>...........................] - ETA: 6:43 - loss: 3.2172 - regression_loss: 2.3858 - classification_loss: 0.8315
 110/1000 [==>...........................] - ETA: 6:42 - loss: 3.2193 - regression_loss: 2.3910 - classification_loss: 0.8284
 111/1000 [==>...........................] - ETA: 6:42 - loss: 3.2205 - regression_loss: 2.3954 - classification_loss: 0.8252
 112/1000 [==>...........................] - ETA: 6:41 - loss: 3.1918 - regression_loss: 2.3740 - classification_loss: 0.8178
 113/1000 [==>...........................] - ETA: 6:41 - loss: 3.1997 - regression_loss: 2.3764 - classification_loss: 0.8234
 114/1000 [==>...........................] - ETA: 6:41 - loss: 3.2139 - regression_loss: 2.3843 - classification_loss: 0.8296
 115/1000 [==>...........................] - ETA: 6:40 - loss: 3.1859 - regression_loss: 2.3636 - classification_loss: 0.8224
 116/1000 [==>...........................] - ETA: 6:40 - loss: 3.1813 - regression_loss: 2.3629 - classification_loss: 0.8184
 117/1000 [==>...........................] - ETA: 6:39 - loss: 3.1851 - regression_loss: 2.3677 - classification_loss: 0.8174
 118/1000 [==>...........................] - ETA: 6:39 - loss: 3.1852 - regression_loss: 2.3700 - classification_loss: 0.8152
 119/1000 [==>...........................] - ETA: 6:38 - loss: 3.1973 - regression_loss: 2.3746 - classification_loss: 0.8227
 120/1000 [==>...........................] - ETA: 6:38 - loss: 3.1707 - regression_loss: 2.3548 - classification_loss: 0.8159
 121/1000 [==>...........................] - ETA: 6:38 - loss: 3.1805 - regression_loss: 2.3600 - classification_loss: 0.8205
 122/1000 [==>...........................] - ETA: 6:37 - loss: 3.1980 - regression_loss: 2.3715 - classification_loss: 0.8265
 123/1000 [==>...........................] - ETA: 6:37 - loss: 3.1720 - regression_loss: 2.3522 - classification_loss: 0.8197
 124/1000 [==>...........................] - ETA: 6:36 - loss: 3.1822 - regression_loss: 2.3633 - classification_loss: 0.8189
 125/1000 [==>...........................] - ETA: 6:36 - loss: 3.1946 - regression_loss: 2.3747 - classification_loss: 0.8199
 126/1000 [==>...........................] - ETA: 6:35 - loss: 3.2068 - regression_loss: 2.3809 - classification_loss: 0.8259
 127/1000 [==>...........................] - ETA: 6:35 - loss: 3.2055 - regression_loss: 2.3802 - classification_loss: 0.8252
 128/1000 [==>...........................] - ETA: 6:34 - loss: 3.2119 - regression_loss: 2.3889 - classification_loss: 0.8230
 129/1000 [==>...........................] - ETA: 6:34 - loss: 3.2063 - regression_loss: 2.3881 - classification_loss: 0.8182
 130/1000 [==>...........................] - ETA: 6:34 - loss: 3.2081 - regression_loss: 2.3918 - classification_loss: 0.8164
 131/1000 [==>...........................] - ETA: 6:33 - loss: 3.2248 - regression_loss: 2.4028 - classification_loss: 0.8220
 132/1000 [==>...........................] - ETA: 6:33 - loss: 3.2549 - regression_loss: 2.4220 - classification_loss: 0.8329
 133/1000 [==>...........................] - ETA: 6:32 - loss: 3.2567 - regression_loss: 2.4245 - classification_loss: 0.8322
 134/1000 [===>..........................] - ETA: 6:32 - loss: 3.2583 - regression_loss: 2.4266 - classification_loss: 0.8317
 135/1000 [===>..........................] - ETA: 6:31 - loss: 3.2781 - regression_loss: 2.4380 - classification_loss: 0.8401
 136/1000 [===>..........................] - ETA: 6:31 - loss: 3.2901 - regression_loss: 2.4463 - classification_loss: 0.8439
 137/1000 [===>..........................] - ETA: 6:30 - loss: 3.2661 - regression_loss: 2.4284 - classification_loss: 0.8377
 138/1000 [===>..........................] - ETA: 6:30 - loss: 3.2425 - regression_loss: 2.4108 - classification_loss: 0.8316
 139/1000 [===>..........................] - ETA: 6:30 - loss: 3.2456 - regression_loss: 2.4108 - classification_loss: 0.8348
 140/1000 [===>..........................] - ETA: 6:29 - loss: 3.2224 - regression_loss: 2.3936 - classification_loss: 0.8288
 141/1000 [===>..........................] - ETA: 6:29 - loss: 3.2242 - regression_loss: 2.3950 - classification_loss: 0.8292
 142/1000 [===>..........................] - ETA: 6:28 - loss: 3.2014 - regression_loss: 2.3781 - classification_loss: 0.8234
 143/1000 [===>..........................] - ETA: 6:28 - loss: 3.2244 - regression_loss: 2.3943 - classification_loss: 0.8300
 144/1000 [===>..........................] - ETA: 6:27 - loss: 3.2421 - regression_loss: 2.4089 - classification_loss: 0.8333
 145/1000 [===>..........................] - ETA: 6:27 - loss: 3.2318 - regression_loss: 2.3923 - classification_loss: 0.8395
 146/1000 [===>..........................] - ETA: 6:26 - loss: 3.2230 - regression_loss: 2.3872 - classification_loss: 0.8358
 147/1000 [===>..........................] - ETA: 6:26 - loss: 3.2383 - regression_loss: 2.3977 - classification_loss: 0.8406
 148/1000 [===>..........................] - ETA: 6:26 - loss: 3.2165 - regression_loss: 2.3815 - classification_loss: 0.8350
 149/1000 [===>..........................] - ETA: 6:25 - loss: 3.2187 - regression_loss: 2.3832 - classification_loss: 0.8355
 150/1000 [===>..........................] - ETA: 6:25 - loss: 3.2276 - regression_loss: 2.3921 - classification_loss: 0.8355
 151/1000 [===>..........................] - ETA: 6:24 - loss: 3.2294 - regression_loss: 2.3925 - classification_loss: 0.8370
 152/1000 [===>..........................] - ETA: 6:24 - loss: 3.2082 - regression_loss: 2.3767 - classification_loss: 0.8315
 153/1000 [===>..........................] - ETA: 6:23 - loss: 3.2098 - regression_loss: 2.3777 - classification_loss: 0.8320
 154/1000 [===>..........................] - ETA: 6:23 - loss: 3.1889 - regression_loss: 2.3623 - classification_loss: 0.8266
 155/1000 [===>..........................] - ETA: 6:22 - loss: 3.1964 - regression_loss: 2.3621 - classification_loss: 0.8342
 156/1000 [===>..........................] - ETA: 6:22 - loss: 3.1913 - regression_loss: 2.3598 - classification_loss: 0.8315
 157/1000 [===>..........................] - ETA: 6:21 - loss: 3.1939 - regression_loss: 2.3624 - classification_loss: 0.8315
 158/1000 [===>..........................] - ETA: 6:21 - loss: 3.1737 - regression_loss: 2.3475 - classification_loss: 0.8262
 159/1000 [===>..........................] - ETA: 6:21 - loss: 3.1736 - regression_loss: 2.3502 - classification_loss: 0.8234
 160/1000 [===>..........................] - ETA: 6:20 - loss: 3.1768 - regression_loss: 2.3525 - classification_loss: 0.8243
 161/1000 [===>..........................] - ETA: 6:20 - loss: 3.1888 - regression_loss: 2.3616 - classification_loss: 0.8272
 162/1000 [===>..........................] - ETA: 6:19 - loss: 3.1717 - regression_loss: 2.3471 - classification_loss: 0.8246
 163/1000 [===>..........................] - ETA: 6:19 - loss: 3.1728 - regression_loss: 2.3493 - classification_loss: 0.8235
 164/1000 [===>..........................] - ETA: 6:18 - loss: 3.1702 - regression_loss: 2.3474 - classification_loss: 0.8229
 165/1000 [===>..........................] - ETA: 6:18 - loss: 3.1741 - regression_loss: 2.3534 - classification_loss: 0.8207
 166/1000 [===>..........................] - ETA: 6:17 - loss: 3.1807 - regression_loss: 2.3532 - classification_loss: 0.8275
 167/1000 [====>.........................] - ETA: 6:17 - loss: 3.1882 - regression_loss: 2.3575 - classification_loss: 0.8307
 168/1000 [====>.........................] - ETA: 6:17 - loss: 3.1957 - regression_loss: 2.3654 - classification_loss: 0.8303
 169/1000 [====>.........................] - ETA: 6:16 - loss: 3.1768 - regression_loss: 2.3514 - classification_loss: 0.8254
 170/1000 [====>.........................] - ETA: 6:16 - loss: 3.1857 - regression_loss: 2.3543 - classification_loss: 0.8314
 171/1000 [====>.........................] - ETA: 6:15 - loss: 3.1844 - regression_loss: 2.3537 - classification_loss: 0.8307
 172/1000 [====>.........................] - ETA: 6:15 - loss: 3.1659 - regression_loss: 2.3400 - classification_loss: 0.8258
 173/1000 [====>.........................] - ETA: 6:14 - loss: 3.1713 - regression_loss: 2.3427 - classification_loss: 0.8286
 174/1000 [====>.........................] - ETA: 6:14 - loss: 3.1748 - regression_loss: 2.3470 - classification_loss: 0.8279
 175/1000 [====>.........................] - ETA: 6:13 - loss: 3.1828 - regression_loss: 2.3524 - classification_loss: 0.8305
 176/1000 [====>.........................] - ETA: 6:13 - loss: 3.1648 - regression_loss: 2.3390 - classification_loss: 0.8257
 177/1000 [====>.........................] - ETA: 6:13 - loss: 3.1631 - regression_loss: 2.3388 - classification_loss: 0.8244
 178/1000 [====>.........................] - ETA: 6:12 - loss: 3.1454 - regression_loss: 2.3256 - classification_loss: 0.8198
 179/1000 [====>.........................] - ETA: 6:12 - loss: 3.1565 - regression_loss: 2.3332 - classification_loss: 0.8233
 180/1000 [====>.........................] - ETA: 6:11 - loss: 3.1573 - regression_loss: 2.3336 - classification_loss: 0.8237
 181/1000 [====>.........................] - ETA: 6:11 - loss: 3.1565 - regression_loss: 2.3350 - classification_loss: 0.8216
 182/1000 [====>.........................] - ETA: 6:10 - loss: 3.1591 - regression_loss: 2.3389 - classification_loss: 0.8202
 183/1000 [====>.........................] - ETA: 6:10 - loss: 3.1747 - regression_loss: 2.3521 - classification_loss: 0.8226
 184/1000 [====>.........................] - ETA: 6:09 - loss: 3.1822 - regression_loss: 2.3535 - classification_loss: 0.8287
 185/1000 [====>.........................] - ETA: 6:09 - loss: 3.1993 - regression_loss: 2.3624 - classification_loss: 0.8369
 186/1000 [====>.........................] - ETA: 6:08 - loss: 3.2130 - regression_loss: 2.3718 - classification_loss: 0.8413
 187/1000 [====>.........................] - ETA: 6:08 - loss: 3.2189 - regression_loss: 2.3749 - classification_loss: 0.8440
 188/1000 [====>.........................] - ETA: 6:08 - loss: 3.2193 - regression_loss: 2.3760 - classification_loss: 0.8433
 189/1000 [====>.........................] - ETA: 6:07 - loss: 3.2176 - regression_loss: 2.3761 - classification_loss: 0.8415
 190/1000 [====>.........................] - ETA: 6:07 - loss: 3.2198 - regression_loss: 2.3744 - classification_loss: 0.8455
 191/1000 [====>.........................] - ETA: 6:06 - loss: 3.2222 - regression_loss: 2.3764 - classification_loss: 0.8457
 192/1000 [====>.........................] - ETA: 6:06 - loss: 3.2301 - regression_loss: 2.3849 - classification_loss: 0.8452
 193/1000 [====>.........................] - ETA: 6:05 - loss: 3.2386 - regression_loss: 2.3876 - classification_loss: 0.8510
 194/1000 [====>.........................] - ETA: 6:05 - loss: 3.2230 - regression_loss: 2.3753 - classification_loss: 0.8477
 195/1000 [====>.........................] - ETA: 6:04 - loss: 3.2263 - regression_loss: 2.3750 - classification_loss: 0.8513
 196/1000 [====>.........................] - ETA: 6:04 - loss: 3.2379 - regression_loss: 2.3839 - classification_loss: 0.8540
 197/1000 [====>.........................] - ETA: 6:03 - loss: 3.2458 - regression_loss: 2.3920 - classification_loss: 0.8538
 198/1000 [====>.........................] - ETA: 6:03 - loss: 3.2317 - regression_loss: 2.3799 - classification_loss: 0.8517
 199/1000 [====>.........................] - ETA: 6:03 - loss: 3.2303 - regression_loss: 2.3803 - classification_loss: 0.8500
 200/1000 [=====>........................] - ETA: 6:02 - loss: 3.2336 - regression_loss: 2.3833 - classification_loss: 0.8503
 201/1000 [=====>........................] - ETA: 6:02 - loss: 3.2176 - regression_loss: 2.3715 - classification_loss: 0.8462
 202/1000 [=====>........................] - ETA: 6:01 - loss: 3.2273 - regression_loss: 2.3796 - classification_loss: 0.8477
 203/1000 [=====>........................] - ETA: 6:01 - loss: 3.2298 - regression_loss: 2.3830 - classification_loss: 0.8468
 204/1000 [=====>........................] - ETA: 6:00 - loss: 3.2140 - regression_loss: 2.3713 - classification_loss: 0.8426
 205/1000 [=====>........................] - ETA: 6:00 - loss: 3.2138 - regression_loss: 2.3729 - classification_loss: 0.8409
 206/1000 [=====>........................] - ETA: 5:59 - loss: 3.2207 - regression_loss: 2.3770 - classification_loss: 0.8437
 207/1000 [=====>........................] - ETA: 5:59 - loss: 3.2051 - regression_loss: 2.3655 - classification_loss: 0.8396
 208/1000 [=====>........................] - ETA: 5:59 - loss: 3.2024 - regression_loss: 2.3635 - classification_loss: 0.8389
 209/1000 [=====>........................] - ETA: 5:58 - loss: 3.2037 - regression_loss: 2.3633 - classification_loss: 0.8404
 210/1000 [=====>........................] - ETA: 5:58 - loss: 3.1886 - regression_loss: 2.3521 - classification_loss: 0.8365
 211/1000 [=====>........................] - ETA: 5:57 - loss: 3.1867 - regression_loss: 2.3516 - classification_loss: 0.8351
 212/1000 [=====>........................] - ETA: 5:57 - loss: 3.1898 - regression_loss: 2.3532 - classification_loss: 0.8367
 213/1000 [=====>........................] - ETA: 5:56 - loss: 3.1749 - regression_loss: 2.3421 - classification_loss: 0.8328
 214/1000 [=====>........................] - ETA: 5:56 - loss: 3.1821 - regression_loss: 2.3465 - classification_loss: 0.8356
 215/1000 [=====>........................] - ETA: 5:55 - loss: 3.1919 - regression_loss: 2.3542 - classification_loss: 0.8377
 216/1000 [=====>........................] - ETA: 5:55 - loss: 3.1771 - regression_loss: 2.3433 - classification_loss: 0.8338
 217/1000 [=====>........................] - ETA: 5:55 - loss: 3.1625 - regression_loss: 2.3325 - classification_loss: 0.8300
 218/1000 [=====>........................] - ETA: 5:54 - loss: 3.1727 - regression_loss: 2.3393 - classification_loss: 0.8334
 219/1000 [=====>........................] - ETA: 5:54 - loss: 3.1800 - regression_loss: 2.3421 - classification_loss: 0.8379
 220/1000 [=====>........................] - ETA: 5:53 - loss: 3.1819 - regression_loss: 2.3453 - classification_loss: 0.8366
 221/1000 [=====>........................] - ETA: 5:53 - loss: 3.1809 - regression_loss: 2.3457 - classification_loss: 0.8353
 222/1000 [=====>........................] - ETA: 5:52 - loss: 3.1907 - regression_loss: 2.3535 - classification_loss: 0.8371
 223/1000 [=====>........................] - ETA: 5:52 - loss: 3.1764 - regression_loss: 2.3430 - classification_loss: 0.8334
 224/1000 [=====>........................] - ETA: 5:51 - loss: 3.1796 - regression_loss: 2.3441 - classification_loss: 0.8355
 225/1000 [=====>........................] - ETA: 5:51 - loss: 3.1876 - regression_loss: 2.3508 - classification_loss: 0.8368
 226/1000 [=====>........................] - ETA: 5:50 - loss: 3.1940 - regression_loss: 2.3549 - classification_loss: 0.8391
 227/1000 [=====>........................] - ETA: 5:50 - loss: 3.1967 - regression_loss: 2.3557 - classification_loss: 0.8410
 228/1000 [=====>........................] - ETA: 5:49 - loss: 3.2030 - regression_loss: 2.3605 - classification_loss: 0.8425
 229/1000 [=====>........................] - ETA: 5:49 - loss: 3.2044 - regression_loss: 2.3601 - classification_loss: 0.8444
 230/1000 [=====>........................] - ETA: 5:49 - loss: 3.2087 - regression_loss: 2.3625 - classification_loss: 0.8461
 231/1000 [=====>........................] - ETA: 5:48 - loss: 3.2089 - regression_loss: 2.3620 - classification_loss: 0.8469
 232/1000 [=====>........................] - ETA: 5:48 - loss: 3.2242 - regression_loss: 2.3749 - classification_loss: 0.8493
 233/1000 [=====>........................] - ETA: 5:47 - loss: 3.2104 - regression_loss: 2.3647 - classification_loss: 0.8457
 234/1000 [======>.......................] - ETA: 5:47 - loss: 3.2149 - regression_loss: 2.3691 - classification_loss: 0.8458
 235/1000 [======>.......................] - ETA: 5:46 - loss: 3.2123 - regression_loss: 2.3675 - classification_loss: 0.8448
 236/1000 [======>.......................] - ETA: 5:46 - loss: 3.1995 - regression_loss: 2.3574 - classification_loss: 0.8420
 237/1000 [======>.......................] - ETA: 5:45 - loss: 3.1860 - regression_loss: 2.3475 - classification_loss: 0.8385
 238/1000 [======>.......................] - ETA: 5:45 - loss: 3.1907 - regression_loss: 2.3502 - classification_loss: 0.8406
 239/1000 [======>.......................] - ETA: 5:44 - loss: 3.1982 - regression_loss: 2.3576 - classification_loss: 0.8406
 240/1000 [======>.......................] - ETA: 5:44 - loss: 3.2024 - regression_loss: 2.3601 - classification_loss: 0.8423
 241/1000 [======>.......................] - ETA: 5:43 - loss: 3.2101 - regression_loss: 2.3683 - classification_loss: 0.8418
 242/1000 [======>.......................] - ETA: 5:43 - loss: 3.2160 - regression_loss: 2.3720 - classification_loss: 0.8440
 243/1000 [======>.......................] - ETA: 5:42 - loss: 3.2161 - regression_loss: 2.3721 - classification_loss: 0.8439
 244/1000 [======>.......................] - ETA: 5:42 - loss: 3.2224 - regression_loss: 2.3759 - classification_loss: 0.8465
 245/1000 [======>.......................] - ETA: 5:42 - loss: 3.2255 - regression_loss: 2.3789 - classification_loss: 0.8466
 246/1000 [======>.......................] - ETA: 5:41 - loss: 3.2302 - regression_loss: 2.3837 - classification_loss: 0.8465
 247/1000 [======>.......................] - ETA: 5:41 - loss: 3.2284 - regression_loss: 2.3824 - classification_loss: 0.8460
 248/1000 [======>.......................] - ETA: 5:40 - loss: 3.2365 - regression_loss: 2.3871 - classification_loss: 0.8494
 249/1000 [======>.......................] - ETA: 5:40 - loss: 3.2407 - regression_loss: 2.3894 - classification_loss: 0.8513
 250/1000 [======>.......................] - ETA: 5:39 - loss: 3.2459 - regression_loss: 2.3934 - classification_loss: 0.8525
 251/1000 [======>.......................] - ETA: 5:39 - loss: 3.2330 - regression_loss: 2.3839 - classification_loss: 0.8491
 252/1000 [======>.......................] - ETA: 5:38 - loss: 3.2363 - regression_loss: 2.3869 - classification_loss: 0.8494
 253/1000 [======>.......................] - ETA: 5:38 - loss: 3.2440 - regression_loss: 2.3924 - classification_loss: 0.8517
 254/1000 [======>.......................] - ETA: 5:38 - loss: 3.2313 - regression_loss: 2.3830 - classification_loss: 0.8483
 255/1000 [======>.......................] - ETA: 5:37 - loss: 3.2186 - regression_loss: 2.3736 - classification_loss: 0.8450
 256/1000 [======>.......................] - ETA: 5:37 - loss: 3.2185 - regression_loss: 2.3747 - classification_loss: 0.8437
 257/1000 [======>.......................] - ETA: 5:36 - loss: 3.2192 - regression_loss: 2.3739 - classification_loss: 0.8453
 258/1000 [======>.......................] - ETA: 5:36 - loss: 3.2196 - regression_loss: 2.3730 - classification_loss: 0.8466
 259/1000 [======>.......................] - ETA: 5:35 - loss: 3.2072 - regression_loss: 2.3638 - classification_loss: 0.8433
 260/1000 [======>.......................] - ETA: 5:35 - loss: 3.1948 - regression_loss: 2.3547 - classification_loss: 0.8401
 261/1000 [======>.......................] - ETA: 5:34 - loss: 3.1954 - regression_loss: 2.3569 - classification_loss: 0.8385
 262/1000 [======>.......................] - ETA: 5:34 - loss: 3.1998 - regression_loss: 2.3608 - classification_loss: 0.8390
 263/1000 [======>.......................] - ETA: 5:34 - loss: 3.2084 - regression_loss: 2.3668 - classification_loss: 0.8416
 264/1000 [======>.......................] - ETA: 5:33 - loss: 3.2145 - regression_loss: 2.3696 - classification_loss: 0.8449
 265/1000 [======>.......................] - ETA: 5:33 - loss: 3.2130 - regression_loss: 2.3695 - classification_loss: 0.8435
 266/1000 [======>.......................] - ETA: 5:32 - loss: 3.2321 - regression_loss: 2.3813 - classification_loss: 0.8508
 267/1000 [=======>......................] - ETA: 5:32 - loss: 3.2384 - regression_loss: 2.3868 - classification_loss: 0.8516
 268/1000 [=======>......................] - ETA: 5:31 - loss: 3.2412 - regression_loss: 2.3894 - classification_loss: 0.8518
 269/1000 [=======>......................] - ETA: 5:31 - loss: 3.2437 - regression_loss: 2.3933 - classification_loss: 0.8504
 270/1000 [=======>......................] - ETA: 5:30 - loss: 3.2405 - regression_loss: 2.3915 - classification_loss: 0.8490
 271/1000 [=======>......................] - ETA: 5:30 - loss: 3.2407 - regression_loss: 2.3930 - classification_loss: 0.8477
 272/1000 [=======>......................] - ETA: 5:29 - loss: 3.2458 - regression_loss: 2.3978 - classification_loss: 0.8480
 273/1000 [=======>......................] - ETA: 5:29 - loss: 3.2428 - regression_loss: 2.3963 - classification_loss: 0.8465
 274/1000 [=======>......................] - ETA: 5:29 - loss: 3.2310 - regression_loss: 2.3876 - classification_loss: 0.8434
 275/1000 [=======>......................] - ETA: 5:28 - loss: 3.2193 - regression_loss: 2.3789 - classification_loss: 0.8403
 276/1000 [=======>......................] - ETA: 5:28 - loss: 3.2217 - regression_loss: 2.3810 - classification_loss: 0.8407
 277/1000 [=======>......................] - ETA: 5:27 - loss: 3.2109 - regression_loss: 2.3724 - classification_loss: 0.8385
 278/1000 [=======>......................] - ETA: 5:27 - loss: 3.2157 - regression_loss: 2.3773 - classification_loss: 0.8384
 279/1000 [=======>......................] - ETA: 5:26 - loss: 3.2155 - regression_loss: 2.3790 - classification_loss: 0.8365
 280/1000 [=======>......................] - ETA: 5:26 - loss: 3.2171 - regression_loss: 2.3812 - classification_loss: 0.8359
 281/1000 [=======>......................] - ETA: 5:25 - loss: 3.2140 - regression_loss: 2.3798 - classification_loss: 0.8343
 282/1000 [=======>......................] - ETA: 5:25 - loss: 3.2027 - regression_loss: 2.3713 - classification_loss: 0.8314
 283/1000 [=======>......................] - ETA: 5:24 - loss: 3.2056 - regression_loss: 2.3719 - classification_loss: 0.8336
 284/1000 [=======>......................] - ETA: 5:24 - loss: 3.2104 - regression_loss: 2.3766 - classification_loss: 0.8338
 285/1000 [=======>......................] - ETA: 5:23 - loss: 3.2170 - regression_loss: 2.3802 - classification_loss: 0.8368
 286/1000 [=======>......................] - ETA: 5:23 - loss: 3.2058 - regression_loss: 2.3719 - classification_loss: 0.8339
 287/1000 [=======>......................] - ETA: 5:23 - loss: 3.1946 - regression_loss: 2.3636 - classification_loss: 0.8310
 288/1000 [=======>......................] - ETA: 5:22 - loss: 3.1972 - regression_loss: 2.3637 - classification_loss: 0.8334
 289/1000 [=======>......................] - ETA: 5:22 - loss: 3.2021 - regression_loss: 2.3661 - classification_loss: 0.8360
 290/1000 [=======>......................] - ETA: 5:21 - loss: 3.1910 - regression_loss: 2.3579 - classification_loss: 0.8331
 291/1000 [=======>......................] - ETA: 5:21 - loss: 3.1913 - regression_loss: 2.3565 - classification_loss: 0.8348
 292/1000 [=======>......................] - ETA: 5:20 - loss: 3.1907 - regression_loss: 2.3562 - classification_loss: 0.8345
 293/1000 [=======>......................] - ETA: 5:20 - loss: 3.1904 - regression_loss: 2.3539 - classification_loss: 0.8365
 294/1000 [=======>......................] - ETA: 5:19 - loss: 3.1954 - regression_loss: 2.3552 - classification_loss: 0.8402
 295/1000 [=======>......................] - ETA: 5:19 - loss: 3.1981 - regression_loss: 2.3581 - classification_loss: 0.8400
 296/1000 [=======>......................] - ETA: 5:18 - loss: 3.1985 - regression_loss: 2.3600 - classification_loss: 0.8384
 297/1000 [=======>......................] - ETA: 5:18 - loss: 3.1982 - regression_loss: 2.3586 - classification_loss: 0.8395
 298/1000 [=======>......................] - ETA: 5:18 - loss: 3.2014 - regression_loss: 2.3598 - classification_loss: 0.8415
 299/1000 [=======>......................] - ETA: 5:17 - loss: 3.1906 - regression_loss: 2.3519 - classification_loss: 0.8387
 300/1000 [========>.....................] - ETA: 5:17 - loss: 3.1913 - regression_loss: 2.3507 - classification_loss: 0.8407
 301/1000 [========>.....................] - ETA: 5:16 - loss: 3.1807 - regression_loss: 2.3429 - classification_loss: 0.8379
 302/1000 [========>.....................] - ETA: 5:16 - loss: 3.1838 - regression_loss: 2.3471 - classification_loss: 0.8367
 303/1000 [========>.....................] - ETA: 5:15 - loss: 3.1851 - regression_loss: 2.3486 - classification_loss: 0.8365
 304/1000 [========>.....................] - ETA: 5:15 - loss: 3.1876 - regression_loss: 2.3496 - classification_loss: 0.8380
 305/1000 [========>.....................] - ETA: 5:14 - loss: 3.1889 - regression_loss: 2.3518 - classification_loss: 0.8371
 306/1000 [========>.....................] - ETA: 5:14 - loss: 3.1785 - regression_loss: 2.3441 - classification_loss: 0.8344
 307/1000 [========>.....................] - ETA: 5:13 - loss: 3.1803 - regression_loss: 2.3460 - classification_loss: 0.8342
 308/1000 [========>.....................] - ETA: 5:13 - loss: 3.1708 - regression_loss: 2.3384 - classification_loss: 0.8323
 309/1000 [========>.....................] - ETA: 5:12 - loss: 3.1723 - regression_loss: 2.3407 - classification_loss: 0.8316
 310/1000 [========>.....................] - ETA: 5:12 - loss: 3.1775 - regression_loss: 2.3455 - classification_loss: 0.8320
 311/1000 [========>.....................] - ETA: 5:12 - loss: 3.1789 - regression_loss: 2.3475 - classification_loss: 0.8314
 312/1000 [========>.....................] - ETA: 5:11 - loss: 3.1833 - regression_loss: 2.3510 - classification_loss: 0.8323
 313/1000 [========>.....................] - ETA: 5:11 - loss: 3.1732 - regression_loss: 2.3435 - classification_loss: 0.8297
 314/1000 [========>.....................] - ETA: 5:10 - loss: 3.1727 - regression_loss: 2.3429 - classification_loss: 0.8298
 315/1000 [========>.....................] - ETA: 5:10 - loss: 3.1747 - regression_loss: 2.3453 - classification_loss: 0.8294
 316/1000 [========>.....................] - ETA: 5:09 - loss: 3.1777 - regression_loss: 2.3488 - classification_loss: 0.8290
 317/1000 [========>.....................] - ETA: 5:09 - loss: 3.1795 - regression_loss: 2.3504 - classification_loss: 0.8292
 318/1000 [========>.....................] - ETA: 5:08 - loss: 3.1695 - regression_loss: 2.3430 - classification_loss: 0.8266
 319/1000 [========>.....................] - ETA: 5:08 - loss: 3.1703 - regression_loss: 2.3442 - classification_loss: 0.8261
 320/1000 [========>.....................] - ETA: 5:07 - loss: 3.1714 - regression_loss: 2.3457 - classification_loss: 0.8257
 321/1000 [========>.....................] - ETA: 5:07 - loss: 3.1825 - regression_loss: 2.3532 - classification_loss: 0.8293
 322/1000 [========>.....................] - ETA: 5:06 - loss: 3.1903 - regression_loss: 2.3584 - classification_loss: 0.8319
 323/1000 [========>.....................] - ETA: 5:06 - loss: 3.1959 - regression_loss: 2.3650 - classification_loss: 0.8309
 324/1000 [========>.....................] - ETA: 5:06 - loss: 3.1950 - regression_loss: 2.3650 - classification_loss: 0.8300
 325/1000 [========>.....................] - ETA: 5:05 - loss: 3.1963 - regression_loss: 2.3665 - classification_loss: 0.8298
 326/1000 [========>.....................] - ETA: 5:05 - loss: 3.1866 - regression_loss: 2.3593 - classification_loss: 0.8273
 327/1000 [========>.....................] - ETA: 5:04 - loss: 3.1769 - regression_loss: 2.3521 - classification_loss: 0.8248
 328/1000 [========>.....................] - ETA: 5:04 - loss: 3.1696 - regression_loss: 2.3449 - classification_loss: 0.8247
 329/1000 [========>.....................] - ETA: 5:03 - loss: 3.1600 - regression_loss: 2.3378 - classification_loss: 0.8222
 330/1000 [========>.....................] - ETA: 5:03 - loss: 3.1610 - regression_loss: 2.3378 - classification_loss: 0.8231
 331/1000 [========>.....................] - ETA: 5:02 - loss: 3.1626 - regression_loss: 2.3401 - classification_loss: 0.8225
 332/1000 [========>.....................] - ETA: 5:02 - loss: 3.1640 - regression_loss: 2.3422 - classification_loss: 0.8217
 333/1000 [========>.....................] - ETA: 5:02 - loss: 3.1676 - regression_loss: 2.3427 - classification_loss: 0.8250
 334/1000 [=========>....................] - ETA: 5:01 - loss: 3.1582 - regression_loss: 2.3357 - classification_loss: 0.8225
 335/1000 [=========>....................] - ETA: 5:01 - loss: 3.1487 - regression_loss: 2.3287 - classification_loss: 0.8200
 336/1000 [=========>....................] - ETA: 5:00 - loss: 3.1530 - regression_loss: 2.3310 - classification_loss: 0.8221
 337/1000 [=========>....................] - ETA: 5:00 - loss: 3.1565 - regression_loss: 2.3334 - classification_loss: 0.8231
 338/1000 [=========>....................] - ETA: 4:59 - loss: 3.1628 - regression_loss: 2.3368 - classification_loss: 0.8260
 339/1000 [=========>....................] - ETA: 4:59 - loss: 3.1662 - regression_loss: 2.3381 - classification_loss: 0.8281
 340/1000 [=========>....................] - ETA: 4:58 - loss: 3.1685 - regression_loss: 2.3403 - classification_loss: 0.8281
 341/1000 [=========>....................] - ETA: 4:58 - loss: 3.1700 - regression_loss: 2.3428 - classification_loss: 0.8272
 342/1000 [=========>....................] - ETA: 4:57 - loss: 3.1714 - regression_loss: 2.3422 - classification_loss: 0.8292
 343/1000 [=========>....................] - ETA: 4:57 - loss: 3.1722 - regression_loss: 2.3429 - classification_loss: 0.8293
 344/1000 [=========>....................] - ETA: 4:56 - loss: 3.1771 - regression_loss: 2.3470 - classification_loss: 0.8301
 345/1000 [=========>....................] - ETA: 4:56 - loss: 3.1775 - regression_loss: 2.3478 - classification_loss: 0.8297
 346/1000 [=========>....................] - ETA: 4:56 - loss: 3.1849 - regression_loss: 2.3518 - classification_loss: 0.8331
 347/1000 [=========>....................] - ETA: 4:55 - loss: 3.1757 - regression_loss: 2.3451 - classification_loss: 0.8307
 348/1000 [=========>....................] - ETA: 4:55 - loss: 3.1839 - regression_loss: 2.3484 - classification_loss: 0.8355
 349/1000 [=========>....................] - ETA: 4:54 - loss: 3.1826 - regression_loss: 2.3484 - classification_loss: 0.8341
 350/1000 [=========>....................] - ETA: 4:54 - loss: 3.1878 - regression_loss: 2.3516 - classification_loss: 0.8362
 351/1000 [=========>....................] - ETA: 4:53 - loss: 3.1913 - regression_loss: 2.3545 - classification_loss: 0.8368
 352/1000 [=========>....................] - ETA: 4:53 - loss: 3.1910 - regression_loss: 2.3550 - classification_loss: 0.8360
 353/1000 [=========>....................] - ETA: 4:52 - loss: 3.1820 - regression_loss: 2.3484 - classification_loss: 0.8336
 354/1000 [=========>....................] - ETA: 4:52 - loss: 3.1835 - regression_loss: 2.3492 - classification_loss: 0.8342
 355/1000 [=========>....................] - ETA: 4:52 - loss: 3.1817 - regression_loss: 2.3478 - classification_loss: 0.8339
 356/1000 [=========>....................] - ETA: 4:51 - loss: 3.1813 - regression_loss: 2.3488 - classification_loss: 0.8325
 357/1000 [=========>....................] - ETA: 4:51 - loss: 3.1866 - regression_loss: 2.3540 - classification_loss: 0.8327
 358/1000 [=========>....................] - ETA: 4:50 - loss: 3.1879 - regression_loss: 2.3549 - classification_loss: 0.8330
 359/1000 [=========>....................] - ETA: 4:50 - loss: 3.1879 - regression_loss: 2.3551 - classification_loss: 0.8328
 360/1000 [=========>....................] - ETA: 4:49 - loss: 3.1879 - regression_loss: 2.3561 - classification_loss: 0.8318
 361/1000 [=========>....................] - ETA: 4:49 - loss: 3.1874 - regression_loss: 2.3548 - classification_loss: 0.8326
 362/1000 [=========>....................] - ETA: 4:48 - loss: 3.1890 - regression_loss: 2.3573 - classification_loss: 0.8316
 363/1000 [=========>....................] - ETA: 4:48 - loss: 3.1802 - regression_loss: 2.3508 - classification_loss: 0.8293
 364/1000 [=========>....................] - ETA: 4:48 - loss: 3.1786 - regression_loss: 2.3490 - classification_loss: 0.8296
 365/1000 [=========>....................] - ETA: 4:47 - loss: 3.1830 - regression_loss: 2.3526 - classification_loss: 0.8304
 366/1000 [=========>....................] - ETA: 4:47 - loss: 3.1744 - regression_loss: 2.3462 - classification_loss: 0.8282
 367/1000 [==========>...................] - ETA: 4:46 - loss: 3.1750 - regression_loss: 2.3460 - classification_loss: 0.8290
 368/1000 [==========>...................] - ETA: 4:46 - loss: 3.1728 - regression_loss: 2.3442 - classification_loss: 0.8286
 369/1000 [==========>...................] - ETA: 4:45 - loss: 3.1776 - regression_loss: 2.3485 - classification_loss: 0.8291
 370/1000 [==========>...................] - ETA: 4:45 - loss: 3.1836 - regression_loss: 2.3533 - classification_loss: 0.8303
 371/1000 [==========>...................] - ETA: 4:44 - loss: 3.1861 - regression_loss: 2.3550 - classification_loss: 0.8310
 372/1000 [==========>...................] - ETA: 4:44 - loss: 3.1775 - regression_loss: 2.3487 - classification_loss: 0.8288
 373/1000 [==========>...................] - ETA: 4:43 - loss: 3.1968 - regression_loss: 2.3424 - classification_loss: 0.8544
 374/1000 [==========>...................] - ETA: 4:43 - loss: 3.2025 - regression_loss: 2.3469 - classification_loss: 0.8556
 375/1000 [==========>...................] - ETA: 4:42 - loss: 3.1939 - regression_loss: 2.3406 - classification_loss: 0.8533
 376/1000 [==========>...................] - ETA: 4:42 - loss: 3.1978 - regression_loss: 2.3439 - classification_loss: 0.8539
 377/1000 [==========>...................] - ETA: 4:42 - loss: 3.2030 - regression_loss: 2.3493 - classification_loss: 0.8537
 378/1000 [==========>...................] - ETA: 4:41 - loss: 3.1945 - regression_loss: 2.3431 - classification_loss: 0.8515
 379/1000 [==========>...................] - ETA: 4:41 - loss: 3.1976 - regression_loss: 2.3451 - classification_loss: 0.8526
 380/1000 [==========>...................] - ETA: 4:40 - loss: 3.1971 - regression_loss: 2.3455 - classification_loss: 0.8516
 381/1000 [==========>...................] - ETA: 4:40 - loss: 3.2028 - regression_loss: 2.3509 - classification_loss: 0.8520
 382/1000 [==========>...................] - ETA: 4:39 - loss: 3.2013 - regression_loss: 2.3501 - classification_loss: 0.8513
 383/1000 [==========>...................] - ETA: 4:39 - loss: 3.2065 - regression_loss: 2.3521 - classification_loss: 0.8544
 384/1000 [==========>...................] - ETA: 4:38 - loss: 3.2117 - regression_loss: 2.3556 - classification_loss: 0.8561
 385/1000 [==========>...................] - ETA: 4:38 - loss: 3.2034 - regression_loss: 2.3495 - classification_loss: 0.8538
 386/1000 [==========>...................] - ETA: 4:37 - loss: 3.2027 - regression_loss: 2.3490 - classification_loss: 0.8538
 387/1000 [==========>...................] - ETA: 4:37 - loss: 3.1944 - regression_loss: 2.3429 - classification_loss: 0.8515
 388/1000 [==========>...................] - ETA: 4:37 - loss: 3.1976 - regression_loss: 2.3458 - classification_loss: 0.8518
 389/1000 [==========>...................] - ETA: 4:36 - loss: 3.1994 - regression_loss: 2.3471 - classification_loss: 0.8524
 390/1000 [==========>...................] - ETA: 4:36 - loss: 3.1912 - regression_loss: 2.3411 - classification_loss: 0.8502
 391/1000 [==========>...................] - ETA: 4:35 - loss: 3.1941 - regression_loss: 2.3434 - classification_loss: 0.8507
 392/1000 [==========>...................] - ETA: 4:35 - loss: 3.1970 - regression_loss: 2.3452 - classification_loss: 0.8518
 393/1000 [==========>...................] - ETA: 4:34 - loss: 3.1954 - regression_loss: 2.3449 - classification_loss: 0.8505
 394/1000 [==========>...................] - ETA: 4:34 - loss: 3.1956 - regression_loss: 2.3451 - classification_loss: 0.8504
 395/1000 [==========>...................] - ETA: 4:33 - loss: 3.1981 - regression_loss: 2.3468 - classification_loss: 0.8513
 396/1000 [==========>...................] - ETA: 4:33 - loss: 3.1901 - regression_loss: 2.3409 - classification_loss: 0.8492
 397/1000 [==========>...................] - ETA: 4:33 - loss: 3.1899 - regression_loss: 2.3417 - classification_loss: 0.8482
 398/1000 [==========>...................] - ETA: 4:32 - loss: 3.1888 - regression_loss: 2.3406 - classification_loss: 0.8481
 399/1000 [==========>...................] - ETA: 4:32 - loss: 3.1808 - regression_loss: 2.3347 - classification_loss: 0.8460
 400/1000 [===========>..................] - ETA: 4:31 - loss: 3.1788 - regression_loss: 2.3333 - classification_loss: 0.8455
 401/1000 [===========>..................] - ETA: 4:31 - loss: 3.1788 - regression_loss: 2.3336 - classification_loss: 0.8452
 402/1000 [===========>..................] - ETA: 4:30 - loss: 3.1798 - regression_loss: 2.3347 - classification_loss: 0.8451
 403/1000 [===========>..................] - ETA: 4:30 - loss: 3.1808 - regression_loss: 2.3361 - classification_loss: 0.8447
 404/1000 [===========>..................] - ETA: 4:29 - loss: 3.1798 - regression_loss: 2.3356 - classification_loss: 0.8441
 405/1000 [===========>..................] - ETA: 4:29 - loss: 3.1719 - regression_loss: 2.3298 - classification_loss: 0.8421
 406/1000 [===========>..................] - ETA: 4:28 - loss: 3.1732 - regression_loss: 2.3311 - classification_loss: 0.8421
 407/1000 [===========>..................] - ETA: 4:28 - loss: 3.1774 - regression_loss: 2.3348 - classification_loss: 0.8426
 408/1000 [===========>..................] - ETA: 4:28 - loss: 3.1697 - regression_loss: 2.3291 - classification_loss: 0.8406
 409/1000 [===========>..................] - ETA: 4:27 - loss: 3.1710 - regression_loss: 2.3314 - classification_loss: 0.8395
 410/1000 [===========>..................] - ETA: 4:27 - loss: 3.1700 - regression_loss: 2.3309 - classification_loss: 0.8391
 411/1000 [===========>..................] - ETA: 4:26 - loss: 3.1623 - regression_loss: 2.3252 - classification_loss: 0.8371
 412/1000 [===========>..................] - ETA: 4:26 - loss: 3.1660 - regression_loss: 2.3294 - classification_loss: 0.8366
 413/1000 [===========>..................] - ETA: 4:25 - loss: 3.1683 - regression_loss: 2.3238 - classification_loss: 0.8445
 414/1000 [===========>..................] - ETA: 4:25 - loss: 3.1713 - regression_loss: 2.3266 - classification_loss: 0.8447
 415/1000 [===========>..................] - ETA: 4:24 - loss: 3.1688 - regression_loss: 2.3252 - classification_loss: 0.8437
 416/1000 [===========>..................] - ETA: 4:24 - loss: 3.1666 - regression_loss: 2.3239 - classification_loss: 0.8427
 417/1000 [===========>..................] - ETA: 4:23 - loss: 3.1590 - regression_loss: 2.3183 - classification_loss: 0.8407
 418/1000 [===========>..................] - ETA: 4:23 - loss: 3.1630 - regression_loss: 2.3194 - classification_loss: 0.8436
 419/1000 [===========>..................] - ETA: 4:23 - loss: 3.1555 - regression_loss: 2.3139 - classification_loss: 0.8416
 420/1000 [===========>..................] - ETA: 4:22 - loss: 3.1574 - regression_loss: 2.3163 - classification_loss: 0.8412
 421/1000 [===========>..................] - ETA: 4:22 - loss: 3.1499 - regression_loss: 2.3108 - classification_loss: 0.8392
 422/1000 [===========>..................] - ETA: 4:21 - loss: 3.1507 - regression_loss: 2.3121 - classification_loss: 0.8386
 423/1000 [===========>..................] - ETA: 4:21 - loss: 3.1604 - regression_loss: 2.3187 - classification_loss: 0.8417
 424/1000 [===========>..................] - ETA: 4:20 - loss: 3.1533 - regression_loss: 2.3132 - classification_loss: 0.8401
 425/1000 [===========>..................] - ETA: 4:20 - loss: 3.1554 - regression_loss: 2.3148 - classification_loss: 0.8405
 426/1000 [===========>..................] - ETA: 4:19 - loss: 3.1563 - regression_loss: 2.3157 - classification_loss: 0.8406
 427/1000 [===========>..................] - ETA: 4:19 - loss: 3.1607 - regression_loss: 2.3188 - classification_loss: 0.8420
 428/1000 [===========>..................] - ETA: 4:18 - loss: 3.1670 - regression_loss: 2.3251 - classification_loss: 0.8418
 429/1000 [===========>..................] - ETA: 4:18 - loss: 3.1704 - regression_loss: 2.3269 - classification_loss: 0.8435
 430/1000 [===========>..................] - ETA: 4:18 - loss: 3.1765 - regression_loss: 2.3304 - classification_loss: 0.8461
 431/1000 [===========>..................] - ETA: 4:17 - loss: 3.1809 - regression_loss: 2.3331 - classification_loss: 0.8478
 432/1000 [===========>..................] - ETA: 4:17 - loss: 3.1853 - regression_loss: 2.3355 - classification_loss: 0.8498
 433/1000 [===========>..................] - ETA: 4:16 - loss: 3.1779 - regression_loss: 2.3301 - classification_loss: 0.8478
 434/1000 [============>.................] - ETA: 4:16 - loss: 3.1808 - regression_loss: 2.3325 - classification_loss: 0.8483
 435/1000 [============>.................] - ETA: 4:15 - loss: 3.1833 - regression_loss: 2.3349 - classification_loss: 0.8484
 436/1000 [============>.................] - ETA: 4:15 - loss: 3.1859 - regression_loss: 2.3371 - classification_loss: 0.8487
 437/1000 [============>.................] - ETA: 4:14 - loss: 3.1888 - regression_loss: 2.3398 - classification_loss: 0.8490
 438/1000 [============>.................] - ETA: 4:14 - loss: 3.1815 - regression_loss: 2.3344 - classification_loss: 0.8471
 439/1000 [============>.................] - ETA: 4:13 - loss: 3.1854 - regression_loss: 2.3371 - classification_loss: 0.8483
 440/1000 [============>.................] - ETA: 4:13 - loss: 3.1879 - regression_loss: 2.3380 - classification_loss: 0.8500
 441/1000 [============>.................] - ETA: 4:13 - loss: 3.1928 - regression_loss: 2.3421 - classification_loss: 0.8507
 442/1000 [============>.................] - ETA: 4:12 - loss: 3.1950 - regression_loss: 2.3439 - classification_loss: 0.8511
 443/1000 [============>.................] - ETA: 4:12 - loss: 3.1951 - regression_loss: 2.3446 - classification_loss: 0.8505
 444/1000 [============>.................] - ETA: 4:11 - loss: 3.1984 - regression_loss: 2.3475 - classification_loss: 0.8509
 445/1000 [============>.................] - ETA: 4:11 - loss: 3.2001 - regression_loss: 2.3481 - classification_loss: 0.8520
 446/1000 [============>.................] - ETA: 4:10 - loss: 3.2003 - regression_loss: 2.3489 - classification_loss: 0.8514
 447/1000 [============>.................] - ETA: 4:10 - loss: 3.1931 - regression_loss: 2.3436 - classification_loss: 0.8495
 448/1000 [============>.................] - ETA: 4:09 - loss: 3.1939 - regression_loss: 2.3448 - classification_loss: 0.8491
 449/1000 [============>.................] - ETA: 4:09 - loss: 3.1868 - regression_loss: 2.3395 - classification_loss: 0.8472
 450/1000 [============>.................] - ETA: 4:09 - loss: 3.1937 - regression_loss: 2.3449 - classification_loss: 0.8488
 451/1000 [============>.................] - ETA: 4:08 - loss: 3.1967 - regression_loss: 2.3470 - classification_loss: 0.8497
 452/1000 [============>.................] - ETA: 4:08 - loss: 3.2010 - regression_loss: 2.3504 - classification_loss: 0.8506
 453/1000 [============>.................] - ETA: 4:07 - loss: 3.2038 - regression_loss: 2.3520 - classification_loss: 0.8518
 454/1000 [============>.................] - ETA: 4:07 - loss: 3.2026 - regression_loss: 2.3516 - classification_loss: 0.8510
 455/1000 [============>.................] - ETA: 4:06 - loss: 3.2066 - regression_loss: 2.3545 - classification_loss: 0.8522
 456/1000 [============>.................] - ETA: 4:06 - loss: 3.2064 - regression_loss: 2.3543 - classification_loss: 0.8521
 457/1000 [============>.................] - ETA: 4:05 - loss: 3.2091 - regression_loss: 2.3566 - classification_loss: 0.8524
 458/1000 [============>.................] - ETA: 4:05 - loss: 3.2096 - regression_loss: 2.3583 - classification_loss: 0.8513
 459/1000 [============>.................] - ETA: 4:04 - loss: 3.2091 - regression_loss: 2.3583 - classification_loss: 0.8508
 460/1000 [============>.................] - ETA: 4:04 - loss: 3.2097 - regression_loss: 2.3590 - classification_loss: 0.8506
 461/1000 [============>.................] - ETA: 4:03 - loss: 3.2129 - regression_loss: 2.3617 - classification_loss: 0.8512
 462/1000 [============>.................] - ETA: 4:03 - loss: 3.2159 - regression_loss: 2.3646 - classification_loss: 0.8513
 463/1000 [============>.................] - ETA: 4:03 - loss: 3.2196 - regression_loss: 2.3686 - classification_loss: 0.8510
 464/1000 [============>.................] - ETA: 4:02 - loss: 3.2222 - regression_loss: 2.3707 - classification_loss: 0.8515
 465/1000 [============>.................] - ETA: 4:02 - loss: 3.2237 - regression_loss: 2.3729 - classification_loss: 0.8508
 466/1000 [============>.................] - ETA: 4:01 - loss: 3.2171 - regression_loss: 2.3678 - classification_loss: 0.8493
 467/1000 [=============>................] - ETA: 4:01 - loss: 3.2177 - regression_loss: 2.3689 - classification_loss: 0.8488
 468/1000 [=============>................] - ETA: 4:00 - loss: 3.2219 - regression_loss: 2.3720 - classification_loss: 0.8499
 469/1000 [=============>................] - ETA: 4:00 - loss: 3.2221 - regression_loss: 2.3725 - classification_loss: 0.8497
 470/1000 [=============>................] - ETA: 3:59 - loss: 3.2264 - regression_loss: 2.3765 - classification_loss: 0.8498
 471/1000 [=============>................] - ETA: 3:59 - loss: 3.2258 - regression_loss: 2.3766 - classification_loss: 0.8492
 472/1000 [=============>................] - ETA: 3:58 - loss: 3.2269 - regression_loss: 2.3781 - classification_loss: 0.8488
 473/1000 [=============>................] - ETA: 3:58 - loss: 3.2201 - regression_loss: 2.3731 - classification_loss: 0.8470
 474/1000 [=============>................] - ETA: 3:58 - loss: 3.2213 - regression_loss: 2.3744 - classification_loss: 0.8470
 475/1000 [=============>................] - ETA: 3:57 - loss: 3.2267 - regression_loss: 2.3767 - classification_loss: 0.8500
 476/1000 [=============>................] - ETA: 3:57 - loss: 3.2279 - regression_loss: 2.3787 - classification_loss: 0.8492
 477/1000 [=============>................] - ETA: 3:56 - loss: 3.2307 - regression_loss: 2.3807 - classification_loss: 0.8500
 478/1000 [=============>................] - ETA: 3:56 - loss: 3.2243 - regression_loss: 2.3757 - classification_loss: 0.8486
 479/1000 [=============>................] - ETA: 3:55 - loss: 3.2242 - regression_loss: 2.3764 - classification_loss: 0.8478
 480/1000 [=============>................] - ETA: 3:55 - loss: 3.2294 - regression_loss: 2.3800 - classification_loss: 0.8494
 481/1000 [=============>................] - ETA: 3:54 - loss: 3.2307 - regression_loss: 2.3809 - classification_loss: 0.8498
 482/1000 [=============>................] - ETA: 3:54 - loss: 3.2293 - regression_loss: 2.3806 - classification_loss: 0.8487
 483/1000 [=============>................] - ETA: 3:53 - loss: 3.2300 - regression_loss: 2.3818 - classification_loss: 0.8482
 484/1000 [=============>................] - ETA: 3:53 - loss: 3.2350 - regression_loss: 2.3871 - classification_loss: 0.8478
 485/1000 [=============>................] - ETA: 3:53 - loss: 3.2397 - regression_loss: 2.3896 - classification_loss: 0.8500
 486/1000 [=============>................] - ETA: 3:52 - loss: 3.2330 - regression_loss: 2.3847 - classification_loss: 0.8483
 487/1000 [=============>................] - ETA: 3:52 - loss: 3.2319 - regression_loss: 2.3842 - classification_loss: 0.8477
 488/1000 [=============>................] - ETA: 3:51 - loss: 3.2352 - regression_loss: 2.3876 - classification_loss: 0.8477
 489/1000 [=============>................] - ETA: 3:51 - loss: 3.2345 - regression_loss: 2.3877 - classification_loss: 0.8469
 490/1000 [=============>................] - ETA: 3:50 - loss: 3.2354 - regression_loss: 2.3888 - classification_loss: 0.8466
 491/1000 [=============>................] - ETA: 3:50 - loss: 3.2288 - regression_loss: 2.3840 - classification_loss: 0.8448
 492/1000 [=============>................] - ETA: 3:49 - loss: 3.2314 - regression_loss: 2.3865 - classification_loss: 0.8449
 493/1000 [=============>................] - ETA: 3:49 - loss: 3.2325 - regression_loss: 2.3881 - classification_loss: 0.8444
 494/1000 [=============>................] - ETA: 3:48 - loss: 3.2368 - regression_loss: 2.3902 - classification_loss: 0.8466
 495/1000 [=============>................] - ETA: 3:48 - loss: 3.2302 - regression_loss: 2.3853 - classification_loss: 0.8449
 496/1000 [=============>................] - ETA: 3:48 - loss: 3.2338 - regression_loss: 2.3879 - classification_loss: 0.8459
 497/1000 [=============>................] - ETA: 3:47 - loss: 3.2273 - regression_loss: 2.3831 - classification_loss: 0.8441
 498/1000 [=============>................] - ETA: 3:47 - loss: 3.2269 - regression_loss: 2.3832 - classification_loss: 0.8438
 499/1000 [=============>................] - ETA: 3:46 - loss: 3.2245 - regression_loss: 2.3819 - classification_loss: 0.8426
 500/1000 [==============>...............] - ETA: 3:46 - loss: 3.2293 - regression_loss: 2.3862 - classification_loss: 0.8431
 501/1000 [==============>...............] - ETA: 3:45 - loss: 3.2294 - regression_loss: 2.3861 - classification_loss: 0.8433
 502/1000 [==============>...............] - ETA: 3:45 - loss: 3.2311 - regression_loss: 2.3884 - classification_loss: 0.8427
 503/1000 [==============>...............] - ETA: 3:44 - loss: 3.2247 - regression_loss: 2.3836 - classification_loss: 0.8410
 504/1000 [==============>...............] - ETA: 3:44 - loss: 3.2241 - regression_loss: 2.3836 - classification_loss: 0.8404
 505/1000 [==============>...............] - ETA: 3:43 - loss: 3.2294 - regression_loss: 2.3881 - classification_loss: 0.8413
 506/1000 [==============>...............] - ETA: 3:43 - loss: 3.2230 - regression_loss: 2.3834 - classification_loss: 0.8396
 507/1000 [==============>...............] - ETA: 3:43 - loss: 3.2168 - regression_loss: 2.3787 - classification_loss: 0.8382
 508/1000 [==============>...............] - ETA: 3:42 - loss: 3.2105 - regression_loss: 2.3740 - classification_loss: 0.8365
 509/1000 [==============>...............] - ETA: 3:42 - loss: 3.2171 - regression_loss: 2.3779 - classification_loss: 0.8392
 510/1000 [==============>...............] - ETA: 3:41 - loss: 3.2184 - regression_loss: 2.3797 - classification_loss: 0.8387
 511/1000 [==============>...............] - ETA: 3:41 - loss: 3.2180 - regression_loss: 2.3803 - classification_loss: 0.8377
 512/1000 [==============>...............] - ETA: 3:40 - loss: 3.2225 - regression_loss: 2.3823 - classification_loss: 0.8402
 513/1000 [==============>...............] - ETA: 3:40 - loss: 3.2163 - regression_loss: 2.3777 - classification_loss: 0.8386
 514/1000 [==============>...............] - ETA: 3:39 - loss: 3.2186 - regression_loss: 2.3786 - classification_loss: 0.8400
 515/1000 [==============>...............] - ETA: 3:39 - loss: 3.2124 - regression_loss: 2.3740 - classification_loss: 0.8383
 516/1000 [==============>...............] - ETA: 3:39 - loss: 3.2113 - regression_loss: 2.3739 - classification_loss: 0.8374
 517/1000 [==============>...............] - ETA: 3:38 - loss: 3.2051 - regression_loss: 2.3693 - classification_loss: 0.8358
 518/1000 [==============>...............] - ETA: 3:38 - loss: 3.2100 - regression_loss: 2.3723 - classification_loss: 0.8376
 519/1000 [==============>...............] - ETA: 3:37 - loss: 3.2106 - regression_loss: 2.3724 - classification_loss: 0.8382
 520/1000 [==============>...............] - ETA: 3:37 - loss: 3.2139 - regression_loss: 2.3752 - classification_loss: 0.8386
 521/1000 [==============>...............] - ETA: 3:36 - loss: 3.2184 - regression_loss: 2.3779 - classification_loss: 0.8405
 522/1000 [==============>...............] - ETA: 3:36 - loss: 3.2205 - regression_loss: 2.3801 - classification_loss: 0.8404
 523/1000 [==============>...............] - ETA: 3:35 - loss: 3.2250 - regression_loss: 2.3827 - classification_loss: 0.8423
 524/1000 [==============>...............] - ETA: 3:35 - loss: 3.2188 - regression_loss: 2.3781 - classification_loss: 0.8407
 525/1000 [==============>...............] - ETA: 3:34 - loss: 3.2213 - regression_loss: 2.3798 - classification_loss: 0.8415
 526/1000 [==============>...............] - ETA: 3:34 - loss: 3.2224 - regression_loss: 2.3805 - classification_loss: 0.8418
 527/1000 [==============>...............] - ETA: 3:34 - loss: 3.2251 - regression_loss: 2.3833 - classification_loss: 0.8418
 528/1000 [==============>...............] - ETA: 3:33 - loss: 3.2282 - regression_loss: 2.3858 - classification_loss: 0.8424
 529/1000 [==============>...............] - ETA: 3:33 - loss: 3.2221 - regression_loss: 2.3813 - classification_loss: 0.8408
 530/1000 [==============>...............] - ETA: 3:32 - loss: 3.2161 - regression_loss: 2.3768 - classification_loss: 0.8392
 531/1000 [==============>...............] - ETA: 3:32 - loss: 3.2167 - regression_loss: 2.3765 - classification_loss: 0.8402
 532/1000 [==============>...............] - ETA: 3:31 - loss: 3.2174 - regression_loss: 2.3779 - classification_loss: 0.8395
 533/1000 [==============>...............] - ETA: 3:31 - loss: 3.2114 - regression_loss: 2.3735 - classification_loss: 0.8379
 534/1000 [===============>..............] - ETA: 3:30 - loss: 3.2054 - regression_loss: 2.3690 - classification_loss: 0.8364
 535/1000 [===============>..............] - ETA: 3:30 - loss: 3.2077 - regression_loss: 2.3716 - classification_loss: 0.8361
 536/1000 [===============>..............] - ETA: 3:29 - loss: 3.2017 - regression_loss: 2.3672 - classification_loss: 0.8345
 537/1000 [===============>..............] - ETA: 3:29 - loss: 3.2009 - regression_loss: 2.3670 - classification_loss: 0.8339
 538/1000 [===============>..............] - ETA: 3:29 - loss: 3.2041 - regression_loss: 2.3697 - classification_loss: 0.8344
 539/1000 [===============>..............] - ETA: 3:28 - loss: 3.2077 - regression_loss: 2.3726 - classification_loss: 0.8351
 540/1000 [===============>..............] - ETA: 3:28 - loss: 3.2083 - regression_loss: 2.3737 - classification_loss: 0.8346
 541/1000 [===============>..............] - ETA: 3:27 - loss: 3.2121 - regression_loss: 2.3757 - classification_loss: 0.8365
 542/1000 [===============>..............] - ETA: 3:27 - loss: 3.2138 - regression_loss: 2.3763 - classification_loss: 0.8375
 543/1000 [===============>..............] - ETA: 3:26 - loss: 3.2142 - regression_loss: 2.3768 - classification_loss: 0.8373
 544/1000 [===============>..............] - ETA: 3:26 - loss: 3.2165 - regression_loss: 2.3783 - classification_loss: 0.8382
 545/1000 [===============>..............] - ETA: 3:25 - loss: 3.2167 - regression_loss: 2.3776 - classification_loss: 0.8391
 546/1000 [===============>..............] - ETA: 3:25 - loss: 3.2110 - regression_loss: 2.3732 - classification_loss: 0.8377
 547/1000 [===============>..............] - ETA: 3:24 - loss: 3.2169 - regression_loss: 2.3773 - classification_loss: 0.8396
 548/1000 [===============>..............] - ETA: 3:24 - loss: 3.2176 - regression_loss: 2.3775 - classification_loss: 0.8401
 549/1000 [===============>..............] - ETA: 3:24 - loss: 3.2198 - regression_loss: 2.3798 - classification_loss: 0.8400
 550/1000 [===============>..............] - ETA: 3:23 - loss: 3.2150 - regression_loss: 2.3755 - classification_loss: 0.8395
 551/1000 [===============>..............] - ETA: 3:23 - loss: 3.2184 - regression_loss: 2.3797 - classification_loss: 0.8387
 552/1000 [===============>..............] - ETA: 3:22 - loss: 3.2126 - regression_loss: 2.3754 - classification_loss: 0.8372
 553/1000 [===============>..............] - ETA: 3:22 - loss: 3.2141 - regression_loss: 2.3768 - classification_loss: 0.8373
 554/1000 [===============>..............] - ETA: 3:21 - loss: 3.2133 - regression_loss: 2.3762 - classification_loss: 0.8371
 555/1000 [===============>..............] - ETA: 3:21 - loss: 3.2153 - regression_loss: 2.3772 - classification_loss: 0.8381
 556/1000 [===============>..............] - ETA: 3:20 - loss: 3.2161 - regression_loss: 2.3779 - classification_loss: 0.8382
 557/1000 [===============>..............] - ETA: 3:20 - loss: 3.2195 - regression_loss: 2.3798 - classification_loss: 0.8397
 558/1000 [===============>..............] - ETA: 3:19 - loss: 3.2138 - regression_loss: 2.3755 - classification_loss: 0.8383
 559/1000 [===============>..............] - ETA: 3:19 - loss: 3.2128 - regression_loss: 2.3756 - classification_loss: 0.8372
 560/1000 [===============>..............] - ETA: 3:19 - loss: 3.2133 - regression_loss: 2.3764 - classification_loss: 0.8369
 561/1000 [===============>..............] - ETA: 3:18 - loss: 3.2163 - regression_loss: 2.3783 - classification_loss: 0.8380
 562/1000 [===============>..............] - ETA: 3:18 - loss: 3.2170 - regression_loss: 2.3792 - classification_loss: 0.8378
 563/1000 [===============>..............] - ETA: 3:17 - loss: 3.2175 - regression_loss: 2.3799 - classification_loss: 0.8376
 564/1000 [===============>..............] - ETA: 3:17 - loss: 3.2195 - regression_loss: 2.3811 - classification_loss: 0.8384
 565/1000 [===============>..............] - ETA: 3:16 - loss: 3.2211 - regression_loss: 2.3826 - classification_loss: 0.8385
 566/1000 [===============>..............] - ETA: 3:16 - loss: 3.2215 - regression_loss: 2.3832 - classification_loss: 0.8383
 567/1000 [================>.............] - ETA: 3:15 - loss: 3.2212 - regression_loss: 2.3834 - classification_loss: 0.8378
 568/1000 [================>.............] - ETA: 3:15 - loss: 3.2219 - regression_loss: 2.3844 - classification_loss: 0.8375
 569/1000 [================>.............] - ETA: 3:15 - loss: 3.2227 - regression_loss: 2.3849 - classification_loss: 0.8379
 570/1000 [================>.............] - ETA: 3:14 - loss: 3.2224 - regression_loss: 2.3848 - classification_loss: 0.8376
 571/1000 [================>.............] - ETA: 3:14 - loss: 3.2260 - regression_loss: 2.3871 - classification_loss: 0.8389
 572/1000 [================>.............] - ETA: 3:13 - loss: 3.2266 - regression_loss: 2.3877 - classification_loss: 0.8389
 573/1000 [================>.............] - ETA: 3:13 - loss: 3.2209 - regression_loss: 2.3835 - classification_loss: 0.8374
 574/1000 [================>.............] - ETA: 3:12 - loss: 3.2211 - regression_loss: 2.3844 - classification_loss: 0.8367
 575/1000 [================>.............] - ETA: 3:12 - loss: 3.2221 - regression_loss: 2.3855 - classification_loss: 0.8366
 576/1000 [================>.............] - ETA: 3:11 - loss: 3.2232 - regression_loss: 2.3864 - classification_loss: 0.8368
 577/1000 [================>.............] - ETA: 3:11 - loss: 3.2216 - regression_loss: 2.3851 - classification_loss: 0.8366
 578/1000 [================>.............] - ETA: 3:10 - loss: 3.2232 - regression_loss: 2.3866 - classification_loss: 0.8366
 579/1000 [================>.............] - ETA: 3:10 - loss: 3.2226 - regression_loss: 2.3870 - classification_loss: 0.8356
 580/1000 [================>.............] - ETA: 3:10 - loss: 3.2264 - regression_loss: 2.3899 - classification_loss: 0.8365
 581/1000 [================>.............] - ETA: 3:09 - loss: 3.2263 - regression_loss: 2.3901 - classification_loss: 0.8362
 582/1000 [================>.............] - ETA: 3:09 - loss: 3.2207 - regression_loss: 2.3860 - classification_loss: 0.8347
 583/1000 [================>.............] - ETA: 3:08 - loss: 3.2152 - regression_loss: 2.3819 - classification_loss: 0.8333
 584/1000 [================>.............] - ETA: 3:08 - loss: 3.2097 - regression_loss: 2.3778 - classification_loss: 0.8319
 585/1000 [================>.............] - ETA: 3:07 - loss: 3.2050 - regression_loss: 2.3737 - classification_loss: 0.8312
 586/1000 [================>.............] - ETA: 3:07 - loss: 3.2064 - regression_loss: 2.3754 - classification_loss: 0.8310
 587/1000 [================>.............] - ETA: 3:06 - loss: 3.2072 - regression_loss: 2.3762 - classification_loss: 0.8310
 588/1000 [================>.............] - ETA: 3:06 - loss: 3.2079 - regression_loss: 2.3774 - classification_loss: 0.8306
 589/1000 [================>.............] - ETA: 3:05 - loss: 3.2120 - regression_loss: 2.3812 - classification_loss: 0.8308
 590/1000 [================>.............] - ETA: 3:05 - loss: 3.2131 - regression_loss: 2.3827 - classification_loss: 0.8304
 591/1000 [================>.............] - ETA: 3:05 - loss: 3.2076 - regression_loss: 2.3786 - classification_loss: 0.8290
 592/1000 [================>.............] - ETA: 3:04 - loss: 3.2023 - regression_loss: 2.3746 - classification_loss: 0.8276
 593/1000 [================>.............] - ETA: 3:04 - loss: 3.2027 - regression_loss: 2.3758 - classification_loss: 0.8269
 594/1000 [================>.............] - ETA: 3:03 - loss: 3.2047 - regression_loss: 2.3783 - classification_loss: 0.8264
 595/1000 [================>.............] - ETA: 3:03 - loss: 3.2046 - regression_loss: 2.3784 - classification_loss: 0.8262
 596/1000 [================>.............] - ETA: 3:02 - loss: 3.1992 - regression_loss: 2.3744 - classification_loss: 0.8248
 597/1000 [================>.............] - ETA: 3:02 - loss: 3.1987 - regression_loss: 2.3737 - classification_loss: 0.8250
 598/1000 [================>.............] - ETA: 3:01 - loss: 3.1934 - regression_loss: 2.3697 - classification_loss: 0.8237
 599/1000 [================>.............] - ETA: 3:01 - loss: 3.1881 - regression_loss: 2.3658 - classification_loss: 0.8223
 600/1000 [=================>............] - ETA: 3:00 - loss: 3.1865 - regression_loss: 2.3651 - classification_loss: 0.8214
 601/1000 [=================>............] - ETA: 3:00 - loss: 3.1873 - regression_loss: 2.3657 - classification_loss: 0.8216
 602/1000 [=================>............] - ETA: 3:00 - loss: 3.1907 - regression_loss: 2.3691 - classification_loss: 0.8216
 603/1000 [=================>............] - ETA: 2:59 - loss: 3.1908 - regression_loss: 2.3693 - classification_loss: 0.8215
 604/1000 [=================>............] - ETA: 2:59 - loss: 3.1855 - regression_loss: 2.3654 - classification_loss: 0.8201
 605/1000 [=================>............] - ETA: 2:58 - loss: 3.1866 - regression_loss: 2.3670 - classification_loss: 0.8196
 606/1000 [=================>............] - ETA: 2:58 - loss: 3.1867 - regression_loss: 2.3661 - classification_loss: 0.8206
 607/1000 [=================>............] - ETA: 2:57 - loss: 3.1901 - regression_loss: 2.3684 - classification_loss: 0.8217
 608/1000 [=================>............] - ETA: 2:57 - loss: 3.1928 - regression_loss: 2.3696 - classification_loss: 0.8231
 609/1000 [=================>............] - ETA: 2:56 - loss: 3.1931 - regression_loss: 2.3701 - classification_loss: 0.8231
 610/1000 [=================>............] - ETA: 2:56 - loss: 3.1973 - regression_loss: 2.3727 - classification_loss: 0.8246
 611/1000 [=================>............] - ETA: 2:56 - loss: 3.1973 - regression_loss: 2.3718 - classification_loss: 0.8255
 612/1000 [=================>............] - ETA: 2:55 - loss: 3.2009 - regression_loss: 2.3743 - classification_loss: 0.8266
 613/1000 [=================>............] - ETA: 2:55 - loss: 3.2045 - regression_loss: 2.3773 - classification_loss: 0.8272
 614/1000 [=================>............] - ETA: 2:54 - loss: 3.2055 - regression_loss: 2.3787 - classification_loss: 0.8268
 615/1000 [=================>............] - ETA: 2:54 - loss: 3.2003 - regression_loss: 2.3748 - classification_loss: 0.8255
 616/1000 [=================>............] - ETA: 2:53 - loss: 3.2013 - regression_loss: 2.3762 - classification_loss: 0.8251
 617/1000 [=================>............] - ETA: 2:53 - loss: 3.1961 - regression_loss: 2.3723 - classification_loss: 0.8237
 618/1000 [=================>............] - ETA: 2:52 - loss: 3.1964 - regression_loss: 2.3729 - classification_loss: 0.8234
 619/1000 [=================>............] - ETA: 2:52 - loss: 3.1974 - regression_loss: 2.3746 - classification_loss: 0.8229
 620/1000 [=================>............] - ETA: 2:51 - loss: 3.1974 - regression_loss: 2.3746 - classification_loss: 0.8228
 621/1000 [=================>............] - ETA: 2:51 - loss: 3.1998 - regression_loss: 2.3754 - classification_loss: 0.8244
 622/1000 [=================>............] - ETA: 2:51 - loss: 3.1947 - regression_loss: 2.3716 - classification_loss: 0.8231
 623/1000 [=================>............] - ETA: 2:50 - loss: 3.1982 - regression_loss: 2.3740 - classification_loss: 0.8241
 624/1000 [=================>............] - ETA: 2:50 - loss: 3.1989 - regression_loss: 2.3753 - classification_loss: 0.8237
 625/1000 [=================>............] - ETA: 2:49 - loss: 3.2037 - regression_loss: 2.3777 - classification_loss: 0.8260
 626/1000 [=================>............] - ETA: 2:49 - loss: 3.1986 - regression_loss: 2.3739 - classification_loss: 0.8247
 627/1000 [=================>............] - ETA: 2:48 - loss: 3.2005 - regression_loss: 2.3746 - classification_loss: 0.8259
 628/1000 [=================>............] - ETA: 2:48 - loss: 3.2033 - regression_loss: 2.3771 - classification_loss: 0.8262
 629/1000 [=================>............] - ETA: 2:47 - loss: 3.2031 - regression_loss: 2.3768 - classification_loss: 0.8264
 630/1000 [=================>............] - ETA: 2:47 - loss: 3.2078 - regression_loss: 2.3790 - classification_loss: 0.8288
 631/1000 [=================>............] - ETA: 2:46 - loss: 3.2076 - regression_loss: 2.3793 - classification_loss: 0.8283
 632/1000 [=================>............] - ETA: 2:46 - loss: 3.2026 - regression_loss: 2.3755 - classification_loss: 0.8270
 633/1000 [=================>............] - ETA: 2:46 - loss: 3.1975 - regression_loss: 2.3718 - classification_loss: 0.8257
 634/1000 [==================>...........] - ETA: 2:45 - loss: 3.1974 - regression_loss: 2.3718 - classification_loss: 0.8256
 635/1000 [==================>...........] - ETA: 2:45 - loss: 3.2032 - regression_loss: 2.3756 - classification_loss: 0.8276
 636/1000 [==================>...........] - ETA: 2:44 - loss: 3.2044 - regression_loss: 2.3769 - classification_loss: 0.8275
 637/1000 [==================>...........] - ETA: 2:44 - loss: 3.2073 - regression_loss: 2.3796 - classification_loss: 0.8277
 638/1000 [==================>...........] - ETA: 2:43 - loss: 3.2110 - regression_loss: 2.3816 - classification_loss: 0.8293
 639/1000 [==================>...........] - ETA: 2:43 - loss: 3.2133 - regression_loss: 2.3826 - classification_loss: 0.8307
 640/1000 [==================>...........] - ETA: 2:42 - loss: 3.2129 - regression_loss: 2.3822 - classification_loss: 0.8307
 641/1000 [==================>...........] - ETA: 2:42 - loss: 3.2126 - regression_loss: 2.3818 - classification_loss: 0.8308
 642/1000 [==================>...........] - ETA: 2:41 - loss: 3.2108 - regression_loss: 2.3807 - classification_loss: 0.8301
 643/1000 [==================>...........] - ETA: 2:41 - loss: 3.2098 - regression_loss: 2.3808 - classification_loss: 0.8291
 644/1000 [==================>...........] - ETA: 2:41 - loss: 3.2049 - regression_loss: 2.3771 - classification_loss: 0.8278
 645/1000 [==================>...........] - ETA: 2:40 - loss: 3.1999 - regression_loss: 2.3734 - classification_loss: 0.8265
 646/1000 [==================>...........] - ETA: 2:40 - loss: 3.2016 - regression_loss: 2.3751 - classification_loss: 0.8265
 647/1000 [==================>...........] - ETA: 2:39 - loss: 3.2042 - regression_loss: 2.3773 - classification_loss: 0.8269
 648/1000 [==================>...........] - ETA: 2:39 - loss: 3.2044 - regression_loss: 2.3779 - classification_loss: 0.8265
 649/1000 [==================>...........] - ETA: 2:38 - loss: 3.2052 - regression_loss: 2.3779 - classification_loss: 0.8273
 650/1000 [==================>...........] - ETA: 2:38 - loss: 3.2054 - regression_loss: 2.3784 - classification_loss: 0.8270
 651/1000 [==================>...........] - ETA: 2:37 - loss: 3.2053 - regression_loss: 2.3787 - classification_loss: 0.8266
 652/1000 [==================>...........] - ETA: 2:37 - loss: 3.2065 - regression_loss: 2.3799 - classification_loss: 0.8267
 653/1000 [==================>...........] - ETA: 2:37 - loss: 3.2079 - regression_loss: 2.3813 - classification_loss: 0.8266
 654/1000 [==================>...........] - ETA: 2:36 - loss: 3.2030 - regression_loss: 2.3777 - classification_loss: 0.8254
 655/1000 [==================>...........] - ETA: 2:36 - loss: 3.2043 - regression_loss: 2.3786 - classification_loss: 0.8257
 656/1000 [==================>...........] - ETA: 2:35 - loss: 3.2084 - regression_loss: 2.3808 - classification_loss: 0.8276
 657/1000 [==================>...........] - ETA: 2:35 - loss: 3.2092 - regression_loss: 2.3819 - classification_loss: 0.8273
 658/1000 [==================>...........] - ETA: 2:34 - loss: 3.2104 - regression_loss: 2.3835 - classification_loss: 0.8269
 659/1000 [==================>...........] - ETA: 2:34 - loss: 3.2098 - regression_loss: 2.3835 - classification_loss: 0.8264
 660/1000 [==================>...........] - ETA: 2:33 - loss: 3.2050 - regression_loss: 2.3798 - classification_loss: 0.8251
 661/1000 [==================>...........] - ETA: 2:33 - loss: 3.2085 - regression_loss: 2.3834 - classification_loss: 0.8251
 662/1000 [==================>...........] - ETA: 2:32 - loss: 3.2113 - regression_loss: 2.3848 - classification_loss: 0.8265
 663/1000 [==================>...........] - ETA: 2:32 - loss: 3.2065 - regression_loss: 2.3812 - classification_loss: 0.8253
 664/1000 [==================>...........] - ETA: 2:32 - loss: 3.2097 - regression_loss: 2.3829 - classification_loss: 0.8268
 665/1000 [==================>...........] - ETA: 2:31 - loss: 3.2088 - regression_loss: 2.3823 - classification_loss: 0.8265
 666/1000 [==================>...........] - ETA: 2:31 - loss: 3.2086 - regression_loss: 2.3826 - classification_loss: 0.8261
 667/1000 [===================>..........] - ETA: 2:30 - loss: 3.2131 - regression_loss: 2.3853 - classification_loss: 0.8278
 668/1000 [===================>..........] - ETA: 2:30 - loss: 3.2164 - regression_loss: 2.3867 - classification_loss: 0.8297
 669/1000 [===================>..........] - ETA: 2:29 - loss: 3.2172 - regression_loss: 2.3880 - classification_loss: 0.8292
 670/1000 [===================>..........] - ETA: 2:29 - loss: 3.2190 - regression_loss: 2.3902 - classification_loss: 0.8288
 671/1000 [===================>..........] - ETA: 2:28 - loss: 3.2183 - regression_loss: 2.3900 - classification_loss: 0.8283
 672/1000 [===================>..........] - ETA: 2:28 - loss: 3.2190 - regression_loss: 2.3910 - classification_loss: 0.8280
 673/1000 [===================>..........] - ETA: 2:27 - loss: 3.2179 - regression_loss: 2.3907 - classification_loss: 0.8273
 674/1000 [===================>..........] - ETA: 2:27 - loss: 3.2191 - regression_loss: 2.3913 - classification_loss: 0.8278
 675/1000 [===================>..........] - ETA: 2:27 - loss: 3.2197 - regression_loss: 2.3921 - classification_loss: 0.8276
 676/1000 [===================>..........] - ETA: 2:26 - loss: 3.2192 - regression_loss: 2.3925 - classification_loss: 0.8267
 677/1000 [===================>..........] - ETA: 2:26 - loss: 3.2207 - regression_loss: 2.3939 - classification_loss: 0.8268
 678/1000 [===================>..........] - ETA: 2:25 - loss: 3.2208 - regression_loss: 2.3942 - classification_loss: 0.8265
 679/1000 [===================>..........] - ETA: 2:25 - loss: 3.2160 - regression_loss: 2.3907 - classification_loss: 0.8253
 680/1000 [===================>..........] - ETA: 2:24 - loss: 3.2157 - regression_loss: 2.3902 - classification_loss: 0.8255
 681/1000 [===================>..........] - ETA: 2:24 - loss: 3.2112 - regression_loss: 2.3867 - classification_loss: 0.8245
 682/1000 [===================>..........] - ETA: 2:23 - loss: 3.2065 - regression_loss: 2.3832 - classification_loss: 0.8233
 683/1000 [===================>..........] - ETA: 2:23 - loss: 3.2090 - regression_loss: 2.3851 - classification_loss: 0.8239
 684/1000 [===================>..........] - ETA: 2:22 - loss: 3.2094 - regression_loss: 2.3861 - classification_loss: 0.8233
 685/1000 [===================>..........] - ETA: 2:22 - loss: 3.2047 - regression_loss: 2.3826 - classification_loss: 0.8221
 686/1000 [===================>..........] - ETA: 2:22 - loss: 3.2001 - regression_loss: 2.3792 - classification_loss: 0.8209
 687/1000 [===================>..........] - ETA: 2:21 - loss: 3.2000 - regression_loss: 2.3787 - classification_loss: 0.8213
 688/1000 [===================>..........] - ETA: 2:21 - loss: 3.2030 - regression_loss: 2.3809 - classification_loss: 0.8221
 689/1000 [===================>..........] - ETA: 2:20 - loss: 3.2039 - regression_loss: 2.3819 - classification_loss: 0.8220
 690/1000 [===================>..........] - ETA: 2:20 - loss: 3.2052 - regression_loss: 2.3822 - classification_loss: 0.8231
 691/1000 [===================>..........] - ETA: 2:19 - loss: 3.2064 - regression_loss: 2.3827 - classification_loss: 0.8238
 692/1000 [===================>..........] - ETA: 2:19 - loss: 3.2080 - regression_loss: 2.3846 - classification_loss: 0.8233
 693/1000 [===================>..........] - ETA: 2:18 - loss: 3.2124 - regression_loss: 2.3878 - classification_loss: 0.8246
 694/1000 [===================>..........] - ETA: 2:18 - loss: 3.2141 - regression_loss: 2.3895 - classification_loss: 0.8247
 695/1000 [===================>..........] - ETA: 2:17 - loss: 3.2158 - regression_loss: 2.3909 - classification_loss: 0.8249
 696/1000 [===================>..........] - ETA: 2:17 - loss: 3.2111 - regression_loss: 2.3874 - classification_loss: 0.8237
 697/1000 [===================>..........] - ETA: 2:17 - loss: 3.2121 - regression_loss: 2.3877 - classification_loss: 0.8243
 698/1000 [===================>..........] - ETA: 2:16 - loss: 3.2075 - regression_loss: 2.3843 - classification_loss: 0.8232
 699/1000 [===================>..........] - ETA: 2:16 - loss: 3.2070 - regression_loss: 2.3845 - classification_loss: 0.8225
 700/1000 [====================>.........] - ETA: 2:15 - loss: 3.2029 - regression_loss: 2.3811 - classification_loss: 0.8217
 701/1000 [====================>.........] - ETA: 2:15 - loss: 3.2065 - regression_loss: 2.3831 - classification_loss: 0.8234
 702/1000 [====================>.........] - ETA: 2:14 - loss: 3.2020 - regression_loss: 2.3797 - classification_loss: 0.8222
 703/1000 [====================>.........] - ETA: 2:14 - loss: 3.1974 - regression_loss: 2.3763 - classification_loss: 0.8211
 704/1000 [====================>.........] - ETA: 2:13 - loss: 3.1929 - regression_loss: 2.3730 - classification_loss: 0.8199
 705/1000 [====================>.........] - ETA: 2:13 - loss: 3.1916 - regression_loss: 2.3717 - classification_loss: 0.8199
 706/1000 [====================>.........] - ETA: 2:13 - loss: 3.1870 - regression_loss: 2.3683 - classification_loss: 0.8187
 707/1000 [====================>.........] - ETA: 2:12 - loss: 3.1868 - regression_loss: 2.3683 - classification_loss: 0.8185
 708/1000 [====================>.........] - ETA: 2:12 - loss: 3.1862 - regression_loss: 2.3683 - classification_loss: 0.8180
 709/1000 [====================>.........] - ETA: 2:11 - loss: 3.1881 - regression_loss: 2.3689 - classification_loss: 0.8192
 710/1000 [====================>.........] - ETA: 2:11 - loss: 3.1916 - regression_loss: 2.3707 - classification_loss: 0.8209
 711/1000 [====================>.........] - ETA: 2:10 - loss: 3.1944 - regression_loss: 2.3717 - classification_loss: 0.8227
 712/1000 [====================>.........] - ETA: 2:10 - loss: 3.1940 - regression_loss: 2.3717 - classification_loss: 0.8223
 713/1000 [====================>.........] - ETA: 2:09 - loss: 3.1993 - regression_loss: 2.3751 - classification_loss: 0.8242
 714/1000 [====================>.........] - ETA: 2:09 - loss: 3.2006 - regression_loss: 2.3768 - classification_loss: 0.8238
 715/1000 [====================>.........] - ETA: 2:08 - loss: 3.2025 - regression_loss: 2.3790 - classification_loss: 0.8235
 716/1000 [====================>.........] - ETA: 2:08 - loss: 3.2057 - regression_loss: 2.3805 - classification_loss: 0.8252
 717/1000 [====================>.........] - ETA: 2:08 - loss: 3.2070 - regression_loss: 2.3814 - classification_loss: 0.8256
 718/1000 [====================>.........] - ETA: 2:07 - loss: 3.2068 - regression_loss: 2.3816 - classification_loss: 0.8252
 719/1000 [====================>.........] - ETA: 2:07 - loss: 3.2104 - regression_loss: 2.3834 - classification_loss: 0.8269
 720/1000 [====================>.........] - ETA: 2:06 - loss: 3.2102 - regression_loss: 2.3837 - classification_loss: 0.8265
 721/1000 [====================>.........] - ETA: 2:06 - loss: 3.2058 - regression_loss: 2.3804 - classification_loss: 0.8254
 722/1000 [====================>.........] - ETA: 2:05 - loss: 3.2074 - regression_loss: 2.3819 - classification_loss: 0.8255
 723/1000 [====================>.........] - ETA: 2:05 - loss: 3.2082 - regression_loss: 2.3831 - classification_loss: 0.8251
 724/1000 [====================>.........] - ETA: 2:04 - loss: 3.2037 - regression_loss: 2.3798 - classification_loss: 0.8239
 725/1000 [====================>.........] - ETA: 2:04 - loss: 3.2055 - regression_loss: 2.3804 - classification_loss: 0.8251
 726/1000 [====================>.........] - ETA: 2:03 - loss: 3.2059 - regression_loss: 2.3807 - classification_loss: 0.8251
 727/1000 [====================>.........] - ETA: 2:03 - loss: 3.2077 - regression_loss: 2.3814 - classification_loss: 0.8263
 728/1000 [====================>.........] - ETA: 2:03 - loss: 3.2095 - regression_loss: 2.3826 - classification_loss: 0.8269
 729/1000 [====================>.........] - ETA: 2:02 - loss: 3.2133 - regression_loss: 2.3861 - classification_loss: 0.8273
 730/1000 [====================>.........] - ETA: 2:02 - loss: 3.2129 - regression_loss: 2.3861 - classification_loss: 0.8268
 731/1000 [====================>.........] - ETA: 2:01 - loss: 3.2085 - regression_loss: 2.3829 - classification_loss: 0.8257
 732/1000 [====================>.........] - ETA: 2:01 - loss: 3.2086 - regression_loss: 2.3826 - classification_loss: 0.8260
 733/1000 [====================>.........] - ETA: 2:00 - loss: 3.2042 - regression_loss: 2.3793 - classification_loss: 0.8248
 734/1000 [=====================>........] - ETA: 2:00 - loss: 3.2050 - regression_loss: 2.3796 - classification_loss: 0.8254
 735/1000 [=====================>........] - ETA: 1:59 - loss: 3.2051 - regression_loss: 2.3798 - classification_loss: 0.8253
 736/1000 [=====================>........] - ETA: 1:59 - loss: 3.2077 - regression_loss: 2.3816 - classification_loss: 0.8261
 737/1000 [=====================>........] - ETA: 1:58 - loss: 3.2084 - regression_loss: 2.3825 - classification_loss: 0.8259
 738/1000 [=====================>........] - ETA: 1:58 - loss: 3.2040 - regression_loss: 2.3793 - classification_loss: 0.8247
 739/1000 [=====================>........] - ETA: 1:58 - loss: 3.2059 - regression_loss: 2.3800 - classification_loss: 0.8259
 740/1000 [=====================>........] - ETA: 1:57 - loss: 3.2069 - regression_loss: 2.3813 - classification_loss: 0.8256
 741/1000 [=====================>........] - ETA: 1:57 - loss: 3.2069 - regression_loss: 2.3818 - classification_loss: 0.8251
 742/1000 [=====================>........] - ETA: 1:56 - loss: 3.2026 - regression_loss: 2.3786 - classification_loss: 0.8239
 743/1000 [=====================>........] - ETA: 1:56 - loss: 3.2033 - regression_loss: 2.3793 - classification_loss: 0.8240
 744/1000 [=====================>........] - ETA: 1:55 - loss: 3.2040 - regression_loss: 2.3795 - classification_loss: 0.8244
 745/1000 [=====================>........] - ETA: 1:55 - loss: 3.2061 - regression_loss: 2.3815 - classification_loss: 0.8246
 746/1000 [=====================>........] - ETA: 1:54 - loss: 3.2081 - regression_loss: 2.3830 - classification_loss: 0.8251
 747/1000 [=====================>........] - ETA: 1:54 - loss: 3.2108 - regression_loss: 2.3847 - classification_loss: 0.8261
 748/1000 [=====================>........] - ETA: 1:53 - loss: 3.2103 - regression_loss: 2.3843 - classification_loss: 0.8260
 749/1000 [=====================>........] - ETA: 1:53 - loss: 3.2107 - regression_loss: 2.3848 - classification_loss: 0.8259
 750/1000 [=====================>........] - ETA: 1:53 - loss: 3.2117 - regression_loss: 2.3864 - classification_loss: 0.8253
 751/1000 [=====================>........] - ETA: 1:52 - loss: 3.2115 - regression_loss: 2.3863 - classification_loss: 0.8252
 752/1000 [=====================>........] - ETA: 1:52 - loss: 3.2144 - regression_loss: 2.3881 - classification_loss: 0.8262
 753/1000 [=====================>........] - ETA: 1:51 - loss: 3.2152 - regression_loss: 2.3886 - classification_loss: 0.8266
 754/1000 [=====================>........] - ETA: 1:51 - loss: 3.2144 - regression_loss: 2.3882 - classification_loss: 0.8262
 755/1000 [=====================>........] - ETA: 1:50 - loss: 3.2152 - regression_loss: 2.3895 - classification_loss: 0.8257
 756/1000 [=====================>........] - ETA: 1:50 - loss: 3.2163 - regression_loss: 2.3904 - classification_loss: 0.8259
 757/1000 [=====================>........] - ETA: 1:49 - loss: 3.2187 - regression_loss: 2.3919 - classification_loss: 0.8268
 758/1000 [=====================>........] - ETA: 1:49 - loss: 3.2209 - regression_loss: 2.3940 - classification_loss: 0.8268
 759/1000 [=====================>........] - ETA: 1:48 - loss: 3.2236 - regression_loss: 2.3965 - classification_loss: 0.8270
 760/1000 [=====================>........] - ETA: 1:48 - loss: 3.2194 - regression_loss: 2.3934 - classification_loss: 0.8260
 761/1000 [=====================>........] - ETA: 1:48 - loss: 3.2194 - regression_loss: 2.3939 - classification_loss: 0.8256
 762/1000 [=====================>........] - ETA: 1:47 - loss: 3.2202 - regression_loss: 2.3942 - classification_loss: 0.8260
 763/1000 [=====================>........] - ETA: 1:47 - loss: 3.2161 - regression_loss: 2.3911 - classification_loss: 0.8250
 764/1000 [=====================>........] - ETA: 1:46 - loss: 3.2153 - regression_loss: 2.3908 - classification_loss: 0.8244
 765/1000 [=====================>........] - ETA: 1:46 - loss: 3.2177 - regression_loss: 2.3932 - classification_loss: 0.8245
 766/1000 [=====================>........] - ETA: 1:45 - loss: 3.2135 - regression_loss: 2.3901 - classification_loss: 0.8234
 767/1000 [======================>.......] - ETA: 1:45 - loss: 3.2093 - regression_loss: 2.3870 - classification_loss: 0.8223
 768/1000 [======================>.......] - ETA: 1:44 - loss: 3.2111 - regression_loss: 2.3879 - classification_loss: 0.8232
 769/1000 [======================>.......] - ETA: 1:44 - loss: 3.2123 - regression_loss: 2.3886 - classification_loss: 0.8238
 770/1000 [======================>.......] - ETA: 1:44 - loss: 3.2151 - regression_loss: 2.3902 - classification_loss: 0.8249
 771/1000 [======================>.......] - ETA: 1:43 - loss: 3.2183 - regression_loss: 2.3924 - classification_loss: 0.8258
 772/1000 [======================>.......] - ETA: 1:43 - loss: 3.2202 - regression_loss: 2.3939 - classification_loss: 0.8263
 773/1000 [======================>.......] - ETA: 1:42 - loss: 3.2212 - regression_loss: 2.3949 - classification_loss: 0.8263
 774/1000 [======================>.......] - ETA: 1:42 - loss: 3.2220 - regression_loss: 2.3948 - classification_loss: 0.8272
 775/1000 [======================>.......] - ETA: 1:41 - loss: 3.2218 - regression_loss: 2.3949 - classification_loss: 0.8270
 776/1000 [======================>.......] - ETA: 1:41 - loss: 3.2177 - regression_loss: 2.3918 - classification_loss: 0.8259
 777/1000 [======================>.......] - ETA: 1:40 - loss: 3.2188 - regression_loss: 2.3930 - classification_loss: 0.8258
 778/1000 [======================>.......] - ETA: 1:40 - loss: 3.2147 - regression_loss: 2.3899 - classification_loss: 0.8248
 779/1000 [======================>.......] - ETA: 1:39 - loss: 3.2158 - regression_loss: 2.3902 - classification_loss: 0.8255
 780/1000 [======================>.......] - ETA: 1:39 - loss: 3.2150 - regression_loss: 2.3899 - classification_loss: 0.8251
 781/1000 [======================>.......] - ETA: 1:39 - loss: 3.2157 - regression_loss: 2.3907 - classification_loss: 0.8249
 782/1000 [======================>.......] - ETA: 1:38 - loss: 3.2153 - regression_loss: 2.3909 - classification_loss: 0.8244
 783/1000 [======================>.......] - ETA: 1:38 - loss: 3.2167 - regression_loss: 2.3925 - classification_loss: 0.8242
 784/1000 [======================>.......] - ETA: 1:37 - loss: 3.2125 - regression_loss: 2.3894 - classification_loss: 0.8231
 785/1000 [======================>.......] - ETA: 1:37 - loss: 3.2085 - regression_loss: 2.3864 - classification_loss: 0.8221
 786/1000 [======================>.......] - ETA: 1:36 - loss: 3.2097 - regression_loss: 2.3874 - classification_loss: 0.8223
 787/1000 [======================>.......] - ETA: 1:36 - loss: 3.2105 - regression_loss: 2.3885 - classification_loss: 0.8221
 788/1000 [======================>.......] - ETA: 1:35 - loss: 3.2111 - regression_loss: 2.3885 - classification_loss: 0.8226
 789/1000 [======================>.......] - ETA: 1:35 - loss: 3.2122 - regression_loss: 2.3892 - classification_loss: 0.8230
 790/1000 [======================>.......] - ETA: 1:34 - loss: 3.2082 - regression_loss: 2.3862 - classification_loss: 0.8220
 791/1000 [======================>.......] - ETA: 1:34 - loss: 3.2041 - regression_loss: 2.3832 - classification_loss: 0.8209
 792/1000 [======================>.......] - ETA: 1:34 - loss: 3.2065 - regression_loss: 2.3851 - classification_loss: 0.8214
 793/1000 [======================>.......] - ETA: 1:33 - loss: 3.2094 - regression_loss: 2.3868 - classification_loss: 0.8225
 794/1000 [======================>.......] - ETA: 1:33 - loss: 3.2053 - regression_loss: 2.3838 - classification_loss: 0.8215
 795/1000 [======================>.......] - ETA: 1:32 - loss: 3.2066 - regression_loss: 2.3856 - classification_loss: 0.8210
 796/1000 [======================>.......] - ETA: 1:32 - loss: 3.2083 - regression_loss: 2.3866 - classification_loss: 0.8217
 797/1000 [======================>.......] - ETA: 1:31 - loss: 3.2081 - regression_loss: 2.3863 - classification_loss: 0.8218
 798/1000 [======================>.......] - ETA: 1:31 - loss: 3.2098 - regression_loss: 2.3872 - classification_loss: 0.8226
 799/1000 [======================>.......] - ETA: 1:30 - loss: 3.2112 - regression_loss: 2.3878 - classification_loss: 0.8234
 800/1000 [=======================>......] - ETA: 1:30 - loss: 3.2128 - regression_loss: 2.3887 - classification_loss: 0.8241
 801/1000 [=======================>......] - ETA: 1:29 - loss: 3.2087 - regression_loss: 2.3857 - classification_loss: 0.8231
 802/1000 [=======================>......] - ETA: 1:29 - loss: 3.2118 - regression_loss: 2.3876 - classification_loss: 0.8242
 803/1000 [=======================>......] - ETA: 1:29 - loss: 3.2134 - regression_loss: 2.3888 - classification_loss: 0.8246
 804/1000 [=======================>......] - ETA: 1:28 - loss: 3.2146 - regression_loss: 2.3903 - classification_loss: 0.8243
 805/1000 [=======================>......] - ETA: 1:28 - loss: 3.2106 - regression_loss: 2.3874 - classification_loss: 0.8232
 806/1000 [=======================>......] - ETA: 1:27 - loss: 3.2066 - regression_loss: 2.3844 - classification_loss: 0.8222
 807/1000 [=======================>......] - ETA: 1:27 - loss: 3.2082 - regression_loss: 2.3853 - classification_loss: 0.8229
 808/1000 [=======================>......] - ETA: 1:26 - loss: 3.2042 - regression_loss: 2.3823 - classification_loss: 0.8219
 809/1000 [=======================>......] - ETA: 1:26 - loss: 3.2003 - regression_loss: 2.3794 - classification_loss: 0.8210
 810/1000 [=======================>......] - ETA: 1:25 - loss: 3.2029 - regression_loss: 2.3806 - classification_loss: 0.8222
 811/1000 [=======================>......] - ETA: 1:25 - loss: 3.2054 - regression_loss: 2.3822 - classification_loss: 0.8232
 812/1000 [=======================>......] - ETA: 1:25 - loss: 3.2068 - regression_loss: 2.3838 - classification_loss: 0.8230
 813/1000 [=======================>......] - ETA: 1:24 - loss: 3.2104 - regression_loss: 2.3864 - classification_loss: 0.8240
 814/1000 [=======================>......] - ETA: 1:24 - loss: 3.2116 - regression_loss: 2.3872 - classification_loss: 0.8245
 815/1000 [=======================>......] - ETA: 1:23 - loss: 3.2121 - regression_loss: 2.3878 - classification_loss: 0.8243
 816/1000 [=======================>......] - ETA: 1:23 - loss: 3.2131 - regression_loss: 2.3882 - classification_loss: 0.8249
 817/1000 [=======================>......] - ETA: 1:22 - loss: 3.2135 - regression_loss: 2.3876 - classification_loss: 0.8259
 818/1000 [=======================>......] - ETA: 1:22 - loss: 3.2096 - regression_loss: 2.3847 - classification_loss: 0.8249
 819/1000 [=======================>......] - ETA: 1:21 - loss: 3.2109 - regression_loss: 2.3859 - classification_loss: 0.8250
 820/1000 [=======================>......] - ETA: 1:21 - loss: 3.2131 - regression_loss: 2.3883 - classification_loss: 0.8248
 821/1000 [=======================>......] - ETA: 1:20 - loss: 3.2125 - regression_loss: 2.3880 - classification_loss: 0.8245
 822/1000 [=======================>......] - ETA: 1:20 - loss: 3.2151 - regression_loss: 2.3898 - classification_loss: 0.8253
 823/1000 [=======================>......] - ETA: 1:20 - loss: 3.2140 - regression_loss: 2.3892 - classification_loss: 0.8248
 824/1000 [=======================>......] - ETA: 1:19 - loss: 3.2168 - regression_loss: 2.3909 - classification_loss: 0.8259
 825/1000 [=======================>......] - ETA: 1:19 - loss: 3.2185 - regression_loss: 2.3914 - classification_loss: 0.8271
 826/1000 [=======================>......] - ETA: 1:18 - loss: 3.2146 - regression_loss: 2.3885 - classification_loss: 0.8261
 827/1000 [=======================>......] - ETA: 1:18 - loss: 3.2107 - regression_loss: 2.3856 - classification_loss: 0.8251
 828/1000 [=======================>......] - ETA: 1:17 - loss: 3.2068 - regression_loss: 2.3827 - classification_loss: 0.8241
 829/1000 [=======================>......] - ETA: 1:17 - loss: 3.2082 - regression_loss: 2.3831 - classification_loss: 0.8251
 830/1000 [=======================>......] - ETA: 1:16 - loss: 3.2074 - regression_loss: 2.3825 - classification_loss: 0.8249
 831/1000 [=======================>......] - ETA: 1:16 - loss: 3.2080 - regression_loss: 2.3834 - classification_loss: 0.8246
 832/1000 [=======================>......] - ETA: 1:15 - loss: 3.2097 - regression_loss: 2.3842 - classification_loss: 0.8255
 833/1000 [=======================>......] - ETA: 1:15 - loss: 3.2093 - regression_loss: 2.3843 - classification_loss: 0.8250
 834/1000 [========================>.....] - ETA: 1:15 - loss: 3.2110 - regression_loss: 2.3851 - classification_loss: 0.8260
 835/1000 [========================>.....] - ETA: 1:14 - loss: 3.2113 - regression_loss: 2.3857 - classification_loss: 0.8257
 836/1000 [========================>.....] - ETA: 1:14 - loss: 3.2107 - regression_loss: 2.3857 - classification_loss: 0.8250
 837/1000 [========================>.....] - ETA: 1:13 - loss: 3.2100 - regression_loss: 2.3853 - classification_loss: 0.8247
 838/1000 [========================>.....] - ETA: 1:13 - loss: 3.2062 - regression_loss: 2.3824 - classification_loss: 0.8237
 839/1000 [========================>.....] - ETA: 1:12 - loss: 3.2080 - regression_loss: 2.3833 - classification_loss: 0.8246
 840/1000 [========================>.....] - ETA: 1:12 - loss: 3.2098 - regression_loss: 2.3850 - classification_loss: 0.8248
 841/1000 [========================>.....] - ETA: 1:11 - loss: 3.2060 - regression_loss: 2.3822 - classification_loss: 0.8239
 842/1000 [========================>.....] - ETA: 1:11 - loss: 3.2022 - regression_loss: 2.3793 - classification_loss: 0.8229
 843/1000 [========================>.....] - ETA: 1:10 - loss: 3.2062 - regression_loss: 2.3819 - classification_loss: 0.8242
 844/1000 [========================>.....] - ETA: 1:10 - loss: 3.2083 - regression_loss: 2.3830 - classification_loss: 0.8252
 845/1000 [========================>.....] - ETA: 1:10 - loss: 3.2045 - regression_loss: 2.3802 - classification_loss: 0.8242
 846/1000 [========================>.....] - ETA: 1:09 - loss: 3.2007 - regression_loss: 2.3774 - classification_loss: 0.8233
 847/1000 [========================>.....] - ETA: 1:09 - loss: 3.1969 - regression_loss: 2.3746 - classification_loss: 0.8223
 848/1000 [========================>.....] - ETA: 1:08 - loss: 3.1981 - regression_loss: 2.3756 - classification_loss: 0.8224
 849/1000 [========================>.....] - ETA: 1:08 - loss: 3.1992 - regression_loss: 2.3758 - classification_loss: 0.8234
 850/1000 [========================>.....] - ETA: 1:07 - loss: 3.1989 - regression_loss: 2.3753 - classification_loss: 0.8235
 851/1000 [========================>.....] - ETA: 1:07 - loss: 3.2002 - regression_loss: 2.3757 - classification_loss: 0.8245
 852/1000 [========================>.....] - ETA: 1:06 - loss: 3.2007 - regression_loss: 2.3756 - classification_loss: 0.8251
 853/1000 [========================>.....] - ETA: 1:06 - loss: 3.2020 - regression_loss: 2.3767 - classification_loss: 0.8253
 854/1000 [========================>.....] - ETA: 1:06 - loss: 3.2012 - regression_loss: 2.3761 - classification_loss: 0.8251
 855/1000 [========================>.....] - ETA: 1:05 - loss: 3.2033 - regression_loss: 2.3773 - classification_loss: 0.8261
 856/1000 [========================>.....] - ETA: 1:05 - loss: 3.1996 - regression_loss: 2.3745 - classification_loss: 0.8251
 857/1000 [========================>.....] - ETA: 1:04 - loss: 3.2012 - regression_loss: 2.3754 - classification_loss: 0.8258
 858/1000 [========================>.....] - ETA: 1:04 - loss: 3.1975 - regression_loss: 2.3727 - classification_loss: 0.8248
 859/1000 [========================>.....] - ETA: 1:03 - loss: 3.1994 - regression_loss: 2.3735 - classification_loss: 0.8258
 860/1000 [========================>.....] - ETA: 1:03 - loss: 3.2013 - regression_loss: 2.3745 - classification_loss: 0.8268
 861/1000 [========================>.....] - ETA: 1:02 - loss: 3.2008 - regression_loss: 2.3745 - classification_loss: 0.8263
 862/1000 [========================>.....] - ETA: 1:02 - loss: 3.2004 - regression_loss: 2.3740 - classification_loss: 0.8264
 863/1000 [========================>.....] - ETA: 1:01 - loss: 3.2026 - regression_loss: 2.3757 - classification_loss: 0.8269
 864/1000 [========================>.....] - ETA: 1:01 - loss: 3.1989 - regression_loss: 2.3730 - classification_loss: 0.8259
 865/1000 [========================>.....] - ETA: 1:01 - loss: 3.1952 - regression_loss: 2.3702 - classification_loss: 0.8250
 866/1000 [========================>.....] - ETA: 1:00 - loss: 3.1973 - regression_loss: 2.3713 - classification_loss: 0.8260
 867/1000 [=========================>....] - ETA: 1:00 - loss: 3.1977 - regression_loss: 2.3710 - classification_loss: 0.8267
 868/1000 [=========================>....] - ETA: 59s - loss: 3.1988 - regression_loss: 2.3715 - classification_loss: 0.8273 
 869/1000 [=========================>....] - ETA: 59s - loss: 3.1951 - regression_loss: 2.3688 - classification_loss: 0.8264
 870/1000 [=========================>....] - ETA: 58s - loss: 3.1963 - regression_loss: 2.3696 - classification_loss: 0.8267
 871/1000 [=========================>....] - ETA: 58s - loss: 3.1926 - regression_loss: 2.3668 - classification_loss: 0.8257
 872/1000 [=========================>....] - ETA: 57s - loss: 3.1944 - regression_loss: 2.3680 - classification_loss: 0.8264
 873/1000 [=========================>....] - ETA: 57s - loss: 3.1960 - regression_loss: 2.3692 - classification_loss: 0.8269
 874/1000 [=========================>....] - ETA: 56s - loss: 3.1967 - regression_loss: 2.3701 - classification_loss: 0.8267
 875/1000 [=========================>....] - ETA: 56s - loss: 3.1986 - regression_loss: 2.3709 - classification_loss: 0.8277
 876/1000 [=========================>....] - ETA: 56s - loss: 3.2002 - regression_loss: 2.3716 - classification_loss: 0.8286
 877/1000 [=========================>....] - ETA: 55s - loss: 3.1965 - regression_loss: 2.3689 - classification_loss: 0.8277
 878/1000 [=========================>....] - ETA: 55s - loss: 3.1978 - regression_loss: 2.3701 - classification_loss: 0.8277
 879/1000 [=========================>....] - ETA: 54s - loss: 3.1998 - regression_loss: 2.3714 - classification_loss: 0.8285
 880/1000 [=========================>....] - ETA: 54s - loss: 3.2014 - regression_loss: 2.3724 - classification_loss: 0.8291
 881/1000 [=========================>....] - ETA: 53s - loss: 3.2030 - regression_loss: 2.3730 - classification_loss: 0.8300
 882/1000 [=========================>....] - ETA: 53s - loss: 3.2028 - regression_loss: 2.3733 - classification_loss: 0.8296
 883/1000 [=========================>....] - ETA: 52s - loss: 3.2046 - regression_loss: 2.3752 - classification_loss: 0.8294
 884/1000 [=========================>....] - ETA: 52s - loss: 3.2082 - regression_loss: 2.3777 - classification_loss: 0.8305
 885/1000 [=========================>....] - ETA: 52s - loss: 3.2103 - regression_loss: 2.3798 - classification_loss: 0.8305
 886/1000 [=========================>....] - ETA: 51s - loss: 3.2111 - regression_loss: 2.3807 - classification_loss: 0.8304
 887/1000 [=========================>....] - ETA: 51s - loss: 3.2166 - regression_loss: 2.3848 - classification_loss: 0.8318
 888/1000 [=========================>....] - ETA: 50s - loss: 3.2165 - regression_loss: 2.3851 - classification_loss: 0.8314
 889/1000 [=========================>....] - ETA: 50s - loss: 3.2176 - regression_loss: 2.3855 - classification_loss: 0.8321
 890/1000 [=========================>....] - ETA: 49s - loss: 3.2189 - regression_loss: 2.3866 - classification_loss: 0.8323
 891/1000 [=========================>....] - ETA: 49s - loss: 3.2193 - regression_loss: 2.3868 - classification_loss: 0.8325
 892/1000 [=========================>....] - ETA: 48s - loss: 3.2188 - regression_loss: 2.3867 - classification_loss: 0.8320
 893/1000 [=========================>....] - ETA: 48s - loss: 3.2152 - regression_loss: 2.3841 - classification_loss: 0.8311
 894/1000 [=========================>....] - ETA: 47s - loss: 3.2148 - regression_loss: 2.3842 - classification_loss: 0.8306
 895/1000 [=========================>....] - ETA: 47s - loss: 3.2163 - regression_loss: 2.3859 - classification_loss: 0.8304
 896/1000 [=========================>....] - ETA: 47s - loss: 3.2155 - regression_loss: 2.3853 - classification_loss: 0.8301
 897/1000 [=========================>....] - ETA: 46s - loss: 3.2151 - regression_loss: 2.3855 - classification_loss: 0.8296
 898/1000 [=========================>....] - ETA: 46s - loss: 3.2154 - regression_loss: 2.3859 - classification_loss: 0.8294
 899/1000 [=========================>....] - ETA: 45s - loss: 3.2155 - regression_loss: 2.3862 - classification_loss: 0.8293
 900/1000 [==========================>...] - ETA: 45s - loss: 3.2150 - regression_loss: 2.3858 - classification_loss: 0.8292
 901/1000 [==========================>...] - ETA: 44s - loss: 3.2153 - regression_loss: 2.3867 - classification_loss: 0.8286
 902/1000 [==========================>...] - ETA: 44s - loss: 3.2171 - regression_loss: 2.3881 - classification_loss: 0.8291
 903/1000 [==========================>...] - ETA: 43s - loss: 3.2166 - regression_loss: 2.3879 - classification_loss: 0.8286
 904/1000 [==========================>...] - ETA: 43s - loss: 3.2184 - regression_loss: 2.3898 - classification_loss: 0.8286
 905/1000 [==========================>...] - ETA: 42s - loss: 3.2190 - regression_loss: 2.3904 - classification_loss: 0.8286
 906/1000 [==========================>...] - ETA: 42s - loss: 3.2201 - regression_loss: 2.3910 - classification_loss: 0.8291
 907/1000 [==========================>...] - ETA: 42s - loss: 3.2209 - regression_loss: 2.3920 - classification_loss: 0.8290
 908/1000 [==========================>...] - ETA: 41s - loss: 3.2203 - regression_loss: 2.3918 - classification_loss: 0.8285
 909/1000 [==========================>...] - ETA: 41s - loss: 3.2198 - regression_loss: 2.3911 - classification_loss: 0.8287
 910/1000 [==========================>...] - ETA: 40s - loss: 3.2199 - regression_loss: 2.3911 - classification_loss: 0.8288
 911/1000 [==========================>...] - ETA: 40s - loss: 3.2218 - regression_loss: 2.3926 - classification_loss: 0.8292
 912/1000 [==========================>...] - ETA: 39s - loss: 3.2219 - regression_loss: 2.3925 - classification_loss: 0.8294
 913/1000 [==========================>...] - ETA: 39s - loss: 3.2231 - regression_loss: 2.3939 - classification_loss: 0.8292
 914/1000 [==========================>...] - ETA: 38s - loss: 3.2202 - regression_loss: 2.3913 - classification_loss: 0.8289
 915/1000 [==========================>...] - ETA: 38s - loss: 3.2171 - regression_loss: 2.3887 - classification_loss: 0.8284
 916/1000 [==========================>...] - ETA: 37s - loss: 3.2136 - regression_loss: 2.3861 - classification_loss: 0.8275
 917/1000 [==========================>...] - ETA: 37s - loss: 3.2135 - regression_loss: 2.3858 - classification_loss: 0.8277
 918/1000 [==========================>...] - ETA: 37s - loss: 3.2141 - regression_loss: 2.3867 - classification_loss: 0.8274
 919/1000 [==========================>...] - ETA: 36s - loss: 3.2141 - regression_loss: 2.3869 - classification_loss: 0.8272
 920/1000 [==========================>...] - ETA: 36s - loss: 3.2106 - regression_loss: 2.3843 - classification_loss: 0.8263
 921/1000 [==========================>...] - ETA: 35s - loss: 3.2113 - regression_loss: 2.3850 - classification_loss: 0.8262
 922/1000 [==========================>...] - ETA: 35s - loss: 3.2124 - regression_loss: 2.3859 - classification_loss: 0.8265
 923/1000 [==========================>...] - ETA: 34s - loss: 3.2137 - regression_loss: 2.3871 - classification_loss: 0.8265
 924/1000 [==========================>...] - ETA: 34s - loss: 3.2155 - regression_loss: 2.3886 - classification_loss: 0.8268
 925/1000 [==========================>...] - ETA: 33s - loss: 3.2162 - regression_loss: 2.3890 - classification_loss: 0.8272
 926/1000 [==========================>...] - ETA: 33s - loss: 3.2171 - regression_loss: 2.3901 - classification_loss: 0.8270
 927/1000 [==========================>...] - ETA: 33s - loss: 3.2184 - regression_loss: 2.3910 - classification_loss: 0.8274
 928/1000 [==========================>...] - ETA: 32s - loss: 3.2203 - regression_loss: 2.3926 - classification_loss: 0.8277
 929/1000 [==========================>...] - ETA: 32s - loss: 3.2176 - regression_loss: 2.3900 - classification_loss: 0.8276
 930/1000 [==========================>...] - ETA: 31s - loss: 3.2185 - regression_loss: 2.3908 - classification_loss: 0.8278
 931/1000 [==========================>...] - ETA: 31s - loss: 3.2193 - regression_loss: 2.3918 - classification_loss: 0.8276
 932/1000 [==========================>...] - ETA: 30s - loss: 3.2204 - regression_loss: 2.3921 - classification_loss: 0.8283
 933/1000 [==========================>...] - ETA: 30s - loss: 3.2175 - regression_loss: 2.3896 - classification_loss: 0.8279
 934/1000 [===========================>..] - ETA: 29s - loss: 3.2191 - regression_loss: 2.3911 - classification_loss: 0.8280
 935/1000 [===========================>..] - ETA: 29s - loss: 3.2157 - regression_loss: 2.3885 - classification_loss: 0.8272
 936/1000 [===========================>..] - ETA: 28s - loss: 3.2169 - regression_loss: 2.3897 - classification_loss: 0.8272
 937/1000 [===========================>..] - ETA: 28s - loss: 3.2177 - regression_loss: 2.3897 - classification_loss: 0.8280
 938/1000 [===========================>..] - ETA: 28s - loss: 3.2190 - regression_loss: 2.3901 - classification_loss: 0.8289
 939/1000 [===========================>..] - ETA: 27s - loss: 3.2197 - regression_loss: 2.3903 - classification_loss: 0.8294
 940/1000 [===========================>..] - ETA: 27s - loss: 3.2163 - regression_loss: 2.3878 - classification_loss: 0.8285
 941/1000 [===========================>..] - ETA: 26s - loss: 3.2129 - regression_loss: 2.3852 - classification_loss: 0.8276
 942/1000 [===========================>..] - ETA: 26s - loss: 3.2126 - regression_loss: 2.3849 - classification_loss: 0.8277
 943/1000 [===========================>..] - ETA: 25s - loss: 3.2134 - regression_loss: 2.3856 - classification_loss: 0.8278
 944/1000 [===========================>..] - ETA: 25s - loss: 3.2129 - regression_loss: 2.3853 - classification_loss: 0.8276
 945/1000 [===========================>..] - ETA: 24s - loss: 3.2140 - regression_loss: 2.3859 - classification_loss: 0.8281
 946/1000 [===========================>..] - ETA: 24s - loss: 3.2146 - regression_loss: 2.3865 - classification_loss: 0.8280
 947/1000 [===========================>..] - ETA: 23s - loss: 3.2144 - regression_loss: 2.3867 - classification_loss: 0.8277
 948/1000 [===========================>..] - ETA: 23s - loss: 3.2147 - regression_loss: 2.3869 - classification_loss: 0.8278
 949/1000 [===========================>..] - ETA: 23s - loss: 3.2155 - regression_loss: 2.3877 - classification_loss: 0.8278
 950/1000 [===========================>..] - ETA: 22s - loss: 3.2167 - regression_loss: 2.3882 - classification_loss: 0.8286
 951/1000 [===========================>..] - ETA: 22s - loss: 3.2187 - regression_loss: 2.3896 - classification_loss: 0.8291
 952/1000 [===========================>..] - ETA: 21s - loss: 3.2183 - regression_loss: 2.3892 - classification_loss: 0.8291
 953/1000 [===========================>..] - ETA: 21s - loss: 3.2193 - regression_loss: 2.3900 - classification_loss: 0.8293
 954/1000 [===========================>..] - ETA: 20s - loss: 3.2160 - regression_loss: 2.3875 - classification_loss: 0.8285
 955/1000 [===========================>..] - ETA: 20s - loss: 3.2182 - regression_loss: 2.3887 - classification_loss: 0.8295
 956/1000 [===========================>..] - ETA: 19s - loss: 3.2149 - regression_loss: 2.3862 - classification_loss: 0.8286
 957/1000 [===========================>..] - ETA: 19s - loss: 3.2155 - regression_loss: 2.3868 - classification_loss: 0.8287
 958/1000 [===========================>..] - ETA: 18s - loss: 3.2122 - regression_loss: 2.3843 - classification_loss: 0.8279
 959/1000 [===========================>..] - ETA: 18s - loss: 3.2134 - regression_loss: 2.3853 - classification_loss: 0.8281
 960/1000 [===========================>..] - ETA: 18s - loss: 3.2161 - regression_loss: 2.3874 - classification_loss: 0.8287
 961/1000 [===========================>..] - ETA: 17s - loss: 3.2163 - regression_loss: 2.3876 - classification_loss: 0.8287
 962/1000 [===========================>..] - ETA: 17s - loss: 3.2130 - regression_loss: 2.3851 - classification_loss: 0.8278
 963/1000 [===========================>..] - ETA: 16s - loss: 3.2151 - regression_loss: 2.3872 - classification_loss: 0.8279
 964/1000 [===========================>..] - ETA: 16s - loss: 3.2178 - regression_loss: 2.3900 - classification_loss: 0.8278
 965/1000 [===========================>..] - ETA: 15s - loss: 3.2145 - regression_loss: 2.3875 - classification_loss: 0.8270
 966/1000 [===========================>..] - ETA: 15s - loss: 3.2173 - regression_loss: 2.3892 - classification_loss: 0.8281
 967/1000 [============================>.] - ETA: 14s - loss: 3.2189 - regression_loss: 2.3905 - classification_loss: 0.8284
 968/1000 [============================>.] - ETA: 14s - loss: 3.2185 - regression_loss: 2.3900 - classification_loss: 0.8285
 969/1000 [============================>.] - ETA: 14s - loss: 3.2190 - regression_loss: 2.3901 - classification_loss: 0.8290
 970/1000 [============================>.] - ETA: 13s - loss: 3.2157 - regression_loss: 2.3876 - classification_loss: 0.8281
 971/1000 [============================>.] - ETA: 13s - loss: 3.2170 - regression_loss: 2.3889 - classification_loss: 0.8281
 972/1000 [============================>.] - ETA: 12s - loss: 3.2188 - regression_loss: 2.3896 - classification_loss: 0.8292
 973/1000 [============================>.] - ETA: 12s - loss: 3.2155 - regression_loss: 2.3871 - classification_loss: 0.8284
 974/1000 [============================>.] - ETA: 11s - loss: 3.2122 - regression_loss: 2.3847 - classification_loss: 0.8275
 975/1000 [============================>.] - ETA: 11s - loss: 3.2142 - regression_loss: 2.3863 - classification_loss: 0.8279
 976/1000 [============================>.] - ETA: 10s - loss: 3.2109 - regression_loss: 2.3839 - classification_loss: 0.8271
 977/1000 [============================>.] - ETA: 10s - loss: 3.2116 - regression_loss: 2.3838 - classification_loss: 0.8278
 978/1000 [============================>.] - ETA: 9s - loss: 3.2083 - regression_loss: 2.3814 - classification_loss: 0.8269 
 979/1000 [============================>.] - ETA: 9s - loss: 3.2081 - regression_loss: 2.3812 - classification_loss: 0.8269
 980/1000 [============================>.] - ETA: 9s - loss: 3.2048 - regression_loss: 2.3787 - classification_loss: 0.8260
 981/1000 [============================>.] - ETA: 8s - loss: 3.2056 - regression_loss: 2.3797 - classification_loss: 0.8260
 982/1000 [============================>.] - ETA: 8s - loss: 3.2062 - regression_loss: 2.3801 - classification_loss: 0.8261
 983/1000 [============================>.] - ETA: 7s - loss: 3.2029 - regression_loss: 2.3776 - classification_loss: 0.8253
 984/1000 [============================>.] - ETA: 7s - loss: 3.2027 - regression_loss: 2.3770 - classification_loss: 0.8256
 985/1000 [============================>.] - ETA: 6s - loss: 3.2034 - regression_loss: 2.3774 - classification_loss: 0.8260
 986/1000 [============================>.] - ETA: 6s - loss: 3.2002 - regression_loss: 2.3750 - classification_loss: 0.8252
 987/1000 [============================>.] - ETA: 5s - loss: 3.1969 - regression_loss: 2.3726 - classification_loss: 0.8243
 988/1000 [============================>.] - ETA: 5s - loss: 3.1990 - regression_loss: 2.3739 - classification_loss: 0.8250
 989/1000 [============================>.] - ETA: 4s - loss: 3.1981 - regression_loss: 2.3733 - classification_loss: 0.8249
 990/1000 [============================>.] - ETA: 4s - loss: 3.2003 - regression_loss: 2.3745 - classification_loss: 0.8258
 991/1000 [============================>.] - ETA: 4s - loss: 3.2003 - regression_loss: 2.3747 - classification_loss: 0.8257
 992/1000 [============================>.] - ETA: 3s - loss: 3.2012 - regression_loss: 2.3754 - classification_loss: 0.8258
 993/1000 [============================>.] - ETA: 3s - loss: 3.2033 - regression_loss: 2.3775 - classification_loss: 0.8258
 994/1000 [============================>.] - ETA: 2s - loss: 3.2043 - regression_loss: 2.3777 - classification_loss: 0.8267
 995/1000 [============================>.] - ETA: 2s - loss: 3.2053 - regression_loss: 2.3780 - classification_loss: 0.8273
 996/1000 [============================>.] - ETA: 1s - loss: 3.2066 - regression_loss: 2.3788 - classification_loss: 0.8278
 997/1000 [============================>.] - ETA: 1s - loss: 3.2070 - regression_loss: 2.3793 - classification_loss: 0.8277
 998/1000 [============================>.] - ETA: 0s - loss: 3.2084 - regression_loss: 2.3802 - classification_loss: 0.8282
 999/1000 [============================>.] - ETA: 0s - loss: 3.2097 - regression_loss: 2.3818 - classification_loss: 0.8279
1000/1000 [==============================] - 452s 452ms/step - loss: 3.2065 - regression_loss: 2.3794 - classification_loss: 0.8271

Epoch 00007: saving model to ./snapshots/resnet50_csv_07.h5
0/2001/2002/2003/2004/2005/2006/2007/2008/2009/20010/20011/20012/20013/20014/20015/20016/20017/20018/20019/20020/20021/20022/20023/20024/20025/20026/20027/20028/20029/20030/20031/20032/20033/20034/20035/20036/20037/20038/20039/20040/20041/20042/20043/20044/20045/20046/20047/20048/20049/20050/20051/20052/20053/20054/20055/20056/20057/20058/20059/20060/20061/20062/20063/20064/20065/20066/20067/20068/20069/20070/20071/20072/20073/20074/20075/20076/20077/20078/20079/20080/20081/20082/20083/20084/20085/20086/20087/20088/20089/20090/20091/20092/20093/20094/20095/20096/20097/20098/20099/200100/200101/200102/200103/200104/200105/200106/200107/200108/200109/200110/200111/200112/200113/200114/200115/200116/200117/200118/200119/200120/200121/200122/200123/200124/200125/200126/200127/200128/200129/200130/200131/200132/200133/200134/200135/200136/200137/200138/200139/200140/200141/200142/200143/200144/200145/200146/200147/200148/200149/200150/200151/200152/200153/200154/200155/200156/200157/200158/200159/200160/200161/200162/200163/200164/200165/200166/200167/200168/200169/200170/200171/200172/200173/200174/200175/200176/200177/200178/200179/200180/200181/200182/200183/200184/200185/200186/200187/200188/200189/200190/200191/200192/200193/200194/200195/200196/200197/200198/200199/2000/2001/2002/2003/2004/2005/2006/2007/2008/2009/20010/20011/20012/20013/20014/20015/20016/20017/20018/20019/20020/20021/20022/20023/20024/20025/20026/20027/20028/20029/20030/20031/20032/20033/20034/20035/20036/20037/20038/20039/20040/20041/20042/20043/20044/20045/20046/20047/20048/20049/20050/20051/20052/20053/20054/20055/20056/20057/20058/20059/20060/20061/20062/20063/20064/20065/20066/20067/20068/20069/20070/20071/20072/20073/20074/20075/20076/20077/20078/20079/20080/20081/20082/20083/20084/20085/20086/20087/20088/20089/20090/20091/20092/20093/20094/20095/20096/20097/20098/20099/200100/200101/200102/200103/200104/200105/200106/200107/200108/200109/200110/200111/200112/200113/200114/200115/200116/200117/200118/200119/200120/200121/200122/200123/200124/200125/200126/200127/200128/200129/200130/200131/200132/200133/200134/200135/200136/200137/200138/200139/200140/200141/200142/200143/200144/200145/200146/200147/200148/200149/200150/200151/200152/200153/200154/200155/200156/200157/200158/200159/200160/200161/200162/200163/200164/200165/200166/200167/200168/200169/200170/200171/200172/200173/200174/200175/200176/200177/200178/200179/200180/200181/200182/200183/200184/200185/200186/200187/200188/200189/200190/200191/200192/200193/200194/200195/200196/200197/200198/200199/200

M 0.1001
N 0.0000
mAP: 0.0500
Epoch 8/30

   1/1000 [..............................] - ETA: 7:14 - loss: 4.1204 - regression_loss: 2.4489 - classification_loss: 1.6715
   2/1000 [..............................] - ETA: 7:19 - loss: 3.6255 - regression_loss: 2.4519 - classification_loss: 1.1737
   3/1000 [..............................] - ETA: 7:22 - loss: 3.7764 - regression_loss: 2.7116 - classification_loss: 1.0648
   4/1000 [..............................] - ETA: 7:24 - loss: 3.5509 - regression_loss: 2.6610 - classification_loss: 0.8899
   5/1000 [..............................] - ETA: 7:25 - loss: 3.3602 - regression_loss: 2.5364 - classification_loss: 0.8238
   6/1000 [..............................] - ETA: 7:19 - loss: 3.6992 - regression_loss: 2.7778 - classification_loss: 0.9213
   7/1000 [..............................] - ETA: 7:19 - loss: 3.1715 - regression_loss: 2.3810 - classification_loss: 0.7905
   8/1000 [..............................] - ETA: 7:20 - loss: 2.7751 - regression_loss: 2.0834 - classification_loss: 0.6917
   9/1000 [..............................] - ETA: 7:21 - loss: 2.7997 - regression_loss: 2.1193 - classification_loss: 0.6803
  10/1000 [..............................] - ETA: 7:20 - loss: 2.8968 - regression_loss: 2.2244 - classification_loss: 0.6724
  11/1000 [..............................] - ETA: 7:20 - loss: 2.9283 - regression_loss: 2.2676 - classification_loss: 0.6607
  12/1000 [..............................] - ETA: 7:20 - loss: 3.0775 - regression_loss: 2.3850 - classification_loss: 0.6926
  13/1000 [..............................] - ETA: 7:20 - loss: 3.2559 - regression_loss: 2.5441 - classification_loss: 0.7118
  14/1000 [..............................] - ETA: 7:20 - loss: 3.0234 - regression_loss: 2.3624 - classification_loss: 0.6610
  15/1000 [..............................] - ETA: 7:17 - loss: 3.0852 - regression_loss: 2.3984 - classification_loss: 0.6868
  16/1000 [..............................] - ETA: 7:17 - loss: 3.0648 - regression_loss: 2.4033 - classification_loss: 0.6615
  17/1000 [..............................] - ETA: 7:17 - loss: 3.0679 - regression_loss: 2.4000 - classification_loss: 0.6679
  18/1000 [..............................] - ETA: 7:17 - loss: 3.2203 - regression_loss: 2.4959 - classification_loss: 0.7244
  19/1000 [..............................] - ETA: 7:18 - loss: 3.1952 - regression_loss: 2.4677 - classification_loss: 0.7275
  20/1000 [..............................] - ETA: 7:17 - loss: 3.2075 - regression_loss: 2.4807 - classification_loss: 0.7268
  21/1000 [..............................] - ETA: 7:17 - loss: 3.2249 - regression_loss: 2.4923 - classification_loss: 0.7326
  22/1000 [..............................] - ETA: 7:17 - loss: 3.2963 - regression_loss: 2.5510 - classification_loss: 0.7452
  23/1000 [..............................] - ETA: 7:17 - loss: 3.1530 - regression_loss: 2.4401 - classification_loss: 0.7128
  24/1000 [..............................] - ETA: 7:16 - loss: 3.2184 - regression_loss: 2.4871 - classification_loss: 0.7313
  25/1000 [..............................] - ETA: 7:16 - loss: 3.2307 - regression_loss: 2.4915 - classification_loss: 0.7392
  26/1000 [..............................] - ETA: 7:16 - loss: 3.1066 - regression_loss: 2.3957 - classification_loss: 0.7110
  27/1000 [..............................] - ETA: 7:16 - loss: 2.9916 - regression_loss: 2.3069 - classification_loss: 0.6846
  28/1000 [..............................] - ETA: 7:16 - loss: 3.0503 - regression_loss: 2.3564 - classification_loss: 0.6939
  29/1000 [..............................] - ETA: 7:15 - loss: 2.9482 - regression_loss: 2.2752 - classification_loss: 0.6731
  30/1000 [..............................] - ETA: 7:15 - loss: 2.9691 - regression_loss: 2.2745 - classification_loss: 0.6946
  31/1000 [..............................] - ETA: 7:15 - loss: 2.8734 - regression_loss: 2.2012 - classification_loss: 0.6722
  32/1000 [..............................] - ETA: 7:15 - loss: 2.8942 - regression_loss: 2.2263 - classification_loss: 0.6679
  33/1000 [..............................] - ETA: 7:14 - loss: 2.8942 - regression_loss: 2.2207 - classification_loss: 0.6735
  34/1000 [>.............................] - ETA: 7:14 - loss: 2.9459 - regression_loss: 2.2516 - classification_loss: 0.6943
  35/1000 [>.............................] - ETA: 7:13 - loss: 2.9586 - regression_loss: 2.2661 - classification_loss: 0.6925
  36/1000 [>.............................] - ETA: 7:13 - loss: 3.0022 - regression_loss: 2.2815 - classification_loss: 0.7207
  37/1000 [>.............................] - ETA: 7:13 - loss: 2.9211 - regression_loss: 2.2199 - classification_loss: 0.7012
  38/1000 [>.............................] - ETA: 7:12 - loss: 2.9621 - regression_loss: 2.2478 - classification_loss: 0.7143
  39/1000 [>.............................] - ETA: 7:12 - loss: 2.9776 - regression_loss: 2.2649 - classification_loss: 0.7127
  40/1000 [>.............................] - ETA: 7:12 - loss: 2.9032 - regression_loss: 2.2083 - classification_loss: 0.6949
  41/1000 [>.............................] - ETA: 7:11 - loss: 2.9031 - regression_loss: 2.2043 - classification_loss: 0.6988
  42/1000 [>.............................] - ETA: 7:11 - loss: 2.9221 - regression_loss: 2.2174 - classification_loss: 0.7047
  43/1000 [>.............................] - ETA: 7:10 - loss: 2.9783 - regression_loss: 2.2684 - classification_loss: 0.7099
  44/1000 [>.............................] - ETA: 7:10 - loss: 2.9996 - regression_loss: 2.2712 - classification_loss: 0.7284
  45/1000 [>.............................] - ETA: 7:10 - loss: 3.0380 - regression_loss: 2.2983 - classification_loss: 0.7396
  46/1000 [>.............................] - ETA: 7:09 - loss: 3.0873 - regression_loss: 2.3299 - classification_loss: 0.7574
  47/1000 [>.............................] - ETA: 7:09 - loss: 3.1236 - regression_loss: 2.3566 - classification_loss: 0.7670
  48/1000 [>.............................] - ETA: 7:08 - loss: 3.1536 - regression_loss: 2.3686 - classification_loss: 0.7850
  49/1000 [>.............................] - ETA: 7:08 - loss: 3.1385 - regression_loss: 2.3613 - classification_loss: 0.7771
  50/1000 [>.............................] - ETA: 7:07 - loss: 3.1440 - regression_loss: 2.3547 - classification_loss: 0.7894
  51/1000 [>.............................] - ETA: 7:07 - loss: 3.1690 - regression_loss: 2.3801 - classification_loss: 0.7890
  52/1000 [>.............................] - ETA: 7:07 - loss: 3.1081 - regression_loss: 2.3343 - classification_loss: 0.7738
  53/1000 [>.............................] - ETA: 7:06 - loss: 3.1062 - regression_loss: 2.3319 - classification_loss: 0.7743
  54/1000 [>.............................] - ETA: 7:06 - loss: 3.1117 - regression_loss: 2.3416 - classification_loss: 0.7700
  55/1000 [>.............................] - ETA: 7:05 - loss: 3.1306 - regression_loss: 2.3669 - classification_loss: 0.7637
  56/1000 [>.............................] - ETA: 7:05 - loss: 3.1231 - regression_loss: 2.3628 - classification_loss: 0.7603
  57/1000 [>.............................] - ETA: 7:04 - loss: 3.1531 - regression_loss: 2.3795 - classification_loss: 0.7737
  58/1000 [>.............................] - ETA: 7:04 - loss: 3.0988 - regression_loss: 2.3384 - classification_loss: 0.7604
  59/1000 [>.............................] - ETA: 7:04 - loss: 3.0937 - regression_loss: 2.3400 - classification_loss: 0.7537
  60/1000 [>.............................] - ETA: 7:03 - loss: 3.0854 - regression_loss: 2.3329 - classification_loss: 0.7525
  61/1000 [>.............................] - ETA: 7:03 - loss: 3.0842 - regression_loss: 2.3344 - classification_loss: 0.7497
  62/1000 [>.............................] - ETA: 7:02 - loss: 3.0838 - regression_loss: 2.3397 - classification_loss: 0.7441
  63/1000 [>.............................] - ETA: 7:02 - loss: 3.0834 - regression_loss: 2.3367 - classification_loss: 0.7468
  64/1000 [>.............................] - ETA: 7:01 - loss: 3.0352 - regression_loss: 2.3001 - classification_loss: 0.7351
  65/1000 [>.............................] - ETA: 7:01 - loss: 3.0584 - regression_loss: 2.3233 - classification_loss: 0.7351
  66/1000 [>.............................] - ETA: 7:01 - loss: 3.0581 - regression_loss: 2.3246 - classification_loss: 0.7334
  67/1000 [=>............................] - ETA: 7:00 - loss: 3.0443 - regression_loss: 2.3158 - classification_loss: 0.7284
  68/1000 [=>............................] - ETA: 7:00 - loss: 3.1045 - regression_loss: 2.3356 - classification_loss: 0.7690
  69/1000 [=>............................] - ETA: 6:59 - loss: 3.0943 - regression_loss: 2.3323 - classification_loss: 0.7620
  70/1000 [=>............................] - ETA: 6:59 - loss: 3.0965 - regression_loss: 2.3354 - classification_loss: 0.7611
  71/1000 [=>............................] - ETA: 6:59 - loss: 3.0548 - regression_loss: 2.3025 - classification_loss: 0.7523
  72/1000 [=>............................] - ETA: 6:58 - loss: 3.0799 - regression_loss: 2.3210 - classification_loss: 0.7589
  73/1000 [=>............................] - ETA: 6:58 - loss: 3.0926 - regression_loss: 2.3358 - classification_loss: 0.7569
  74/1000 [=>............................] - ETA: 6:57 - loss: 3.0509 - regression_loss: 2.3042 - classification_loss: 0.7467
  75/1000 [=>............................] - ETA: 6:57 - loss: 3.0645 - regression_loss: 2.3079 - classification_loss: 0.7566
  76/1000 [=>............................] - ETA: 6:56 - loss: 3.0924 - regression_loss: 2.3315 - classification_loss: 0.7610
  77/1000 [=>............................] - ETA: 6:56 - loss: 3.0922 - regression_loss: 2.3328 - classification_loss: 0.7594
  78/1000 [=>............................] - ETA: 6:55 - loss: 3.1107 - regression_loss: 2.3406 - classification_loss: 0.7701
  79/1000 [=>............................] - ETA: 6:55 - loss: 3.1308 - regression_loss: 2.3480 - classification_loss: 0.7829
  80/1000 [=>............................] - ETA: 6:55 - loss: 3.1551 - regression_loss: 2.3663 - classification_loss: 0.7888
  81/1000 [=>............................] - ETA: 6:54 - loss: 3.1161 - regression_loss: 2.3370 - classification_loss: 0.7791
  82/1000 [=>............................] - ETA: 6:54 - loss: 3.0807 - regression_loss: 2.3085 - classification_loss: 0.7722
  83/1000 [=>............................] - ETA: 6:53 - loss: 3.0438 - regression_loss: 2.2807 - classification_loss: 0.7631
  84/1000 [=>............................] - ETA: 6:53 - loss: 3.0498 - regression_loss: 2.2819 - classification_loss: 0.7678
  85/1000 [=>............................] - ETA: 6:52 - loss: 3.0497 - regression_loss: 2.2858 - classification_loss: 0.7640
  86/1000 [=>............................] - ETA: 6:52 - loss: 3.0624 - regression_loss: 2.2917 - classification_loss: 0.7708
  87/1000 [=>............................] - ETA: 6:51 - loss: 3.0683 - regression_loss: 2.2964 - classification_loss: 0.7719
  88/1000 [=>............................] - ETA: 6:51 - loss: 3.0334 - regression_loss: 2.2703 - classification_loss: 0.7632
  89/1000 [=>............................] - ETA: 6:50 - loss: 3.0352 - regression_loss: 2.2744 - classification_loss: 0.7609
  90/1000 [=>............................] - ETA: 6:50 - loss: 3.0526 - regression_loss: 2.2902 - classification_loss: 0.7624
  91/1000 [=>............................] - ETA: 6:50 - loss: 3.0668 - regression_loss: 2.2992 - classification_loss: 0.7676
  92/1000 [=>............................] - ETA: 6:49 - loss: 3.0900 - regression_loss: 2.3147 - classification_loss: 0.7753
  93/1000 [=>............................] - ETA: 6:49 - loss: 3.0986 - regression_loss: 2.3217 - classification_loss: 0.7769
  94/1000 [=>............................] - ETA: 6:48 - loss: 3.1095 - regression_loss: 2.3343 - classification_loss: 0.7752
  95/1000 [=>............................] - ETA: 6:48 - loss: 3.1067 - regression_loss: 2.3341 - classification_loss: 0.7726
  96/1000 [=>............................] - ETA: 6:47 - loss: 3.0744 - regression_loss: 2.3098 - classification_loss: 0.7646
  97/1000 [=>............................] - ETA: 6:47 - loss: 3.0695 - regression_loss: 2.3052 - classification_loss: 0.7643
  98/1000 [=>............................] - ETA: 6:46 - loss: 3.0655 - regression_loss: 2.3036 - classification_loss: 0.7619
  99/1000 [=>............................] - ETA: 6:46 - loss: 3.0766 - regression_loss: 2.3084 - classification_loss: 0.7682
 100/1000 [==>...........................] - ETA: 6:45 - loss: 3.0849 - regression_loss: 2.3173 - classification_loss: 0.7676
 101/1000 [==>...........................] - ETA: 6:45 - loss: 3.0835 - regression_loss: 2.3134 - classification_loss: 0.7701
 102/1000 [==>...........................] - ETA: 6:45 - loss: 3.0533 - regression_loss: 2.2907 - classification_loss: 0.7626
 103/1000 [==>...........................] - ETA: 6:44 - loss: 3.0548 - regression_loss: 2.2957 - classification_loss: 0.7592
 104/1000 [==>...........................] - ETA: 6:44 - loss: 3.0255 - regression_loss: 2.2736 - classification_loss: 0.7519
 105/1000 [==>...........................] - ETA: 6:43 - loss: 3.0315 - regression_loss: 2.2803 - classification_loss: 0.7511
 106/1000 [==>...........................] - ETA: 6:43 - loss: 3.0352 - regression_loss: 2.2799 - classification_loss: 0.7553
 107/1000 [==>...........................] - ETA: 6:42 - loss: 3.0422 - regression_loss: 2.2881 - classification_loss: 0.7542
 108/1000 [==>...........................] - ETA: 6:42 - loss: 3.0420 - regression_loss: 2.2857 - classification_loss: 0.7564
 109/1000 [==>...........................] - ETA: 6:41 - loss: 3.0408 - regression_loss: 2.2839 - classification_loss: 0.7568
 110/1000 [==>...........................] - ETA: 6:41 - loss: 3.0408 - regression_loss: 2.2883 - classification_loss: 0.7526
 111/1000 [==>...........................] - ETA: 6:41 - loss: 3.0418 - regression_loss: 2.2894 - classification_loss: 0.7525
 112/1000 [==>...........................] - ETA: 6:40 - loss: 3.0687 - regression_loss: 2.3168 - classification_loss: 0.7519
 113/1000 [==>...........................] - ETA: 6:40 - loss: 3.0649 - regression_loss: 2.3145 - classification_loss: 0.7505
 114/1000 [==>...........................] - ETA: 6:39 - loss: 3.0669 - regression_loss: 2.3167 - classification_loss: 0.7502
 115/1000 [==>...........................] - ETA: 6:39 - loss: 3.0809 - regression_loss: 2.3306 - classification_loss: 0.7503
 116/1000 [==>...........................] - ETA: 6:39 - loss: 3.0882 - regression_loss: 2.3410 - classification_loss: 0.7472
 117/1000 [==>...........................] - ETA: 6:38 - loss: 3.1009 - regression_loss: 2.3519 - classification_loss: 0.7490
 118/1000 [==>...........................] - ETA: 6:38 - loss: 3.1034 - regression_loss: 2.3541 - classification_loss: 0.7493
 119/1000 [==>...........................] - ETA: 6:37 - loss: 3.0809 - regression_loss: 2.3343 - classification_loss: 0.7465
 120/1000 [==>...........................] - ETA: 6:37 - loss: 3.0940 - regression_loss: 2.3410 - classification_loss: 0.7530
 121/1000 [==>...........................] - ETA: 6:36 - loss: 3.0996 - regression_loss: 2.3478 - classification_loss: 0.7518
 122/1000 [==>...........................] - ETA: 6:36 - loss: 3.0742 - regression_loss: 2.3285 - classification_loss: 0.7457
 123/1000 [==>...........................] - ETA: 6:35 - loss: 3.0674 - regression_loss: 2.3255 - classification_loss: 0.7420
 124/1000 [==>...........................] - ETA: 6:35 - loss: 3.0786 - regression_loss: 2.3392 - classification_loss: 0.7394
 125/1000 [==>...........................] - ETA: 6:35 - loss: 3.0864 - regression_loss: 2.3487 - classification_loss: 0.7377
 126/1000 [==>...........................] - ETA: 6:34 - loss: 3.0866 - regression_loss: 2.3467 - classification_loss: 0.7399
 127/1000 [==>...........................] - ETA: 6:34 - loss: 3.1037 - regression_loss: 2.3631 - classification_loss: 0.7406
 128/1000 [==>...........................] - ETA: 6:33 - loss: 3.1177 - regression_loss: 2.3734 - classification_loss: 0.7443
 129/1000 [==>...........................] - ETA: 6:33 - loss: 3.1266 - regression_loss: 2.3736 - classification_loss: 0.7530
 130/1000 [==>...........................] - ETA: 6:32 - loss: 3.1479 - regression_loss: 2.3826 - classification_loss: 0.7653
 131/1000 [==>...........................] - ETA: 6:32 - loss: 3.1669 - regression_loss: 2.3950 - classification_loss: 0.7719
 132/1000 [==>...........................] - ETA: 6:32 - loss: 3.1675 - regression_loss: 2.3946 - classification_loss: 0.7729
 133/1000 [==>...........................] - ETA: 6:31 - loss: 3.1663 - regression_loss: 2.3969 - classification_loss: 0.7694
 134/1000 [===>..........................] - ETA: 6:31 - loss: 3.1774 - regression_loss: 2.4038 - classification_loss: 0.7736
 135/1000 [===>..........................] - ETA: 6:30 - loss: 3.1538 - regression_loss: 2.3860 - classification_loss: 0.7679
 136/1000 [===>..........................] - ETA: 6:30 - loss: 3.1306 - regression_loss: 2.3684 - classification_loss: 0.7622
 137/1000 [===>..........................] - ETA: 6:29 - loss: 3.1078 - regression_loss: 2.3512 - classification_loss: 0.7566
 138/1000 [===>..........................] - ETA: 6:29 - loss: 3.1158 - regression_loss: 2.3579 - classification_loss: 0.7579
 139/1000 [===>..........................] - ETA: 6:29 - loss: 3.0934 - regression_loss: 2.3409 - classification_loss: 0.7525
 140/1000 [===>..........................] - ETA: 6:28 - loss: 3.0993 - regression_loss: 2.3469 - classification_loss: 0.7524
 141/1000 [===>..........................] - ETA: 6:28 - loss: 3.1035 - regression_loss: 2.3526 - classification_loss: 0.7509
 142/1000 [===>..........................] - ETA: 6:27 - loss: 3.1032 - regression_loss: 2.3536 - classification_loss: 0.7496
 143/1000 [===>..........................] - ETA: 6:27 - loss: 3.1109 - regression_loss: 2.3578 - classification_loss: 0.7531
 144/1000 [===>..........................] - ETA: 6:26 - loss: 3.1245 - regression_loss: 2.3673 - classification_loss: 0.7573
 145/1000 [===>..........................] - ETA: 6:26 - loss: 3.1304 - regression_loss: 2.3699 - classification_loss: 0.7605
 146/1000 [===>..........................] - ETA: 6:25 - loss: 3.1332 - regression_loss: 2.3745 - classification_loss: 0.7588
 147/1000 [===>..........................] - ETA: 6:25 - loss: 3.1380 - regression_loss: 2.3811 - classification_loss: 0.7569
 148/1000 [===>..........................] - ETA: 6:25 - loss: 3.1411 - regression_loss: 2.3848 - classification_loss: 0.7564
 149/1000 [===>..........................] - ETA: 6:24 - loss: 3.1205 - regression_loss: 2.3688 - classification_loss: 0.7518
 150/1000 [===>..........................] - ETA: 6:24 - loss: 3.1201 - regression_loss: 2.3706 - classification_loss: 0.7495
 151/1000 [===>..........................] - ETA: 6:23 - loss: 3.1313 - regression_loss: 2.3756 - classification_loss: 0.7557
 152/1000 [===>..........................] - ETA: 6:23 - loss: 3.1108 - regression_loss: 2.3600 - classification_loss: 0.7508
 153/1000 [===>..........................] - ETA: 6:22 - loss: 3.0905 - regression_loss: 2.3446 - classification_loss: 0.7459
 154/1000 [===>..........................] - ETA: 6:22 - loss: 3.0941 - regression_loss: 2.3472 - classification_loss: 0.7469
 155/1000 [===>..........................] - ETA: 6:21 - loss: 3.0920 - regression_loss: 2.3479 - classification_loss: 0.7441
 156/1000 [===>..........................] - ETA: 6:21 - loss: 3.1033 - regression_loss: 2.3532 - classification_loss: 0.7501
 157/1000 [===>..........................] - ETA: 6:21 - loss: 3.0836 - regression_loss: 2.3382 - classification_loss: 0.7453
 158/1000 [===>..........................] - ETA: 6:20 - loss: 3.0640 - regression_loss: 2.3234 - classification_loss: 0.7406
 159/1000 [===>..........................] - ETA: 6:20 - loss: 3.0749 - regression_loss: 2.3286 - classification_loss: 0.7463
 160/1000 [===>..........................] - ETA: 6:19 - loss: 3.0861 - regression_loss: 2.3399 - classification_loss: 0.7462
 161/1000 [===>..........................] - ETA: 6:19 - loss: 3.0897 - regression_loss: 2.3445 - classification_loss: 0.7452
 162/1000 [===>..........................] - ETA: 6:18 - loss: 3.0913 - regression_loss: 2.3450 - classification_loss: 0.7462
 163/1000 [===>..........................] - ETA: 6:18 - loss: 3.1071 - regression_loss: 2.3533 - classification_loss: 0.7539
 164/1000 [===>..........................] - ETA: 6:17 - loss: 3.1170 - regression_loss: 2.3557 - classification_loss: 0.7613
 165/1000 [===>..........................] - ETA: 6:17 - loss: 3.1152 - regression_loss: 2.3552 - classification_loss: 0.7600
 166/1000 [===>..........................] - ETA: 6:16 - loss: 3.0964 - regression_loss: 2.3410 - classification_loss: 0.7554
 167/1000 [====>.........................] - ETA: 6:16 - loss: 3.0779 - regression_loss: 2.3270 - classification_loss: 0.7509
 168/1000 [====>.........................] - ETA: 6:15 - loss: 3.0596 - regression_loss: 2.3131 - classification_loss: 0.7464
 169/1000 [====>.........................] - ETA: 6:15 - loss: 3.0611 - regression_loss: 2.3160 - classification_loss: 0.7451
 170/1000 [====>.........................] - ETA: 6:14 - loss: 3.0550 - regression_loss: 2.3123 - classification_loss: 0.7427
 171/1000 [====>.........................] - ETA: 6:14 - loss: 3.0371 - regression_loss: 2.2988 - classification_loss: 0.7384
 172/1000 [====>.........................] - ETA: 6:13 - loss: 3.0372 - regression_loss: 2.2990 - classification_loss: 0.7382
 173/1000 [====>.........................] - ETA: 6:13 - loss: 3.0365 - regression_loss: 2.3005 - classification_loss: 0.7361
 174/1000 [====>.........................] - ETA: 6:13 - loss: 3.0282 - regression_loss: 2.2946 - classification_loss: 0.7336
 175/1000 [====>.........................] - ETA: 6:12 - loss: 3.0321 - regression_loss: 2.2986 - classification_loss: 0.7335
 176/1000 [====>.........................] - ETA: 6:12 - loss: 3.0269 - regression_loss: 2.2956 - classification_loss: 0.7313
 177/1000 [====>.........................] - ETA: 6:11 - loss: 3.0178 - regression_loss: 2.2826 - classification_loss: 0.7352
 178/1000 [====>.........................] - ETA: 6:11 - loss: 3.0321 - regression_loss: 2.2898 - classification_loss: 0.7424
 179/1000 [====>.........................] - ETA: 6:10 - loss: 3.0437 - regression_loss: 2.2961 - classification_loss: 0.7476
 180/1000 [====>.........................] - ETA: 6:10 - loss: 3.0528 - regression_loss: 2.3033 - classification_loss: 0.7495
 181/1000 [====>.........................] - ETA: 6:10 - loss: 3.0571 - regression_loss: 2.3068 - classification_loss: 0.7502
 182/1000 [====>.........................] - ETA: 6:09 - loss: 3.0580 - regression_loss: 2.3105 - classification_loss: 0.7475
 183/1000 [====>.........................] - ETA: 6:09 - loss: 3.0412 - regression_loss: 2.2978 - classification_loss: 0.7434
 184/1000 [====>.........................] - ETA: 6:08 - loss: 3.0515 - regression_loss: 2.3011 - classification_loss: 0.7504
 185/1000 [====>.........................] - ETA: 6:08 - loss: 3.0594 - regression_loss: 2.3073 - classification_loss: 0.7521
 186/1000 [====>.........................] - ETA: 6:07 - loss: 3.0537 - regression_loss: 2.3032 - classification_loss: 0.7505
 187/1000 [====>.........................] - ETA: 6:07 - loss: 3.0555 - regression_loss: 2.3021 - classification_loss: 0.7534
 188/1000 [====>.........................] - ETA: 6:07 - loss: 3.0426 - regression_loss: 2.2899 - classification_loss: 0.7527
 189/1000 [====>.........................] - ETA: 6:06 - loss: 3.0508 - regression_loss: 2.2972 - classification_loss: 0.7537
 190/1000 [====>.........................] - ETA: 6:06 - loss: 3.0617 - regression_loss: 2.3029 - classification_loss: 0.7588
 191/1000 [====>.........................] - ETA: 6:05 - loss: 3.0776 - regression_loss: 2.3127 - classification_loss: 0.7649
 192/1000 [====>.........................] - ETA: 6:05 - loss: 3.0826 - regression_loss: 2.3139 - classification_loss: 0.7686
 193/1000 [====>.........................] - ETA: 6:04 - loss: 3.0868 - regression_loss: 2.3196 - classification_loss: 0.7673
 194/1000 [====>.........................] - ETA: 6:04 - loss: 3.0852 - regression_loss: 2.3191 - classification_loss: 0.7661
 195/1000 [====>.........................] - ETA: 6:03 - loss: 3.0835 - regression_loss: 2.3184 - classification_loss: 0.7651
 196/1000 [====>.........................] - ETA: 6:03 - loss: 3.0869 - regression_loss: 2.3238 - classification_loss: 0.7631
 197/1000 [====>.........................] - ETA: 6:02 - loss: 3.0713 - regression_loss: 2.3120 - classification_loss: 0.7593
 198/1000 [====>.........................] - ETA: 6:02 - loss: 3.0756 - regression_loss: 2.3157 - classification_loss: 0.7599
 199/1000 [====>.........................] - ETA: 6:02 - loss: 3.0822 - regression_loss: 2.3190 - classification_loss: 0.7632
 200/1000 [=====>........................] - ETA: 6:01 - loss: 3.0668 - regression_loss: 2.3074 - classification_loss: 0.7594
 201/1000 [=====>........................] - ETA: 6:01 - loss: 3.0680 - regression_loss: 2.3105 - classification_loss: 0.7574
 202/1000 [=====>........................] - ETA: 6:00 - loss: 3.0720 - regression_loss: 2.3147 - classification_loss: 0.7573
 203/1000 [=====>........................] - ETA: 6:00 - loss: 3.0808 - regression_loss: 2.3205 - classification_loss: 0.7603
 204/1000 [=====>........................] - ETA: 5:59 - loss: 3.0657 - regression_loss: 2.3091 - classification_loss: 0.7566
 205/1000 [=====>........................] - ETA: 5:59 - loss: 3.0667 - regression_loss: 2.3090 - classification_loss: 0.7578
 206/1000 [=====>........................] - ETA: 5:58 - loss: 3.0673 - regression_loss: 2.3098 - classification_loss: 0.7575
 207/1000 [=====>........................] - ETA: 5:58 - loss: 3.0732 - regression_loss: 2.3107 - classification_loss: 0.7625
 208/1000 [=====>........................] - ETA: 5:57 - loss: 3.0584 - regression_loss: 2.2996 - classification_loss: 0.7588
 209/1000 [=====>........................] - ETA: 5:57 - loss: 3.0662 - regression_loss: 2.3049 - classification_loss: 0.7613
 210/1000 [=====>........................] - ETA: 5:56 - loss: 3.0516 - regression_loss: 2.2939 - classification_loss: 0.7577
 211/1000 [=====>........................] - ETA: 5:56 - loss: 3.0512 - regression_loss: 2.2932 - classification_loss: 0.7580
 212/1000 [=====>........................] - ETA: 5:56 - loss: 3.0563 - regression_loss: 2.2979 - classification_loss: 0.7584
 213/1000 [=====>........................] - ETA: 5:55 - loss: 3.0713 - regression_loss: 2.3126 - classification_loss: 0.7587
 214/1000 [=====>........................] - ETA: 5:55 - loss: 3.0793 - regression_loss: 2.3166 - classification_loss: 0.7627
 215/1000 [=====>........................] - ETA: 5:54 - loss: 3.0793 - regression_loss: 2.3187 - classification_loss: 0.7606
 216/1000 [=====>........................] - ETA: 5:54 - loss: 3.0847 - regression_loss: 2.3189 - classification_loss: 0.7658
 217/1000 [=====>........................] - ETA: 5:53 - loss: 3.0816 - regression_loss: 2.3174 - classification_loss: 0.7643
 218/1000 [=====>........................] - ETA: 5:53 - loss: 3.0799 - regression_loss: 2.3175 - classification_loss: 0.7624
 219/1000 [=====>........................] - ETA: 5:53 - loss: 3.0798 - regression_loss: 2.3151 - classification_loss: 0.7647
 220/1000 [=====>........................] - ETA: 5:52 - loss: 3.0658 - regression_loss: 2.3045 - classification_loss: 0.7613
 221/1000 [=====>........................] - ETA: 5:52 - loss: 3.0811 - regression_loss: 2.3207 - classification_loss: 0.7604
 222/1000 [=====>........................] - ETA: 5:51 - loss: 3.0859 - regression_loss: 2.3240 - classification_loss: 0.7619
 223/1000 [=====>........................] - ETA: 5:51 - loss: 3.0973 - regression_loss: 2.3355 - classification_loss: 0.7618
 224/1000 [=====>........................] - ETA: 5:50 - loss: 3.0836 - regression_loss: 2.3251 - classification_loss: 0.7585
 225/1000 [=====>........................] - ETA: 5:50 - loss: 3.0884 - regression_loss: 2.3312 - classification_loss: 0.7572
 226/1000 [=====>........................] - ETA: 5:49 - loss: 3.0925 - regression_loss: 2.3345 - classification_loss: 0.7580
 227/1000 [=====>........................] - ETA: 5:49 - loss: 3.0922 - regression_loss: 2.3345 - classification_loss: 0.7577
 228/1000 [=====>........................] - ETA: 5:48 - loss: 3.0947 - regression_loss: 2.3373 - classification_loss: 0.7574
 229/1000 [=====>........................] - ETA: 5:48 - loss: 3.0855 - regression_loss: 2.3271 - classification_loss: 0.7584
 230/1000 [=====>........................] - ETA: 5:47 - loss: 3.0851 - regression_loss: 2.3284 - classification_loss: 0.7567
 231/1000 [=====>........................] - ETA: 5:47 - loss: 3.0717 - regression_loss: 2.3183 - classification_loss: 0.7534
 232/1000 [=====>........................] - ETA: 5:47 - loss: 3.0845 - regression_loss: 2.3245 - classification_loss: 0.7600
 233/1000 [=====>........................] - ETA: 5:46 - loss: 3.0713 - regression_loss: 2.3145 - classification_loss: 0.7567
 234/1000 [======>.......................] - ETA: 5:46 - loss: 3.0782 - regression_loss: 2.3182 - classification_loss: 0.7600
 235/1000 [======>.......................] - ETA: 5:45 - loss: 3.0814 - regression_loss: 2.3217 - classification_loss: 0.7597
 236/1000 [======>.......................] - ETA: 5:45 - loss: 3.0893 - regression_loss: 2.3262 - classification_loss: 0.7631
 237/1000 [======>.......................] - ETA: 5:44 - loss: 3.0964 - regression_loss: 2.3299 - classification_loss: 0.7665
 238/1000 [======>.......................] - ETA: 5:44 - loss: 3.0834 - regression_loss: 2.3202 - classification_loss: 0.7632
 239/1000 [======>.......................] - ETA: 5:43 - loss: 3.0811 - regression_loss: 2.3190 - classification_loss: 0.7621
 240/1000 [======>.......................] - ETA: 5:43 - loss: 3.0683 - regression_loss: 2.3093 - classification_loss: 0.7590
 241/1000 [======>.......................] - ETA: 5:42 - loss: 3.0555 - regression_loss: 2.2997 - classification_loss: 0.7558
 242/1000 [======>.......................] - ETA: 5:42 - loss: 3.0436 - regression_loss: 2.2902 - classification_loss: 0.7534
 243/1000 [======>.......................] - ETA: 5:42 - loss: 3.0459 - regression_loss: 2.2921 - classification_loss: 0.7538
 244/1000 [======>.......................] - ETA: 5:41 - loss: 3.0483 - regression_loss: 2.2941 - classification_loss: 0.7542
 245/1000 [======>.......................] - ETA: 5:41 - loss: 3.0488 - regression_loss: 2.2956 - classification_loss: 0.7531
 246/1000 [======>.......................] - ETA: 5:40 - loss: 3.0511 - regression_loss: 2.2954 - classification_loss: 0.7557
 247/1000 [======>.......................] - ETA: 5:40 - loss: 3.0387 - regression_loss: 2.2861 - classification_loss: 0.7526
 248/1000 [======>.......................] - ETA: 5:39 - loss: 3.0419 - regression_loss: 2.2853 - classification_loss: 0.7566
 249/1000 [======>.......................] - ETA: 5:39 - loss: 3.0297 - regression_loss: 2.2761 - classification_loss: 0.7535
 250/1000 [======>.......................] - ETA: 5:38 - loss: 3.0312 - regression_loss: 2.2778 - classification_loss: 0.7533
 251/1000 [======>.......................] - ETA: 5:38 - loss: 3.0356 - regression_loss: 2.2816 - classification_loss: 0.7541
 252/1000 [======>.......................] - ETA: 5:38 - loss: 3.0236 - regression_loss: 2.2725 - classification_loss: 0.7511
 253/1000 [======>.......................] - ETA: 5:37 - loss: 3.0281 - regression_loss: 2.2767 - classification_loss: 0.7514
 254/1000 [======>.......................] - ETA: 5:37 - loss: 3.0162 - regression_loss: 2.2677 - classification_loss: 0.7485
 255/1000 [======>.......................] - ETA: 5:36 - loss: 3.0044 - regression_loss: 2.2588 - classification_loss: 0.7456
 256/1000 [======>.......................] - ETA: 5:36 - loss: 3.0044 - regression_loss: 2.2593 - classification_loss: 0.7452
 257/1000 [======>.......................] - ETA: 5:35 - loss: 3.0103 - regression_loss: 2.2611 - classification_loss: 0.7492
 258/1000 [======>.......................] - ETA: 5:35 - loss: 3.0186 - regression_loss: 2.2664 - classification_loss: 0.7522
 259/1000 [======>.......................] - ETA: 5:34 - loss: 3.0296 - regression_loss: 2.2738 - classification_loss: 0.7558
 260/1000 [======>.......................] - ETA: 5:34 - loss: 3.0321 - regression_loss: 2.2760 - classification_loss: 0.7561
 261/1000 [======>.......................] - ETA: 5:33 - loss: 3.0206 - regression_loss: 2.2673 - classification_loss: 0.7532
 262/1000 [======>.......................] - ETA: 5:33 - loss: 3.0090 - regression_loss: 2.2587 - classification_loss: 0.7504
 263/1000 [======>.......................] - ETA: 5:32 - loss: 2.9976 - regression_loss: 2.2501 - classification_loss: 0.7475
 264/1000 [======>.......................] - ETA: 5:32 - loss: 2.9972 - regression_loss: 2.2509 - classification_loss: 0.7463
 265/1000 [======>.......................] - ETA: 5:32 - loss: 3.0031 - regression_loss: 2.2534 - classification_loss: 0.7497
 266/1000 [======>.......................] - ETA: 5:31 - loss: 3.0112 - regression_loss: 2.2573 - classification_loss: 0.7539
 267/1000 [=======>......................] - ETA: 5:31 - loss: 2.9999 - regression_loss: 2.2488 - classification_loss: 0.7511
 268/1000 [=======>......................] - ETA: 5:30 - loss: 3.0101 - regression_loss: 2.2564 - classification_loss: 0.7538
 269/1000 [=======>......................] - ETA: 5:30 - loss: 3.0129 - regression_loss: 2.2572 - classification_loss: 0.7556
 270/1000 [=======>......................] - ETA: 5:29 - loss: 3.0142 - regression_loss: 2.2590 - classification_loss: 0.7552
 271/1000 [=======>......................] - ETA: 5:29 - loss: 3.0194 - regression_loss: 2.2588 - classification_loss: 0.7605
 272/1000 [=======>......................] - ETA: 5:28 - loss: 3.0268 - regression_loss: 2.2635 - classification_loss: 0.7633
 273/1000 [=======>......................] - ETA: 5:28 - loss: 3.0313 - regression_loss: 2.2649 - classification_loss: 0.7664
 274/1000 [=======>......................] - ETA: 5:28 - loss: 3.0374 - regression_loss: 2.2707 - classification_loss: 0.7667
 275/1000 [=======>......................] - ETA: 5:27 - loss: 3.0431 - regression_loss: 2.2774 - classification_loss: 0.7657
 276/1000 [=======>......................] - ETA: 5:27 - loss: 3.0440 - regression_loss: 2.2775 - classification_loss: 0.7666
 277/1000 [=======>......................] - ETA: 5:26 - loss: 3.0515 - regression_loss: 2.2816 - classification_loss: 0.7699
 278/1000 [=======>......................] - ETA: 5:26 - loss: 3.0584 - regression_loss: 2.2872 - classification_loss: 0.7712
 279/1000 [=======>......................] - ETA: 5:25 - loss: 3.0637 - regression_loss: 2.2902 - classification_loss: 0.7735
 280/1000 [=======>......................] - ETA: 5:25 - loss: 3.0774 - regression_loss: 2.3010 - classification_loss: 0.7764
 281/1000 [=======>......................] - ETA: 5:24 - loss: 3.0784 - regression_loss: 2.3013 - classification_loss: 0.7772
 282/1000 [=======>......................] - ETA: 5:24 - loss: 3.0675 - regression_loss: 2.2931 - classification_loss: 0.7744
 283/1000 [=======>......................] - ETA: 5:24 - loss: 3.0676 - regression_loss: 2.2936 - classification_loss: 0.7739
 284/1000 [=======>......................] - ETA: 5:23 - loss: 3.0715 - regression_loss: 2.2985 - classification_loss: 0.7730
 285/1000 [=======>......................] - ETA: 5:23 - loss: 3.0758 - regression_loss: 2.3026 - classification_loss: 0.7732
 286/1000 [=======>......................] - ETA: 5:22 - loss: 3.0815 - regression_loss: 2.3079 - classification_loss: 0.7735
 287/1000 [=======>......................] - ETA: 5:22 - loss: 3.0827 - regression_loss: 2.3088 - classification_loss: 0.7739
 288/1000 [=======>......................] - ETA: 5:21 - loss: 3.0848 - regression_loss: 2.3106 - classification_loss: 0.7741
 289/1000 [=======>......................] - ETA: 5:21 - loss: 3.0901 - regression_loss: 2.3129 - classification_loss: 0.7772
 290/1000 [=======>......................] - ETA: 5:20 - loss: 3.0914 - regression_loss: 2.3135 - classification_loss: 0.7779
 291/1000 [=======>......................] - ETA: 5:20 - loss: 3.0953 - regression_loss: 2.3181 - classification_loss: 0.7772
 292/1000 [=======>......................] - ETA: 5:20 - loss: 3.0953 - regression_loss: 2.3185 - classification_loss: 0.7768
 293/1000 [=======>......................] - ETA: 5:19 - loss: 3.0991 - regression_loss: 2.3223 - classification_loss: 0.7768
 294/1000 [=======>......................] - ETA: 5:19 - loss: 3.0968 - regression_loss: 2.3212 - classification_loss: 0.7755
 295/1000 [=======>......................] - ETA: 5:18 - loss: 3.1016 - regression_loss: 2.3242 - classification_loss: 0.7773
 296/1000 [=======>......................] - ETA: 5:18 - loss: 3.1009 - regression_loss: 2.3241 - classification_loss: 0.7768
 297/1000 [=======>......................] - ETA: 5:17 - loss: 3.1030 - regression_loss: 2.3269 - classification_loss: 0.7762
 298/1000 [=======>......................] - ETA: 5:17 - loss: 3.0932 - regression_loss: 2.3191 - classification_loss: 0.7741
 299/1000 [=======>......................] - ETA: 5:16 - loss: 3.0943 - regression_loss: 2.3209 - classification_loss: 0.7735
 300/1000 [========>.....................] - ETA: 5:16 - loss: 3.0976 - regression_loss: 2.3242 - classification_loss: 0.7734
 301/1000 [========>.....................] - ETA: 5:16 - loss: 3.0970 - regression_loss: 2.3249 - classification_loss: 0.7722
 302/1000 [========>.....................] - ETA: 5:15 - loss: 3.0972 - regression_loss: 2.3257 - classification_loss: 0.7715
 303/1000 [========>.....................] - ETA: 5:15 - loss: 3.1001 - regression_loss: 2.3284 - classification_loss: 0.7717
 304/1000 [========>.....................] - ETA: 5:14 - loss: 3.0998 - regression_loss: 2.3284 - classification_loss: 0.7714
 305/1000 [========>.....................] - ETA: 5:14 - loss: 3.0897 - regression_loss: 2.3208 - classification_loss: 0.7689
 306/1000 [========>.....................] - ETA: 5:13 - loss: 3.0862 - regression_loss: 2.3180 - classification_loss: 0.7682
 307/1000 [========>.....................] - ETA: 5:13 - loss: 3.0761 - regression_loss: 2.3104 - classification_loss: 0.7657
 308/1000 [========>.....................] - ETA: 5:12 - loss: 3.0681 - regression_loss: 2.3029 - classification_loss: 0.7652
 309/1000 [========>.....................] - ETA: 5:12 - loss: 3.0593 - regression_loss: 2.2955 - classification_loss: 0.7639
 310/1000 [========>.....................] - ETA: 5:11 - loss: 3.0614 - regression_loss: 2.2983 - classification_loss: 0.7632
 311/1000 [========>.....................] - ETA: 5:11 - loss: 3.0604 - regression_loss: 2.2982 - classification_loss: 0.7622
 312/1000 [========>.....................] - ETA: 5:10 - loss: 3.0639 - regression_loss: 2.3007 - classification_loss: 0.7633
 313/1000 [========>.....................] - ETA: 5:10 - loss: 3.0681 - regression_loss: 2.3033 - classification_loss: 0.7648
 314/1000 [========>.....................] - ETA: 5:10 - loss: 3.0726 - regression_loss: 2.3078 - classification_loss: 0.7648
 315/1000 [========>.....................] - ETA: 5:09 - loss: 3.0722 - regression_loss: 2.3075 - classification_loss: 0.7647
 316/1000 [========>.....................] - ETA: 5:09 - loss: 3.0761 - regression_loss: 2.3089 - classification_loss: 0.7673
 317/1000 [========>.....................] - ETA: 5:08 - loss: 3.0802 - regression_loss: 2.3120 - classification_loss: 0.7681
 318/1000 [========>.....................] - ETA: 5:08 - loss: 3.0705 - regression_loss: 2.3047 - classification_loss: 0.7657
 319/1000 [========>.....................] - ETA: 5:07 - loss: 3.0710 - regression_loss: 2.3062 - classification_loss: 0.7648
 320/1000 [========>.....................] - ETA: 5:07 - loss: 3.0768 - regression_loss: 2.3107 - classification_loss: 0.7661
 321/1000 [========>.....................] - ETA: 5:06 - loss: 3.0798 - regression_loss: 2.3129 - classification_loss: 0.7669
 322/1000 [========>.....................] - ETA: 5:06 - loss: 3.0854 - regression_loss: 2.3167 - classification_loss: 0.7687
 323/1000 [========>.....................] - ETA: 5:06 - loss: 3.0889 - regression_loss: 2.3177 - classification_loss: 0.7712
 324/1000 [========>.....................] - ETA: 5:05 - loss: 3.0889 - regression_loss: 2.3160 - classification_loss: 0.7729
 325/1000 [========>.....................] - ETA: 5:05 - loss: 3.0905 - regression_loss: 2.3178 - classification_loss: 0.7727
 326/1000 [========>.....................] - ETA: 5:04 - loss: 3.0916 - regression_loss: 2.3188 - classification_loss: 0.7728
 327/1000 [========>.....................] - ETA: 5:04 - loss: 3.0942 - regression_loss: 2.3215 - classification_loss: 0.7727
 328/1000 [========>.....................] - ETA: 5:03 - loss: 3.0930 - regression_loss: 2.3216 - classification_loss: 0.7714
 329/1000 [========>.....................] - ETA: 5:03 - loss: 3.0837 - regression_loss: 2.3146 - classification_loss: 0.7691
 330/1000 [========>.....................] - ETA: 5:02 - loss: 3.0824 - regression_loss: 2.3140 - classification_loss: 0.7684
 331/1000 [========>.....................] - ETA: 5:02 - loss: 3.0804 - regression_loss: 2.3131 - classification_loss: 0.7672
 332/1000 [========>.....................] - ETA: 5:01 - loss: 3.0774 - regression_loss: 2.3111 - classification_loss: 0.7663
 333/1000 [========>.....................] - ETA: 5:01 - loss: 3.0812 - regression_loss: 2.3151 - classification_loss: 0.7661
 334/1000 [=========>....................] - ETA: 5:01 - loss: 3.0817 - regression_loss: 2.3149 - classification_loss: 0.7668
 335/1000 [=========>....................] - ETA: 5:00 - loss: 3.0815 - regression_loss: 2.3162 - classification_loss: 0.7653
 336/1000 [=========>....................] - ETA: 5:00 - loss: 3.0833 - regression_loss: 2.3179 - classification_loss: 0.7654
 337/1000 [=========>....................] - ETA: 4:59 - loss: 3.0933 - regression_loss: 2.3238 - classification_loss: 0.7695
 338/1000 [=========>....................] - ETA: 4:59 - loss: 3.0972 - regression_loss: 2.3277 - classification_loss: 0.7695
 339/1000 [=========>....................] - ETA: 4:58 - loss: 3.0965 - regression_loss: 2.3279 - classification_loss: 0.7686
 340/1000 [=========>....................] - ETA: 4:58 - loss: 3.0975 - regression_loss: 2.3300 - classification_loss: 0.7675
 341/1000 [=========>....................] - ETA: 4:57 - loss: 3.0885 - regression_loss: 2.3232 - classification_loss: 0.7653
 342/1000 [=========>....................] - ETA: 4:57 - loss: 3.0797 - regression_loss: 2.3164 - classification_loss: 0.7633
 343/1000 [=========>....................] - ETA: 4:56 - loss: 3.0794 - regression_loss: 2.3163 - classification_loss: 0.7632
 344/1000 [=========>....................] - ETA: 4:56 - loss: 3.0831 - regression_loss: 2.3187 - classification_loss: 0.7644
 345/1000 [=========>....................] - ETA: 4:56 - loss: 3.0808 - regression_loss: 2.3174 - classification_loss: 0.7634
 346/1000 [=========>....................] - ETA: 4:55 - loss: 3.0721 - regression_loss: 2.3107 - classification_loss: 0.7614
 347/1000 [=========>....................] - ETA: 4:55 - loss: 3.0750 - regression_loss: 2.3132 - classification_loss: 0.7617
 348/1000 [=========>....................] - ETA: 4:54 - loss: 3.0666 - regression_loss: 2.3066 - classification_loss: 0.7600
 349/1000 [=========>....................] - ETA: 4:54 - loss: 3.0777 - regression_loss: 2.3158 - classification_loss: 0.7619
 350/1000 [=========>....................] - ETA: 4:53 - loss: 3.0695 - regression_loss: 2.3092 - classification_loss: 0.7603
 351/1000 [=========>....................] - ETA: 4:53 - loss: 3.0722 - regression_loss: 2.3116 - classification_loss: 0.7606
 352/1000 [=========>....................] - ETA: 4:52 - loss: 3.0697 - regression_loss: 2.3096 - classification_loss: 0.7601
 353/1000 [=========>....................] - ETA: 4:52 - loss: 3.0713 - regression_loss: 2.3114 - classification_loss: 0.7598
 354/1000 [=========>....................] - ETA: 4:51 - loss: 3.0788 - regression_loss: 2.3169 - classification_loss: 0.7620
 355/1000 [=========>....................] - ETA: 4:51 - loss: 3.0702 - regression_loss: 2.3103 - classification_loss: 0.7598
 356/1000 [=========>....................] - ETA: 4:51 - loss: 3.0615 - regression_loss: 2.3038 - classification_loss: 0.7577
 357/1000 [=========>....................] - ETA: 4:50 - loss: 3.0649 - regression_loss: 2.3061 - classification_loss: 0.7589
 358/1000 [=========>....................] - ETA: 4:50 - loss: 3.0645 - regression_loss: 2.3064 - classification_loss: 0.7581
 359/1000 [=========>....................] - ETA: 4:49 - loss: 3.0663 - regression_loss: 2.3079 - classification_loss: 0.7584
 360/1000 [=========>....................] - ETA: 4:49 - loss: 3.0718 - regression_loss: 2.3133 - classification_loss: 0.7585
 361/1000 [=========>....................] - ETA: 4:48 - loss: 3.0734 - regression_loss: 2.3145 - classification_loss: 0.7589
 362/1000 [=========>....................] - ETA: 4:48 - loss: 3.0825 - regression_loss: 2.3232 - classification_loss: 0.7593
 363/1000 [=========>....................] - ETA: 4:47 - loss: 3.0740 - regression_loss: 2.3168 - classification_loss: 0.7572
 364/1000 [=========>....................] - ETA: 4:47 - loss: 3.0655 - regression_loss: 2.3104 - classification_loss: 0.7552
 365/1000 [=========>....................] - ETA: 4:47 - loss: 3.0698 - regression_loss: 2.3130 - classification_loss: 0.7568
 366/1000 [=========>....................] - ETA: 4:46 - loss: 3.0746 - regression_loss: 2.3148 - classification_loss: 0.7598
 367/1000 [==========>...................] - ETA: 4:46 - loss: 3.0778 - regression_loss: 2.3162 - classification_loss: 0.7616
 368/1000 [==========>...................] - ETA: 4:45 - loss: 3.0694 - regression_loss: 2.3099 - classification_loss: 0.7595
 369/1000 [==========>...................] - ETA: 4:45 - loss: 3.0611 - regression_loss: 2.3036 - classification_loss: 0.7575
 370/1000 [==========>...................] - ETA: 4:44 - loss: 3.0528 - regression_loss: 2.2974 - classification_loss: 0.7554
 371/1000 [==========>...................] - ETA: 4:44 - loss: 3.0446 - regression_loss: 2.2912 - classification_loss: 0.7534
 372/1000 [==========>...................] - ETA: 4:43 - loss: 3.0486 - regression_loss: 2.2953 - classification_loss: 0.7534
 373/1000 [==========>...................] - ETA: 4:43 - loss: 3.0554 - regression_loss: 2.2993 - classification_loss: 0.7562
 374/1000 [==========>...................] - ETA: 4:42 - loss: 3.0609 - regression_loss: 2.3038 - classification_loss: 0.7571
 375/1000 [==========>...................] - ETA: 4:42 - loss: 3.0585 - regression_loss: 2.3018 - classification_loss: 0.7566
 376/1000 [==========>...................] - ETA: 4:42 - loss: 3.0660 - regression_loss: 2.3055 - classification_loss: 0.7605
 377/1000 [==========>...................] - ETA: 4:41 - loss: 3.0667 - regression_loss: 2.3051 - classification_loss: 0.7616
 378/1000 [==========>...................] - ETA: 4:41 - loss: 3.0721 - regression_loss: 2.3060 - classification_loss: 0.7661
 379/1000 [==========>...................] - ETA: 4:40 - loss: 3.0793 - regression_loss: 2.3091 - classification_loss: 0.7702
 380/1000 [==========>...................] - ETA: 4:40 - loss: 3.0831 - regression_loss: 2.3096 - classification_loss: 0.7735
 381/1000 [==========>...................] - ETA: 4:39 - loss: 3.0876 - regression_loss: 2.3117 - classification_loss: 0.7759
 382/1000 [==========>...................] - ETA: 4:39 - loss: 3.0880 - regression_loss: 2.3118 - classification_loss: 0.7761
 383/1000 [==========>...................] - ETA: 4:38 - loss: 3.0917 - regression_loss: 2.3145 - classification_loss: 0.7773
 384/1000 [==========>...................] - ETA: 4:38 - loss: 3.0959 - regression_loss: 2.3184 - classification_loss: 0.7775
 385/1000 [==========>...................] - ETA: 4:38 - loss: 3.0984 - regression_loss: 2.3192 - classification_loss: 0.7791
 386/1000 [==========>...................] - ETA: 4:37 - loss: 3.0903 - regression_loss: 2.3132 - classification_loss: 0.7771
 387/1000 [==========>...................] - ETA: 4:37 - loss: 3.0943 - regression_loss: 2.3159 - classification_loss: 0.7785
 388/1000 [==========>...................] - ETA: 4:36 - loss: 3.0940 - regression_loss: 2.3144 - classification_loss: 0.7796
 389/1000 [==========>...................] - ETA: 4:36 - loss: 3.0924 - regression_loss: 2.3140 - classification_loss: 0.7785
 390/1000 [==========>...................] - ETA: 4:35 - loss: 3.0937 - regression_loss: 2.3150 - classification_loss: 0.7788
 391/1000 [==========>...................] - ETA: 4:35 - loss: 3.0948 - regression_loss: 2.3170 - classification_loss: 0.7778
 392/1000 [==========>...................] - ETA: 4:34 - loss: 3.0952 - regression_loss: 2.3183 - classification_loss: 0.7769
 393/1000 [==========>...................] - ETA: 4:34 - loss: 3.0967 - regression_loss: 2.3198 - classification_loss: 0.7769
 394/1000 [==========>...................] - ETA: 4:33 - loss: 3.0888 - regression_loss: 2.3139 - classification_loss: 0.7749
 395/1000 [==========>...................] - ETA: 4:33 - loss: 3.0810 - regression_loss: 2.3080 - classification_loss: 0.7730
 396/1000 [==========>...................] - ETA: 4:33 - loss: 3.0838 - regression_loss: 2.3100 - classification_loss: 0.7738
 397/1000 [==========>...................] - ETA: 4:32 - loss: 3.0891 - regression_loss: 2.3143 - classification_loss: 0.7748
 398/1000 [==========>...................] - ETA: 4:32 - loss: 3.0870 - regression_loss: 2.3135 - classification_loss: 0.7735
 399/1000 [==========>...................] - ETA: 4:31 - loss: 3.0900 - regression_loss: 2.3142 - classification_loss: 0.7758
 400/1000 [===========>..................] - ETA: 4:31 - loss: 3.0926 - regression_loss: 2.3152 - classification_loss: 0.7774
 401/1000 [===========>..................] - ETA: 4:30 - loss: 3.0961 - regression_loss: 2.3179 - classification_loss: 0.7781
 402/1000 [===========>..................] - ETA: 4:30 - loss: 3.0888 - regression_loss: 2.3122 - classification_loss: 0.7766
 403/1000 [===========>..................] - ETA: 4:29 - loss: 3.0936 - regression_loss: 2.3151 - classification_loss: 0.7785
 404/1000 [===========>..................] - ETA: 4:29 - loss: 3.0959 - regression_loss: 2.3156 - classification_loss: 0.7803
 405/1000 [===========>..................] - ETA: 4:28 - loss: 3.0883 - regression_loss: 2.3099 - classification_loss: 0.7784
 406/1000 [===========>..................] - ETA: 4:28 - loss: 3.0916 - regression_loss: 2.3133 - classification_loss: 0.7782
 407/1000 [===========>..................] - ETA: 4:28 - loss: 3.0967 - regression_loss: 2.3165 - classification_loss: 0.7803
 408/1000 [===========>..................] - ETA: 4:27 - loss: 3.0993 - regression_loss: 2.3180 - classification_loss: 0.7813
 409/1000 [===========>..................] - ETA: 4:27 - loss: 3.0997 - regression_loss: 2.3177 - classification_loss: 0.7819
 410/1000 [===========>..................] - ETA: 4:26 - loss: 3.0921 - regression_loss: 2.3121 - classification_loss: 0.7800
 411/1000 [===========>..................] - ETA: 4:26 - loss: 3.0961 - regression_loss: 2.3163 - classification_loss: 0.7798
 412/1000 [===========>..................] - ETA: 4:25 - loss: 3.0886 - regression_loss: 2.3107 - classification_loss: 0.7779
 413/1000 [===========>..................] - ETA: 4:25 - loss: 3.0893 - regression_loss: 2.3116 - classification_loss: 0.7778
 414/1000 [===========>..................] - ETA: 4:24 - loss: 3.0895 - regression_loss: 2.3110 - classification_loss: 0.7785
 415/1000 [===========>..................] - ETA: 4:24 - loss: 3.0906 - regression_loss: 2.3118 - classification_loss: 0.7789
 416/1000 [===========>..................] - ETA: 4:24 - loss: 3.0943 - regression_loss: 2.3154 - classification_loss: 0.7789
 417/1000 [===========>..................] - ETA: 4:23 - loss: 3.0942 - regression_loss: 2.3154 - classification_loss: 0.7788
 418/1000 [===========>..................] - ETA: 4:23 - loss: 3.0926 - regression_loss: 2.3141 - classification_loss: 0.7785
 419/1000 [===========>..................] - ETA: 4:22 - loss: 3.0853 - regression_loss: 2.3086 - classification_loss: 0.7767
 420/1000 [===========>..................] - ETA: 4:22 - loss: 3.0896 - regression_loss: 2.3112 - classification_loss: 0.7784
 421/1000 [===========>..................] - ETA: 4:21 - loss: 3.0935 - regression_loss: 2.3135 - classification_loss: 0.7800
 422/1000 [===========>..................] - ETA: 4:21 - loss: 3.0945 - regression_loss: 2.3147 - classification_loss: 0.7798
 423/1000 [===========>..................] - ETA: 4:20 - loss: 3.0960 - regression_loss: 2.3167 - classification_loss: 0.7793
 424/1000 [===========>..................] - ETA: 4:20 - loss: 3.0979 - regression_loss: 2.3166 - classification_loss: 0.7813
 425/1000 [===========>..................] - ETA: 4:19 - loss: 3.0997 - regression_loss: 2.3193 - classification_loss: 0.7804
 426/1000 [===========>..................] - ETA: 4:19 - loss: 3.0924 - regression_loss: 2.3138 - classification_loss: 0.7786
 427/1000 [===========>..................] - ETA: 4:18 - loss: 3.0852 - regression_loss: 2.3084 - classification_loss: 0.7768
 428/1000 [===========>..................] - ETA: 4:18 - loss: 3.0893 - regression_loss: 2.3129 - classification_loss: 0.7765
 429/1000 [===========>..................] - ETA: 4:18 - loss: 3.0823 - regression_loss: 2.3075 - classification_loss: 0.7749
 430/1000 [===========>..................] - ETA: 4:17 - loss: 3.0879 - regression_loss: 2.3100 - classification_loss: 0.7779
 431/1000 [===========>..................] - ETA: 4:17 - loss: 3.0941 - regression_loss: 2.3132 - classification_loss: 0.7809
 432/1000 [===========>..................] - ETA: 4:16 - loss: 3.0960 - regression_loss: 2.3157 - classification_loss: 0.7802
 433/1000 [===========>..................] - ETA: 4:16 - loss: 3.0888 - regression_loss: 2.3104 - classification_loss: 0.7784
 434/1000 [============>.................] - ETA: 4:15 - loss: 3.0954 - regression_loss: 2.3152 - classification_loss: 0.7803
 435/1000 [============>.................] - ETA: 4:15 - loss: 3.0986 - regression_loss: 2.3169 - classification_loss: 0.7817
 436/1000 [============>.................] - ETA: 4:14 - loss: 3.1008 - regression_loss: 2.3185 - classification_loss: 0.7822
 437/1000 [============>.................] - ETA: 4:14 - loss: 3.0937 - regression_loss: 2.3132 - classification_loss: 0.7804
 438/1000 [============>.................] - ETA: 4:13 - loss: 3.0958 - regression_loss: 2.3149 - classification_loss: 0.7809
 439/1000 [============>.................] - ETA: 4:13 - loss: 3.0976 - regression_loss: 2.3171 - classification_loss: 0.7806
 440/1000 [============>.................] - ETA: 4:13 - loss: 3.1000 - regression_loss: 2.3176 - classification_loss: 0.7824
 441/1000 [============>.................] - ETA: 4:12 - loss: 3.0983 - regression_loss: 2.3172 - classification_loss: 0.7811
 442/1000 [============>.................] - ETA: 4:12 - loss: 3.0980 - regression_loss: 2.3175 - classification_loss: 0.7805
 443/1000 [============>.................] - ETA: 4:11 - loss: 3.1026 - regression_loss: 2.3208 - classification_loss: 0.7818
 444/1000 [============>.................] - ETA: 4:11 - loss: 3.1020 - regression_loss: 2.3212 - classification_loss: 0.7808
 445/1000 [============>.................] - ETA: 4:10 - loss: 3.1049 - regression_loss: 2.3231 - classification_loss: 0.7817
 446/1000 [============>.................] - ETA: 4:10 - loss: 3.1068 - regression_loss: 2.3249 - classification_loss: 0.7820
 447/1000 [============>.................] - ETA: 4:09 - loss: 3.1055 - regression_loss: 2.3244 - classification_loss: 0.7810
 448/1000 [============>.................] - ETA: 4:09 - loss: 3.1103 - regression_loss: 2.3286 - classification_loss: 0.7817
 449/1000 [============>.................] - ETA: 4:09 - loss: 3.1034 - regression_loss: 2.3234 - classification_loss: 0.7800
 450/1000 [============>.................] - ETA: 4:08 - loss: 3.1037 - regression_loss: 2.3242 - classification_loss: 0.7795
 451/1000 [============>.................] - ETA: 4:08 - loss: 3.1010 - regression_loss: 2.3190 - classification_loss: 0.7819
 452/1000 [============>.................] - ETA: 4:07 - loss: 3.1003 - regression_loss: 2.3195 - classification_loss: 0.7808
 453/1000 [============>.................] - ETA: 4:07 - loss: 3.1004 - regression_loss: 2.3198 - classification_loss: 0.7806
 454/1000 [============>.................] - ETA: 4:06 - loss: 3.1042 - regression_loss: 2.3227 - classification_loss: 0.7815
 455/1000 [============>.................] - ETA: 4:06 - loss: 3.1075 - regression_loss: 2.3265 - classification_loss: 0.7809
 456/1000 [============>.................] - ETA: 4:05 - loss: 3.1114 - regression_loss: 2.3288 - classification_loss: 0.7826
 457/1000 [============>.................] - ETA: 4:05 - loss: 3.1126 - regression_loss: 2.3307 - classification_loss: 0.7819
 458/1000 [============>.................] - ETA: 4:04 - loss: 3.1058 - regression_loss: 2.3256 - classification_loss: 0.7802
 459/1000 [============>.................] - ETA: 4:04 - loss: 3.1074 - regression_loss: 2.3275 - classification_loss: 0.7799
 460/1000 [============>.................] - ETA: 4:04 - loss: 3.1109 - regression_loss: 2.3296 - classification_loss: 0.7813
 461/1000 [============>.................] - ETA: 4:03 - loss: 3.1124 - regression_loss: 2.3316 - classification_loss: 0.7809
 462/1000 [============>.................] - ETA: 4:03 - loss: 3.1143 - regression_loss: 2.3336 - classification_loss: 0.7807
 463/1000 [============>.................] - ETA: 4:02 - loss: 3.1127 - regression_loss: 2.3327 - classification_loss: 0.7800
 464/1000 [============>.................] - ETA: 4:02 - loss: 3.1146 - regression_loss: 2.3341 - classification_loss: 0.7805
 465/1000 [============>.................] - ETA: 4:01 - loss: 3.1173 - regression_loss: 2.3359 - classification_loss: 0.7814
 466/1000 [============>.................] - ETA: 4:01 - loss: 3.1217 - regression_loss: 2.3397 - classification_loss: 0.7821
 467/1000 [=============>................] - ETA: 4:00 - loss: 3.1151 - regression_loss: 2.3347 - classification_loss: 0.7804
 468/1000 [=============>................] - ETA: 4:00 - loss: 3.1086 - regression_loss: 2.3297 - classification_loss: 0.7790
 469/1000 [=============>................] - ETA: 3:59 - loss: 3.1144 - regression_loss: 2.3335 - classification_loss: 0.7809
 470/1000 [=============>................] - ETA: 3:59 - loss: 3.1156 - regression_loss: 2.3353 - classification_loss: 0.7803
 471/1000 [=============>................] - ETA: 3:59 - loss: 3.1154 - regression_loss: 2.3351 - classification_loss: 0.7802
 472/1000 [=============>................] - ETA: 3:58 - loss: 3.1088 - regression_loss: 2.3302 - classification_loss: 0.7786
 473/1000 [=============>................] - ETA: 3:58 - loss: 3.1105 - regression_loss: 2.3308 - classification_loss: 0.7797
 474/1000 [=============>................] - ETA: 3:57 - loss: 3.1122 - regression_loss: 2.3321 - classification_loss: 0.7801
 475/1000 [=============>................] - ETA: 3:57 - loss: 3.1120 - regression_loss: 2.3314 - classification_loss: 0.7806
 476/1000 [=============>................] - ETA: 3:56 - loss: 3.1104 - regression_loss: 2.3304 - classification_loss: 0.7800
 477/1000 [=============>................] - ETA: 3:56 - loss: 3.1137 - regression_loss: 2.3327 - classification_loss: 0.7810
 478/1000 [=============>................] - ETA: 3:55 - loss: 3.1153 - regression_loss: 2.3334 - classification_loss: 0.7820
 479/1000 [=============>................] - ETA: 3:55 - loss: 3.1088 - regression_loss: 2.3285 - classification_loss: 0.7803
 480/1000 [=============>................] - ETA: 3:55 - loss: 3.1079 - regression_loss: 2.3284 - classification_loss: 0.7795
 481/1000 [=============>................] - ETA: 3:54 - loss: 3.1117 - regression_loss: 2.3309 - classification_loss: 0.7808
 482/1000 [=============>................] - ETA: 3:54 - loss: 3.1125 - regression_loss: 2.3312 - classification_loss: 0.7813
 483/1000 [=============>................] - ETA: 3:53 - loss: 3.1158 - regression_loss: 2.3332 - classification_loss: 0.7826
 484/1000 [=============>................] - ETA: 3:53 - loss: 3.1190 - regression_loss: 2.3353 - classification_loss: 0.7837
 485/1000 [=============>................] - ETA: 3:52 - loss: 3.1196 - regression_loss: 2.3363 - classification_loss: 0.7834
 486/1000 [=============>................] - ETA: 3:52 - loss: 3.1223 - regression_loss: 2.3378 - classification_loss: 0.7845
 487/1000 [=============>................] - ETA: 3:51 - loss: 3.1159 - regression_loss: 2.3330 - classification_loss: 0.7829
 488/1000 [=============>................] - ETA: 3:51 - loss: 3.1168 - regression_loss: 2.3339 - classification_loss: 0.7829
 489/1000 [=============>................] - ETA: 3:50 - loss: 3.1104 - regression_loss: 2.3291 - classification_loss: 0.7813
 490/1000 [=============>................] - ETA: 3:50 - loss: 3.1099 - regression_loss: 2.3291 - classification_loss: 0.7807
 491/1000 [=============>................] - ETA: 3:50 - loss: 3.1095 - regression_loss: 2.3280 - classification_loss: 0.7815
 492/1000 [=============>................] - ETA: 3:49 - loss: 3.1121 - regression_loss: 2.3293 - classification_loss: 0.7829
 493/1000 [=============>................] - ETA: 3:49 - loss: 3.1122 - regression_loss: 2.3287 - classification_loss: 0.7835
 494/1000 [=============>................] - ETA: 3:48 - loss: 3.1128 - regression_loss: 2.3293 - classification_loss: 0.7835
 495/1000 [=============>................] - ETA: 3:48 - loss: 3.1120 - regression_loss: 2.3292 - classification_loss: 0.7828
 496/1000 [=============>................] - ETA: 3:47 - loss: 3.1057 - regression_loss: 2.3245 - classification_loss: 0.7812
 497/1000 [=============>................] - ETA: 3:47 - loss: 3.1076 - regression_loss: 2.3266 - classification_loss: 0.7810
 498/1000 [=============>................] - ETA: 3:46 - loss: 3.1073 - regression_loss: 2.3268 - classification_loss: 0.7805
 499/1000 [=============>................] - ETA: 3:46 - loss: 3.1093 - regression_loss: 2.3276 - classification_loss: 0.7817
 500/1000 [==============>...............] - ETA: 3:45 - loss: 3.1133 - regression_loss: 2.3299 - classification_loss: 0.7834
 501/1000 [==============>...............] - ETA: 3:45 - loss: 3.1150 - regression_loss: 2.3315 - classification_loss: 0.7835
 502/1000 [==============>...............] - ETA: 3:45 - loss: 3.1163 - regression_loss: 2.3324 - classification_loss: 0.7839
 503/1000 [==============>...............] - ETA: 3:44 - loss: 3.1174 - regression_loss: 2.3335 - classification_loss: 0.7838
 504/1000 [==============>...............] - ETA: 3:44 - loss: 3.1173 - regression_loss: 2.3335 - classification_loss: 0.7838
 505/1000 [==============>...............] - ETA: 3:43 - loss: 3.1181 - regression_loss: 2.3344 - classification_loss: 0.7837
 506/1000 [==============>...............] - ETA: 3:43 - loss: 3.1119 - regression_loss: 2.3298 - classification_loss: 0.7821
 507/1000 [==============>...............] - ETA: 3:42 - loss: 3.1127 - regression_loss: 2.3313 - classification_loss: 0.7815
 508/1000 [==============>...............] - ETA: 3:42 - loss: 3.1128 - regression_loss: 2.3315 - classification_loss: 0.7814
 509/1000 [==============>...............] - ETA: 3:41 - loss: 3.1140 - regression_loss: 2.3316 - classification_loss: 0.7824
 510/1000 [==============>...............] - ETA: 3:41 - loss: 3.1139 - regression_loss: 2.3322 - classification_loss: 0.7818
 511/1000 [==============>...............] - ETA: 3:40 - loss: 3.1157 - regression_loss: 2.3343 - classification_loss: 0.7814
 512/1000 [==============>...............] - ETA: 3:40 - loss: 3.1156 - regression_loss: 2.3346 - classification_loss: 0.7810
 513/1000 [==============>...............] - ETA: 3:40 - loss: 3.1154 - regression_loss: 2.3349 - classification_loss: 0.7805
 514/1000 [==============>...............] - ETA: 3:39 - loss: 3.1167 - regression_loss: 2.3360 - classification_loss: 0.7807
 515/1000 [==============>...............] - ETA: 3:39 - loss: 3.1106 - regression_loss: 2.3315 - classification_loss: 0.7791
 516/1000 [==============>...............] - ETA: 3:38 - loss: 3.1046 - regression_loss: 2.3270 - classification_loss: 0.7776
 517/1000 [==============>...............] - ETA: 3:38 - loss: 3.1033 - regression_loss: 2.3264 - classification_loss: 0.7769
 518/1000 [==============>...............] - ETA: 3:37 - loss: 3.1047 - regression_loss: 2.3280 - classification_loss: 0.7768
 519/1000 [==============>...............] - ETA: 3:37 - loss: 3.1054 - regression_loss: 2.3288 - classification_loss: 0.7766
 520/1000 [==============>...............] - ETA: 3:36 - loss: 3.1089 - regression_loss: 2.3323 - classification_loss: 0.7766
 521/1000 [==============>...............] - ETA: 3:36 - loss: 3.1112 - regression_loss: 2.3332 - classification_loss: 0.7780
 522/1000 [==============>...............] - ETA: 3:36 - loss: 3.1112 - regression_loss: 2.3330 - classification_loss: 0.7782
 523/1000 [==============>...............] - ETA: 3:35 - loss: 3.1115 - regression_loss: 2.3327 - classification_loss: 0.7787
 524/1000 [==============>...............] - ETA: 3:35 - loss: 3.1125 - regression_loss: 2.3337 - classification_loss: 0.7787
 525/1000 [==============>...............] - ETA: 3:34 - loss: 3.1065 - regression_loss: 2.3293 - classification_loss: 0.7773
 526/1000 [==============>...............] - ETA: 3:34 - loss: 3.1097 - regression_loss: 2.3325 - classification_loss: 0.7772
 527/1000 [==============>...............] - ETA: 3:33 - loss: 3.1038 - regression_loss: 2.3281 - classification_loss: 0.7757
 528/1000 [==============>...............] - ETA: 3:33 - loss: 3.1047 - regression_loss: 2.3280 - classification_loss: 0.7766
 529/1000 [==============>...............] - ETA: 3:32 - loss: 3.1040 - regression_loss: 2.3284 - classification_loss: 0.7756
 530/1000 [==============>...............] - ETA: 3:32 - loss: 3.1029 - regression_loss: 2.3277 - classification_loss: 0.7752
 531/1000 [==============>...............] - ETA: 3:31 - loss: 3.1023 - regression_loss: 2.3281 - classification_loss: 0.7742
 532/1000 [==============>...............] - ETA: 3:31 - loss: 3.1037 - regression_loss: 2.3301 - classification_loss: 0.7737
 533/1000 [==============>...............] - ETA: 3:31 - loss: 3.1058 - regression_loss: 2.3320 - classification_loss: 0.7738
 534/1000 [===============>..............] - ETA: 3:30 - loss: 3.1038 - regression_loss: 2.3299 - classification_loss: 0.7739
 535/1000 [===============>..............] - ETA: 3:30 - loss: 3.1018 - regression_loss: 2.3288 - classification_loss: 0.7730
 536/1000 [===============>..............] - ETA: 3:29 - loss: 3.1009 - regression_loss: 2.3288 - classification_loss: 0.7721
 537/1000 [===============>..............] - ETA: 3:29 - loss: 3.1027 - regression_loss: 2.3297 - classification_loss: 0.7730
 538/1000 [===============>..............] - ETA: 3:28 - loss: 3.0969 - regression_loss: 2.3253 - classification_loss: 0.7716
 539/1000 [===============>..............] - ETA: 3:28 - loss: 3.0912 - regression_loss: 2.3210 - classification_loss: 0.7702
 540/1000 [===============>..............] - ETA: 3:27 - loss: 3.0889 - regression_loss: 2.3197 - classification_loss: 0.7692
 541/1000 [===============>..............] - ETA: 3:27 - loss: 3.0864 - regression_loss: 2.3154 - classification_loss: 0.7710
 542/1000 [===============>..............] - ETA: 3:26 - loss: 3.0865 - regression_loss: 2.3111 - classification_loss: 0.7754
 543/1000 [===============>..............] - ETA: 3:26 - loss: 3.0876 - regression_loss: 2.3113 - classification_loss: 0.7763
 544/1000 [===============>..............] - ETA: 3:26 - loss: 3.0896 - regression_loss: 2.3133 - classification_loss: 0.7762
 545/1000 [===============>..............] - ETA: 3:25 - loss: 3.0907 - regression_loss: 2.3131 - classification_loss: 0.7776
 546/1000 [===============>..............] - ETA: 3:25 - loss: 3.0924 - regression_loss: 2.3146 - classification_loss: 0.7778
 547/1000 [===============>..............] - ETA: 3:24 - loss: 3.0940 - regression_loss: 2.3162 - classification_loss: 0.7779
 548/1000 [===============>..............] - ETA: 3:24 - loss: 3.0970 - regression_loss: 2.3176 - classification_loss: 0.7794
 549/1000 [===============>..............] - ETA: 3:23 - loss: 3.0990 - regression_loss: 2.3188 - classification_loss: 0.7802
 550/1000 [===============>..............] - ETA: 3:23 - loss: 3.1011 - regression_loss: 2.3204 - classification_loss: 0.7807
 551/1000 [===============>..............] - ETA: 3:22 - loss: 3.1026 - regression_loss: 2.3225 - classification_loss: 0.7801
 552/1000 [===============>..............] - ETA: 3:22 - loss: 3.0971 - regression_loss: 2.3183 - classification_loss: 0.7788
 553/1000 [===============>..............] - ETA: 3:21 - loss: 3.0915 - regression_loss: 2.3141 - classification_loss: 0.7774
 554/1000 [===============>..............] - ETA: 3:21 - loss: 3.0940 - regression_loss: 2.3162 - classification_loss: 0.7778
 555/1000 [===============>..............] - ETA: 3:21 - loss: 3.0924 - regression_loss: 2.3152 - classification_loss: 0.7772
 556/1000 [===============>..............] - ETA: 3:20 - loss: 3.0868 - regression_loss: 2.3110 - classification_loss: 0.7758
 557/1000 [===============>..............] - ETA: 3:20 - loss: 3.0813 - regression_loss: 2.3069 - classification_loss: 0.7744
 558/1000 [===============>..............] - ETA: 3:19 - loss: 3.0835 - regression_loss: 2.3077 - classification_loss: 0.7758
 559/1000 [===============>..............] - ETA: 3:19 - loss: 3.0844 - regression_loss: 2.3081 - classification_loss: 0.7764
 560/1000 [===============>..............] - ETA: 3:18 - loss: 3.0848 - regression_loss: 2.3089 - classification_loss: 0.7759
 561/1000 [===============>..............] - ETA: 3:18 - loss: 3.0860 - regression_loss: 2.3089 - classification_loss: 0.7771
 562/1000 [===============>..............] - ETA: 3:17 - loss: 3.0887 - regression_loss: 2.3114 - classification_loss: 0.7773
 563/1000 [===============>..............] - ETA: 3:17 - loss: 3.0895 - regression_loss: 2.3123 - classification_loss: 0.7772
 564/1000 [===============>..............] - ETA: 3:17 - loss: 3.0840 - regression_loss: 2.3082 - classification_loss: 0.7758
 565/1000 [===============>..............] - ETA: 3:16 - loss: 3.0865 - regression_loss: 2.3102 - classification_loss: 0.7763
 566/1000 [===============>..............] - ETA: 3:16 - loss: 3.0886 - regression_loss: 2.3108 - classification_loss: 0.7778
 567/1000 [================>.............] - ETA: 3:15 - loss: 3.0917 - regression_loss: 2.3131 - classification_loss: 0.7785
 568/1000 [================>.............] - ETA: 3:15 - loss: 3.0932 - regression_loss: 2.3148 - classification_loss: 0.7784
 569/1000 [================>.............] - ETA: 3:14 - loss: 3.0974 - regression_loss: 2.3176 - classification_loss: 0.7798
 570/1000 [================>.............] - ETA: 3:14 - loss: 3.0920 - regression_loss: 2.3136 - classification_loss: 0.7784
 571/1000 [================>.............] - ETA: 3:13 - loss: 3.0974 - regression_loss: 2.3172 - classification_loss: 0.7802
 572/1000 [================>.............] - ETA: 3:13 - loss: 3.0961 - regression_loss: 2.3166 - classification_loss: 0.7795
 573/1000 [================>.............] - ETA: 3:12 - loss: 3.0992 - regression_loss: 2.3196 - classification_loss: 0.7796
 574/1000 [================>.............] - ETA: 3:12 - loss: 3.0938 - regression_loss: 2.3155 - classification_loss: 0.7783
 575/1000 [================>.............] - ETA: 3:12 - loss: 3.0975 - regression_loss: 2.3178 - classification_loss: 0.7797
 576/1000 [================>.............] - ETA: 3:11 - loss: 3.0999 - regression_loss: 2.3197 - classification_loss: 0.7801
 577/1000 [================>.............] - ETA: 3:11 - loss: 3.1012 - regression_loss: 2.3214 - classification_loss: 0.7799
 578/1000 [================>.............] - ETA: 3:10 - loss: 3.1049 - regression_loss: 2.3238 - classification_loss: 0.7810
 579/1000 [================>.............] - ETA: 3:10 - loss: 3.1089 - regression_loss: 2.3270 - classification_loss: 0.7819
 580/1000 [================>.............] - ETA: 3:09 - loss: 3.1070 - regression_loss: 2.3262 - classification_loss: 0.7808
 581/1000 [================>.............] - ETA: 3:09 - loss: 3.1066 - regression_loss: 2.3266 - classification_loss: 0.7800
 582/1000 [================>.............] - ETA: 3:08 - loss: 3.1077 - regression_loss: 2.3267 - classification_loss: 0.7810
 583/1000 [================>.............] - ETA: 3:08 - loss: 3.1081 - regression_loss: 2.3277 - classification_loss: 0.7805
 584/1000 [================>.............] - ETA: 3:07 - loss: 3.1089 - regression_loss: 2.3286 - classification_loss: 0.7803
 585/1000 [================>.............] - ETA: 3:07 - loss: 3.1127 - regression_loss: 2.3316 - classification_loss: 0.7810
 586/1000 [================>.............] - ETA: 3:07 - loss: 3.1073 - regression_loss: 2.3277 - classification_loss: 0.7797
 587/1000 [================>.............] - ETA: 3:06 - loss: 3.1111 - regression_loss: 2.3307 - classification_loss: 0.7804
 588/1000 [================>.............] - ETA: 3:06 - loss: 3.1125 - regression_loss: 2.3319 - classification_loss: 0.7807
 589/1000 [================>.............] - ETA: 3:05 - loss: 3.1072 - regression_loss: 2.3279 - classification_loss: 0.7793
 590/1000 [================>.............] - ETA: 3:05 - loss: 3.1097 - regression_loss: 2.3292 - classification_loss: 0.7804
 591/1000 [================>.............] - ETA: 3:04 - loss: 3.1106 - regression_loss: 2.3307 - classification_loss: 0.7800
 592/1000 [================>.............] - ETA: 3:04 - loss: 3.1119 - regression_loss: 2.3305 - classification_loss: 0.7814
 593/1000 [================>.............] - ETA: 3:03 - loss: 3.1067 - regression_loss: 2.3266 - classification_loss: 0.7800
 594/1000 [================>.............] - ETA: 3:03 - loss: 3.1067 - regression_loss: 2.3270 - classification_loss: 0.7797
 595/1000 [================>.............] - ETA: 3:02 - loss: 3.1015 - regression_loss: 2.3231 - classification_loss: 0.7784
 596/1000 [================>.............] - ETA: 3:02 - loss: 3.1045 - regression_loss: 2.3253 - classification_loss: 0.7792
 597/1000 [================>.............] - ETA: 3:02 - loss: 3.1074 - regression_loss: 2.3276 - classification_loss: 0.7798
 598/1000 [================>.............] - ETA: 3:01 - loss: 3.1095 - regression_loss: 2.3295 - classification_loss: 0.7800
 599/1000 [================>.............] - ETA: 3:01 - loss: 3.1100 - regression_loss: 2.3297 - classification_loss: 0.7803
 600/1000 [=================>............] - ETA: 3:00 - loss: 3.1102 - regression_loss: 2.3297 - classification_loss: 0.7805
 601/1000 [=================>............] - ETA: 3:00 - loss: 3.1051 - regression_loss: 2.3258 - classification_loss: 0.7792
 602/1000 [=================>............] - ETA: 2:59 - loss: 3.1062 - regression_loss: 2.3258 - classification_loss: 0.7804
 603/1000 [=================>............] - ETA: 2:59 - loss: 3.1097 - regression_loss: 2.3286 - classification_loss: 0.7812
 604/1000 [=================>............] - ETA: 2:58 - loss: 3.1141 - regression_loss: 2.3314 - classification_loss: 0.7827
 605/1000 [=================>............] - ETA: 2:58 - loss: 3.1147 - regression_loss: 2.3320 - classification_loss: 0.7827
 606/1000 [=================>............] - ETA: 2:58 - loss: 3.1096 - regression_loss: 2.3281 - classification_loss: 0.7814
 607/1000 [=================>............] - ETA: 2:57 - loss: 3.1126 - regression_loss: 2.3304 - classification_loss: 0.7823
 608/1000 [=================>............] - ETA: 2:57 - loss: 3.1117 - regression_loss: 2.3295 - classification_loss: 0.7822
 609/1000 [=================>............] - ETA: 2:56 - loss: 3.1122 - regression_loss: 2.3301 - classification_loss: 0.7821
 610/1000 [=================>............] - ETA: 2:56 - loss: 3.1113 - regression_loss: 2.3298 - classification_loss: 0.7815
 611/1000 [=================>............] - ETA: 2:55 - loss: 3.1126 - regression_loss: 2.3308 - classification_loss: 0.7819
 612/1000 [=================>............] - ETA: 2:55 - loss: 3.1138 - regression_loss: 2.3313 - classification_loss: 0.7825
 613/1000 [=================>............] - ETA: 2:54 - loss: 3.1153 - regression_loss: 2.3330 - classification_loss: 0.7823
 614/1000 [=================>............] - ETA: 2:54 - loss: 3.1148 - regression_loss: 2.3331 - classification_loss: 0.7817
 615/1000 [=================>............] - ETA: 2:53 - loss: 3.1173 - regression_loss: 2.3354 - classification_loss: 0.7819
 616/1000 [=================>............] - ETA: 2:53 - loss: 3.1177 - regression_loss: 2.3363 - classification_loss: 0.7814
 617/1000 [=================>............] - ETA: 2:53 - loss: 3.1181 - regression_loss: 2.3371 - classification_loss: 0.7810
 618/1000 [=================>............] - ETA: 2:52 - loss: 3.1212 - regression_loss: 2.3390 - classification_loss: 0.7822
 619/1000 [=================>............] - ETA: 2:52 - loss: 3.1209 - regression_loss: 2.3389 - classification_loss: 0.7820
 620/1000 [=================>............] - ETA: 2:51 - loss: 3.1235 - regression_loss: 2.3412 - classification_loss: 0.7823
 621/1000 [=================>............] - ETA: 2:51 - loss: 3.1250 - regression_loss: 2.3426 - classification_loss: 0.7823
 622/1000 [=================>............] - ETA: 2:50 - loss: 3.1295 - regression_loss: 2.3457 - classification_loss: 0.7838
 623/1000 [=================>............] - ETA: 2:50 - loss: 3.1253 - regression_loss: 2.3420 - classification_loss: 0.7834
 624/1000 [=================>............] - ETA: 2:49 - loss: 3.1247 - regression_loss: 2.3415 - classification_loss: 0.7831
 625/1000 [=================>............] - ETA: 2:49 - loss: 3.1239 - regression_loss: 2.3414 - classification_loss: 0.7825
 626/1000 [=================>............] - ETA: 2:49 - loss: 3.1227 - regression_loss: 2.3408 - classification_loss: 0.7819
 627/1000 [=================>............] - ETA: 2:48 - loss: 3.1230 - regression_loss: 2.3414 - classification_loss: 0.7816
 628/1000 [=================>............] - ETA: 2:48 - loss: 3.1244 - regression_loss: 2.3432 - classification_loss: 0.7812
 629/1000 [=================>............] - ETA: 2:47 - loss: 3.1196 - regression_loss: 2.3395 - classification_loss: 0.7801
 630/1000 [=================>............] - ETA: 2:47 - loss: 3.1185 - regression_loss: 2.3389 - classification_loss: 0.7796
 631/1000 [=================>............] - ETA: 2:46 - loss: 3.1136 - regression_loss: 2.3352 - classification_loss: 0.7784
 632/1000 [=================>............] - ETA: 2:46 - loss: 3.1155 - regression_loss: 2.3371 - classification_loss: 0.7783
 633/1000 [=================>............] - ETA: 2:45 - loss: 3.1106 - regression_loss: 2.3335 - classification_loss: 0.7772
 634/1000 [==================>...........] - ETA: 2:45 - loss: 3.1124 - regression_loss: 2.3346 - classification_loss: 0.7778
 635/1000 [==================>...........] - ETA: 2:44 - loss: 3.1075 - regression_loss: 2.3309 - classification_loss: 0.7766
 636/1000 [==================>...........] - ETA: 2:44 - loss: 3.1026 - regression_loss: 2.3273 - classification_loss: 0.7753
 637/1000 [==================>...........] - ETA: 2:44 - loss: 3.0977 - regression_loss: 2.3236 - classification_loss: 0.7741
 638/1000 [==================>...........] - ETA: 2:43 - loss: 3.0984 - regression_loss: 2.3239 - classification_loss: 0.7745
 639/1000 [==================>...........] - ETA: 2:43 - loss: 3.0985 - regression_loss: 2.3242 - classification_loss: 0.7743
 640/1000 [==================>...........] - ETA: 2:42 - loss: 3.0985 - regression_loss: 2.3243 - classification_loss: 0.7742
 641/1000 [==================>...........] - ETA: 2:42 - loss: 3.0990 - regression_loss: 2.3247 - classification_loss: 0.7743
 642/1000 [==================>...........] - ETA: 2:41 - loss: 3.1033 - regression_loss: 2.3285 - classification_loss: 0.7748
 643/1000 [==================>...........] - ETA: 2:41 - loss: 3.0985 - regression_loss: 2.3249 - classification_loss: 0.7736
 644/1000 [==================>...........] - ETA: 2:40 - loss: 3.1020 - regression_loss: 2.3273 - classification_loss: 0.7748
 645/1000 [==================>...........] - ETA: 2:40 - loss: 3.1059 - regression_loss: 2.3295 - classification_loss: 0.7764
 646/1000 [==================>...........] - ETA: 2:40 - loss: 3.1037 - regression_loss: 2.3281 - classification_loss: 0.7756
 647/1000 [==================>...........] - ETA: 2:39 - loss: 3.1054 - regression_loss: 2.3291 - classification_loss: 0.7763
 648/1000 [==================>...........] - ETA: 2:39 - loss: 3.1114 - regression_loss: 2.3322 - classification_loss: 0.7792
 649/1000 [==================>...........] - ETA: 2:38 - loss: 3.1119 - regression_loss: 2.3327 - classification_loss: 0.7792
 650/1000 [==================>...........] - ETA: 2:38 - loss: 3.1142 - regression_loss: 2.3340 - classification_loss: 0.7802
 651/1000 [==================>...........] - ETA: 2:37 - loss: 3.1101 - regression_loss: 2.3304 - classification_loss: 0.7797
 652/1000 [==================>...........] - ETA: 2:37 - loss: 3.1115 - regression_loss: 2.3313 - classification_loss: 0.7802
 653/1000 [==================>...........] - ETA: 2:36 - loss: 3.1068 - regression_loss: 2.3278 - classification_loss: 0.7790
 654/1000 [==================>...........] - ETA: 2:36 - loss: 3.1062 - regression_loss: 2.3275 - classification_loss: 0.7787
 655/1000 [==================>...........] - ETA: 2:35 - loss: 3.1089 - regression_loss: 2.3291 - classification_loss: 0.7798
 656/1000 [==================>...........] - ETA: 2:35 - loss: 3.1041 - regression_loss: 2.3256 - classification_loss: 0.7786
 657/1000 [==================>...........] - ETA: 2:35 - loss: 3.1074 - regression_loss: 2.3282 - classification_loss: 0.7791
 658/1000 [==================>...........] - ETA: 2:34 - loss: 3.1027 - regression_loss: 2.3247 - classification_loss: 0.7780
 659/1000 [==================>...........] - ETA: 2:34 - loss: 3.1079 - regression_loss: 2.3284 - classification_loss: 0.7795
 660/1000 [==================>...........] - ETA: 2:33 - loss: 3.1078 - regression_loss: 2.3288 - classification_loss: 0.7789
 661/1000 [==================>...........] - ETA: 2:33 - loss: 3.1031 - regression_loss: 2.3253 - classification_loss: 0.7778
 662/1000 [==================>...........] - ETA: 2:32 - loss: 3.1045 - regression_loss: 2.3267 - classification_loss: 0.7778
 663/1000 [==================>...........] - ETA: 2:32 - loss: 3.0998 - regression_loss: 2.3231 - classification_loss: 0.7766
 664/1000 [==================>...........] - ETA: 2:31 - loss: 3.0992 - regression_loss: 2.3229 - classification_loss: 0.7764
 665/1000 [==================>...........] - ETA: 2:31 - loss: 3.0986 - regression_loss: 2.3221 - classification_loss: 0.7765
 666/1000 [==================>...........] - ETA: 2:30 - loss: 3.1013 - regression_loss: 2.3232 - classification_loss: 0.7781
 667/1000 [===================>..........] - ETA: 2:30 - loss: 3.1028 - regression_loss: 2.3248 - classification_loss: 0.7780
 668/1000 [===================>..........] - ETA: 2:30 - loss: 3.1060 - regression_loss: 2.3265 - classification_loss: 0.7795
 669/1000 [===================>..........] - ETA: 2:29 - loss: 3.1014 - regression_loss: 2.3230 - classification_loss: 0.7783
 670/1000 [===================>..........] - ETA: 2:29 - loss: 3.1029 - regression_loss: 2.3235 - classification_loss: 0.7794
 671/1000 [===================>..........] - ETA: 2:28 - loss: 3.1043 - regression_loss: 2.3248 - classification_loss: 0.7794
 672/1000 [===================>..........] - ETA: 2:28 - loss: 3.1047 - regression_loss: 2.3257 - classification_loss: 0.7789
 673/1000 [===================>..........] - ETA: 2:27 - loss: 3.1044 - regression_loss: 2.3250 - classification_loss: 0.7793
 674/1000 [===================>..........] - ETA: 2:27 - loss: 3.1075 - regression_loss: 2.3267 - classification_loss: 0.7808
 675/1000 [===================>..........] - ETA: 2:26 - loss: 3.1080 - regression_loss: 2.3274 - classification_loss: 0.7806
 676/1000 [===================>..........] - ETA: 2:26 - loss: 3.1118 - regression_loss: 2.3292 - classification_loss: 0.7827
 677/1000 [===================>..........] - ETA: 2:26 - loss: 3.1138 - regression_loss: 2.3304 - classification_loss: 0.7834
 678/1000 [===================>..........] - ETA: 2:25 - loss: 3.1136 - regression_loss: 2.3299 - classification_loss: 0.7836
 679/1000 [===================>..........] - ETA: 2:25 - loss: 3.1090 - regression_loss: 2.3265 - classification_loss: 0.7825
 680/1000 [===================>..........] - ETA: 2:24 - loss: 3.1097 - regression_loss: 2.3277 - classification_loss: 0.7820
 681/1000 [===================>..........] - ETA: 2:24 - loss: 3.1051 - regression_loss: 2.3243 - classification_loss: 0.7808
 682/1000 [===================>..........] - ETA: 2:23 - loss: 3.1006 - regression_loss: 2.3209 - classification_loss: 0.7797
 683/1000 [===================>..........] - ETA: 2:23 - loss: 3.1010 - regression_loss: 2.3215 - classification_loss: 0.7795
 684/1000 [===================>..........] - ETA: 2:22 - loss: 3.0965 - regression_loss: 2.3181 - classification_loss: 0.7784
 685/1000 [===================>..........] - ETA: 2:22 - loss: 3.0958 - regression_loss: 2.3180 - classification_loss: 0.7778
 686/1000 [===================>..........] - ETA: 2:21 - loss: 3.0959 - regression_loss: 2.3182 - classification_loss: 0.7776
 687/1000 [===================>..........] - ETA: 2:21 - loss: 3.0961 - regression_loss: 2.3188 - classification_loss: 0.7773
 688/1000 [===================>..........] - ETA: 2:21 - loss: 3.0989 - regression_loss: 2.3203 - classification_loss: 0.7786
 689/1000 [===================>..........] - ETA: 2:20 - loss: 3.0994 - regression_loss: 2.3204 - classification_loss: 0.7790
 690/1000 [===================>..........] - ETA: 2:20 - loss: 3.1013 - regression_loss: 2.3216 - classification_loss: 0.7797
 691/1000 [===================>..........] - ETA: 2:19 - loss: 3.1023 - regression_loss: 2.3214 - classification_loss: 0.7809
 692/1000 [===================>..........] - ETA: 2:19 - loss: 3.1032 - regression_loss: 2.3212 - classification_loss: 0.7819
 693/1000 [===================>..........] - ETA: 2:18 - loss: 3.0987 - regression_loss: 2.3179 - classification_loss: 0.7808
 694/1000 [===================>..........] - ETA: 2:18 - loss: 3.1021 - regression_loss: 2.3199 - classification_loss: 0.7823
 695/1000 [===================>..........] - ETA: 2:17 - loss: 3.1029 - regression_loss: 2.3201 - classification_loss: 0.7828
 696/1000 [===================>..........] - ETA: 2:17 - loss: 3.1021 - regression_loss: 2.3197 - classification_loss: 0.7824
 697/1000 [===================>..........] - ETA: 2:16 - loss: 3.1023 - regression_loss: 2.3203 - classification_loss: 0.7820
 698/1000 [===================>..........] - ETA: 2:16 - loss: 3.0979 - regression_loss: 2.3170 - classification_loss: 0.7809
 699/1000 [===================>..........] - ETA: 2:16 - loss: 3.0984 - regression_loss: 2.3173 - classification_loss: 0.7811
 700/1000 [====================>.........] - ETA: 2:15 - loss: 3.1024 - regression_loss: 2.3196 - classification_loss: 0.7828
 701/1000 [====================>.........] - ETA: 2:15 - loss: 3.1075 - regression_loss: 2.3222 - classification_loss: 0.7853
 702/1000 [====================>.........] - ETA: 2:14 - loss: 3.1091 - regression_loss: 2.3230 - classification_loss: 0.7861
 703/1000 [====================>.........] - ETA: 2:14 - loss: 3.1094 - regression_loss: 2.3236 - classification_loss: 0.7857
 704/1000 [====================>.........] - ETA: 2:13 - loss: 3.1050 - regression_loss: 2.3203 - classification_loss: 0.7846
 705/1000 [====================>.........] - ETA: 2:13 - loss: 3.1067 - regression_loss: 2.3220 - classification_loss: 0.7847
 706/1000 [====================>.........] - ETA: 2:12 - loss: 3.1074 - regression_loss: 2.3229 - classification_loss: 0.7845
 707/1000 [====================>.........] - ETA: 2:12 - loss: 3.1088 - regression_loss: 2.3245 - classification_loss: 0.7843
 708/1000 [====================>.........] - ETA: 2:11 - loss: 3.1095 - regression_loss: 2.3255 - classification_loss: 0.7841
 709/1000 [====================>.........] - ETA: 2:11 - loss: 3.1052 - regression_loss: 2.3222 - classification_loss: 0.7830
 710/1000 [====================>.........] - ETA: 2:11 - loss: 3.1056 - regression_loss: 2.3221 - classification_loss: 0.7836
 711/1000 [====================>.........] - ETA: 2:10 - loss: 3.1056 - regression_loss: 2.3226 - classification_loss: 0.7830
 712/1000 [====================>.........] - ETA: 2:10 - loss: 3.1091 - regression_loss: 2.3255 - classification_loss: 0.7836
 713/1000 [====================>.........] - ETA: 2:09 - loss: 3.1047 - regression_loss: 2.3223 - classification_loss: 0.7825
 714/1000 [====================>.........] - ETA: 2:09 - loss: 3.1175 - regression_loss: 2.3333 - classification_loss: 0.7842
 715/1000 [====================>.........] - ETA: 2:08 - loss: 3.1173 - regression_loss: 2.3337 - classification_loss: 0.7836
 716/1000 [====================>.........] - ETA: 2:08 - loss: 3.1176 - regression_loss: 2.3340 - classification_loss: 0.7836
 717/1000 [====================>.........] - ETA: 2:07 - loss: 3.1185 - regression_loss: 2.3349 - classification_loss: 0.7836
 718/1000 [====================>.........] - ETA: 2:07 - loss: 3.1151 - regression_loss: 2.3317 - classification_loss: 0.7834
 719/1000 [====================>.........] - ETA: 2:07 - loss: 3.1151 - regression_loss: 2.3319 - classification_loss: 0.7833
 720/1000 [====================>.........] - ETA: 2:06 - loss: 3.1169 - regression_loss: 2.3330 - classification_loss: 0.7839
 721/1000 [====================>.........] - ETA: 2:06 - loss: 3.1176 - regression_loss: 2.3332 - classification_loss: 0.7844
 722/1000 [====================>.........] - ETA: 2:05 - loss: 3.1185 - regression_loss: 2.3336 - classification_loss: 0.7849
 723/1000 [====================>.........] - ETA: 2:05 - loss: 3.1196 - regression_loss: 2.3344 - classification_loss: 0.7852
 724/1000 [====================>.........] - ETA: 2:04 - loss: 3.1153 - regression_loss: 2.3312 - classification_loss: 0.7841
 725/1000 [====================>.........] - ETA: 2:04 - loss: 3.1159 - regression_loss: 2.3315 - classification_loss: 0.7844
 726/1000 [====================>.........] - ETA: 2:03 - loss: 3.1168 - regression_loss: 2.3323 - classification_loss: 0.7845
 727/1000 [====================>.........] - ETA: 2:03 - loss: 3.1183 - regression_loss: 2.3327 - classification_loss: 0.7856
 728/1000 [====================>.........] - ETA: 2:02 - loss: 3.1140 - regression_loss: 2.3295 - classification_loss: 0.7845
 729/1000 [====================>.........] - ETA: 2:02 - loss: 3.1100 - regression_loss: 2.3263 - classification_loss: 0.7836
 730/1000 [====================>.........] - ETA: 2:02 - loss: 3.1057 - regression_loss: 2.3231 - classification_loss: 0.7826
 731/1000 [====================>.........] - ETA: 2:01 - loss: 3.1080 - regression_loss: 2.3249 - classification_loss: 0.7830
 732/1000 [====================>.........] - ETA: 2:01 - loss: 3.1098 - regression_loss: 2.3257 - classification_loss: 0.7841
 733/1000 [====================>.........] - ETA: 2:00 - loss: 3.1111 - regression_loss: 2.3269 - classification_loss: 0.7842
 734/1000 [=====================>........] - ETA: 2:00 - loss: 3.1146 - regression_loss: 2.3285 - classification_loss: 0.7861
 735/1000 [=====================>........] - ETA: 1:59 - loss: 3.1151 - regression_loss: 2.3286 - classification_loss: 0.7865
 736/1000 [=====================>........] - ETA: 1:59 - loss: 3.1182 - regression_loss: 2.3313 - classification_loss: 0.7869
 737/1000 [=====================>........] - ETA: 1:58 - loss: 3.1190 - regression_loss: 2.3321 - classification_loss: 0.7869
 738/1000 [=====================>........] - ETA: 1:58 - loss: 3.1148 - regression_loss: 2.3289 - classification_loss: 0.7859
 739/1000 [=====================>........] - ETA: 1:57 - loss: 3.1107 - regression_loss: 2.3258 - classification_loss: 0.7849
 740/1000 [=====================>........] - ETA: 1:57 - loss: 3.1120 - regression_loss: 2.3270 - classification_loss: 0.7849
 741/1000 [=====================>........] - ETA: 1:57 - loss: 3.1131 - regression_loss: 2.3274 - classification_loss: 0.7856
 742/1000 [=====================>........] - ETA: 1:56 - loss: 3.1150 - regression_loss: 2.3288 - classification_loss: 0.7862
 743/1000 [=====================>........] - ETA: 1:56 - loss: 3.1154 - regression_loss: 2.3291 - classification_loss: 0.7864
 744/1000 [=====================>........] - ETA: 1:55 - loss: 3.1173 - regression_loss: 2.3304 - classification_loss: 0.7869
 745/1000 [=====================>........] - ETA: 1:55 - loss: 3.1184 - regression_loss: 2.3310 - classification_loss: 0.7875
 746/1000 [=====================>........] - ETA: 1:54 - loss: 3.1188 - regression_loss: 2.3317 - classification_loss: 0.7871
 747/1000 [=====================>........] - ETA: 1:54 - loss: 3.1204 - regression_loss: 2.3323 - classification_loss: 0.7881
 748/1000 [=====================>........] - ETA: 1:53 - loss: 3.1162 - regression_loss: 2.3291 - classification_loss: 0.7871
 749/1000 [=====================>........] - ETA: 1:53 - loss: 3.1174 - regression_loss: 2.3298 - classification_loss: 0.7876
 750/1000 [=====================>........] - ETA: 1:53 - loss: 3.1177 - regression_loss: 2.3298 - classification_loss: 0.7880
 751/1000 [=====================>........] - ETA: 1:52 - loss: 3.1199 - regression_loss: 2.3317 - classification_loss: 0.7882
 752/1000 [=====================>........] - ETA: 1:52 - loss: 3.1157 - regression_loss: 2.3286 - classification_loss: 0.7871
 753/1000 [=====================>........] - ETA: 1:51 - loss: 3.1166 - regression_loss: 2.3294 - classification_loss: 0.7872
 754/1000 [=====================>........] - ETA: 1:51 - loss: 3.1179 - regression_loss: 2.3302 - classification_loss: 0.7877
 755/1000 [=====================>........] - ETA: 1:50 - loss: 3.1214 - regression_loss: 2.3335 - classification_loss: 0.7879
 756/1000 [=====================>........] - ETA: 1:50 - loss: 3.1173 - regression_loss: 2.3304 - classification_loss: 0.7869
 757/1000 [=====================>........] - ETA: 1:49 - loss: 3.1168 - regression_loss: 2.3302 - classification_loss: 0.7867
 758/1000 [=====================>........] - ETA: 1:49 - loss: 3.1166 - regression_loss: 2.3300 - classification_loss: 0.7866
 759/1000 [=====================>........] - ETA: 1:48 - loss: 3.1220 - regression_loss: 2.3353 - classification_loss: 0.7867
 760/1000 [=====================>........] - ETA: 1:48 - loss: 3.1179 - regression_loss: 2.3322 - classification_loss: 0.7857
 761/1000 [=====================>........] - ETA: 1:48 - loss: 3.1180 - regression_loss: 2.3322 - classification_loss: 0.7858
 762/1000 [=====================>........] - ETA: 1:47 - loss: 3.1211 - regression_loss: 2.3340 - classification_loss: 0.7871
 763/1000 [=====================>........] - ETA: 1:47 - loss: 3.1170 - regression_loss: 2.3309 - classification_loss: 0.7861
 764/1000 [=====================>........] - ETA: 1:46 - loss: 3.1129 - regression_loss: 2.3278 - classification_loss: 0.7851
 765/1000 [=====================>........] - ETA: 1:46 - loss: 3.1136 - regression_loss: 2.3285 - classification_loss: 0.7851
 766/1000 [=====================>........] - ETA: 1:45 - loss: 3.1096 - regression_loss: 2.3255 - classification_loss: 0.7841
 767/1000 [======================>.......] - ETA: 1:45 - loss: 3.1121 - regression_loss: 2.3279 - classification_loss: 0.7842
 768/1000 [======================>.......] - ETA: 1:44 - loss: 3.1123 - regression_loss: 2.3280 - classification_loss: 0.7842
 769/1000 [======================>.......] - ETA: 1:44 - loss: 3.1121 - regression_loss: 2.3280 - classification_loss: 0.7840
 770/1000 [======================>.......] - ETA: 1:43 - loss: 3.1123 - regression_loss: 2.3285 - classification_loss: 0.7838
 771/1000 [======================>.......] - ETA: 1:43 - loss: 3.1153 - regression_loss: 2.3303 - classification_loss: 0.7850
 772/1000 [======================>.......] - ETA: 1:43 - loss: 3.1158 - regression_loss: 2.3307 - classification_loss: 0.7851
 773/1000 [======================>.......] - ETA: 1:42 - loss: 3.1182 - regression_loss: 2.3326 - classification_loss: 0.7856
 774/1000 [======================>.......] - ETA: 1:42 - loss: 3.1141 - regression_loss: 2.3296 - classification_loss: 0.7846
 775/1000 [======================>.......] - ETA: 1:41 - loss: 3.1143 - regression_loss: 2.3298 - classification_loss: 0.7845
 776/1000 [======================>.......] - ETA: 1:41 - loss: 3.1156 - regression_loss: 2.3306 - classification_loss: 0.7850
 777/1000 [======================>.......] - ETA: 1:40 - loss: 3.1177 - regression_loss: 2.3316 - classification_loss: 0.7860
 778/1000 [======================>.......] - ETA: 1:40 - loss: 3.1189 - regression_loss: 2.3323 - classification_loss: 0.7866
 779/1000 [======================>.......] - ETA: 1:39 - loss: 3.1149 - regression_loss: 2.3293 - classification_loss: 0.7856
 780/1000 [======================>.......] - ETA: 1:39 - loss: 3.1156 - regression_loss: 2.3292 - classification_loss: 0.7864
 781/1000 [======================>.......] - ETA: 1:38 - loss: 3.1150 - regression_loss: 2.3290 - classification_loss: 0.7860
 782/1000 [======================>.......] - ETA: 1:38 - loss: 3.1177 - regression_loss: 2.3308 - classification_loss: 0.7868
 783/1000 [======================>.......] - ETA: 1:38 - loss: 3.1137 - regression_loss: 2.3279 - classification_loss: 0.7858
 784/1000 [======================>.......] - ETA: 1:37 - loss: 3.1148 - regression_loss: 2.3294 - classification_loss: 0.7854
 785/1000 [======================>.......] - ETA: 1:37 - loss: 3.1184 - regression_loss: 2.3312 - classification_loss: 0.7872
 786/1000 [======================>.......] - ETA: 1:36 - loss: 3.1187 - regression_loss: 2.3312 - classification_loss: 0.7875
 787/1000 [======================>.......] - ETA: 1:36 - loss: 3.1204 - regression_loss: 2.3322 - classification_loss: 0.7882
 788/1000 [======================>.......] - ETA: 1:35 - loss: 3.1164 - regression_loss: 2.3293 - classification_loss: 0.7872
 789/1000 [======================>.......] - ETA: 1:35 - loss: 3.1160 - regression_loss: 2.3293 - classification_loss: 0.7867
 790/1000 [======================>.......] - ETA: 1:34 - loss: 3.1175 - regression_loss: 2.3302 - classification_loss: 0.7873
 791/1000 [======================>.......] - ETA: 1:34 - loss: 3.1136 - regression_loss: 2.3272 - classification_loss: 0.7863
 792/1000 [======================>.......] - ETA: 1:34 - loss: 3.1096 - regression_loss: 2.3243 - classification_loss: 0.7853
 793/1000 [======================>.......] - ETA: 1:33 - loss: 3.1101 - regression_loss: 2.3247 - classification_loss: 0.7854
 794/1000 [======================>.......] - ETA: 1:33 - loss: 3.1129 - regression_loss: 2.3261 - classification_loss: 0.7868
 795/1000 [======================>.......] - ETA: 1:32 - loss: 3.1090 - regression_loss: 2.3232 - classification_loss: 0.7858
 796/1000 [======================>.......] - ETA: 1:32 - loss: 3.1091 - regression_loss: 2.3236 - classification_loss: 0.7855
 797/1000 [======================>.......] - ETA: 1:31 - loss: 3.1108 - regression_loss: 2.3243 - classification_loss: 0.7865
 798/1000 [======================>.......] - ETA: 1:31 - loss: 3.1123 - regression_loss: 2.3259 - classification_loss: 0.7864
 799/1000 [======================>.......] - ETA: 1:30 - loss: 3.1134 - regression_loss: 2.3265 - classification_loss: 0.7870
 800/1000 [=======================>......] - ETA: 1:30 - loss: 3.1149 - regression_loss: 2.3270 - classification_loss: 0.7878
 801/1000 [=======================>......] - ETA: 1:29 - loss: 3.1110 - regression_loss: 2.3241 - classification_loss: 0.7868
 802/1000 [=======================>......] - ETA: 1:29 - loss: 3.1071 - regression_loss: 2.3212 - classification_loss: 0.7858
 803/1000 [=======================>......] - ETA: 1:29 - loss: 3.1060 - regression_loss: 2.3204 - classification_loss: 0.7856
 804/1000 [=======================>......] - ETA: 1:28 - loss: 3.1063 - regression_loss: 2.3205 - classification_loss: 0.7858
 805/1000 [=======================>......] - ETA: 1:28 - loss: 3.1088 - regression_loss: 2.3215 - classification_loss: 0.7874
 806/1000 [=======================>......] - ETA: 1:27 - loss: 3.1051 - regression_loss: 2.3186 - classification_loss: 0.7865
 807/1000 [=======================>......] - ETA: 1:27 - loss: 3.1051 - regression_loss: 2.3190 - classification_loss: 0.7861
 808/1000 [=======================>......] - ETA: 1:26 - loss: 3.1061 - regression_loss: 2.3198 - classification_loss: 0.7863
 809/1000 [=======================>......] - ETA: 1:26 - loss: 3.1056 - regression_loss: 2.3197 - classification_loss: 0.7859
 810/1000 [=======================>......] - ETA: 1:25 - loss: 3.1094 - regression_loss: 2.3219 - classification_loss: 0.7875
 811/1000 [=======================>......] - ETA: 1:25 - loss: 3.1056 - regression_loss: 2.3191 - classification_loss: 0.7865
 812/1000 [=======================>......] - ETA: 1:24 - loss: 3.1079 - regression_loss: 2.3201 - classification_loss: 0.7877
 813/1000 [=======================>......] - ETA: 1:24 - loss: 3.1110 - regression_loss: 2.3223 - classification_loss: 0.7887
 814/1000 [=======================>......] - ETA: 1:24 - loss: 3.1132 - regression_loss: 2.3234 - classification_loss: 0.7898
 815/1000 [=======================>......] - ETA: 1:23 - loss: 3.1161 - regression_loss: 2.3249 - classification_loss: 0.7912
 816/1000 [=======================>......] - ETA: 1:23 - loss: 3.1188 - regression_loss: 2.3266 - classification_loss: 0.7922
 817/1000 [=======================>......] - ETA: 1:22 - loss: 3.1149 - regression_loss: 2.3237 - classification_loss: 0.7912
 818/1000 [=======================>......] - ETA: 1:22 - loss: 3.1158 - regression_loss: 2.3240 - classification_loss: 0.7918
 819/1000 [=======================>......] - ETA: 1:21 - loss: 3.1120 - regression_loss: 2.3212 - classification_loss: 0.7908
 820/1000 [=======================>......] - ETA: 1:21 - loss: 3.1124 - regression_loss: 2.3217 - classification_loss: 0.7907
 821/1000 [=======================>......] - ETA: 1:20 - loss: 3.1145 - regression_loss: 2.3229 - classification_loss: 0.7916
 822/1000 [=======================>......] - ETA: 1:20 - loss: 3.1107 - regression_loss: 2.3201 - classification_loss: 0.7906
 823/1000 [=======================>......] - ETA: 1:19 - loss: 3.1101 - regression_loss: 2.3199 - classification_loss: 0.7902
 824/1000 [=======================>......] - ETA: 1:19 - loss: 3.1123 - regression_loss: 2.3217 - classification_loss: 0.7906
 825/1000 [=======================>......] - ETA: 1:19 - loss: 3.1139 - regression_loss: 2.3234 - classification_loss: 0.7905
 826/1000 [=======================>......] - ETA: 1:18 - loss: 3.1101 - regression_loss: 2.3206 - classification_loss: 0.7896
 827/1000 [=======================>......] - ETA: 1:18 - loss: 3.1120 - regression_loss: 2.3222 - classification_loss: 0.7898
 828/1000 [=======================>......] - ETA: 1:17 - loss: 3.1142 - regression_loss: 2.3240 - classification_loss: 0.7902
 829/1000 [=======================>......] - ETA: 1:17 - loss: 3.1104 - regression_loss: 2.3212 - classification_loss: 0.7893
 830/1000 [=======================>......] - ETA: 1:16 - loss: 3.1118 - regression_loss: 2.3219 - classification_loss: 0.7899
 831/1000 [=======================>......] - ETA: 1:16 - loss: 3.1132 - regression_loss: 2.3222 - classification_loss: 0.7910
 832/1000 [=======================>......] - ETA: 1:15 - loss: 3.1095 - regression_loss: 2.3194 - classification_loss: 0.7901
 833/1000 [=======================>......] - ETA: 1:15 - loss: 3.1116 - regression_loss: 2.3208 - classification_loss: 0.7908
 834/1000 [========================>.....] - ETA: 1:15 - loss: 3.1140 - regression_loss: 2.3225 - classification_loss: 0.7914
 835/1000 [========================>.....] - ETA: 1:14 - loss: 3.1141 - regression_loss: 2.3230 - classification_loss: 0.7911
 836/1000 [========================>.....] - ETA: 1:14 - loss: 3.1148 - regression_loss: 2.3242 - classification_loss: 0.7906
 837/1000 [========================>.....] - ETA: 1:13 - loss: 3.1156 - regression_loss: 2.3251 - classification_loss: 0.7905
 838/1000 [========================>.....] - ETA: 1:13 - loss: 3.1119 - regression_loss: 2.3223 - classification_loss: 0.7895
 839/1000 [========================>.....] - ETA: 1:12 - loss: 3.1126 - regression_loss: 2.3231 - classification_loss: 0.7895
 840/1000 [========================>.....] - ETA: 1:12 - loss: 3.1140 - regression_loss: 2.3235 - classification_loss: 0.7906
 841/1000 [========================>.....] - ETA: 1:11 - loss: 3.1148 - regression_loss: 2.3244 - classification_loss: 0.7904
 842/1000 [========================>.....] - ETA: 1:11 - loss: 3.1142 - regression_loss: 2.3238 - classification_loss: 0.7904
 843/1000 [========================>.....] - ETA: 1:10 - loss: 3.1105 - regression_loss: 2.3210 - classification_loss: 0.7895
 844/1000 [========================>.....] - ETA: 1:10 - loss: 3.1108 - regression_loss: 2.3216 - classification_loss: 0.7892
 845/1000 [========================>.....] - ETA: 1:10 - loss: 3.1102 - regression_loss: 2.3215 - classification_loss: 0.7887
 846/1000 [========================>.....] - ETA: 1:09 - loss: 3.1121 - regression_loss: 2.3224 - classification_loss: 0.7897
 847/1000 [========================>.....] - ETA: 1:09 - loss: 3.1119 - regression_loss: 2.3224 - classification_loss: 0.7894
 848/1000 [========================>.....] - ETA: 1:08 - loss: 3.1115 - regression_loss: 2.3225 - classification_loss: 0.7890
 849/1000 [========================>.....] - ETA: 1:08 - loss: 3.1122 - regression_loss: 2.3229 - classification_loss: 0.7893
 850/1000 [========================>.....] - ETA: 1:07 - loss: 3.1131 - regression_loss: 2.3230 - classification_loss: 0.7901
 851/1000 [========================>.....] - ETA: 1:07 - loss: 3.1154 - regression_loss: 2.3252 - classification_loss: 0.7902
 852/1000 [========================>.....] - ETA: 1:06 - loss: 3.1118 - regression_loss: 2.3225 - classification_loss: 0.7893
 853/1000 [========================>.....] - ETA: 1:06 - loss: 3.1081 - regression_loss: 2.3198 - classification_loss: 0.7883
 854/1000 [========================>.....] - ETA: 1:05 - loss: 3.1114 - regression_loss: 2.3217 - classification_loss: 0.7898
 855/1000 [========================>.....] - ETA: 1:05 - loss: 3.1113 - regression_loss: 2.3221 - classification_loss: 0.7892
 856/1000 [========================>.....] - ETA: 1:05 - loss: 3.1117 - regression_loss: 2.3225 - classification_loss: 0.7892
 857/1000 [========================>.....] - ETA: 1:04 - loss: 3.1080 - regression_loss: 2.3198 - classification_loss: 0.7883
 858/1000 [========================>.....] - ETA: 1:04 - loss: 3.1079 - regression_loss: 2.3200 - classification_loss: 0.7879
 859/1000 [========================>.....] - ETA: 1:03 - loss: 3.1043 - regression_loss: 2.3173 - classification_loss: 0.7870
 860/1000 [========================>.....] - ETA: 1:03 - loss: 3.1046 - regression_loss: 2.3181 - classification_loss: 0.7865
 861/1000 [========================>.....] - ETA: 1:02 - loss: 3.1074 - regression_loss: 2.3204 - classification_loss: 0.7871
 862/1000 [========================>.....] - ETA: 1:02 - loss: 3.1065 - regression_loss: 2.3199 - classification_loss: 0.7866
 863/1000 [========================>.....] - ETA: 1:01 - loss: 3.1029 - regression_loss: 2.3173 - classification_loss: 0.7857
 864/1000 [========================>.....] - ETA: 1:01 - loss: 3.1059 - regression_loss: 2.3195 - classification_loss: 0.7865
 865/1000 [========================>.....] - ETA: 1:01 - loss: 3.1057 - regression_loss: 2.3195 - classification_loss: 0.7862
 866/1000 [========================>.....] - ETA: 1:00 - loss: 3.1071 - regression_loss: 2.3200 - classification_loss: 0.7871
 867/1000 [=========================>....] - ETA: 1:00 - loss: 3.1078 - regression_loss: 2.3201 - classification_loss: 0.7877
 868/1000 [=========================>....] - ETA: 59s - loss: 3.1042 - regression_loss: 2.3174 - classification_loss: 0.7868 
 869/1000 [=========================>....] - ETA: 59s - loss: 3.1040 - regression_loss: 2.3173 - classification_loss: 0.7867
 870/1000 [=========================>....] - ETA: 58s - loss: 3.1068 - regression_loss: 2.3191 - classification_loss: 0.7878
 871/1000 [=========================>....] - ETA: 58s - loss: 3.1080 - regression_loss: 2.3201 - classification_loss: 0.7879
 872/1000 [=========================>....] - ETA: 57s - loss: 3.1096 - regression_loss: 2.3213 - classification_loss: 0.7883
 873/1000 [=========================>....] - ETA: 57s - loss: 3.1111 - regression_loss: 2.3225 - classification_loss: 0.7886
 874/1000 [=========================>....] - ETA: 56s - loss: 3.1115 - regression_loss: 2.3223 - classification_loss: 0.7892
 875/1000 [=========================>....] - ETA: 56s - loss: 3.1108 - regression_loss: 2.3222 - classification_loss: 0.7887
 876/1000 [=========================>....] - ETA: 56s - loss: 3.1073 - regression_loss: 2.3195 - classification_loss: 0.7878
 877/1000 [=========================>....] - ETA: 55s - loss: 3.1096 - regression_loss: 2.3207 - classification_loss: 0.7889
 878/1000 [=========================>....] - ETA: 55s - loss: 3.1062 - regression_loss: 2.3180 - classification_loss: 0.7882
 879/1000 [=========================>....] - ETA: 54s - loss: 3.1047 - regression_loss: 2.3171 - classification_loss: 0.7876
 880/1000 [=========================>....] - ETA: 54s - loss: 3.1053 - regression_loss: 2.3179 - classification_loss: 0.7874
 881/1000 [=========================>....] - ETA: 53s - loss: 3.1044 - regression_loss: 2.3174 - classification_loss: 0.7870
 882/1000 [=========================>....] - ETA: 53s - loss: 3.1009 - regression_loss: 2.3148 - classification_loss: 0.7861
 883/1000 [=========================>....] - ETA: 52s - loss: 3.1024 - regression_loss: 2.3150 - classification_loss: 0.7874
 884/1000 [=========================>....] - ETA: 52s - loss: 3.1039 - regression_loss: 2.3162 - classification_loss: 0.7878
 885/1000 [=========================>....] - ETA: 51s - loss: 3.1057 - regression_loss: 2.3175 - classification_loss: 0.7882
 886/1000 [=========================>....] - ETA: 51s - loss: 3.1066 - regression_loss: 2.3179 - classification_loss: 0.7887
 887/1000 [=========================>....] - ETA: 51s - loss: 3.1072 - regression_loss: 2.3185 - classification_loss: 0.7886
 888/1000 [=========================>....] - ETA: 50s - loss: 3.1094 - regression_loss: 2.3198 - classification_loss: 0.7896
 889/1000 [=========================>....] - ETA: 50s - loss: 3.1097 - regression_loss: 2.3200 - classification_loss: 0.7897
 890/1000 [=========================>....] - ETA: 49s - loss: 3.1111 - regression_loss: 2.3214 - classification_loss: 0.7897
 891/1000 [=========================>....] - ETA: 49s - loss: 3.1142 - regression_loss: 2.3240 - classification_loss: 0.7902
 892/1000 [=========================>....] - ETA: 48s - loss: 3.1149 - regression_loss: 2.3248 - classification_loss: 0.7901
 893/1000 [=========================>....] - ETA: 48s - loss: 3.1119 - regression_loss: 2.3222 - classification_loss: 0.7897
 894/1000 [=========================>....] - ETA: 47s - loss: 3.1116 - regression_loss: 2.3216 - classification_loss: 0.7900
 895/1000 [=========================>....] - ETA: 47s - loss: 3.1115 - regression_loss: 2.3215 - classification_loss: 0.7899
 896/1000 [=========================>....] - ETA: 47s - loss: 3.1080 - regression_loss: 2.3189 - classification_loss: 0.7891
 897/1000 [=========================>....] - ETA: 46s - loss: 3.1101 - regression_loss: 2.3203 - classification_loss: 0.7898
 898/1000 [=========================>....] - ETA: 46s - loss: 3.1094 - regression_loss: 2.3200 - classification_loss: 0.7894
 899/1000 [=========================>....] - ETA: 45s - loss: 3.1084 - regression_loss: 2.3196 - classification_loss: 0.7888
 900/1000 [==========================>...] - ETA: 45s - loss: 3.1049 - regression_loss: 2.3170 - classification_loss: 0.7879
 901/1000 [==========================>...] - ETA: 44s - loss: 3.1070 - regression_loss: 2.3183 - classification_loss: 0.7887
 902/1000 [==========================>...] - ETA: 44s - loss: 3.1105 - regression_loss: 2.3204 - classification_loss: 0.7901
 903/1000 [==========================>...] - ETA: 43s - loss: 3.1116 - regression_loss: 2.3204 - classification_loss: 0.7912
 904/1000 [==========================>...] - ETA: 43s - loss: 3.1136 - regression_loss: 2.3217 - classification_loss: 0.7918
 905/1000 [==========================>...] - ETA: 42s - loss: 3.1149 - regression_loss: 2.3227 - classification_loss: 0.7923
 906/1000 [==========================>...] - ETA: 42s - loss: 3.1159 - regression_loss: 2.3239 - classification_loss: 0.7920
 907/1000 [==========================>...] - ETA: 42s - loss: 3.1168 - regression_loss: 2.3250 - classification_loss: 0.7919
 908/1000 [==========================>...] - ETA: 41s - loss: 3.1165 - regression_loss: 2.3250 - classification_loss: 0.7915
 909/1000 [==========================>...] - ETA: 41s - loss: 3.1161 - regression_loss: 2.3251 - classification_loss: 0.7910
 910/1000 [==========================>...] - ETA: 40s - loss: 3.1190 - regression_loss: 2.3276 - classification_loss: 0.7913
 911/1000 [==========================>...] - ETA: 40s - loss: 3.1189 - regression_loss: 2.3279 - classification_loss: 0.7911
 912/1000 [==========================>...] - ETA: 39s - loss: 3.1155 - regression_loss: 2.3253 - classification_loss: 0.7902
 913/1000 [==========================>...] - ETA: 39s - loss: 3.1177 - regression_loss: 2.3269 - classification_loss: 0.7908
 914/1000 [==========================>...] - ETA: 38s - loss: 3.1190 - regression_loss: 2.3280 - classification_loss: 0.7910
 915/1000 [==========================>...] - ETA: 38s - loss: 3.1202 - regression_loss: 2.3293 - classification_loss: 0.7910
 916/1000 [==========================>...] - ETA: 37s - loss: 3.1176 - regression_loss: 2.3267 - classification_loss: 0.7908
 917/1000 [==========================>...] - ETA: 37s - loss: 3.1177 - regression_loss: 2.3267 - classification_loss: 0.7910
 918/1000 [==========================>...] - ETA: 37s - loss: 3.1192 - regression_loss: 2.3280 - classification_loss: 0.7912
 919/1000 [==========================>...] - ETA: 36s - loss: 3.1207 - regression_loss: 2.3293 - classification_loss: 0.7914
 920/1000 [==========================>...] - ETA: 36s - loss: 3.1173 - regression_loss: 2.3267 - classification_loss: 0.7906
 921/1000 [==========================>...] - ETA: 35s - loss: 3.1177 - regression_loss: 2.3272 - classification_loss: 0.7905
 922/1000 [==========================>...] - ETA: 35s - loss: 3.1181 - regression_loss: 2.3278 - classification_loss: 0.7903
 923/1000 [==========================>...] - ETA: 34s - loss: 3.1186 - regression_loss: 2.3285 - classification_loss: 0.7901
 924/1000 [==========================>...] - ETA: 34s - loss: 3.1153 - regression_loss: 2.3260 - classification_loss: 0.7893
 925/1000 [==========================>...] - ETA: 33s - loss: 3.1166 - regression_loss: 2.3274 - classification_loss: 0.7892
 926/1000 [==========================>...] - ETA: 33s - loss: 3.1133 - regression_loss: 2.3249 - classification_loss: 0.7884
 927/1000 [==========================>...] - ETA: 32s - loss: 3.1139 - regression_loss: 2.3250 - classification_loss: 0.7889
 928/1000 [==========================>...] - ETA: 32s - loss: 3.1143 - regression_loss: 2.3259 - classification_loss: 0.7884
 929/1000 [==========================>...] - ETA: 32s - loss: 3.1109 - regression_loss: 2.3234 - classification_loss: 0.7876
 930/1000 [==========================>...] - ETA: 31s - loss: 3.1076 - regression_loss: 2.3209 - classification_loss: 0.7867
 931/1000 [==========================>...] - ETA: 31s - loss: 3.1043 - regression_loss: 2.3184 - classification_loss: 0.7859
 932/1000 [==========================>...] - ETA: 30s - loss: 3.1043 - regression_loss: 2.3189 - classification_loss: 0.7854
 933/1000 [==========================>...] - ETA: 30s - loss: 3.1047 - regression_loss: 2.3190 - classification_loss: 0.7857
 934/1000 [===========================>..] - ETA: 29s - loss: 3.1046 - regression_loss: 2.3191 - classification_loss: 0.7855
 935/1000 [===========================>..] - ETA: 29s - loss: 3.1087 - regression_loss: 2.3234 - classification_loss: 0.7853
 936/1000 [===========================>..] - ETA: 28s - loss: 3.1103 - regression_loss: 2.3249 - classification_loss: 0.7854
 937/1000 [===========================>..] - ETA: 28s - loss: 3.1070 - regression_loss: 2.3224 - classification_loss: 0.7846
 938/1000 [===========================>..] - ETA: 28s - loss: 3.1092 - regression_loss: 2.3248 - classification_loss: 0.7844
 939/1000 [===========================>..] - ETA: 27s - loss: 3.1116 - regression_loss: 2.3265 - classification_loss: 0.7851
 940/1000 [===========================>..] - ETA: 27s - loss: 3.1106 - regression_loss: 2.3258 - classification_loss: 0.7848
 941/1000 [===========================>..] - ETA: 26s - loss: 3.1127 - regression_loss: 2.3264 - classification_loss: 0.7863
 942/1000 [===========================>..] - ETA: 26s - loss: 3.1125 - regression_loss: 2.3268 - classification_loss: 0.7857
 943/1000 [===========================>..] - ETA: 25s - loss: 3.1092 - regression_loss: 2.3244 - classification_loss: 0.7849
 944/1000 [===========================>..] - ETA: 25s - loss: 3.1060 - regression_loss: 2.3219 - classification_loss: 0.7841
 945/1000 [===========================>..] - ETA: 24s - loss: 3.1068 - regression_loss: 2.3229 - classification_loss: 0.7839
 946/1000 [===========================>..] - ETA: 24s - loss: 3.1081 - regression_loss: 2.3238 - classification_loss: 0.7843
 947/1000 [===========================>..] - ETA: 23s - loss: 3.1098 - regression_loss: 2.3252 - classification_loss: 0.7846
 948/1000 [===========================>..] - ETA: 23s - loss: 3.1097 - regression_loss: 2.3253 - classification_loss: 0.7844
 949/1000 [===========================>..] - ETA: 23s - loss: 3.1125 - regression_loss: 2.3271 - classification_loss: 0.7854
 950/1000 [===========================>..] - ETA: 22s - loss: 3.1139 - regression_loss: 2.3275 - classification_loss: 0.7864
 951/1000 [===========================>..] - ETA: 22s - loss: 3.1150 - regression_loss: 2.3285 - classification_loss: 0.7865
 952/1000 [===========================>..] - ETA: 21s - loss: 3.1160 - regression_loss: 2.3301 - classification_loss: 0.7859
 953/1000 [===========================>..] - ETA: 21s - loss: 3.1161 - regression_loss: 2.3292 - classification_loss: 0.7868
 954/1000 [===========================>..] - ETA: 20s - loss: 3.1156 - regression_loss: 2.3290 - classification_loss: 0.7866
 955/1000 [===========================>..] - ETA: 20s - loss: 3.1156 - regression_loss: 2.3288 - classification_loss: 0.7868
 956/1000 [===========================>..] - ETA: 19s - loss: 3.1190 - regression_loss: 2.3308 - classification_loss: 0.7882
 957/1000 [===========================>..] - ETA: 19s - loss: 3.1200 - regression_loss: 2.3323 - classification_loss: 0.7877
 958/1000 [===========================>..] - ETA: 18s - loss: 3.1220 - regression_loss: 2.3337 - classification_loss: 0.7883
 959/1000 [===========================>..] - ETA: 18s - loss: 3.1232 - regression_loss: 2.3349 - classification_loss: 0.7883
 960/1000 [===========================>..] - ETA: 18s - loss: 3.1264 - regression_loss: 2.3371 - classification_loss: 0.7894
 961/1000 [===========================>..] - ETA: 17s - loss: 3.1282 - regression_loss: 2.3383 - classification_loss: 0.7899
 962/1000 [===========================>..] - ETA: 17s - loss: 3.1299 - regression_loss: 2.3393 - classification_loss: 0.7906
 963/1000 [===========================>..] - ETA: 16s - loss: 3.1267 - regression_loss: 2.3369 - classification_loss: 0.7898
 964/1000 [===========================>..] - ETA: 16s - loss: 3.1271 - regression_loss: 2.3375 - classification_loss: 0.7895
 965/1000 [===========================>..] - ETA: 15s - loss: 3.1268 - regression_loss: 2.3377 - classification_loss: 0.7891
 966/1000 [===========================>..] - ETA: 15s - loss: 3.1236 - regression_loss: 2.3353 - classification_loss: 0.7883
 967/1000 [============================>.] - ETA: 14s - loss: 3.1241 - regression_loss: 2.3360 - classification_loss: 0.7881
 968/1000 [============================>.] - ETA: 14s - loss: 3.1271 - regression_loss: 2.3380 - classification_loss: 0.7891
 969/1000 [============================>.] - ETA: 14s - loss: 3.1287 - regression_loss: 2.3390 - classification_loss: 0.7898
 970/1000 [============================>.] - ETA: 13s - loss: 3.1255 - regression_loss: 2.3366 - classification_loss: 0.7890
 971/1000 [============================>.] - ETA: 13s - loss: 3.1262 - regression_loss: 2.3377 - classification_loss: 0.7885
 972/1000 [============================>.] - ETA: 12s - loss: 3.1248 - regression_loss: 2.3369 - classification_loss: 0.7879
 973/1000 [============================>.] - ETA: 12s - loss: 3.1216 - regression_loss: 2.3345 - classification_loss: 0.7871
 974/1000 [============================>.] - ETA: 11s - loss: 3.1218 - regression_loss: 2.3345 - classification_loss: 0.7873
 975/1000 [============================>.] - ETA: 11s - loss: 3.1239 - regression_loss: 2.3358 - classification_loss: 0.7881
 976/1000 [============================>.] - ETA: 10s - loss: 3.1263 - regression_loss: 2.3375 - classification_loss: 0.7887
 977/1000 [============================>.] - ETA: 10s - loss: 3.1284 - regression_loss: 2.3390 - classification_loss: 0.7894
 978/1000 [============================>.] - ETA: 9s - loss: 3.1304 - regression_loss: 2.3411 - classification_loss: 0.7893 
 979/1000 [============================>.] - ETA: 9s - loss: 3.1331 - regression_loss: 2.3427 - classification_loss: 0.7904
 980/1000 [============================>.] - ETA: 9s - loss: 3.1332 - regression_loss: 2.3427 - classification_loss: 0.7905
 981/1000 [============================>.] - ETA: 8s - loss: 3.1337 - regression_loss: 2.3430 - classification_loss: 0.7906
 982/1000 [============================>.] - ETA: 8s - loss: 3.1346 - regression_loss: 2.3440 - classification_loss: 0.7906
 983/1000 [============================>.] - ETA: 7s - loss: 3.1352 - regression_loss: 2.3449 - classification_loss: 0.7903
 984/1000 [============================>.] - ETA: 7s - loss: 3.1355 - regression_loss: 2.3451 - classification_loss: 0.7904
 985/1000 [============================>.] - ETA: 6s - loss: 3.1324 - regression_loss: 2.3428 - classification_loss: 0.7896
 986/1000 [============================>.] - ETA: 6s - loss: 3.1334 - regression_loss: 2.3438 - classification_loss: 0.7896
 987/1000 [============================>.] - ETA: 5s - loss: 3.1334 - regression_loss: 2.3440 - classification_loss: 0.7894
 988/1000 [============================>.] - ETA: 5s - loss: 3.1353 - regression_loss: 2.3457 - classification_loss: 0.7896
 989/1000 [============================>.] - ETA: 4s - loss: 3.1321 - regression_loss: 2.3433 - classification_loss: 0.7888
 990/1000 [============================>.] - ETA: 4s - loss: 3.1290 - regression_loss: 2.3410 - classification_loss: 0.7880
 991/1000 [============================>.] - ETA: 4s - loss: 3.1288 - regression_loss: 2.3411 - classification_loss: 0.7877
 992/1000 [============================>.] - ETA: 3s - loss: 3.1285 - regression_loss: 2.3410 - classification_loss: 0.7875
 993/1000 [============================>.] - ETA: 3s - loss: 3.1297 - regression_loss: 2.3422 - classification_loss: 0.7875
 994/1000 [============================>.] - ETA: 2s - loss: 3.1312 - regression_loss: 2.3426 - classification_loss: 0.7886
 995/1000 [============================>.] - ETA: 2s - loss: 3.1317 - regression_loss: 2.3431 - classification_loss: 0.7885
 996/1000 [============================>.] - ETA: 1s - loss: 3.1285 - regression_loss: 2.3408 - classification_loss: 0.7878
 997/1000 [============================>.] - ETA: 1s - loss: 3.1303 - regression_loss: 2.3426 - classification_loss: 0.7877
 998/1000 [============================>.] - ETA: 0s - loss: 3.1318 - regression_loss: 2.3435 - classification_loss: 0.7883
 999/1000 [============================>.] - ETA: 0s - loss: 3.1324 - regression_loss: 2.3441 - classification_loss: 0.7883
1000/1000 [==============================] - 452s 452ms/step - loss: 3.1322 - regression_loss: 2.3442 - classification_loss: 0.7880

Epoch 00008: saving model to ./snapshots/resnet50_csv_08.h5
0/2001/2002/2003/2004/2005/2006/2007/2008/2009/20010/20011/20012/20013/20014/20015/20016/20017/20018/20019/20020/20021/20022/20023/20024/20025/20026/20027/20028/20029/20030/20031/20032/20033/20034/20035/20036/20037/20038/20039/20040/20041/20042/20043/20044/20045/20046/20047/20048/20049/20050/20051/20052/20053/20054/20055/20056/20057/20058/20059/20060/20061/20062/20063/20064/20065/20066/20067/20068/20069/20070/20071/20072/20073/20074/20075/20076/20077/20078/20079/20080/20081/20082/20083/20084/20085/20086/20087/20088/20089/20090/20091/20092/20093/20094/20095/20096/20097/20098/20099/200100/200101/200102/200103/200104/200105/200106/200107/200108/200109/200110/200111/200112/200113/200114/200115/200116/200117/200118/200119/200120/200121/200122/200123/200124/200125/200126/200127/200128/200129/200130/200131/200132/200133/200134/200135/200136/200137/200138/200139/200140/200141/200142/200143/200144/200145/200146/200147/200148/200149/200150/200151/200152/200153/200154/200155/200156/200157/200158/200159/200160/200161/200162/200163/200164/200165/200166/200167/200168/200169/200170/200171/200172/200173/200174/200175/200176/200177/200178/200179/200180/200181/200182/200183/200184/200185/200186/200187/200188/200189/200190/200191/200192/200193/200194/200195/200196/200197/200198/200199/2000/2001/2002/2003/2004/2005/2006/2007/2008/2009/20010/20011/20012/20013/20014/20015/20016/20017/20018/20019/20020/20021/20022/20023/20024/20025/20026/20027/20028/20029/20030/20031/20032/20033/20034/20035/20036/20037/20038/20039/20040/20041/20042/20043/20044/20045/20046/20047/20048/20049/20050/20051/20052/20053/20054/20055/20056/20057/20058/20059/20060/20061/20062/20063/20064/20065/20066/20067/20068/20069/20070/20071/20072/20073/20074/20075/20076/20077/20078/20079/20080/20081/20082/20083/20084/20085/20086/20087/20088/20089/20090/20091/20092/20093/20094/20095/20096/20097/20098/20099/200100/200101/200102/200103/200104/200105/200106/200107/200108/200109/200110/200111/200112/200113/200114/200115/200116/200117/200118/200119/200120/200121/200122/200123/200124/200125/200126/200127/200128/200129/200130/200131/200132/200133/200134/200135/200136/200137/200138/200139/200140/200141/200142/200143/200144/200145/200146/200147/200148/200149/200150/200151/200152/200153/200154/200155/200156/200157/200158/200159/200160/200161/200162/200163/200164/200165/200166/200167/200168/200169/200170/200171/200172/200173/200174/200175/200176/200177/200178/200179/200180/200181/200182/200183/200184/200185/200186/200187/200188/200189/200190/200191/200192/200193/200194/200195/200196/200197/200198/200199/200

M 0.0893
N 0.0007
mAP: 0.0450
Epoch 9/30

   1/1000 [..............................] - ETA: 7:22 - loss: 4.5288 - regression_loss: 3.1503 - classification_loss: 1.3784
   2/1000 [..............................] - ETA: 7:25 - loss: 3.7854 - regression_loss: 2.8830 - classification_loss: 0.9024
   3/1000 [..............................] - ETA: 7:28 - loss: 3.6022 - regression_loss: 2.6911 - classification_loss: 0.9111
   4/1000 [..............................] - ETA: 7:28 - loss: 3.4610 - regression_loss: 2.7089 - classification_loss: 0.7521
   5/1000 [..............................] - ETA: 7:29 - loss: 3.3354 - regression_loss: 2.6275 - classification_loss: 0.7079
   6/1000 [..............................] - ETA: 7:30 - loss: 3.2356 - regression_loss: 2.6124 - classification_loss: 0.6232
   7/1000 [..............................] - ETA: 7:31 - loss: 2.7734 - regression_loss: 2.2392 - classification_loss: 0.5341
   8/1000 [..............................] - ETA: 7:30 - loss: 2.4267 - regression_loss: 1.9593 - classification_loss: 0.4674
   9/1000 [..............................] - ETA: 7:29 - loss: 2.4203 - regression_loss: 1.9767 - classification_loss: 0.4437
  10/1000 [..............................] - ETA: 7:28 - loss: 2.5121 - regression_loss: 2.0374 - classification_loss: 0.4747
  11/1000 [..............................] - ETA: 7:28 - loss: 2.2837 - regression_loss: 1.8522 - classification_loss: 0.4315
  12/1000 [..............................] - ETA: 7:27 - loss: 2.0934 - regression_loss: 1.6978 - classification_loss: 0.3956
  13/1000 [..............................] - ETA: 7:27 - loss: 2.3489 - regression_loss: 1.8559 - classification_loss: 0.4930
  14/1000 [..............................] - ETA: 7:26 - loss: 2.4897 - regression_loss: 1.9233 - classification_loss: 0.5663
  15/1000 [..............................] - ETA: 7:23 - loss: 2.6482 - regression_loss: 2.0169 - classification_loss: 0.6313
  16/1000 [..............................] - ETA: 7:22 - loss: 2.4827 - regression_loss: 1.8909 - classification_loss: 0.5918
  17/1000 [..............................] - ETA: 7:22 - loss: 2.3366 - regression_loss: 1.7796 - classification_loss: 0.5570
  18/1000 [..............................] - ETA: 7:21 - loss: 2.4862 - regression_loss: 1.9059 - classification_loss: 0.5803
  19/1000 [..............................] - ETA: 7:21 - loss: 2.6334 - regression_loss: 2.0119 - classification_loss: 0.6215
  20/1000 [..............................] - ETA: 7:20 - loss: 2.5017 - regression_loss: 1.9113 - classification_loss: 0.5904
  21/1000 [..............................] - ETA: 7:20 - loss: 2.5966 - regression_loss: 2.0032 - classification_loss: 0.5934
  22/1000 [..............................] - ETA: 7:20 - loss: 2.4786 - regression_loss: 1.9122 - classification_loss: 0.5664
  23/1000 [..............................] - ETA: 7:19 - loss: 2.3715 - regression_loss: 1.8290 - classification_loss: 0.5425
  24/1000 [..............................] - ETA: 7:19 - loss: 2.4587 - regression_loss: 1.8933 - classification_loss: 0.5655
  25/1000 [..............................] - ETA: 7:19 - loss: 2.5277 - regression_loss: 1.9398 - classification_loss: 0.5879
  26/1000 [..............................] - ETA: 7:19 - loss: 2.6047 - regression_loss: 2.0090 - classification_loss: 0.5957
  27/1000 [..............................] - ETA: 7:18 - loss: 2.6557 - regression_loss: 2.0388 - classification_loss: 0.6168
  28/1000 [..............................] - ETA: 7:18 - loss: 2.7143 - regression_loss: 2.1008 - classification_loss: 0.6136
  29/1000 [..............................] - ETA: 7:18 - loss: 2.7628 - regression_loss: 2.1409 - classification_loss: 0.6218
  30/1000 [..............................] - ETA: 7:18 - loss: 2.8965 - regression_loss: 2.2150 - classification_loss: 0.6815
  31/1000 [..............................] - ETA: 7:17 - loss: 2.9263 - regression_loss: 2.1991 - classification_loss: 0.7272
  32/1000 [..............................] - ETA: 7:17 - loss: 2.9812 - regression_loss: 2.2147 - classification_loss: 0.7665
  33/1000 [..............................] - ETA: 7:16 - loss: 2.9974 - regression_loss: 2.2404 - classification_loss: 0.7570
  34/1000 [>.............................] - ETA: 7:15 - loss: 2.9100 - regression_loss: 2.1745 - classification_loss: 0.7355
  35/1000 [>.............................] - ETA: 7:15 - loss: 2.9296 - regression_loss: 2.1960 - classification_loss: 0.7335
  36/1000 [>.............................] - ETA: 7:15 - loss: 2.9341 - regression_loss: 2.2000 - classification_loss: 0.7341
  37/1000 [>.............................] - ETA: 7:14 - loss: 2.8548 - regression_loss: 2.1406 - classification_loss: 0.7143
  38/1000 [>.............................] - ETA: 7:14 - loss: 2.8599 - regression_loss: 2.1444 - classification_loss: 0.7154
  39/1000 [>.............................] - ETA: 7:13 - loss: 2.8785 - regression_loss: 2.1558 - classification_loss: 0.7227
  40/1000 [>.............................] - ETA: 7:13 - loss: 2.8941 - regression_loss: 2.1780 - classification_loss: 0.7162
  41/1000 [>.............................] - ETA: 7:13 - loss: 2.8842 - regression_loss: 2.1769 - classification_loss: 0.7073
  42/1000 [>.............................] - ETA: 7:12 - loss: 2.9065 - regression_loss: 2.1992 - classification_loss: 0.7072
  43/1000 [>.............................] - ETA: 7:12 - loss: 2.8411 - regression_loss: 2.1481 - classification_loss: 0.6930
  44/1000 [>.............................] - ETA: 7:11 - loss: 2.8744 - regression_loss: 2.1811 - classification_loss: 0.6933
  45/1000 [>.............................] - ETA: 7:11 - loss: 2.8832 - regression_loss: 2.1941 - classification_loss: 0.6891
  46/1000 [>.............................] - ETA: 7:11 - loss: 2.8934 - regression_loss: 2.2080 - classification_loss: 0.6854
  47/1000 [>.............................] - ETA: 7:10 - loss: 2.8857 - regression_loss: 2.2069 - classification_loss: 0.6787
  48/1000 [>.............................] - ETA: 7:10 - loss: 2.8255 - regression_loss: 2.1609 - classification_loss: 0.6646
  49/1000 [>.............................] - ETA: 7:09 - loss: 2.8512 - regression_loss: 2.1798 - classification_loss: 0.6713
  50/1000 [>.............................] - ETA: 7:09 - loss: 2.8606 - regression_loss: 2.1834 - classification_loss: 0.6772
  51/1000 [>.............................] - ETA: 7:09 - loss: 2.8639 - regression_loss: 2.1901 - classification_loss: 0.6739
  52/1000 [>.............................] - ETA: 7:08 - loss: 2.9065 - regression_loss: 2.2112 - classification_loss: 0.6953
  53/1000 [>.............................] - ETA: 7:07 - loss: 2.9213 - regression_loss: 2.2256 - classification_loss: 0.6957
  54/1000 [>.............................] - ETA: 7:07 - loss: 2.9414 - regression_loss: 2.2460 - classification_loss: 0.6954
  55/1000 [>.............................] - ETA: 7:06 - loss: 2.9686 - regression_loss: 2.2744 - classification_loss: 0.6942
  56/1000 [>.............................] - ETA: 7:06 - loss: 2.9977 - regression_loss: 2.2964 - classification_loss: 0.7012
  57/1000 [>.............................] - ETA: 7:05 - loss: 3.0079 - regression_loss: 2.3029 - classification_loss: 0.7049
  58/1000 [>.............................] - ETA: 7:05 - loss: 3.0104 - regression_loss: 2.3062 - classification_loss: 0.7042
  59/1000 [>.............................] - ETA: 7:05 - loss: 2.9594 - regression_loss: 2.2671 - classification_loss: 0.6923
  60/1000 [>.............................] - ETA: 7:04 - loss: 3.0043 - regression_loss: 2.2872 - classification_loss: 0.7172
  61/1000 [>.............................] - ETA: 7:04 - loss: 3.0248 - regression_loss: 2.3012 - classification_loss: 0.7235
  62/1000 [>.............................] - ETA: 7:04 - loss: 3.0453 - regression_loss: 2.3242 - classification_loss: 0.7210
  63/1000 [>.............................] - ETA: 7:03 - loss: 3.0412 - regression_loss: 2.3244 - classification_loss: 0.7168
  64/1000 [>.............................] - ETA: 7:03 - loss: 3.0417 - regression_loss: 2.3298 - classification_loss: 0.7118
  65/1000 [>.............................] - ETA: 7:02 - loss: 3.0619 - regression_loss: 2.3428 - classification_loss: 0.7191
  66/1000 [>.............................] - ETA: 7:02 - loss: 3.0734 - regression_loss: 2.3479 - classification_loss: 0.7255
  67/1000 [=>............................] - ETA: 7:02 - loss: 3.0275 - regression_loss: 2.3129 - classification_loss: 0.7146
  68/1000 [=>............................] - ETA: 7:01 - loss: 3.0230 - regression_loss: 2.3139 - classification_loss: 0.7091
  69/1000 [=>............................] - ETA: 7:01 - loss: 3.0154 - regression_loss: 2.3101 - classification_loss: 0.7053
  70/1000 [=>............................] - ETA: 7:00 - loss: 3.0083 - regression_loss: 2.3101 - classification_loss: 0.6982
  71/1000 [=>............................] - ETA: 7:00 - loss: 3.0114 - regression_loss: 2.3146 - classification_loss: 0.6968
  72/1000 [=>............................] - ETA: 6:59 - loss: 2.9696 - regression_loss: 2.2825 - classification_loss: 0.6871
  73/1000 [=>............................] - ETA: 6:58 - loss: 2.9289 - regression_loss: 2.2512 - classification_loss: 0.6777
  74/1000 [=>............................] - ETA: 6:58 - loss: 2.9351 - regression_loss: 2.2605 - classification_loss: 0.6746
  75/1000 [=>............................] - ETA: 6:58 - loss: 2.8960 - regression_loss: 2.2304 - classification_loss: 0.6656
  76/1000 [=>............................] - ETA: 6:57 - loss: 2.8996 - regression_loss: 2.2346 - classification_loss: 0.6650
  77/1000 [=>............................] - ETA: 6:57 - loss: 2.9106 - regression_loss: 2.2374 - classification_loss: 0.6733
  78/1000 [=>............................] - ETA: 6:56 - loss: 2.9492 - regression_loss: 2.2542 - classification_loss: 0.6950
  79/1000 [=>............................] - ETA: 6:56 - loss: 2.9627 - regression_loss: 2.2704 - classification_loss: 0.6923
  80/1000 [=>............................] - ETA: 6:55 - loss: 2.9689 - regression_loss: 2.2794 - classification_loss: 0.6895
  81/1000 [=>............................] - ETA: 6:55 - loss: 2.9943 - regression_loss: 2.2960 - classification_loss: 0.6983
  82/1000 [=>............................] - ETA: 6:54 - loss: 3.0014 - regression_loss: 2.3081 - classification_loss: 0.6933
  83/1000 [=>............................] - ETA: 6:54 - loss: 3.0065 - regression_loss: 2.3088 - classification_loss: 0.6977
  84/1000 [=>............................] - ETA: 6:53 - loss: 3.0393 - regression_loss: 2.3290 - classification_loss: 0.7102
  85/1000 [=>............................] - ETA: 6:53 - loss: 3.0692 - regression_loss: 2.3536 - classification_loss: 0.7156
  86/1000 [=>............................] - ETA: 6:52 - loss: 3.0336 - regression_loss: 2.3263 - classification_loss: 0.7073
  87/1000 [=>............................] - ETA: 6:52 - loss: 2.9987 - regression_loss: 2.2995 - classification_loss: 0.6992
  88/1000 [=>............................] - ETA: 6:51 - loss: 3.0167 - regression_loss: 2.3144 - classification_loss: 0.7023
  89/1000 [=>............................] - ETA: 6:51 - loss: 3.0280 - regression_loss: 2.3239 - classification_loss: 0.7041
  90/1000 [=>............................] - ETA: 6:51 - loss: 3.0293 - regression_loss: 2.3243 - classification_loss: 0.7050
  91/1000 [=>............................] - ETA: 6:50 - loss: 3.0578 - regression_loss: 2.3398 - classification_loss: 0.7180
  92/1000 [=>............................] - ETA: 6:50 - loss: 3.0665 - regression_loss: 2.3476 - classification_loss: 0.7189
  93/1000 [=>............................] - ETA: 6:49 - loss: 3.0379 - regression_loss: 2.3224 - classification_loss: 0.7155
  94/1000 [=>............................] - ETA: 6:49 - loss: 3.0617 - regression_loss: 2.3383 - classification_loss: 0.7234
  95/1000 [=>............................] - ETA: 6:48 - loss: 3.0712 - regression_loss: 2.3457 - classification_loss: 0.7256
  96/1000 [=>............................] - ETA: 6:48 - loss: 3.0859 - regression_loss: 2.3534 - classification_loss: 0.7325
  97/1000 [=>............................] - ETA: 6:47 - loss: 3.0856 - regression_loss: 2.3561 - classification_loss: 0.7295
  98/1000 [=>............................] - ETA: 6:47 - loss: 3.0541 - regression_loss: 2.3321 - classification_loss: 0.7220
  99/1000 [=>............................] - ETA: 6:46 - loss: 3.0577 - regression_loss: 2.3364 - classification_loss: 0.7213
 100/1000 [==>...........................] - ETA: 6:46 - loss: 3.0851 - regression_loss: 2.3540 - classification_loss: 0.7311
 101/1000 [==>...........................] - ETA: 6:45 - loss: 3.0927 - regression_loss: 2.3578 - classification_loss: 0.7349
 102/1000 [==>...........................] - ETA: 6:45 - loss: 3.0936 - regression_loss: 2.3597 - classification_loss: 0.7339
 103/1000 [==>...........................] - ETA: 6:45 - loss: 3.1009 - regression_loss: 2.3645 - classification_loss: 0.7364
 104/1000 [==>...........................] - ETA: 6:44 - loss: 3.0928 - regression_loss: 2.3597 - classification_loss: 0.7331
 105/1000 [==>...........................] - ETA: 6:44 - loss: 3.1151 - regression_loss: 2.3739 - classification_loss: 0.7411
 106/1000 [==>...........................] - ETA: 6:43 - loss: 3.1163 - regression_loss: 2.3775 - classification_loss: 0.7389
 107/1000 [==>...........................] - ETA: 6:43 - loss: 3.1213 - regression_loss: 2.3832 - classification_loss: 0.7381
 108/1000 [==>...........................] - ETA: 6:42 - loss: 3.0964 - regression_loss: 2.3611 - classification_loss: 0.7353
 109/1000 [==>...........................] - ETA: 6:42 - loss: 3.0680 - regression_loss: 2.3394 - classification_loss: 0.7286
 110/1000 [==>...........................] - ETA: 6:41 - loss: 3.0636 - regression_loss: 2.3386 - classification_loss: 0.7250
 111/1000 [==>...........................] - ETA: 6:41 - loss: 3.0797 - regression_loss: 2.3469 - classification_loss: 0.7328
 112/1000 [==>...........................] - ETA: 6:41 - loss: 3.0522 - regression_loss: 2.3259 - classification_loss: 0.7262
 113/1000 [==>...........................] - ETA: 6:40 - loss: 3.0617 - regression_loss: 2.3295 - classification_loss: 0.7322
 114/1000 [==>...........................] - ETA: 6:40 - loss: 3.0669 - regression_loss: 2.3356 - classification_loss: 0.7313
 115/1000 [==>...........................] - ETA: 6:39 - loss: 3.0403 - regression_loss: 2.3153 - classification_loss: 0.7250
 116/1000 [==>...........................] - ETA: 6:39 - loss: 3.0525 - regression_loss: 2.3237 - classification_loss: 0.7288
 117/1000 [==>...........................] - ETA: 6:39 - loss: 3.0664 - regression_loss: 2.3367 - classification_loss: 0.7298
 118/1000 [==>...........................] - ETA: 6:38 - loss: 3.0800 - regression_loss: 2.3400 - classification_loss: 0.7399
 119/1000 [==>...........................] - ETA: 6:38 - loss: 3.0834 - regression_loss: 2.3437 - classification_loss: 0.7397
 120/1000 [==>...........................] - ETA: 6:37 - loss: 3.0911 - regression_loss: 2.3466 - classification_loss: 0.7445
 121/1000 [==>...........................] - ETA: 6:37 - loss: 3.1042 - regression_loss: 2.3585 - classification_loss: 0.7457
 122/1000 [==>...........................] - ETA: 6:36 - loss: 3.1024 - regression_loss: 2.3582 - classification_loss: 0.7442
 123/1000 [==>...........................] - ETA: 6:36 - loss: 3.1141 - regression_loss: 2.3622 - classification_loss: 0.7518
 124/1000 [==>...........................] - ETA: 6:36 - loss: 3.1255 - regression_loss: 2.3734 - classification_loss: 0.7521
 125/1000 [==>...........................] - ETA: 6:35 - loss: 3.1270 - regression_loss: 2.3785 - classification_loss: 0.7485
 126/1000 [==>...........................] - ETA: 6:35 - loss: 3.1247 - regression_loss: 2.3792 - classification_loss: 0.7455
 127/1000 [==>...........................] - ETA: 6:34 - loss: 3.1346 - regression_loss: 2.3848 - classification_loss: 0.7497
 128/1000 [==>...........................] - ETA: 6:34 - loss: 3.1435 - regression_loss: 2.3940 - classification_loss: 0.7495
 129/1000 [==>...........................] - ETA: 6:33 - loss: 3.1191 - regression_loss: 2.3754 - classification_loss: 0.7437
 130/1000 [==>...........................] - ETA: 6:33 - loss: 3.1186 - regression_loss: 2.3784 - classification_loss: 0.7402
 131/1000 [==>...........................] - ETA: 6:32 - loss: 3.1351 - regression_loss: 2.3927 - classification_loss: 0.7424
 132/1000 [==>...........................] - ETA: 6:32 - loss: 3.1114 - regression_loss: 2.3746 - classification_loss: 0.7368
 133/1000 [==>...........................] - ETA: 6:31 - loss: 3.1131 - regression_loss: 2.3714 - classification_loss: 0.7418
 134/1000 [===>..........................] - ETA: 6:31 - loss: 3.0904 - regression_loss: 2.3537 - classification_loss: 0.7368
 135/1000 [===>..........................] - ETA: 6:30 - loss: 3.0679 - regression_loss: 2.3362 - classification_loss: 0.7317
 136/1000 [===>..........................] - ETA: 6:30 - loss: 3.0614 - regression_loss: 2.3328 - classification_loss: 0.7286
 137/1000 [===>..........................] - ETA: 6:29 - loss: 3.0785 - regression_loss: 2.3430 - classification_loss: 0.7355
 138/1000 [===>..........................] - ETA: 6:29 - loss: 3.0841 - regression_loss: 2.3482 - classification_loss: 0.7359
 139/1000 [===>..........................] - ETA: 6:28 - loss: 3.0798 - regression_loss: 2.3468 - classification_loss: 0.7330
 140/1000 [===>..........................] - ETA: 6:28 - loss: 3.0775 - regression_loss: 2.3469 - classification_loss: 0.7306
 141/1000 [===>..........................] - ETA: 6:28 - loss: 3.0875 - regression_loss: 2.3515 - classification_loss: 0.7360
 142/1000 [===>..........................] - ETA: 6:27 - loss: 3.1020 - regression_loss: 2.3623 - classification_loss: 0.7397
 143/1000 [===>..........................] - ETA: 6:27 - loss: 3.1148 - regression_loss: 2.3736 - classification_loss: 0.7413
 144/1000 [===>..........................] - ETA: 6:26 - loss: 3.1216 - regression_loss: 2.3784 - classification_loss: 0.7432
 145/1000 [===>..........................] - ETA: 6:26 - loss: 3.1300 - regression_loss: 2.3855 - classification_loss: 0.7445
 146/1000 [===>..........................] - ETA: 6:25 - loss: 3.1229 - regression_loss: 2.3818 - classification_loss: 0.7411
 147/1000 [===>..........................] - ETA: 6:25 - loss: 3.1304 - regression_loss: 2.3862 - classification_loss: 0.7441
 148/1000 [===>..........................] - ETA: 6:24 - loss: 3.1092 - regression_loss: 2.3701 - classification_loss: 0.7391
 149/1000 [===>..........................] - ETA: 6:24 - loss: 3.1100 - regression_loss: 2.3699 - classification_loss: 0.7401
 150/1000 [===>..........................] - ETA: 6:24 - loss: 3.0893 - regression_loss: 2.3541 - classification_loss: 0.7352
 151/1000 [===>..........................] - ETA: 6:23 - loss: 3.1032 - regression_loss: 2.3626 - classification_loss: 0.7406
 152/1000 [===>..........................] - ETA: 6:23 - loss: 3.1078 - regression_loss: 2.3653 - classification_loss: 0.7425
 153/1000 [===>..........................] - ETA: 6:22 - loss: 3.1142 - regression_loss: 2.3702 - classification_loss: 0.7440
 154/1000 [===>..........................] - ETA: 6:22 - loss: 3.1315 - regression_loss: 2.3808 - classification_loss: 0.7507
 155/1000 [===>..........................] - ETA: 6:21 - loss: 3.1497 - regression_loss: 2.3941 - classification_loss: 0.7556
 156/1000 [===>..........................] - ETA: 6:21 - loss: 3.1512 - regression_loss: 2.3939 - classification_loss: 0.7572
 157/1000 [===>..........................] - ETA: 6:21 - loss: 3.1559 - regression_loss: 2.3941 - classification_loss: 0.7618
 158/1000 [===>..........................] - ETA: 6:20 - loss: 3.1563 - regression_loss: 2.3951 - classification_loss: 0.7612
 159/1000 [===>..........................] - ETA: 6:20 - loss: 3.1695 - regression_loss: 2.4041 - classification_loss: 0.7653
 160/1000 [===>..........................] - ETA: 6:19 - loss: 3.1728 - regression_loss: 2.4082 - classification_loss: 0.7645
 161/1000 [===>..........................] - ETA: 6:18 - loss: 3.1691 - regression_loss: 2.4066 - classification_loss: 0.7625
 162/1000 [===>..........................] - ETA: 6:18 - loss: 3.1763 - regression_loss: 2.4086 - classification_loss: 0.7677
 163/1000 [===>..........................] - ETA: 6:18 - loss: 3.1582 - regression_loss: 2.3938 - classification_loss: 0.7644
 164/1000 [===>..........................] - ETA: 6:17 - loss: 3.1524 - regression_loss: 2.3894 - classification_loss: 0.7630
 165/1000 [===>..........................] - ETA: 6:17 - loss: 3.1632 - regression_loss: 2.3976 - classification_loss: 0.7656
 166/1000 [===>..........................] - ETA: 6:16 - loss: 3.1584 - regression_loss: 2.3959 - classification_loss: 0.7625
 167/1000 [====>.........................] - ETA: 6:16 - loss: 3.1687 - regression_loss: 2.4022 - classification_loss: 0.7665
 168/1000 [====>.........................] - ETA: 6:15 - loss: 3.1707 - regression_loss: 2.4001 - classification_loss: 0.7705
 169/1000 [====>.........................] - ETA: 6:15 - loss: 3.1644 - regression_loss: 2.3961 - classification_loss: 0.7683
 170/1000 [====>.........................] - ETA: 6:15 - loss: 3.1628 - regression_loss: 2.3976 - classification_loss: 0.7651
 171/1000 [====>.........................] - ETA: 6:14 - loss: 3.1638 - regression_loss: 2.3996 - classification_loss: 0.7642
 172/1000 [====>.........................] - ETA: 6:14 - loss: 3.1658 - regression_loss: 2.4039 - classification_loss: 0.7618
 173/1000 [====>.........................] - ETA: 6:13 - loss: 3.1690 - regression_loss: 2.4049 - classification_loss: 0.7641
 174/1000 [====>.........................] - ETA: 6:13 - loss: 3.1720 - regression_loss: 2.4064 - classification_loss: 0.7656
 175/1000 [====>.........................] - ETA: 6:12 - loss: 3.1539 - regression_loss: 2.3927 - classification_loss: 0.7612
 176/1000 [====>.........................] - ETA: 6:12 - loss: 3.1635 - regression_loss: 2.3984 - classification_loss: 0.7651
 177/1000 [====>.........................] - ETA: 6:11 - loss: 3.1699 - regression_loss: 2.4035 - classification_loss: 0.7664
 178/1000 [====>.........................] - ETA: 6:11 - loss: 3.1664 - regression_loss: 2.4025 - classification_loss: 0.7639
 179/1000 [====>.........................] - ETA: 6:11 - loss: 3.1705 - regression_loss: 2.4079 - classification_loss: 0.7626
 180/1000 [====>.........................] - ETA: 6:10 - loss: 3.1738 - regression_loss: 2.4127 - classification_loss: 0.7611
 181/1000 [====>.........................] - ETA: 6:10 - loss: 3.1912 - regression_loss: 2.4283 - classification_loss: 0.7629
 182/1000 [====>.........................] - ETA: 6:09 - loss: 3.1895 - regression_loss: 2.4278 - classification_loss: 0.7617
 183/1000 [====>.........................] - ETA: 6:09 - loss: 3.1721 - regression_loss: 2.4145 - classification_loss: 0.7576
 184/1000 [====>.........................] - ETA: 6:08 - loss: 3.1549 - regression_loss: 2.4014 - classification_loss: 0.7535
 185/1000 [====>.........................] - ETA: 6:08 - loss: 3.1548 - regression_loss: 2.4026 - classification_loss: 0.7522
 186/1000 [====>.........................] - ETA: 6:07 - loss: 3.1491 - regression_loss: 2.3897 - classification_loss: 0.7594
 187/1000 [====>.........................] - ETA: 6:07 - loss: 3.1460 - regression_loss: 2.3879 - classification_loss: 0.7581
 188/1000 [====>.........................] - ETA: 6:07 - loss: 3.1517 - regression_loss: 2.3927 - classification_loss: 0.7590
 189/1000 [====>.........................] - ETA: 6:06 - loss: 3.1557 - regression_loss: 2.3980 - classification_loss: 0.7576
 190/1000 [====>.........................] - ETA: 6:06 - loss: 3.1597 - regression_loss: 2.4006 - classification_loss: 0.7591
 191/1000 [====>.........................] - ETA: 6:05 - loss: 3.1432 - regression_loss: 2.3881 - classification_loss: 0.7551
 192/1000 [====>.........................] - ETA: 6:05 - loss: 3.1505 - regression_loss: 2.3924 - classification_loss: 0.7581
 193/1000 [====>.........................] - ETA: 6:04 - loss: 3.1342 - regression_loss: 2.3800 - classification_loss: 0.7542
 194/1000 [====>.........................] - ETA: 6:04 - loss: 3.1472 - regression_loss: 2.3885 - classification_loss: 0.7587
 195/1000 [====>.........................] - ETA: 6:03 - loss: 3.1517 - regression_loss: 2.3901 - classification_loss: 0.7616
 196/1000 [====>.........................] - ETA: 6:03 - loss: 3.1493 - regression_loss: 2.3891 - classification_loss: 0.7601
 197/1000 [====>.........................] - ETA: 6:02 - loss: 3.1613 - regression_loss: 2.3966 - classification_loss: 0.7648
 198/1000 [====>.........................] - ETA: 6:02 - loss: 3.1762 - regression_loss: 2.4108 - classification_loss: 0.7654
 199/1000 [====>.........................] - ETA: 6:02 - loss: 3.1801 - regression_loss: 2.4138 - classification_loss: 0.7663
 200/1000 [=====>........................] - ETA: 6:01 - loss: 3.1889 - regression_loss: 2.4198 - classification_loss: 0.7691
 201/1000 [=====>........................] - ETA: 6:01 - loss: 3.1937 - regression_loss: 2.4240 - classification_loss: 0.7697
 202/1000 [=====>........................] - ETA: 6:00 - loss: 3.2049 - regression_loss: 2.4335 - classification_loss: 0.7714
 203/1000 [=====>........................] - ETA: 6:00 - loss: 3.2050 - regression_loss: 2.4332 - classification_loss: 0.7718
 204/1000 [=====>........................] - ETA: 5:59 - loss: 3.2045 - regression_loss: 2.4309 - classification_loss: 0.7735
 205/1000 [=====>........................] - ETA: 5:59 - loss: 3.2095 - regression_loss: 2.4354 - classification_loss: 0.7741
 206/1000 [=====>........................] - ETA: 5:59 - loss: 3.2169 - regression_loss: 2.4412 - classification_loss: 0.7757
 207/1000 [=====>........................] - ETA: 5:58 - loss: 3.2270 - regression_loss: 2.4496 - classification_loss: 0.7774
 208/1000 [=====>........................] - ETA: 5:58 - loss: 3.2309 - regression_loss: 2.4549 - classification_loss: 0.7760
 209/1000 [=====>........................] - ETA: 5:57 - loss: 3.2341 - regression_loss: 2.4564 - classification_loss: 0.7777
 210/1000 [=====>........................] - ETA: 5:57 - loss: 3.2422 - regression_loss: 2.4625 - classification_loss: 0.7797
 211/1000 [=====>........................] - ETA: 5:56 - loss: 3.2440 - regression_loss: 2.4641 - classification_loss: 0.7798
 212/1000 [=====>........................] - ETA: 5:56 - loss: 3.2424 - regression_loss: 2.4621 - classification_loss: 0.7803
 213/1000 [=====>........................] - ETA: 5:55 - loss: 3.2272 - regression_loss: 2.4506 - classification_loss: 0.7766
 214/1000 [=====>........................] - ETA: 5:55 - loss: 3.2380 - regression_loss: 2.4584 - classification_loss: 0.7796
 215/1000 [=====>........................] - ETA: 5:55 - loss: 3.2350 - regression_loss: 2.4565 - classification_loss: 0.7786
 216/1000 [=====>........................] - ETA: 5:54 - loss: 3.2395 - regression_loss: 2.4606 - classification_loss: 0.7790
 217/1000 [=====>........................] - ETA: 5:54 - loss: 3.2381 - regression_loss: 2.4591 - classification_loss: 0.7790
 218/1000 [=====>........................] - ETA: 5:53 - loss: 3.2393 - regression_loss: 2.4618 - classification_loss: 0.7775
 219/1000 [=====>........................] - ETA: 5:53 - loss: 3.2245 - regression_loss: 2.4505 - classification_loss: 0.7740
 220/1000 [=====>........................] - ETA: 5:52 - loss: 3.2265 - regression_loss: 2.4533 - classification_loss: 0.7732
 221/1000 [=====>........................] - ETA: 5:52 - loss: 3.2277 - regression_loss: 2.4553 - classification_loss: 0.7724
 222/1000 [=====>........................] - ETA: 5:51 - loss: 3.2319 - regression_loss: 2.4594 - classification_loss: 0.7725
 223/1000 [=====>........................] - ETA: 5:51 - loss: 3.2362 - regression_loss: 2.4634 - classification_loss: 0.7727
 224/1000 [=====>........................] - ETA: 5:51 - loss: 3.2374 - regression_loss: 2.4665 - classification_loss: 0.7709
 225/1000 [=====>........................] - ETA: 5:50 - loss: 3.2415 - regression_loss: 2.4711 - classification_loss: 0.7704
 226/1000 [=====>........................] - ETA: 5:50 - loss: 3.2383 - regression_loss: 2.4701 - classification_loss: 0.7683
 227/1000 [=====>........................] - ETA: 5:49 - loss: 3.2426 - regression_loss: 2.4758 - classification_loss: 0.7667
 228/1000 [=====>........................] - ETA: 5:49 - loss: 3.2420 - regression_loss: 2.4768 - classification_loss: 0.7652
 229/1000 [=====>........................] - ETA: 5:48 - loss: 3.2420 - regression_loss: 2.4774 - classification_loss: 0.7647
 230/1000 [=====>........................] - ETA: 5:48 - loss: 3.2420 - regression_loss: 2.4775 - classification_loss: 0.7645
 231/1000 [=====>........................] - ETA: 5:47 - loss: 3.2534 - regression_loss: 2.4877 - classification_loss: 0.7657
 232/1000 [=====>........................] - ETA: 5:47 - loss: 3.2568 - regression_loss: 2.4908 - classification_loss: 0.7659
 233/1000 [=====>........................] - ETA: 5:46 - loss: 3.2589 - regression_loss: 2.4934 - classification_loss: 0.7655
 234/1000 [======>.......................] - ETA: 5:46 - loss: 3.2565 - regression_loss: 2.4927 - classification_loss: 0.7638
 235/1000 [======>.......................] - ETA: 5:46 - loss: 3.2563 - regression_loss: 2.4938 - classification_loss: 0.7626
 236/1000 [======>.......................] - ETA: 5:45 - loss: 3.2518 - regression_loss: 2.4910 - classification_loss: 0.7607
 237/1000 [======>.......................] - ETA: 5:45 - loss: 3.2554 - regression_loss: 2.4945 - classification_loss: 0.7609
 238/1000 [======>.......................] - ETA: 5:44 - loss: 3.2586 - regression_loss: 2.4986 - classification_loss: 0.7601
 239/1000 [======>.......................] - ETA: 5:44 - loss: 3.2588 - regression_loss: 2.4991 - classification_loss: 0.7597
 240/1000 [======>.......................] - ETA: 5:43 - loss: 3.2458 - regression_loss: 2.4886 - classification_loss: 0.7571
 241/1000 [======>.......................] - ETA: 5:43 - loss: 3.2323 - regression_loss: 2.4783 - classification_loss: 0.7540
 242/1000 [======>.......................] - ETA: 5:42 - loss: 3.2275 - regression_loss: 2.4758 - classification_loss: 0.7517
 243/1000 [======>.......................] - ETA: 5:42 - loss: 3.2313 - regression_loss: 2.4795 - classification_loss: 0.7518
 244/1000 [======>.......................] - ETA: 5:41 - loss: 3.2255 - regression_loss: 2.4694 - classification_loss: 0.7561
 245/1000 [======>.......................] - ETA: 5:41 - loss: 3.2223 - regression_loss: 2.4681 - classification_loss: 0.7542
 246/1000 [======>.......................] - ETA: 5:41 - loss: 3.2305 - regression_loss: 2.4739 - classification_loss: 0.7567
 247/1000 [======>.......................] - ETA: 5:40 - loss: 3.2327 - regression_loss: 2.4755 - classification_loss: 0.7572
 248/1000 [======>.......................] - ETA: 5:40 - loss: 3.2288 - regression_loss: 2.4732 - classification_loss: 0.7556
 249/1000 [======>.......................] - ETA: 5:39 - loss: 3.2158 - regression_loss: 2.4632 - classification_loss: 0.7526
 250/1000 [======>.......................] - ETA: 5:39 - loss: 3.2127 - regression_loss: 2.4614 - classification_loss: 0.7513
 251/1000 [======>.......................] - ETA: 5:38 - loss: 3.1999 - regression_loss: 2.4516 - classification_loss: 0.7483
 252/1000 [======>.......................] - ETA: 5:38 - loss: 3.2104 - regression_loss: 2.4578 - classification_loss: 0.7526
 253/1000 [======>.......................] - ETA: 5:37 - loss: 3.2127 - regression_loss: 2.4583 - classification_loss: 0.7544
 254/1000 [======>.......................] - ETA: 5:37 - loss: 3.2188 - regression_loss: 2.4622 - classification_loss: 0.7566
 255/1000 [======>.......................] - ETA: 5:37 - loss: 3.2337 - regression_loss: 2.4759 - classification_loss: 0.7578
 256/1000 [======>.......................] - ETA: 5:36 - loss: 3.2389 - regression_loss: 2.4781 - classification_loss: 0.7608
 257/1000 [======>.......................] - ETA: 5:36 - loss: 3.2414 - regression_loss: 2.4799 - classification_loss: 0.7615
 258/1000 [======>.......................] - ETA: 5:35 - loss: 3.2288 - regression_loss: 2.4703 - classification_loss: 0.7585
 259/1000 [======>.......................] - ETA: 5:35 - loss: 3.2256 - regression_loss: 2.4686 - classification_loss: 0.7570
 260/1000 [======>.......................] - ETA: 5:34 - loss: 3.2262 - regression_loss: 2.4681 - classification_loss: 0.7581
 261/1000 [======>.......................] - ETA: 5:34 - loss: 3.2234 - regression_loss: 2.4657 - classification_loss: 0.7577
 262/1000 [======>.......................] - ETA: 5:33 - loss: 3.2275 - regression_loss: 2.4695 - classification_loss: 0.7579
 263/1000 [======>.......................] - ETA: 5:33 - loss: 3.2291 - regression_loss: 2.4710 - classification_loss: 0.7581
 264/1000 [======>.......................] - ETA: 5:32 - loss: 3.2329 - regression_loss: 2.4727 - classification_loss: 0.7602
 265/1000 [======>.......................] - ETA: 5:32 - loss: 3.2207 - regression_loss: 2.4634 - classification_loss: 0.7574
 266/1000 [======>.......................] - ETA: 5:32 - loss: 3.2086 - regression_loss: 2.4541 - classification_loss: 0.7545
 267/1000 [=======>......................] - ETA: 5:31 - loss: 3.1966 - regression_loss: 2.4449 - classification_loss: 0.7517
 268/1000 [=======>......................] - ETA: 5:31 - loss: 3.1953 - regression_loss: 2.4445 - classification_loss: 0.7508
 269/1000 [=======>......................] - ETA: 5:30 - loss: 3.1934 - regression_loss: 2.4442 - classification_loss: 0.7492
 270/1000 [=======>......................] - ETA: 5:30 - loss: 3.1816 - regression_loss: 2.4351 - classification_loss: 0.7464
 271/1000 [=======>......................] - ETA: 5:29 - loss: 3.1698 - regression_loss: 2.4261 - classification_loss: 0.7437
 272/1000 [=======>......................] - ETA: 5:29 - loss: 3.1582 - regression_loss: 2.4172 - classification_loss: 0.7409
 273/1000 [=======>......................] - ETA: 5:28 - loss: 3.1607 - regression_loss: 2.4186 - classification_loss: 0.7421
 274/1000 [=======>......................] - ETA: 5:28 - loss: 3.1495 - regression_loss: 2.4098 - classification_loss: 0.7397
 275/1000 [=======>......................] - ETA: 5:27 - loss: 3.1560 - regression_loss: 2.4146 - classification_loss: 0.7415
 276/1000 [=======>......................] - ETA: 5:27 - loss: 3.1605 - regression_loss: 2.4173 - classification_loss: 0.7432
 277/1000 [=======>......................] - ETA: 5:27 - loss: 3.1643 - regression_loss: 2.4176 - classification_loss: 0.7467
 278/1000 [=======>......................] - ETA: 5:26 - loss: 3.1529 - regression_loss: 2.4089 - classification_loss: 0.7440
 279/1000 [=======>......................] - ETA: 5:26 - loss: 3.1416 - regression_loss: 2.4002 - classification_loss: 0.7413
 280/1000 [=======>......................] - ETA: 5:25 - loss: 3.1573 - regression_loss: 2.4094 - classification_loss: 0.7479
 281/1000 [=======>......................] - ETA: 5:25 - loss: 3.1630 - regression_loss: 2.4127 - classification_loss: 0.7503
 282/1000 [=======>......................] - ETA: 5:24 - loss: 3.1647 - regression_loss: 2.4120 - classification_loss: 0.7527
 283/1000 [=======>......................] - ETA: 5:24 - loss: 3.1587 - regression_loss: 2.4078 - classification_loss: 0.7510
 284/1000 [=======>......................] - ETA: 5:23 - loss: 3.1598 - regression_loss: 2.4072 - classification_loss: 0.7527
 285/1000 [=======>......................] - ETA: 5:23 - loss: 3.1717 - regression_loss: 2.4163 - classification_loss: 0.7554
 286/1000 [=======>......................] - ETA: 5:23 - loss: 3.1683 - regression_loss: 2.4145 - classification_loss: 0.7538
 287/1000 [=======>......................] - ETA: 5:22 - loss: 3.1704 - regression_loss: 2.4165 - classification_loss: 0.7539
 288/1000 [=======>......................] - ETA: 5:22 - loss: 3.1771 - regression_loss: 2.4203 - classification_loss: 0.7568
 289/1000 [=======>......................] - ETA: 5:21 - loss: 3.1783 - regression_loss: 2.4227 - classification_loss: 0.7556
 290/1000 [=======>......................] - ETA: 5:21 - loss: 3.1861 - regression_loss: 2.4265 - classification_loss: 0.7596
 291/1000 [=======>......................] - ETA: 5:20 - loss: 3.1751 - regression_loss: 2.4182 - classification_loss: 0.7570
 292/1000 [=======>......................] - ETA: 5:20 - loss: 3.1754 - regression_loss: 2.4182 - classification_loss: 0.7572
 293/1000 [=======>......................] - ETA: 5:19 - loss: 3.1645 - regression_loss: 2.4099 - classification_loss: 0.7546
 294/1000 [=======>......................] - ETA: 5:19 - loss: 3.1612 - regression_loss: 2.4073 - classification_loss: 0.7539
 295/1000 [=======>......................] - ETA: 5:18 - loss: 3.1681 - regression_loss: 2.4105 - classification_loss: 0.7577
 296/1000 [=======>......................] - ETA: 5:18 - loss: 3.1724 - regression_loss: 2.4108 - classification_loss: 0.7617
 297/1000 [=======>......................] - ETA: 5:18 - loss: 3.1746 - regression_loss: 2.4092 - classification_loss: 0.7653
 298/1000 [=======>......................] - ETA: 5:17 - loss: 3.1805 - regression_loss: 2.4129 - classification_loss: 0.7676
 299/1000 [=======>......................] - ETA: 5:17 - loss: 3.1858 - regression_loss: 2.4152 - classification_loss: 0.7706
 300/1000 [========>.....................] - ETA: 5:16 - loss: 3.1753 - regression_loss: 2.4072 - classification_loss: 0.7681
 301/1000 [========>.....................] - ETA: 5:16 - loss: 3.1865 - regression_loss: 2.4151 - classification_loss: 0.7714
 302/1000 [========>.....................] - ETA: 5:15 - loss: 3.1849 - regression_loss: 2.4121 - classification_loss: 0.7728
 303/1000 [========>.....................] - ETA: 5:15 - loss: 3.1906 - regression_loss: 2.4168 - classification_loss: 0.7738
 304/1000 [========>.....................] - ETA: 5:14 - loss: 3.1810 - regression_loss: 2.4088 - classification_loss: 0.7722
 305/1000 [========>.....................] - ETA: 5:14 - loss: 3.1796 - regression_loss: 2.4074 - classification_loss: 0.7723
 306/1000 [========>.....................] - ETA: 5:14 - loss: 3.1774 - regression_loss: 2.4058 - classification_loss: 0.7716
 307/1000 [========>.....................] - ETA: 5:13 - loss: 3.1801 - regression_loss: 2.4066 - classification_loss: 0.7736
 308/1000 [========>.....................] - ETA: 5:13 - loss: 3.1796 - regression_loss: 2.4051 - classification_loss: 0.7745
 309/1000 [========>.....................] - ETA: 5:12 - loss: 3.1802 - regression_loss: 2.4063 - classification_loss: 0.7739
 310/1000 [========>.....................] - ETA: 5:12 - loss: 3.1815 - regression_loss: 2.4092 - classification_loss: 0.7723
 311/1000 [========>.....................] - ETA: 5:11 - loss: 3.1798 - regression_loss: 2.4089 - classification_loss: 0.7709
 312/1000 [========>.....................] - ETA: 5:11 - loss: 3.1826 - regression_loss: 2.4119 - classification_loss: 0.7706
 313/1000 [========>.....................] - ETA: 5:10 - loss: 3.1801 - regression_loss: 2.4112 - classification_loss: 0.7689
 314/1000 [========>.....................] - ETA: 5:10 - loss: 3.1820 - regression_loss: 2.4110 - classification_loss: 0.7711
 315/1000 [========>.....................] - ETA: 5:09 - loss: 3.1838 - regression_loss: 2.4129 - classification_loss: 0.7709
 316/1000 [========>.....................] - ETA: 5:09 - loss: 3.1738 - regression_loss: 2.4053 - classification_loss: 0.7685
 317/1000 [========>.....................] - ETA: 5:09 - loss: 3.1758 - regression_loss: 2.4084 - classification_loss: 0.7674
 318/1000 [========>.....................] - ETA: 5:08 - loss: 3.1831 - regression_loss: 2.4134 - classification_loss: 0.7697
 319/1000 [========>.....................] - ETA: 5:08 - loss: 3.1845 - regression_loss: 2.4149 - classification_loss: 0.7696
 320/1000 [========>.....................] - ETA: 5:07 - loss: 3.1833 - regression_loss: 2.4149 - classification_loss: 0.7685
 321/1000 [========>.....................] - ETA: 5:07 - loss: 3.1882 - regression_loss: 2.4176 - classification_loss: 0.7706
 322/1000 [========>.....................] - ETA: 5:06 - loss: 3.1941 - regression_loss: 2.4207 - classification_loss: 0.7734
 323/1000 [========>.....................] - ETA: 5:06 - loss: 3.1987 - regression_loss: 2.4250 - classification_loss: 0.7737
 324/1000 [========>.....................] - ETA: 5:05 - loss: 3.1888 - regression_loss: 2.4175 - classification_loss: 0.7714
 325/1000 [========>.....................] - ETA: 5:05 - loss: 3.1791 - regression_loss: 2.4101 - classification_loss: 0.7690
 326/1000 [========>.....................] - ETA: 5:04 - loss: 3.1693 - regression_loss: 2.4027 - classification_loss: 0.7666
 327/1000 [========>.....................] - ETA: 5:04 - loss: 3.1727 - regression_loss: 2.4061 - classification_loss: 0.7666
 328/1000 [========>.....................] - ETA: 5:04 - loss: 3.1630 - regression_loss: 2.3988 - classification_loss: 0.7643
 329/1000 [========>.....................] - ETA: 5:03 - loss: 3.1618 - regression_loss: 2.3979 - classification_loss: 0.7639
 330/1000 [========>.....................] - ETA: 5:03 - loss: 3.1629 - regression_loss: 2.3996 - classification_loss: 0.7633
 331/1000 [========>.....................] - ETA: 5:02 - loss: 3.1674 - regression_loss: 2.4025 - classification_loss: 0.7649
 332/1000 [========>.....................] - ETA: 5:02 - loss: 3.1693 - regression_loss: 2.4018 - classification_loss: 0.7676
 333/1000 [========>.....................] - ETA: 5:01 - loss: 3.1705 - regression_loss: 2.4020 - classification_loss: 0.7685
 334/1000 [=========>....................] - ETA: 5:01 - loss: 3.1744 - regression_loss: 2.4040 - classification_loss: 0.7704
 335/1000 [=========>....................] - ETA: 5:00 - loss: 3.1649 - regression_loss: 2.3969 - classification_loss: 0.7681
 336/1000 [=========>....................] - ETA: 5:00 - loss: 3.1643 - regression_loss: 2.3971 - classification_loss: 0.7672
 337/1000 [=========>....................] - ETA: 5:00 - loss: 3.1677 - regression_loss: 2.3993 - classification_loss: 0.7684
 338/1000 [=========>....................] - ETA: 4:59 - loss: 3.1678 - regression_loss: 2.3998 - classification_loss: 0.7679
 339/1000 [=========>....................] - ETA: 4:59 - loss: 3.1718 - regression_loss: 2.4038 - classification_loss: 0.7680
 340/1000 [=========>....................] - ETA: 4:58 - loss: 3.1624 - regression_loss: 2.3967 - classification_loss: 0.7657
 341/1000 [=========>....................] - ETA: 4:58 - loss: 3.1642 - regression_loss: 2.3973 - classification_loss: 0.7669
 342/1000 [=========>....................] - ETA: 4:57 - loss: 3.1640 - regression_loss: 2.3974 - classification_loss: 0.7665
 343/1000 [=========>....................] - ETA: 4:57 - loss: 3.1659 - regression_loss: 2.3981 - classification_loss: 0.7678
 344/1000 [=========>....................] - ETA: 4:56 - loss: 3.1567 - regression_loss: 2.3912 - classification_loss: 0.7655
 345/1000 [=========>....................] - ETA: 4:56 - loss: 3.1585 - regression_loss: 2.3934 - classification_loss: 0.7651
 346/1000 [=========>....................] - ETA: 4:56 - loss: 3.1627 - regression_loss: 2.3959 - classification_loss: 0.7668
 347/1000 [=========>....................] - ETA: 4:55 - loss: 3.1664 - regression_loss: 2.3975 - classification_loss: 0.7688
 348/1000 [=========>....................] - ETA: 4:55 - loss: 3.1709 - regression_loss: 2.4002 - classification_loss: 0.7707
 349/1000 [=========>....................] - ETA: 4:54 - loss: 3.1716 - regression_loss: 2.4014 - classification_loss: 0.7702
 350/1000 [=========>....................] - ETA: 4:54 - loss: 3.1742 - regression_loss: 2.4041 - classification_loss: 0.7701
 351/1000 [=========>....................] - ETA: 4:53 - loss: 3.1744 - regression_loss: 2.4055 - classification_loss: 0.7689
 352/1000 [=========>....................] - ETA: 4:53 - loss: 3.1768 - regression_loss: 2.4071 - classification_loss: 0.7698
 353/1000 [=========>....................] - ETA: 4:52 - loss: 3.1780 - regression_loss: 2.4084 - classification_loss: 0.7696
 354/1000 [=========>....................] - ETA: 4:52 - loss: 3.1690 - regression_loss: 2.4016 - classification_loss: 0.7675
 355/1000 [=========>....................] - ETA: 4:51 - loss: 3.1698 - regression_loss: 2.4031 - classification_loss: 0.7667
 356/1000 [=========>....................] - ETA: 4:51 - loss: 3.1707 - regression_loss: 2.4043 - classification_loss: 0.7664
 357/1000 [=========>....................] - ETA: 4:51 - loss: 3.1733 - regression_loss: 2.4064 - classification_loss: 0.7670
 358/1000 [=========>....................] - ETA: 4:50 - loss: 3.1645 - regression_loss: 2.3996 - classification_loss: 0.7649
 359/1000 [=========>....................] - ETA: 4:50 - loss: 3.1723 - regression_loss: 2.4044 - classification_loss: 0.7679
 360/1000 [=========>....................] - ETA: 4:49 - loss: 3.1660 - regression_loss: 2.3978 - classification_loss: 0.7682
 361/1000 [=========>....................] - ETA: 4:49 - loss: 3.1682 - regression_loss: 2.3999 - classification_loss: 0.7683
 362/1000 [=========>....................] - ETA: 4:48 - loss: 3.1701 - regression_loss: 2.4025 - classification_loss: 0.7677
 363/1000 [=========>....................] - ETA: 4:48 - loss: 3.1729 - regression_loss: 2.4048 - classification_loss: 0.7682
 364/1000 [=========>....................] - ETA: 4:47 - loss: 3.1746 - regression_loss: 2.4052 - classification_loss: 0.7693
 365/1000 [=========>....................] - ETA: 4:47 - loss: 3.1763 - regression_loss: 2.4062 - classification_loss: 0.7701
 366/1000 [=========>....................] - ETA: 4:46 - loss: 3.1780 - regression_loss: 2.4069 - classification_loss: 0.7712
 367/1000 [==========>...................] - ETA: 4:46 - loss: 3.1694 - regression_loss: 2.4003 - classification_loss: 0.7691
 368/1000 [==========>...................] - ETA: 4:46 - loss: 3.1780 - regression_loss: 2.4045 - classification_loss: 0.7735
 369/1000 [==========>...................] - ETA: 4:45 - loss: 3.1814 - regression_loss: 2.4068 - classification_loss: 0.7747
 370/1000 [==========>...................] - ETA: 4:45 - loss: 3.1754 - regression_loss: 2.4003 - classification_loss: 0.7751
 371/1000 [==========>...................] - ETA: 4:44 - loss: 3.1740 - regression_loss: 2.4002 - classification_loss: 0.7739
 372/1000 [==========>...................] - ETA: 4:44 - loss: 3.1780 - regression_loss: 2.4002 - classification_loss: 0.7778
 373/1000 [==========>...................] - ETA: 4:43 - loss: 3.1869 - regression_loss: 2.4040 - classification_loss: 0.7829
 374/1000 [==========>...................] - ETA: 4:43 - loss: 3.1955 - regression_loss: 2.4100 - classification_loss: 0.7854
 375/1000 [==========>...................] - ETA: 4:42 - loss: 3.2008 - regression_loss: 2.4132 - classification_loss: 0.7876
 376/1000 [==========>...................] - ETA: 4:42 - loss: 3.1923 - regression_loss: 2.4068 - classification_loss: 0.7855
 377/1000 [==========>...................] - ETA: 4:41 - loss: 3.1925 - regression_loss: 2.4078 - classification_loss: 0.7848
 378/1000 [==========>...................] - ETA: 4:41 - loss: 3.1913 - regression_loss: 2.4072 - classification_loss: 0.7840
 379/1000 [==========>...................] - ETA: 4:41 - loss: 3.1940 - regression_loss: 2.4084 - classification_loss: 0.7856
 380/1000 [==========>...................] - ETA: 4:40 - loss: 3.1871 - regression_loss: 2.4021 - classification_loss: 0.7850
 381/1000 [==========>...................] - ETA: 4:40 - loss: 3.1886 - regression_loss: 2.4028 - classification_loss: 0.7858
 382/1000 [==========>...................] - ETA: 4:39 - loss: 3.1941 - regression_loss: 2.4059 - classification_loss: 0.7882
 383/1000 [==========>...................] - ETA: 4:39 - loss: 3.1974 - regression_loss: 2.4095 - classification_loss: 0.7878
 384/1000 [==========>...................] - ETA: 4:38 - loss: 3.1990 - regression_loss: 2.4122 - classification_loss: 0.7868
 385/1000 [==========>...................] - ETA: 4:38 - loss: 3.1906 - regression_loss: 2.4059 - classification_loss: 0.7847
 386/1000 [==========>...................] - ETA: 4:37 - loss: 3.1824 - regression_loss: 2.3997 - classification_loss: 0.7827
 387/1000 [==========>...................] - ETA: 4:37 - loss: 3.1843 - regression_loss: 2.3998 - classification_loss: 0.7845
 388/1000 [==========>...................] - ETA: 4:36 - loss: 3.1854 - regression_loss: 2.4003 - classification_loss: 0.7851
 389/1000 [==========>...................] - ETA: 4:36 - loss: 3.1772 - regression_loss: 2.3942 - classification_loss: 0.7831
 390/1000 [==========>...................] - ETA: 4:36 - loss: 3.1834 - regression_loss: 2.4010 - classification_loss: 0.7824
 391/1000 [==========>...................] - ETA: 4:35 - loss: 3.1834 - regression_loss: 2.4009 - classification_loss: 0.7824
 392/1000 [==========>...................] - ETA: 4:35 - loss: 3.1819 - regression_loss: 2.4004 - classification_loss: 0.7815
 393/1000 [==========>...................] - ETA: 4:34 - loss: 3.1858 - regression_loss: 2.4037 - classification_loss: 0.7821
 394/1000 [==========>...................] - ETA: 4:34 - loss: 3.1870 - regression_loss: 2.4029 - classification_loss: 0.7841
 395/1000 [==========>...................] - ETA: 4:33 - loss: 3.1790 - regression_loss: 2.3968 - classification_loss: 0.7821
 396/1000 [==========>...................] - ETA: 4:33 - loss: 3.1792 - regression_loss: 2.3977 - classification_loss: 0.7816
 397/1000 [==========>...................] - ETA: 4:32 - loss: 3.1808 - regression_loss: 2.4000 - classification_loss: 0.7808
 398/1000 [==========>...................] - ETA: 4:32 - loss: 3.1817 - regression_loss: 2.4017 - classification_loss: 0.7800
 399/1000 [==========>...................] - ETA: 4:31 - loss: 3.1806 - regression_loss: 2.4008 - classification_loss: 0.7797
 400/1000 [===========>..................] - ETA: 4:31 - loss: 3.1824 - regression_loss: 2.4009 - classification_loss: 0.7815
 401/1000 [===========>..................] - ETA: 4:31 - loss: 3.1829 - regression_loss: 2.4023 - classification_loss: 0.7806
 402/1000 [===========>..................] - ETA: 4:30 - loss: 3.1750 - regression_loss: 2.3963 - classification_loss: 0.7786
 403/1000 [===========>..................] - ETA: 4:30 - loss: 3.1773 - regression_loss: 2.3966 - classification_loss: 0.7807
 404/1000 [===========>..................] - ETA: 4:29 - loss: 3.1774 - regression_loss: 2.3965 - classification_loss: 0.7809
 405/1000 [===========>..................] - ETA: 4:29 - loss: 3.1813 - regression_loss: 2.3993 - classification_loss: 0.7820
 406/1000 [===========>..................] - ETA: 4:28 - loss: 3.1801 - regression_loss: 2.3987 - classification_loss: 0.7814
 407/1000 [===========>..................] - ETA: 4:28 - loss: 3.1841 - regression_loss: 2.4008 - classification_loss: 0.7833
 408/1000 [===========>..................] - ETA: 4:27 - loss: 3.1840 - regression_loss: 2.4020 - classification_loss: 0.7820
 409/1000 [===========>..................] - ETA: 4:27 - loss: 3.1832 - regression_loss: 2.4027 - classification_loss: 0.7806
 410/1000 [===========>..................] - ETA: 4:27 - loss: 3.1862 - regression_loss: 2.4053 - classification_loss: 0.7808
 411/1000 [===========>..................] - ETA: 4:26 - loss: 3.1843 - regression_loss: 2.4034 - classification_loss: 0.7809
 412/1000 [===========>..................] - ETA: 4:26 - loss: 3.1835 - regression_loss: 2.4037 - classification_loss: 0.7798
 413/1000 [===========>..................] - ETA: 4:25 - loss: 3.1899 - regression_loss: 2.4073 - classification_loss: 0.7827
 414/1000 [===========>..................] - ETA: 4:25 - loss: 3.1875 - regression_loss: 2.4063 - classification_loss: 0.7812
 415/1000 [===========>..................] - ETA: 4:24 - loss: 3.1893 - regression_loss: 2.4073 - classification_loss: 0.7820
 416/1000 [===========>..................] - ETA: 4:24 - loss: 3.1896 - regression_loss: 2.4082 - classification_loss: 0.7814
 417/1000 [===========>..................] - ETA: 4:23 - loss: 3.1820 - regression_loss: 2.4024 - classification_loss: 0.7795
 418/1000 [===========>..................] - ETA: 4:23 - loss: 3.1850 - regression_loss: 2.4054 - classification_loss: 0.7796
 419/1000 [===========>..................] - ETA: 4:22 - loss: 3.1867 - regression_loss: 2.4061 - classification_loss: 0.7806
 420/1000 [===========>..................] - ETA: 4:22 - loss: 3.1907 - regression_loss: 2.4087 - classification_loss: 0.7820
 421/1000 [===========>..................] - ETA: 4:22 - loss: 3.1927 - regression_loss: 2.4103 - classification_loss: 0.7823
 422/1000 [===========>..................] - ETA: 4:21 - loss: 3.1986 - regression_loss: 2.4137 - classification_loss: 0.7849
 423/1000 [===========>..................] - ETA: 4:21 - loss: 3.2009 - regression_loss: 2.4164 - classification_loss: 0.7845
 424/1000 [===========>..................] - ETA: 4:20 - loss: 3.2009 - regression_loss: 2.4166 - classification_loss: 0.7843
 425/1000 [===========>..................] - ETA: 4:20 - loss: 3.2012 - regression_loss: 2.4170 - classification_loss: 0.7842
 426/1000 [===========>..................] - ETA: 4:19 - loss: 3.2059 - regression_loss: 2.4203 - classification_loss: 0.7857
 427/1000 [===========>..................] - ETA: 4:19 - loss: 3.2044 - regression_loss: 2.4199 - classification_loss: 0.7845
 428/1000 [===========>..................] - ETA: 4:18 - loss: 3.2028 - regression_loss: 2.4190 - classification_loss: 0.7838
 429/1000 [===========>..................] - ETA: 4:18 - loss: 3.2078 - regression_loss: 2.4236 - classification_loss: 0.7842
 430/1000 [===========>..................] - ETA: 4:17 - loss: 3.2067 - regression_loss: 2.4234 - classification_loss: 0.7833
 431/1000 [===========>..................] - ETA: 4:17 - loss: 3.1993 - regression_loss: 2.4178 - classification_loss: 0.7815
 432/1000 [===========>..................] - ETA: 4:17 - loss: 3.1919 - regression_loss: 2.4122 - classification_loss: 0.7797
 433/1000 [===========>..................] - ETA: 4:16 - loss: 3.1904 - regression_loss: 2.4100 - classification_loss: 0.7804
 434/1000 [============>.................] - ETA: 4:16 - loss: 3.1957 - regression_loss: 2.4130 - classification_loss: 0.7827
 435/1000 [============>.................] - ETA: 4:15 - loss: 3.2011 - regression_loss: 2.4177 - classification_loss: 0.7834
 436/1000 [============>.................] - ETA: 4:15 - loss: 3.2003 - regression_loss: 2.4169 - classification_loss: 0.7834
 437/1000 [============>.................] - ETA: 4:14 - loss: 3.1996 - regression_loss: 2.4168 - classification_loss: 0.7828
 438/1000 [============>.................] - ETA: 4:14 - loss: 3.1923 - regression_loss: 2.4113 - classification_loss: 0.7811
 439/1000 [============>.................] - ETA: 4:13 - loss: 3.1944 - regression_loss: 2.4134 - classification_loss: 0.7810
 440/1000 [============>.................] - ETA: 4:13 - loss: 3.2011 - regression_loss: 2.4174 - classification_loss: 0.7837
 441/1000 [============>.................] - ETA: 4:12 - loss: 3.2043 - regression_loss: 2.4192 - classification_loss: 0.7851
 442/1000 [============>.................] - ETA: 4:12 - loss: 3.2029 - regression_loss: 2.4184 - classification_loss: 0.7845
 443/1000 [============>.................] - ETA: 4:12 - loss: 3.2053 - regression_loss: 2.4204 - classification_loss: 0.7850
 444/1000 [============>.................] - ETA: 4:11 - loss: 3.2070 - regression_loss: 2.4220 - classification_loss: 0.7850
 445/1000 [============>.................] - ETA: 4:11 - loss: 3.2055 - regression_loss: 2.4210 - classification_loss: 0.7845
 446/1000 [============>.................] - ETA: 4:10 - loss: 3.2055 - regression_loss: 2.4204 - classification_loss: 0.7851
 447/1000 [============>.................] - ETA: 4:10 - loss: 3.2039 - regression_loss: 2.4196 - classification_loss: 0.7843
 448/1000 [============>.................] - ETA: 4:09 - loss: 3.2035 - regression_loss: 2.4202 - classification_loss: 0.7833
 449/1000 [============>.................] - ETA: 4:09 - loss: 3.2027 - regression_loss: 2.4192 - classification_loss: 0.7835
 450/1000 [============>.................] - ETA: 4:08 - loss: 3.2029 - regression_loss: 2.4181 - classification_loss: 0.7848
 451/1000 [============>.................] - ETA: 4:08 - loss: 3.2025 - regression_loss: 2.4184 - classification_loss: 0.7841
 452/1000 [============>.................] - ETA: 4:07 - loss: 3.2036 - regression_loss: 2.4193 - classification_loss: 0.7843
 453/1000 [============>.................] - ETA: 4:07 - loss: 3.2077 - regression_loss: 2.4220 - classification_loss: 0.7857
 454/1000 [============>.................] - ETA: 4:07 - loss: 3.2006 - regression_loss: 2.4166 - classification_loss: 0.7840
 455/1000 [============>.................] - ETA: 4:06 - loss: 3.1936 - regression_loss: 2.4113 - classification_loss: 0.7823
 456/1000 [============>.................] - ETA: 4:06 - loss: 3.1921 - regression_loss: 2.4097 - classification_loss: 0.7824
 457/1000 [============>.................] - ETA: 4:05 - loss: 3.1878 - regression_loss: 2.4044 - classification_loss: 0.7834
 458/1000 [============>.................] - ETA: 4:05 - loss: 3.1898 - regression_loss: 2.4054 - classification_loss: 0.7844
 459/1000 [============>.................] - ETA: 4:04 - loss: 3.1914 - regression_loss: 2.4076 - classification_loss: 0.7837
 460/1000 [============>.................] - ETA: 4:04 - loss: 3.1955 - regression_loss: 2.4114 - classification_loss: 0.7841
 461/1000 [============>.................] - ETA: 4:03 - loss: 3.1960 - regression_loss: 2.4124 - classification_loss: 0.7835
 462/1000 [============>.................] - ETA: 4:03 - loss: 3.1988 - regression_loss: 2.4149 - classification_loss: 0.7840
 463/1000 [============>.................] - ETA: 4:02 - loss: 3.2001 - regression_loss: 2.4162 - classification_loss: 0.7838
 464/1000 [============>.................] - ETA: 4:02 - loss: 3.1999 - regression_loss: 2.4171 - classification_loss: 0.7828
 465/1000 [============>.................] - ETA: 4:02 - loss: 3.1993 - regression_loss: 2.4174 - classification_loss: 0.7819
 466/1000 [============>.................] - ETA: 4:01 - loss: 3.1978 - regression_loss: 2.4169 - classification_loss: 0.7810
 467/1000 [=============>................] - ETA: 4:01 - loss: 3.1910 - regression_loss: 2.4117 - classification_loss: 0.7793
 468/1000 [=============>................] - ETA: 4:00 - loss: 3.1842 - regression_loss: 2.4065 - classification_loss: 0.7777
 469/1000 [=============>................] - ETA: 4:00 - loss: 3.1846 - regression_loss: 2.4068 - classification_loss: 0.7778
 470/1000 [=============>................] - ETA: 3:59 - loss: 3.1779 - regression_loss: 2.4017 - classification_loss: 0.7762
 471/1000 [=============>................] - ETA: 3:59 - loss: 3.1788 - regression_loss: 2.4027 - classification_loss: 0.7760
 472/1000 [=============>................] - ETA: 3:58 - loss: 3.1775 - regression_loss: 2.4018 - classification_loss: 0.7757
 473/1000 [=============>................] - ETA: 3:58 - loss: 3.1811 - regression_loss: 2.4050 - classification_loss: 0.7760
 474/1000 [=============>................] - ETA: 3:57 - loss: 3.1744 - regression_loss: 2.4000 - classification_loss: 0.7744
 475/1000 [=============>................] - ETA: 3:57 - loss: 3.1737 - regression_loss: 2.4004 - classification_loss: 0.7733
 476/1000 [=============>................] - ETA: 3:57 - loss: 3.1758 - regression_loss: 2.4025 - classification_loss: 0.7733
 477/1000 [=============>................] - ETA: 3:56 - loss: 3.1694 - regression_loss: 2.3975 - classification_loss: 0.7719
 478/1000 [=============>................] - ETA: 3:56 - loss: 3.1730 - regression_loss: 2.4007 - classification_loss: 0.7722
 479/1000 [=============>................] - ETA: 3:55 - loss: 3.1727 - regression_loss: 2.3996 - classification_loss: 0.7731
 480/1000 [=============>................] - ETA: 3:55 - loss: 3.1661 - regression_loss: 2.3946 - classification_loss: 0.7715
 481/1000 [=============>................] - ETA: 3:54 - loss: 3.1655 - regression_loss: 2.3932 - classification_loss: 0.7723
 482/1000 [=============>................] - ETA: 3:54 - loss: 3.1589 - regression_loss: 2.3882 - classification_loss: 0.7707
 483/1000 [=============>................] - ETA: 3:53 - loss: 3.1607 - regression_loss: 2.3898 - classification_loss: 0.7709
 484/1000 [=============>................] - ETA: 3:53 - loss: 3.1590 - regression_loss: 2.3880 - classification_loss: 0.7710
 485/1000 [=============>................] - ETA: 3:52 - loss: 3.1525 - regression_loss: 2.3831 - classification_loss: 0.7694
 486/1000 [=============>................] - ETA: 3:52 - loss: 3.1528 - regression_loss: 2.3835 - classification_loss: 0.7693
 487/1000 [=============>................] - ETA: 3:52 - loss: 3.1553 - regression_loss: 2.3858 - classification_loss: 0.7695
 488/1000 [=============>................] - ETA: 3:51 - loss: 3.1563 - regression_loss: 2.3851 - classification_loss: 0.7712
 489/1000 [=============>................] - ETA: 3:51 - loss: 3.1599 - regression_loss: 2.3875 - classification_loss: 0.7724
 490/1000 [=============>................] - ETA: 3:50 - loss: 3.1609 - regression_loss: 2.3882 - classification_loss: 0.7727
 491/1000 [=============>................] - ETA: 3:50 - loss: 3.1627 - regression_loss: 2.3896 - classification_loss: 0.7731
 492/1000 [=============>................] - ETA: 3:49 - loss: 3.1653 - regression_loss: 2.3922 - classification_loss: 0.7731
 493/1000 [=============>................] - ETA: 3:49 - loss: 3.1650 - regression_loss: 2.3909 - classification_loss: 0.7742
 494/1000 [=============>................] - ETA: 3:48 - loss: 3.1670 - regression_loss: 2.3914 - classification_loss: 0.7756
 495/1000 [=============>................] - ETA: 3:48 - loss: 3.1606 - regression_loss: 2.3865 - classification_loss: 0.7741
 496/1000 [=============>................] - ETA: 3:48 - loss: 3.1544 - regression_loss: 2.3817 - classification_loss: 0.7727
 497/1000 [=============>................] - ETA: 3:47 - loss: 3.1574 - regression_loss: 2.3837 - classification_loss: 0.7737
 498/1000 [=============>................] - ETA: 3:47 - loss: 3.1595 - regression_loss: 2.3848 - classification_loss: 0.7747
 499/1000 [=============>................] - ETA: 3:46 - loss: 3.1532 - regression_loss: 2.3800 - classification_loss: 0.7731
 500/1000 [==============>...............] - ETA: 3:46 - loss: 3.1559 - regression_loss: 2.3815 - classification_loss: 0.7744
 501/1000 [==============>...............] - ETA: 3:45 - loss: 3.1590 - regression_loss: 2.3838 - classification_loss: 0.7752
 502/1000 [==============>...............] - ETA: 3:45 - loss: 3.1527 - regression_loss: 2.3791 - classification_loss: 0.7736
 503/1000 [==============>...............] - ETA: 3:44 - loss: 3.1544 - regression_loss: 2.3804 - classification_loss: 0.7740
 504/1000 [==============>...............] - ETA: 3:44 - loss: 3.1481 - regression_loss: 2.3757 - classification_loss: 0.7725
 505/1000 [==============>...............] - ETA: 3:43 - loss: 3.1420 - regression_loss: 2.3710 - classification_loss: 0.7710
 506/1000 [==============>...............] - ETA: 3:43 - loss: 3.1436 - regression_loss: 2.3724 - classification_loss: 0.7712
 507/1000 [==============>...............] - ETA: 3:43 - loss: 3.1432 - regression_loss: 2.3725 - classification_loss: 0.7707
 508/1000 [==============>...............] - ETA: 3:42 - loss: 3.1370 - regression_loss: 2.3678 - classification_loss: 0.7692
 509/1000 [==============>...............] - ETA: 3:42 - loss: 3.1308 - regression_loss: 2.3632 - classification_loss: 0.7677
 510/1000 [==============>...............] - ETA: 3:41 - loss: 3.1308 - regression_loss: 2.3632 - classification_loss: 0.7677
 511/1000 [==============>...............] - ETA: 3:41 - loss: 3.1247 - regression_loss: 2.3585 - classification_loss: 0.7662
 512/1000 [==============>...............] - ETA: 3:40 - loss: 3.1273 - regression_loss: 2.3599 - classification_loss: 0.7674
 513/1000 [==============>...............] - ETA: 3:40 - loss: 3.1212 - regression_loss: 2.3553 - classification_loss: 0.7659
 514/1000 [==============>...............] - ETA: 3:39 - loss: 3.1209 - regression_loss: 2.3548 - classification_loss: 0.7661
 515/1000 [==============>...............] - ETA: 3:39 - loss: 3.1260 - regression_loss: 2.3581 - classification_loss: 0.7678
 516/1000 [==============>...............] - ETA: 3:38 - loss: 3.1199 - regression_loss: 2.3536 - classification_loss: 0.7663
 517/1000 [==============>...............] - ETA: 3:38 - loss: 3.1139 - regression_loss: 2.3490 - classification_loss: 0.7648
 518/1000 [==============>...............] - ETA: 3:38 - loss: 3.1160 - regression_loss: 2.3494 - classification_loss: 0.7666
 519/1000 [==============>...............] - ETA: 3:37 - loss: 3.1153 - regression_loss: 2.3488 - classification_loss: 0.7665
 520/1000 [==============>...............] - ETA: 3:37 - loss: 3.1170 - regression_loss: 2.3502 - classification_loss: 0.7667
 521/1000 [==============>...............] - ETA: 3:36 - loss: 3.1110 - regression_loss: 2.3457 - classification_loss: 0.7653
 522/1000 [==============>...............] - ETA: 3:36 - loss: 3.1127 - regression_loss: 2.3466 - classification_loss: 0.7662
 523/1000 [==============>...............] - ETA: 3:35 - loss: 3.1170 - regression_loss: 2.3490 - classification_loss: 0.7681
 524/1000 [==============>...............] - ETA: 3:35 - loss: 3.1214 - regression_loss: 2.3518 - classification_loss: 0.7695
 525/1000 [==============>...............] - ETA: 3:34 - loss: 3.1154 - regression_loss: 2.3474 - classification_loss: 0.7681
 526/1000 [==============>...............] - ETA: 3:34 - loss: 3.1095 - regression_loss: 2.3429 - classification_loss: 0.7666
 527/1000 [==============>...............] - ETA: 3:34 - loss: 3.1082 - regression_loss: 2.3423 - classification_loss: 0.7659
 528/1000 [==============>...............] - ETA: 3:33 - loss: 3.1023 - regression_loss: 2.3379 - classification_loss: 0.7644
 529/1000 [==============>...............] - ETA: 3:33 - loss: 3.0965 - regression_loss: 2.3335 - classification_loss: 0.7630
 530/1000 [==============>...............] - ETA: 3:32 - loss: 3.1002 - regression_loss: 2.3358 - classification_loss: 0.7645
 531/1000 [==============>...............] - ETA: 3:32 - loss: 3.1035 - regression_loss: 2.3376 - classification_loss: 0.7660
 532/1000 [==============>...............] - ETA: 3:31 - loss: 3.0977 - regression_loss: 2.3332 - classification_loss: 0.7645
 533/1000 [==============>...............] - ETA: 3:31 - loss: 3.1033 - regression_loss: 2.3368 - classification_loss: 0.7665
 534/1000 [===============>..............] - ETA: 3:30 - loss: 3.1046 - regression_loss: 2.3378 - classification_loss: 0.7667
 535/1000 [===============>..............] - ETA: 3:30 - loss: 3.1061 - regression_loss: 2.3378 - classification_loss: 0.7683
 536/1000 [===============>..............] - ETA: 3:29 - loss: 3.1003 - regression_loss: 2.3334 - classification_loss: 0.7669
 537/1000 [===============>..............] - ETA: 3:29 - loss: 3.1014 - regression_loss: 2.3338 - classification_loss: 0.7676
 538/1000 [===============>..............] - ETA: 3:29 - loss: 3.0957 - regression_loss: 2.3295 - classification_loss: 0.7662
 539/1000 [===============>..............] - ETA: 3:28 - loss: 3.0960 - regression_loss: 2.3300 - classification_loss: 0.7660
 540/1000 [===============>..............] - ETA: 3:28 - loss: 3.0976 - regression_loss: 2.3305 - classification_loss: 0.7671
 541/1000 [===============>..............] - ETA: 3:27 - loss: 3.0978 - regression_loss: 2.3305 - classification_loss: 0.7674
 542/1000 [===============>..............] - ETA: 3:27 - loss: 3.0921 - regression_loss: 2.3262 - classification_loss: 0.7659
 543/1000 [===============>..............] - ETA: 3:26 - loss: 3.0937 - regression_loss: 2.3270 - classification_loss: 0.7667
 544/1000 [===============>..............] - ETA: 3:26 - loss: 3.0964 - regression_loss: 2.3290 - classification_loss: 0.7674
 545/1000 [===============>..............] - ETA: 3:25 - loss: 3.0961 - regression_loss: 2.3279 - classification_loss: 0.7682
 546/1000 [===============>..............] - ETA: 3:25 - loss: 3.0965 - regression_loss: 2.3292 - classification_loss: 0.7673
 547/1000 [===============>..............] - ETA: 3:24 - loss: 3.0909 - regression_loss: 2.3249 - classification_loss: 0.7659
 548/1000 [===============>..............] - ETA: 3:24 - loss: 3.0921 - regression_loss: 2.3243 - classification_loss: 0.7678
 549/1000 [===============>..............] - ETA: 3:24 - loss: 3.0918 - regression_loss: 2.3245 - classification_loss: 0.7673
 550/1000 [===============>..............] - ETA: 3:23 - loss: 3.0971 - regression_loss: 2.3270 - classification_loss: 0.7701
 551/1000 [===============>..............] - ETA: 3:23 - loss: 3.1019 - regression_loss: 2.3309 - classification_loss: 0.7710
 552/1000 [===============>..............] - ETA: 3:22 - loss: 3.1035 - regression_loss: 2.3318 - classification_loss: 0.7717
 553/1000 [===============>..............] - ETA: 3:22 - loss: 3.1050 - regression_loss: 2.3324 - classification_loss: 0.7725
 554/1000 [===============>..............] - ETA: 3:21 - loss: 3.1060 - regression_loss: 2.3337 - classification_loss: 0.7723
 555/1000 [===============>..............] - ETA: 3:21 - loss: 3.1085 - regression_loss: 2.3348 - classification_loss: 0.7737
 556/1000 [===============>..............] - ETA: 3:20 - loss: 3.1029 - regression_loss: 2.3306 - classification_loss: 0.7723
 557/1000 [===============>..............] - ETA: 3:20 - loss: 3.1040 - regression_loss: 2.3321 - classification_loss: 0.7720
 558/1000 [===============>..............] - ETA: 3:19 - loss: 3.0987 - regression_loss: 2.3279 - classification_loss: 0.7708
 559/1000 [===============>..............] - ETA: 3:19 - loss: 3.1006 - regression_loss: 2.3299 - classification_loss: 0.7707
 560/1000 [===============>..............] - ETA: 3:19 - loss: 3.1021 - regression_loss: 2.3305 - classification_loss: 0.7717
 561/1000 [===============>..............] - ETA: 3:18 - loss: 3.1047 - regression_loss: 2.3327 - classification_loss: 0.7719
 562/1000 [===============>..............] - ETA: 3:18 - loss: 3.0993 - regression_loss: 2.3286 - classification_loss: 0.7707
 563/1000 [===============>..............] - ETA: 3:17 - loss: 3.0938 - regression_loss: 2.3244 - classification_loss: 0.7694
 564/1000 [===============>..............] - ETA: 3:17 - loss: 3.0952 - regression_loss: 2.3251 - classification_loss: 0.7701
 565/1000 [===============>..............] - ETA: 3:16 - loss: 3.0934 - regression_loss: 2.3239 - classification_loss: 0.7695
 566/1000 [===============>..............] - ETA: 3:16 - loss: 3.0930 - regression_loss: 2.3234 - classification_loss: 0.7696
 567/1000 [================>.............] - ETA: 3:15 - loss: 3.0947 - regression_loss: 2.3252 - classification_loss: 0.7696
 568/1000 [================>.............] - ETA: 3:15 - loss: 3.0975 - regression_loss: 2.3271 - classification_loss: 0.7704
 569/1000 [================>.............] - ETA: 3:14 - loss: 3.0983 - regression_loss: 2.3282 - classification_loss: 0.7702
 570/1000 [================>.............] - ETA: 3:14 - loss: 3.0929 - regression_loss: 2.3241 - classification_loss: 0.7688
 571/1000 [================>.............] - ETA: 3:14 - loss: 3.0911 - regression_loss: 2.3232 - classification_loss: 0.7679
 572/1000 [================>.............] - ETA: 3:13 - loss: 3.0921 - regression_loss: 2.3241 - classification_loss: 0.7679
 573/1000 [================>.............] - ETA: 3:13 - loss: 3.0903 - regression_loss: 2.3230 - classification_loss: 0.7673
 574/1000 [================>.............] - ETA: 3:12 - loss: 3.0934 - regression_loss: 2.3263 - classification_loss: 0.7671
 575/1000 [================>.............] - ETA: 3:12 - loss: 3.0945 - regression_loss: 2.3273 - classification_loss: 0.7672
 576/1000 [================>.............] - ETA: 3:11 - loss: 3.0966 - regression_loss: 2.3298 - classification_loss: 0.7668
 577/1000 [================>.............] - ETA: 3:11 - loss: 3.0973 - regression_loss: 2.3310 - classification_loss: 0.7663
 578/1000 [================>.............] - ETA: 3:10 - loss: 3.1091 - regression_loss: 2.3370 - classification_loss: 0.7721
 579/1000 [================>.............] - ETA: 3:10 - loss: 3.1037 - regression_loss: 2.3330 - classification_loss: 0.7707
 580/1000 [================>.............] - ETA: 3:10 - loss: 3.1078 - regression_loss: 2.3357 - classification_loss: 0.7721
 581/1000 [================>.............] - ETA: 3:09 - loss: 3.1024 - regression_loss: 2.3317 - classification_loss: 0.7708
 582/1000 [================>.............] - ETA: 3:09 - loss: 3.1042 - regression_loss: 2.3321 - classification_loss: 0.7721
 583/1000 [================>.............] - ETA: 3:08 - loss: 3.1029 - regression_loss: 2.3307 - classification_loss: 0.7722
 584/1000 [================>.............] - ETA: 3:08 - loss: 3.1041 - regression_loss: 2.3324 - classification_loss: 0.7716
 585/1000 [================>.............] - ETA: 3:07 - loss: 3.0988 - regression_loss: 2.3285 - classification_loss: 0.7703
 586/1000 [================>.............] - ETA: 3:07 - loss: 3.0994 - regression_loss: 2.3281 - classification_loss: 0.7713
 587/1000 [================>.............] - ETA: 3:06 - loss: 3.1023 - regression_loss: 2.3304 - classification_loss: 0.7719
 588/1000 [================>.............] - ETA: 3:06 - loss: 3.0971 - regression_loss: 2.3264 - classification_loss: 0.7706
 589/1000 [================>.............] - ETA: 3:05 - loss: 3.0918 - regression_loss: 2.3225 - classification_loss: 0.7693
 590/1000 [================>.............] - ETA: 3:05 - loss: 3.0939 - regression_loss: 2.3246 - classification_loss: 0.7693
 591/1000 [================>.............] - ETA: 3:05 - loss: 3.0974 - regression_loss: 2.3270 - classification_loss: 0.7705
 592/1000 [================>.............] - ETA: 3:04 - loss: 3.0966 - regression_loss: 2.3267 - classification_loss: 0.7699
 593/1000 [================>.............] - ETA: 3:04 - loss: 3.0971 - regression_loss: 2.3261 - classification_loss: 0.7710
 594/1000 [================>.............] - ETA: 3:03 - loss: 3.0960 - regression_loss: 2.3256 - classification_loss: 0.7704
 595/1000 [================>.............] - ETA: 3:03 - loss: 3.0908 - regression_loss: 2.3217 - classification_loss: 0.7691
 596/1000 [================>.............] - ETA: 3:02 - loss: 3.0856 - regression_loss: 2.3178 - classification_loss: 0.7678
 597/1000 [================>.............] - ETA: 3:02 - loss: 3.0879 - regression_loss: 2.3192 - classification_loss: 0.7687
 598/1000 [================>.............] - ETA: 3:01 - loss: 3.0887 - regression_loss: 2.3197 - classification_loss: 0.7690
 599/1000 [================>.............] - ETA: 3:01 - loss: 3.0893 - regression_loss: 2.3201 - classification_loss: 0.7692
 600/1000 [=================>............] - ETA: 3:00 - loss: 3.0841 - regression_loss: 2.3162 - classification_loss: 0.7679
 601/1000 [=================>............] - ETA: 3:00 - loss: 3.0850 - regression_loss: 2.3172 - classification_loss: 0.7677
 602/1000 [=================>............] - ETA: 3:00 - loss: 3.0904 - regression_loss: 2.3205 - classification_loss: 0.7700
 603/1000 [=================>............] - ETA: 2:59 - loss: 3.0905 - regression_loss: 2.3201 - classification_loss: 0.7704
 604/1000 [=================>............] - ETA: 2:59 - loss: 3.0916 - regression_loss: 2.3213 - classification_loss: 0.7703
 605/1000 [=================>............] - ETA: 2:58 - loss: 3.0943 - regression_loss: 2.3230 - classification_loss: 0.7712
 606/1000 [=================>............] - ETA: 2:58 - loss: 3.0962 - regression_loss: 2.3241 - classification_loss: 0.7721
 607/1000 [=================>............] - ETA: 2:57 - loss: 3.0974 - regression_loss: 2.3259 - classification_loss: 0.7715
 608/1000 [=================>............] - ETA: 2:57 - loss: 3.1004 - regression_loss: 2.3278 - classification_loss: 0.7726
 609/1000 [=================>............] - ETA: 2:56 - loss: 3.1018 - regression_loss: 2.3296 - classification_loss: 0.7723
 610/1000 [=================>............] - ETA: 2:56 - loss: 3.1038 - regression_loss: 2.3320 - classification_loss: 0.7718
 611/1000 [=================>............] - ETA: 2:56 - loss: 3.0989 - regression_loss: 2.3282 - classification_loss: 0.7707
 612/1000 [=================>............] - ETA: 2:55 - loss: 3.0946 - regression_loss: 2.3244 - classification_loss: 0.7703
 613/1000 [=================>............] - ETA: 2:55 - loss: 3.0928 - regression_loss: 2.3231 - classification_loss: 0.7697
 614/1000 [=================>............] - ETA: 2:54 - loss: 3.0944 - regression_loss: 2.3239 - classification_loss: 0.7705
 615/1000 [=================>............] - ETA: 2:54 - loss: 3.0973 - regression_loss: 2.3260 - classification_loss: 0.7713
 616/1000 [=================>............] - ETA: 2:53 - loss: 3.0922 - regression_loss: 2.3222 - classification_loss: 0.7701
 617/1000 [=================>............] - ETA: 2:53 - loss: 3.0917 - regression_loss: 2.3220 - classification_loss: 0.7697
 618/1000 [=================>............] - ETA: 2:52 - loss: 3.0954 - regression_loss: 2.3260 - classification_loss: 0.7695
 619/1000 [=================>............] - ETA: 2:52 - loss: 3.0953 - regression_loss: 2.3264 - classification_loss: 0.7690
 620/1000 [=================>............] - ETA: 2:51 - loss: 3.0945 - regression_loss: 2.3258 - classification_loss: 0.7687
 621/1000 [=================>............] - ETA: 2:51 - loss: 3.0960 - regression_loss: 2.3273 - classification_loss: 0.7687
 622/1000 [=================>............] - ETA: 2:51 - loss: 3.0980 - regression_loss: 2.3291 - classification_loss: 0.7688
 623/1000 [=================>............] - ETA: 2:50 - loss: 3.0930 - regression_loss: 2.3254 - classification_loss: 0.7676
 624/1000 [=================>............] - ETA: 2:50 - loss: 3.0880 - regression_loss: 2.3217 - classification_loss: 0.7663
 625/1000 [=================>............] - ETA: 2:49 - loss: 3.0831 - regression_loss: 2.3180 - classification_loss: 0.7651
 626/1000 [=================>............] - ETA: 2:49 - loss: 3.0849 - regression_loss: 2.3184 - classification_loss: 0.7666
 627/1000 [=================>............] - ETA: 2:48 - loss: 3.0855 - regression_loss: 2.3193 - classification_loss: 0.7662
 628/1000 [=================>............] - ETA: 2:48 - loss: 3.0896 - regression_loss: 2.3220 - classification_loss: 0.7676
 629/1000 [=================>............] - ETA: 2:47 - loss: 3.0913 - regression_loss: 2.3237 - classification_loss: 0.7677
 630/1000 [=================>............] - ETA: 2:47 - loss: 3.0941 - regression_loss: 2.3258 - classification_loss: 0.7683
 631/1000 [=================>............] - ETA: 2:47 - loss: 3.0893 - regression_loss: 2.3222 - classification_loss: 0.7672
 632/1000 [=================>............] - ETA: 2:46 - loss: 3.0909 - regression_loss: 2.3234 - classification_loss: 0.7675
 633/1000 [=================>............] - ETA: 2:46 - loss: 3.0898 - regression_loss: 2.3227 - classification_loss: 0.7671
 634/1000 [==================>...........] - ETA: 2:45 - loss: 3.0924 - regression_loss: 2.3248 - classification_loss: 0.7676
 635/1000 [==================>...........] - ETA: 2:45 - loss: 3.0939 - regression_loss: 2.3261 - classification_loss: 0.7678
 636/1000 [==================>...........] - ETA: 2:44 - loss: 3.0945 - regression_loss: 2.3271 - classification_loss: 0.7674
 637/1000 [==================>...........] - ETA: 2:44 - loss: 3.0945 - regression_loss: 2.3278 - classification_loss: 0.7667
 638/1000 [==================>...........] - ETA: 2:43 - loss: 3.0939 - regression_loss: 2.3265 - classification_loss: 0.7674
 639/1000 [==================>...........] - ETA: 2:43 - loss: 3.0946 - regression_loss: 2.3268 - classification_loss: 0.7678
 640/1000 [==================>...........] - ETA: 2:42 - loss: 3.0940 - regression_loss: 2.3270 - classification_loss: 0.7670
 641/1000 [==================>...........] - ETA: 2:42 - loss: 3.0944 - regression_loss: 2.3263 - classification_loss: 0.7681
 642/1000 [==================>...........] - ETA: 2:42 - loss: 3.0926 - regression_loss: 2.3251 - classification_loss: 0.7674
 643/1000 [==================>...........] - ETA: 2:41 - loss: 3.0878 - regression_loss: 2.3215 - classification_loss: 0.7663
 644/1000 [==================>...........] - ETA: 2:41 - loss: 3.0900 - regression_loss: 2.3226 - classification_loss: 0.7673
 645/1000 [==================>...........] - ETA: 2:40 - loss: 3.0852 - regression_loss: 2.3190 - classification_loss: 0.7661
 646/1000 [==================>...........] - ETA: 2:40 - loss: 3.0851 - regression_loss: 2.3189 - classification_loss: 0.7662
 647/1000 [==================>...........] - ETA: 2:39 - loss: 3.0860 - regression_loss: 2.3194 - classification_loss: 0.7666
 648/1000 [==================>...........] - ETA: 2:39 - loss: 3.0863 - regression_loss: 2.3202 - classification_loss: 0.7661
 649/1000 [==================>...........] - ETA: 2:38 - loss: 3.0870 - regression_loss: 2.3204 - classification_loss: 0.7666
 650/1000 [==================>...........] - ETA: 2:38 - loss: 3.0900 - regression_loss: 2.3222 - classification_loss: 0.7678
 651/1000 [==================>...........] - ETA: 2:37 - loss: 3.0908 - regression_loss: 2.3230 - classification_loss: 0.7678
 652/1000 [==================>...........] - ETA: 2:37 - loss: 3.0944 - regression_loss: 2.3255 - classification_loss: 0.7689
 653/1000 [==================>...........] - ETA: 2:37 - loss: 3.0951 - regression_loss: 2.3264 - classification_loss: 0.7687
 654/1000 [==================>...........] - ETA: 2:36 - loss: 3.0978 - regression_loss: 2.3289 - classification_loss: 0.7689
 655/1000 [==================>...........] - ETA: 2:36 - loss: 3.0931 - regression_loss: 2.3253 - classification_loss: 0.7678
 656/1000 [==================>...........] - ETA: 2:35 - loss: 3.0939 - regression_loss: 2.3265 - classification_loss: 0.7674
 657/1000 [==================>...........] - ETA: 2:35 - loss: 3.0892 - regression_loss: 2.3229 - classification_loss: 0.7663
 658/1000 [==================>...........] - ETA: 2:34 - loss: 3.0899 - regression_loss: 2.3233 - classification_loss: 0.7666
 659/1000 [==================>...........] - ETA: 2:34 - loss: 3.0905 - regression_loss: 2.3240 - classification_loss: 0.7665
 660/1000 [==================>...........] - ETA: 2:33 - loss: 3.0921 - regression_loss: 2.3258 - classification_loss: 0.7663
 661/1000 [==================>...........] - ETA: 2:33 - loss: 3.0922 - regression_loss: 2.3260 - classification_loss: 0.7663
 662/1000 [==================>...........] - ETA: 2:32 - loss: 3.0941 - regression_loss: 2.3281 - classification_loss: 0.7660
 663/1000 [==================>...........] - ETA: 2:32 - loss: 3.0945 - regression_loss: 2.3290 - classification_loss: 0.7655
 664/1000 [==================>...........] - ETA: 2:32 - loss: 3.0939 - regression_loss: 2.3287 - classification_loss: 0.7652
 665/1000 [==================>...........] - ETA: 2:31 - loss: 3.0956 - regression_loss: 2.3302 - classification_loss: 0.7654
 666/1000 [==================>...........] - ETA: 2:31 - loss: 3.0978 - regression_loss: 2.3313 - classification_loss: 0.7665
 667/1000 [===================>..........] - ETA: 2:30 - loss: 3.0933 - regression_loss: 2.3278 - classification_loss: 0.7655
 668/1000 [===================>..........] - ETA: 2:30 - loss: 3.0949 - regression_loss: 2.3293 - classification_loss: 0.7656
 669/1000 [===================>..........] - ETA: 2:29 - loss: 3.0956 - regression_loss: 2.3303 - classification_loss: 0.7654
 670/1000 [===================>..........] - ETA: 2:29 - loss: 3.0958 - regression_loss: 2.3307 - classification_loss: 0.7651
 671/1000 [===================>..........] - ETA: 2:28 - loss: 3.0964 - regression_loss: 2.3309 - classification_loss: 0.7655
 672/1000 [===================>..........] - ETA: 2:28 - loss: 3.0981 - regression_loss: 2.3326 - classification_loss: 0.7656
 673/1000 [===================>..........] - ETA: 2:28 - loss: 3.0975 - regression_loss: 2.3322 - classification_loss: 0.7653
 674/1000 [===================>..........] - ETA: 2:27 - loss: 3.0996 - regression_loss: 2.3344 - classification_loss: 0.7651
 675/1000 [===================>..........] - ETA: 2:27 - loss: 3.1013 - regression_loss: 2.3365 - classification_loss: 0.7648
 676/1000 [===================>..........] - ETA: 2:26 - loss: 3.1030 - regression_loss: 2.3386 - classification_loss: 0.7644
 677/1000 [===================>..........] - ETA: 2:26 - loss: 3.1032 - regression_loss: 2.3389 - classification_loss: 0.7644
 678/1000 [===================>..........] - ETA: 2:25 - loss: 3.1021 - regression_loss: 2.3383 - classification_loss: 0.7638
 679/1000 [===================>..........] - ETA: 2:25 - loss: 3.0983 - regression_loss: 2.3348 - classification_loss: 0.7634
 680/1000 [===================>..........] - ETA: 2:24 - loss: 3.1009 - regression_loss: 2.3375 - classification_loss: 0.7634
 681/1000 [===================>..........] - ETA: 2:24 - loss: 3.1033 - regression_loss: 2.3400 - classification_loss: 0.7633
 682/1000 [===================>..........] - ETA: 2:23 - loss: 3.1037 - regression_loss: 2.3406 - classification_loss: 0.7630
 683/1000 [===================>..........] - ETA: 2:23 - loss: 3.1037 - regression_loss: 2.3410 - classification_loss: 0.7627
 684/1000 [===================>..........] - ETA: 2:23 - loss: 3.0996 - regression_loss: 2.3375 - classification_loss: 0.7621
 685/1000 [===================>..........] - ETA: 2:22 - loss: 3.0992 - regression_loss: 2.3376 - classification_loss: 0.7616
 686/1000 [===================>..........] - ETA: 2:22 - loss: 3.1013 - regression_loss: 2.3390 - classification_loss: 0.7623
 687/1000 [===================>..........] - ETA: 2:21 - loss: 3.1023 - regression_loss: 2.3403 - classification_loss: 0.7620
 688/1000 [===================>..........] - ETA: 2:21 - loss: 3.1041 - regression_loss: 2.3418 - classification_loss: 0.7623
 689/1000 [===================>..........] - ETA: 2:20 - loss: 3.1079 - regression_loss: 2.3451 - classification_loss: 0.7628
 690/1000 [===================>..........] - ETA: 2:20 - loss: 3.1034 - regression_loss: 2.3417 - classification_loss: 0.7617
 691/1000 [===================>..........] - ETA: 2:19 - loss: 3.1036 - regression_loss: 2.3424 - classification_loss: 0.7612
 692/1000 [===================>..........] - ETA: 2:19 - loss: 3.1028 - regression_loss: 2.3419 - classification_loss: 0.7609
 693/1000 [===================>..........] - ETA: 2:18 - loss: 3.1048 - regression_loss: 2.3440 - classification_loss: 0.7607
 694/1000 [===================>..........] - ETA: 2:18 - loss: 3.1043 - regression_loss: 2.3440 - classification_loss: 0.7603
 695/1000 [===================>..........] - ETA: 2:18 - loss: 3.1041 - regression_loss: 2.3443 - classification_loss: 0.7598
 696/1000 [===================>..........] - ETA: 2:17 - loss: 3.1068 - regression_loss: 2.3465 - classification_loss: 0.7604
 697/1000 [===================>..........] - ETA: 2:17 - loss: 3.1059 - regression_loss: 2.3458 - classification_loss: 0.7601
 698/1000 [===================>..........] - ETA: 2:16 - loss: 3.1089 - regression_loss: 2.3488 - classification_loss: 0.7601
 699/1000 [===================>..........] - ETA: 2:16 - loss: 3.1099 - regression_loss: 2.3498 - classification_loss: 0.7602
 700/1000 [====================>.........] - ETA: 2:15 - loss: 3.1113 - regression_loss: 2.3512 - classification_loss: 0.7601
 701/1000 [====================>.........] - ETA: 2:15 - loss: 3.1137 - regression_loss: 2.3527 - classification_loss: 0.7609
 702/1000 [====================>.........] - ETA: 2:14 - loss: 3.1092 - regression_loss: 2.3494 - classification_loss: 0.7598
 703/1000 [====================>.........] - ETA: 2:14 - loss: 3.1101 - regression_loss: 2.3503 - classification_loss: 0.7598
 704/1000 [====================>.........] - ETA: 2:13 - loss: 3.1121 - regression_loss: 2.3518 - classification_loss: 0.7604
 705/1000 [====================>.........] - ETA: 2:13 - loss: 3.1134 - regression_loss: 2.3534 - classification_loss: 0.7601
 706/1000 [====================>.........] - ETA: 2:13 - loss: 3.1155 - regression_loss: 2.3562 - classification_loss: 0.7593
 707/1000 [====================>.........] - ETA: 2:12 - loss: 3.1152 - regression_loss: 2.3562 - classification_loss: 0.7591
 708/1000 [====================>.........] - ETA: 2:12 - loss: 3.1154 - regression_loss: 2.3568 - classification_loss: 0.7586
 709/1000 [====================>.........] - ETA: 2:11 - loss: 3.1160 - regression_loss: 2.3576 - classification_loss: 0.7583
 710/1000 [====================>.........] - ETA: 2:11 - loss: 3.1117 - regression_loss: 2.3543 - classification_loss: 0.7574
 711/1000 [====================>.........] - ETA: 2:10 - loss: 3.1123 - regression_loss: 2.3546 - classification_loss: 0.7577
 712/1000 [====================>.........] - ETA: 2:10 - loss: 3.1142 - regression_loss: 2.3564 - classification_loss: 0.7578
 713/1000 [====================>.........] - ETA: 2:09 - loss: 3.1147 - regression_loss: 2.3569 - classification_loss: 0.7577
 714/1000 [====================>.........] - ETA: 2:09 - loss: 3.1149 - regression_loss: 2.3576 - classification_loss: 0.7573
 715/1000 [====================>.........] - ETA: 2:08 - loss: 3.1155 - regression_loss: 2.3584 - classification_loss: 0.7571
 716/1000 [====================>.........] - ETA: 2:08 - loss: 3.1170 - regression_loss: 2.3597 - classification_loss: 0.7573
 717/1000 [====================>.........] - ETA: 2:08 - loss: 3.1130 - regression_loss: 2.3564 - classification_loss: 0.7565
 718/1000 [====================>.........] - ETA: 2:07 - loss: 3.1141 - regression_loss: 2.3576 - classification_loss: 0.7565
 719/1000 [====================>.........] - ETA: 2:07 - loss: 3.1176 - regression_loss: 2.3604 - classification_loss: 0.7572
 720/1000 [====================>.........] - ETA: 2:06 - loss: 3.1204 - regression_loss: 2.3623 - classification_loss: 0.7582
 721/1000 [====================>.........] - ETA: 2:06 - loss: 3.1213 - regression_loss: 2.3635 - classification_loss: 0.7577
 722/1000 [====================>.........] - ETA: 2:05 - loss: 3.1169 - regression_loss: 2.3602 - classification_loss: 0.7567
 723/1000 [====================>.........] - ETA: 2:05 - loss: 3.1171 - regression_loss: 2.3607 - classification_loss: 0.7563
 724/1000 [====================>.........] - ETA: 2:04 - loss: 3.1128 - regression_loss: 2.3575 - classification_loss: 0.7553
 725/1000 [====================>.........] - ETA: 2:04 - loss: 3.1124 - regression_loss: 2.3575 - classification_loss: 0.7549
 726/1000 [====================>.........] - ETA: 2:04 - loss: 3.1085 - regression_loss: 2.3543 - classification_loss: 0.7542
 727/1000 [====================>.........] - ETA: 2:03 - loss: 3.1090 - regression_loss: 2.3547 - classification_loss: 0.7543
 728/1000 [====================>.........] - ETA: 2:03 - loss: 3.1100 - regression_loss: 2.3554 - classification_loss: 0.7546
 729/1000 [====================>.........] - ETA: 2:02 - loss: 3.1118 - regression_loss: 2.3571 - classification_loss: 0.7547
 730/1000 [====================>.........] - ETA: 2:02 - loss: 3.1118 - regression_loss: 2.3571 - classification_loss: 0.7547
 731/1000 [====================>.........] - ETA: 2:01 - loss: 3.1076 - regression_loss: 2.3539 - classification_loss: 0.7536
 732/1000 [====================>.........] - ETA: 2:01 - loss: 3.1083 - regression_loss: 2.3545 - classification_loss: 0.7538
 733/1000 [====================>.........] - ETA: 2:00 - loss: 3.1041 - regression_loss: 2.3513 - classification_loss: 0.7528
 734/1000 [=====================>........] - ETA: 2:00 - loss: 3.0999 - regression_loss: 2.3481 - classification_loss: 0.7518
 735/1000 [=====================>........] - ETA: 1:59 - loss: 3.1020 - regression_loss: 2.3486 - classification_loss: 0.7534
 736/1000 [=====================>........] - ETA: 1:59 - loss: 3.1032 - regression_loss: 2.3498 - classification_loss: 0.7534
 737/1000 [=====================>........] - ETA: 1:59 - loss: 3.1094 - regression_loss: 2.3533 - classification_loss: 0.7561
 738/1000 [=====================>........] - ETA: 1:58 - loss: 3.1101 - regression_loss: 2.3536 - classification_loss: 0.7564
 739/1000 [=====================>........] - ETA: 1:58 - loss: 3.1093 - regression_loss: 2.3537 - classification_loss: 0.7557
 740/1000 [=====================>........] - ETA: 1:57 - loss: 3.1051 - regression_loss: 2.3505 - classification_loss: 0.7547
 741/1000 [=====================>........] - ETA: 1:57 - loss: 3.1009 - regression_loss: 2.3473 - classification_loss: 0.7536
 742/1000 [=====================>........] - ETA: 1:56 - loss: 3.0968 - regression_loss: 2.3441 - classification_loss: 0.7526
 743/1000 [=====================>........] - ETA: 1:56 - loss: 3.0987 - regression_loss: 2.3462 - classification_loss: 0.7525
 744/1000 [=====================>........] - ETA: 1:55 - loss: 3.1020 - regression_loss: 2.3478 - classification_loss: 0.7542
 745/1000 [=====================>........] - ETA: 1:55 - loss: 3.1017 - regression_loss: 2.3477 - classification_loss: 0.7540
 746/1000 [=====================>........] - ETA: 1:54 - loss: 3.1031 - regression_loss: 2.3484 - classification_loss: 0.7547
 747/1000 [=====================>........] - ETA: 1:54 - loss: 3.1071 - regression_loss: 2.3505 - classification_loss: 0.7566
 748/1000 [=====================>........] - ETA: 1:54 - loss: 3.1030 - regression_loss: 2.3473 - classification_loss: 0.7556
 749/1000 [=====================>........] - ETA: 1:53 - loss: 3.0988 - regression_loss: 2.3442 - classification_loss: 0.7546
 750/1000 [=====================>........] - ETA: 1:53 - loss: 3.0986 - regression_loss: 2.3437 - classification_loss: 0.7550
 751/1000 [=====================>........] - ETA: 1:52 - loss: 3.1007 - regression_loss: 2.3443 - classification_loss: 0.7564
 752/1000 [=====================>........] - ETA: 1:52 - loss: 3.1037 - regression_loss: 2.3462 - classification_loss: 0.7574
 753/1000 [=====================>........] - ETA: 1:51 - loss: 3.1070 - regression_loss: 2.3478 - classification_loss: 0.7592
 754/1000 [=====================>........] - ETA: 1:51 - loss: 3.1067 - regression_loss: 2.3476 - classification_loss: 0.7590
 755/1000 [=====================>........] - ETA: 1:50 - loss: 3.1056 - regression_loss: 2.3470 - classification_loss: 0.7586
 756/1000 [=====================>........] - ETA: 1:50 - loss: 3.1059 - regression_loss: 2.3462 - classification_loss: 0.7597
 757/1000 [=====================>........] - ETA: 1:49 - loss: 3.1084 - regression_loss: 2.3490 - classification_loss: 0.7594
 758/1000 [=====================>........] - ETA: 1:49 - loss: 3.1043 - regression_loss: 2.3459 - classification_loss: 0.7584
 759/1000 [=====================>........] - ETA: 1:49 - loss: 3.1002 - regression_loss: 2.3428 - classification_loss: 0.7574
 760/1000 [=====================>........] - ETA: 1:48 - loss: 3.0962 - regression_loss: 2.3397 - classification_loss: 0.7565
 761/1000 [=====================>........] - ETA: 1:48 - loss: 3.1004 - regression_loss: 2.3434 - classification_loss: 0.7570
 762/1000 [=====================>........] - ETA: 1:47 - loss: 3.1004 - regression_loss: 2.3435 - classification_loss: 0.7569
 763/1000 [=====================>........] - ETA: 1:47 - loss: 3.1026 - regression_loss: 2.3454 - classification_loss: 0.7572
 764/1000 [=====================>........] - ETA: 1:46 - loss: 3.1056 - regression_loss: 2.3468 - classification_loss: 0.7588
 765/1000 [=====================>........] - ETA: 1:46 - loss: 3.1056 - regression_loss: 2.3474 - classification_loss: 0.7582
 766/1000 [=====================>........] - ETA: 1:45 - loss: 3.1015 - regression_loss: 2.3443 - classification_loss: 0.7572
 767/1000 [======================>.......] - ETA: 1:45 - loss: 3.1015 - regression_loss: 2.3447 - classification_loss: 0.7569
 768/1000 [======================>.......] - ETA: 1:45 - loss: 3.1011 - regression_loss: 2.3444 - classification_loss: 0.7567
 769/1000 [======================>.......] - ETA: 1:44 - loss: 3.1030 - regression_loss: 2.3457 - classification_loss: 0.7572
 770/1000 [======================>.......] - ETA: 1:44 - loss: 3.0989 - regression_loss: 2.3427 - classification_loss: 0.7563
 771/1000 [======================>.......] - ETA: 1:43 - loss: 3.1023 - regression_loss: 2.3444 - classification_loss: 0.7579
 772/1000 [======================>.......] - ETA: 1:43 - loss: 3.1041 - regression_loss: 2.3454 - classification_loss: 0.7587
 773/1000 [======================>.......] - ETA: 1:42 - loss: 3.1037 - regression_loss: 2.3454 - classification_loss: 0.7583
 774/1000 [======================>.......] - ETA: 1:42 - loss: 3.1033 - regression_loss: 2.3455 - classification_loss: 0.7577
 775/1000 [======================>.......] - ETA: 1:41 - loss: 3.1032 - regression_loss: 2.3455 - classification_loss: 0.7577
 776/1000 [======================>.......] - ETA: 1:41 - loss: 3.1042 - regression_loss: 2.3468 - classification_loss: 0.7574
 777/1000 [======================>.......] - ETA: 1:40 - loss: 3.1036 - regression_loss: 2.3467 - classification_loss: 0.7569
 778/1000 [======================>.......] - ETA: 1:40 - loss: 3.1047 - regression_loss: 2.3468 - classification_loss: 0.7579
 779/1000 [======================>.......] - ETA: 1:40 - loss: 3.1074 - regression_loss: 2.3485 - classification_loss: 0.7589
 780/1000 [======================>.......] - ETA: 1:39 - loss: 3.1081 - regression_loss: 2.3491 - classification_loss: 0.7589
 781/1000 [======================>.......] - ETA: 1:39 - loss: 3.1112 - regression_loss: 2.3518 - classification_loss: 0.7593
 782/1000 [======================>.......] - ETA: 1:38 - loss: 3.1109 - regression_loss: 2.3519 - classification_loss: 0.7590
 783/1000 [======================>.......] - ETA: 1:38 - loss: 3.1134 - regression_loss: 2.3537 - classification_loss: 0.7598
 784/1000 [======================>.......] - ETA: 1:37 - loss: 3.1095 - regression_loss: 2.3507 - classification_loss: 0.7588
 785/1000 [======================>.......] - ETA: 1:37 - loss: 3.1107 - regression_loss: 2.3521 - classification_loss: 0.7586
 786/1000 [======================>.......] - ETA: 1:36 - loss: 3.1068 - regression_loss: 2.3491 - classification_loss: 0.7577
 787/1000 [======================>.......] - ETA: 1:36 - loss: 3.1074 - regression_loss: 2.3492 - classification_loss: 0.7582
 788/1000 [======================>.......] - ETA: 1:35 - loss: 3.1035 - regression_loss: 2.3462 - classification_loss: 0.7572
 789/1000 [======================>.......] - ETA: 1:35 - loss: 3.1032 - regression_loss: 2.3456 - classification_loss: 0.7576
 790/1000 [======================>.......] - ETA: 1:35 - loss: 3.1042 - regression_loss: 2.3470 - classification_loss: 0.7572
 791/1000 [======================>.......] - ETA: 1:34 - loss: 3.1057 - regression_loss: 2.3482 - classification_loss: 0.7574
 792/1000 [======================>.......] - ETA: 1:34 - loss: 3.1017 - regression_loss: 2.3453 - classification_loss: 0.7565
 793/1000 [======================>.......] - ETA: 1:33 - loss: 3.1033 - regression_loss: 2.3461 - classification_loss: 0.7573
 794/1000 [======================>.......] - ETA: 1:33 - loss: 3.1067 - regression_loss: 2.3486 - classification_loss: 0.7581
 795/1000 [======================>.......] - ETA: 1:32 - loss: 3.1090 - regression_loss: 2.3505 - classification_loss: 0.7585
 796/1000 [======================>.......] - ETA: 1:32 - loss: 3.1099 - regression_loss: 2.3511 - classification_loss: 0.7587
 797/1000 [======================>.......] - ETA: 1:31 - loss: 3.1121 - regression_loss: 2.3524 - classification_loss: 0.7597
 798/1000 [======================>.......] - ETA: 1:31 - loss: 3.1133 - regression_loss: 2.3534 - classification_loss: 0.7599
 799/1000 [======================>.......] - ETA: 1:30 - loss: 3.1126 - regression_loss: 2.3532 - classification_loss: 0.7593
 800/1000 [=======================>......] - ETA: 1:30 - loss: 3.1141 - regression_loss: 2.3552 - classification_loss: 0.7589
 801/1000 [=======================>......] - ETA: 1:30 - loss: 3.1146 - regression_loss: 2.3555 - classification_loss: 0.7592
 802/1000 [=======================>......] - ETA: 1:29 - loss: 3.1146 - regression_loss: 2.3556 - classification_loss: 0.7591
 803/1000 [=======================>......] - ETA: 1:29 - loss: 3.1146 - regression_loss: 2.3557 - classification_loss: 0.7589
 804/1000 [=======================>......] - ETA: 1:28 - loss: 3.1134 - regression_loss: 2.3551 - classification_loss: 0.7583
 805/1000 [=======================>......] - ETA: 1:28 - loss: 3.1095 - regression_loss: 2.3522 - classification_loss: 0.7573
 806/1000 [=======================>......] - ETA: 1:27 - loss: 3.1102 - regression_loss: 2.3525 - classification_loss: 0.7576
 807/1000 [=======================>......] - ETA: 1:27 - loss: 3.1112 - regression_loss: 2.3534 - classification_loss: 0.7578
 808/1000 [=======================>......] - ETA: 1:26 - loss: 3.1115 - regression_loss: 2.3533 - classification_loss: 0.7582
 809/1000 [=======================>......] - ETA: 1:26 - loss: 3.1104 - regression_loss: 2.3528 - classification_loss: 0.7576
 810/1000 [=======================>......] - ETA: 1:26 - loss: 3.1125 - regression_loss: 2.3542 - classification_loss: 0.7583
 811/1000 [=======================>......] - ETA: 1:25 - loss: 3.1124 - regression_loss: 2.3543 - classification_loss: 0.7580
 812/1000 [=======================>......] - ETA: 1:25 - loss: 3.1142 - regression_loss: 2.3563 - classification_loss: 0.7579
 813/1000 [=======================>......] - ETA: 1:24 - loss: 3.1104 - regression_loss: 2.3534 - classification_loss: 0.7570
 814/1000 [=======================>......] - ETA: 1:24 - loss: 3.1066 - regression_loss: 2.3505 - classification_loss: 0.7561
 815/1000 [=======================>......] - ETA: 1:23 - loss: 3.1105 - regression_loss: 2.3546 - classification_loss: 0.7559
 816/1000 [=======================>......] - ETA: 1:23 - loss: 3.1136 - regression_loss: 2.3574 - classification_loss: 0.7562
 817/1000 [=======================>......] - ETA: 1:22 - loss: 3.1127 - regression_loss: 2.3566 - classification_loss: 0.7561
 818/1000 [=======================>......] - ETA: 1:22 - loss: 3.1119 - regression_loss: 2.3562 - classification_loss: 0.7557
 819/1000 [=======================>......] - ETA: 1:21 - loss: 3.1123 - regression_loss: 2.3566 - classification_loss: 0.7557
 820/1000 [=======================>......] - ETA: 1:21 - loss: 3.1085 - regression_loss: 2.3537 - classification_loss: 0.7548
 821/1000 [=======================>......] - ETA: 1:21 - loss: 3.1089 - regression_loss: 2.3539 - classification_loss: 0.7551
 822/1000 [=======================>......] - ETA: 1:20 - loss: 3.1052 - regression_loss: 2.3510 - classification_loss: 0.7541
 823/1000 [=======================>......] - ETA: 1:20 - loss: 3.1014 - regression_loss: 2.3482 - classification_loss: 0.7532
 824/1000 [=======================>......] - ETA: 1:19 - loss: 3.0976 - regression_loss: 2.3453 - classification_loss: 0.7523
 825/1000 [=======================>......] - ETA: 1:19 - loss: 3.0991 - regression_loss: 2.3455 - classification_loss: 0.7536
 826/1000 [=======================>......] - ETA: 1:18 - loss: 3.0994 - regression_loss: 2.3458 - classification_loss: 0.7536
 827/1000 [=======================>......] - ETA: 1:18 - loss: 3.1006 - regression_loss: 2.3468 - classification_loss: 0.7538
 828/1000 [=======================>......] - ETA: 1:17 - loss: 3.1028 - regression_loss: 2.3482 - classification_loss: 0.7546
 829/1000 [=======================>......] - ETA: 1:17 - loss: 3.0991 - regression_loss: 2.3454 - classification_loss: 0.7536
 830/1000 [=======================>......] - ETA: 1:16 - loss: 3.0995 - regression_loss: 2.3457 - classification_loss: 0.7538
 831/1000 [=======================>......] - ETA: 1:16 - loss: 3.1027 - regression_loss: 2.3476 - classification_loss: 0.7552
 832/1000 [=======================>......] - ETA: 1:16 - loss: 3.1014 - regression_loss: 2.3465 - classification_loss: 0.7549
 833/1000 [=======================>......] - ETA: 1:15 - loss: 3.1011 - regression_loss: 2.3464 - classification_loss: 0.7547
 834/1000 [========================>.....] - ETA: 1:15 - loss: 3.0973 - regression_loss: 2.3436 - classification_loss: 0.7538
 835/1000 [========================>.....] - ETA: 1:14 - loss: 3.1010 - regression_loss: 2.3453 - classification_loss: 0.7557
 836/1000 [========================>.....] - ETA: 1:14 - loss: 3.1004 - regression_loss: 2.3450 - classification_loss: 0.7554
 837/1000 [========================>.....] - ETA: 1:13 - loss: 3.1028 - regression_loss: 2.3461 - classification_loss: 0.7567
 838/1000 [========================>.....] - ETA: 1:13 - loss: 3.1034 - regression_loss: 2.3465 - classification_loss: 0.7568
 839/1000 [========================>.....] - ETA: 1:12 - loss: 3.1034 - regression_loss: 2.3465 - classification_loss: 0.7569
 840/1000 [========================>.....] - ETA: 1:12 - loss: 3.1029 - regression_loss: 2.3462 - classification_loss: 0.7567
 841/1000 [========================>.....] - ETA: 1:11 - loss: 3.1047 - regression_loss: 2.3475 - classification_loss: 0.7572
 842/1000 [========================>.....] - ETA: 1:11 - loss: 3.1052 - regression_loss: 2.3476 - classification_loss: 0.7576
 843/1000 [========================>.....] - ETA: 1:11 - loss: 3.1066 - regression_loss: 2.3480 - classification_loss: 0.7586
 844/1000 [========================>.....] - ETA: 1:10 - loss: 3.1091 - regression_loss: 2.3503 - classification_loss: 0.7589
 845/1000 [========================>.....] - ETA: 1:10 - loss: 3.1089 - regression_loss: 2.3503 - classification_loss: 0.7586
 846/1000 [========================>.....] - ETA: 1:09 - loss: 3.1100 - regression_loss: 2.3509 - classification_loss: 0.7590
 847/1000 [========================>.....] - ETA: 1:09 - loss: 3.1110 - regression_loss: 2.3509 - classification_loss: 0.7601
 848/1000 [========================>.....] - ETA: 1:08 - loss: 3.1115 - regression_loss: 2.3503 - classification_loss: 0.7612
 849/1000 [========================>.....] - ETA: 1:08 - loss: 3.1120 - regression_loss: 2.3509 - classification_loss: 0.7611
 850/1000 [========================>.....] - ETA: 1:07 - loss: 3.1128 - regression_loss: 2.3505 - classification_loss: 0.7623
 851/1000 [========================>.....] - ETA: 1:07 - loss: 3.1092 - regression_loss: 2.3477 - classification_loss: 0.7614
 852/1000 [========================>.....] - ETA: 1:06 - loss: 3.1096 - regression_loss: 2.3486 - classification_loss: 0.7610
 853/1000 [========================>.....] - ETA: 1:06 - loss: 3.1065 - regression_loss: 2.3458 - classification_loss: 0.7607
 854/1000 [========================>.....] - ETA: 1:06 - loss: 3.1086 - regression_loss: 2.3478 - classification_loss: 0.7608
 855/1000 [========================>.....] - ETA: 1:05 - loss: 3.1049 - regression_loss: 2.3451 - classification_loss: 0.7599
 856/1000 [========================>.....] - ETA: 1:05 - loss: 3.1044 - regression_loss: 2.3447 - classification_loss: 0.7597
 857/1000 [========================>.....] - ETA: 1:04 - loss: 3.1007 - regression_loss: 2.3419 - classification_loss: 0.7588
 858/1000 [========================>.....] - ETA: 1:04 - loss: 3.1014 - regression_loss: 2.3425 - classification_loss: 0.7589
 859/1000 [========================>.....] - ETA: 1:03 - loss: 3.0982 - regression_loss: 2.3397 - classification_loss: 0.7584
 860/1000 [========================>.....] - ETA: 1:03 - loss: 3.0946 - regression_loss: 2.3370 - classification_loss: 0.7575
 861/1000 [========================>.....] - ETA: 1:02 - loss: 3.0957 - regression_loss: 2.3374 - classification_loss: 0.7583
 862/1000 [========================>.....] - ETA: 1:02 - loss: 3.0954 - regression_loss: 2.3373 - classification_loss: 0.7581
 863/1000 [========================>.....] - ETA: 1:02 - loss: 3.0918 - regression_loss: 2.3346 - classification_loss: 0.7572
 864/1000 [========================>.....] - ETA: 1:01 - loss: 3.0922 - regression_loss: 2.3350 - classification_loss: 0.7572
 865/1000 [========================>.....] - ETA: 1:01 - loss: 3.0928 - regression_loss: 2.3355 - classification_loss: 0.7574
 866/1000 [========================>.....] - ETA: 1:00 - loss: 3.0893 - regression_loss: 2.3328 - classification_loss: 0.7565
 867/1000 [=========================>....] - ETA: 1:00 - loss: 3.0913 - regression_loss: 2.3344 - classification_loss: 0.7569
 868/1000 [=========================>....] - ETA: 59s - loss: 3.0920 - regression_loss: 2.3348 - classification_loss: 0.7572 
 869/1000 [=========================>....] - ETA: 59s - loss: 3.0930 - regression_loss: 2.3356 - classification_loss: 0.7573
 870/1000 [=========================>....] - ETA: 58s - loss: 3.0953 - regression_loss: 2.3371 - classification_loss: 0.7581
 871/1000 [=========================>....] - ETA: 58s - loss: 3.0917 - regression_loss: 2.3345 - classification_loss: 0.7572
 872/1000 [=========================>....] - ETA: 57s - loss: 3.0915 - regression_loss: 2.3343 - classification_loss: 0.7572
 873/1000 [=========================>....] - ETA: 57s - loss: 3.0954 - regression_loss: 2.3373 - classification_loss: 0.7581
 874/1000 [=========================>....] - ETA: 57s - loss: 3.0918 - regression_loss: 2.3346 - classification_loss: 0.7572
 875/1000 [=========================>....] - ETA: 56s - loss: 3.0883 - regression_loss: 2.3319 - classification_loss: 0.7563
 876/1000 [=========================>....] - ETA: 56s - loss: 3.0848 - regression_loss: 2.3293 - classification_loss: 0.7555
 877/1000 [=========================>....] - ETA: 55s - loss: 3.0869 - regression_loss: 2.3302 - classification_loss: 0.7567
 878/1000 [=========================>....] - ETA: 55s - loss: 3.0864 - regression_loss: 2.3296 - classification_loss: 0.7569
 879/1000 [=========================>....] - ETA: 54s - loss: 3.0885 - regression_loss: 2.3306 - classification_loss: 0.7580
 880/1000 [=========================>....] - ETA: 54s - loss: 3.0891 - regression_loss: 2.3310 - classification_loss: 0.7581
 881/1000 [=========================>....] - ETA: 53s - loss: 3.0895 - regression_loss: 2.3308 - classification_loss: 0.7587
 882/1000 [=========================>....] - ETA: 53s - loss: 3.0904 - regression_loss: 2.3319 - classification_loss: 0.7586
 883/1000 [=========================>....] - ETA: 52s - loss: 3.0869 - regression_loss: 2.3292 - classification_loss: 0.7577
 884/1000 [=========================>....] - ETA: 52s - loss: 3.0898 - regression_loss: 2.3304 - classification_loss: 0.7595
 885/1000 [=========================>....] - ETA: 52s - loss: 3.0913 - regression_loss: 2.3312 - classification_loss: 0.7602
 886/1000 [=========================>....] - ETA: 51s - loss: 3.0934 - regression_loss: 2.3321 - classification_loss: 0.7613
 887/1000 [=========================>....] - ETA: 51s - loss: 3.0938 - regression_loss: 2.3326 - classification_loss: 0.7612
 888/1000 [=========================>....] - ETA: 50s - loss: 3.0935 - regression_loss: 2.3323 - classification_loss: 0.7612
 889/1000 [=========================>....] - ETA: 50s - loss: 3.0940 - regression_loss: 2.3321 - classification_loss: 0.7619
 890/1000 [=========================>....] - ETA: 49s - loss: 3.0905 - regression_loss: 2.3295 - classification_loss: 0.7610
 891/1000 [=========================>....] - ETA: 49s - loss: 3.0913 - regression_loss: 2.3296 - classification_loss: 0.7617
 892/1000 [=========================>....] - ETA: 48s - loss: 3.0911 - regression_loss: 2.3295 - classification_loss: 0.7616
 893/1000 [=========================>....] - ETA: 48s - loss: 3.0931 - regression_loss: 2.3311 - classification_loss: 0.7620
 894/1000 [=========================>....] - ETA: 47s - loss: 3.0897 - regression_loss: 2.3285 - classification_loss: 0.7612
 895/1000 [=========================>....] - ETA: 47s - loss: 3.0863 - regression_loss: 2.3259 - classification_loss: 0.7604
 896/1000 [=========================>....] - ETA: 47s - loss: 3.0855 - regression_loss: 2.3252 - classification_loss: 0.7603
 897/1000 [=========================>....] - ETA: 46s - loss: 3.0862 - regression_loss: 2.3257 - classification_loss: 0.7604
 898/1000 [=========================>....] - ETA: 46s - loss: 3.0871 - regression_loss: 2.3267 - classification_loss: 0.7604
 899/1000 [=========================>....] - ETA: 45s - loss: 3.0882 - regression_loss: 2.3275 - classification_loss: 0.7606
 900/1000 [==========================>...] - ETA: 45s - loss: 3.0870 - regression_loss: 2.3266 - classification_loss: 0.7603
 901/1000 [==========================>...] - ETA: 44s - loss: 3.0892 - regression_loss: 2.3281 - classification_loss: 0.7611
 902/1000 [==========================>...] - ETA: 44s - loss: 3.0919 - regression_loss: 2.3298 - classification_loss: 0.7621
 903/1000 [==========================>...] - ETA: 43s - loss: 3.0923 - regression_loss: 2.3298 - classification_loss: 0.7625
 904/1000 [==========================>...] - ETA: 43s - loss: 3.0889 - regression_loss: 2.3272 - classification_loss: 0.7617
 905/1000 [==========================>...] - ETA: 43s - loss: 3.0899 - regression_loss: 2.3275 - classification_loss: 0.7624
 906/1000 [==========================>...] - ETA: 42s - loss: 3.0905 - regression_loss: 2.3283 - classification_loss: 0.7622
 907/1000 [==========================>...] - ETA: 42s - loss: 3.0871 - regression_loss: 2.3258 - classification_loss: 0.7613
 908/1000 [==========================>...] - ETA: 41s - loss: 3.0864 - regression_loss: 2.3252 - classification_loss: 0.7612
 909/1000 [==========================>...] - ETA: 41s - loss: 3.0830 - regression_loss: 2.3227 - classification_loss: 0.7603
 910/1000 [==========================>...] - ETA: 40s - loss: 3.0796 - regression_loss: 2.3201 - classification_loss: 0.7595
 911/1000 [==========================>...] - ETA: 40s - loss: 3.0799 - regression_loss: 2.3204 - classification_loss: 0.7595
 912/1000 [==========================>...] - ETA: 39s - loss: 3.0766 - regression_loss: 2.3179 - classification_loss: 0.7587
 913/1000 [==========================>...] - ETA: 39s - loss: 3.0790 - regression_loss: 2.3196 - classification_loss: 0.7594
 914/1000 [==========================>...] - ETA: 38s - loss: 3.0789 - regression_loss: 2.3198 - classification_loss: 0.7591
 915/1000 [==========================>...] - ETA: 38s - loss: 3.0757 - regression_loss: 2.3173 - classification_loss: 0.7584
 916/1000 [==========================>...] - ETA: 38s - loss: 3.0742 - regression_loss: 2.3159 - classification_loss: 0.7584
 917/1000 [==========================>...] - ETA: 37s - loss: 3.0757 - regression_loss: 2.3172 - classification_loss: 0.7586
 918/1000 [==========================>...] - ETA: 37s - loss: 3.0784 - regression_loss: 2.3194 - classification_loss: 0.7590
 919/1000 [==========================>...] - ETA: 36s - loss: 3.0750 - regression_loss: 2.3168 - classification_loss: 0.7582
 920/1000 [==========================>...] - ETA: 36s - loss: 3.0786 - regression_loss: 2.3194 - classification_loss: 0.7592
 921/1000 [==========================>...] - ETA: 35s - loss: 3.0793 - regression_loss: 2.3198 - classification_loss: 0.7595
 922/1000 [==========================>...] - ETA: 35s - loss: 3.0759 - regression_loss: 2.3173 - classification_loss: 0.7586
 923/1000 [==========================>...] - ETA: 34s - loss: 3.0775 - regression_loss: 2.3179 - classification_loss: 0.7596
 924/1000 [==========================>...] - ETA: 34s - loss: 3.0792 - regression_loss: 2.3195 - classification_loss: 0.7597
 925/1000 [==========================>...] - ETA: 33s - loss: 3.0788 - regression_loss: 2.3197 - classification_loss: 0.7591
 926/1000 [==========================>...] - ETA: 33s - loss: 3.0813 - regression_loss: 2.3211 - classification_loss: 0.7602
 927/1000 [==========================>...] - ETA: 33s - loss: 3.0815 - regression_loss: 2.3215 - classification_loss: 0.7600
 928/1000 [==========================>...] - ETA: 32s - loss: 3.0781 - regression_loss: 2.3190 - classification_loss: 0.7592
 929/1000 [==========================>...] - ETA: 32s - loss: 3.0748 - regression_loss: 2.3165 - classification_loss: 0.7583
 930/1000 [==========================>...] - ETA: 31s - loss: 3.0758 - regression_loss: 2.3179 - classification_loss: 0.7580
 931/1000 [==========================>...] - ETA: 31s - loss: 3.0725 - regression_loss: 2.3154 - classification_loss: 0.7571
 932/1000 [==========================>...] - ETA: 30s - loss: 3.0729 - regression_loss: 2.3152 - classification_loss: 0.7577
 933/1000 [==========================>...] - ETA: 30s - loss: 3.0696 - regression_loss: 2.3127 - classification_loss: 0.7569
 934/1000 [===========================>..] - ETA: 29s - loss: 3.0731 - regression_loss: 2.3149 - classification_loss: 0.7581
 935/1000 [===========================>..] - ETA: 29s - loss: 3.0733 - regression_loss: 2.3145 - classification_loss: 0.7589
 936/1000 [===========================>..] - ETA: 28s - loss: 3.0701 - regression_loss: 2.3120 - classification_loss: 0.7581
 937/1000 [===========================>..] - ETA: 28s - loss: 3.0707 - regression_loss: 2.3128 - classification_loss: 0.7579
 938/1000 [===========================>..] - ETA: 28s - loss: 3.0732 - regression_loss: 2.3144 - classification_loss: 0.7588
 939/1000 [===========================>..] - ETA: 27s - loss: 3.0723 - regression_loss: 2.3139 - classification_loss: 0.7584
 940/1000 [===========================>..] - ETA: 27s - loss: 3.0735 - regression_loss: 2.3150 - classification_loss: 0.7586
 941/1000 [===========================>..] - ETA: 26s - loss: 3.0747 - regression_loss: 2.3162 - classification_loss: 0.7585
 942/1000 [===========================>..] - ETA: 26s - loss: 3.0715 - regression_loss: 2.3137 - classification_loss: 0.7577
 943/1000 [===========================>..] - ETA: 25s - loss: 3.0735 - regression_loss: 2.3147 - classification_loss: 0.7588
 944/1000 [===========================>..] - ETA: 25s - loss: 3.0736 - regression_loss: 2.3147 - classification_loss: 0.7589
 945/1000 [===========================>..] - ETA: 24s - loss: 3.0753 - regression_loss: 2.3155 - classification_loss: 0.7598
 946/1000 [===========================>..] - ETA: 24s - loss: 3.0758 - regression_loss: 2.3159 - classification_loss: 0.7599
 947/1000 [===========================>..] - ETA: 23s - loss: 3.0778 - regression_loss: 2.3169 - classification_loss: 0.7608
 948/1000 [===========================>..] - ETA: 23s - loss: 3.0800 - regression_loss: 2.3183 - classification_loss: 0.7616
 949/1000 [===========================>..] - ETA: 23s - loss: 3.0823 - regression_loss: 2.3198 - classification_loss: 0.7626
 950/1000 [===========================>..] - ETA: 22s - loss: 3.0833 - regression_loss: 2.3205 - classification_loss: 0.7628
 951/1000 [===========================>..] - ETA: 22s - loss: 3.0842 - regression_loss: 2.3216 - classification_loss: 0.7627
 952/1000 [===========================>..] - ETA: 21s - loss: 3.0847 - regression_loss: 2.3221 - classification_loss: 0.7626
 953/1000 [===========================>..] - ETA: 21s - loss: 3.0871 - regression_loss: 2.3234 - classification_loss: 0.7637
 954/1000 [===========================>..] - ETA: 20s - loss: 3.0839 - regression_loss: 2.3210 - classification_loss: 0.7629
 955/1000 [===========================>..] - ETA: 20s - loss: 3.0807 - regression_loss: 2.3185 - classification_loss: 0.7621
 956/1000 [===========================>..] - ETA: 19s - loss: 3.0774 - regression_loss: 2.3161 - classification_loss: 0.7613
 957/1000 [===========================>..] - ETA: 19s - loss: 3.0772 - regression_loss: 2.3163 - classification_loss: 0.7610
 958/1000 [===========================>..] - ETA: 19s - loss: 3.0791 - regression_loss: 2.3170 - classification_loss: 0.7621
 959/1000 [===========================>..] - ETA: 18s - loss: 3.0801 - regression_loss: 2.3183 - classification_loss: 0.7618
 960/1000 [===========================>..] - ETA: 18s - loss: 3.0793 - regression_loss: 2.3179 - classification_loss: 0.7613
 961/1000 [===========================>..] - ETA: 17s - loss: 3.0802 - regression_loss: 2.3187 - classification_loss: 0.7615
 962/1000 [===========================>..] - ETA: 17s - loss: 3.0791 - regression_loss: 2.3181 - classification_loss: 0.7610
 963/1000 [===========================>..] - ETA: 16s - loss: 3.0759 - regression_loss: 2.3157 - classification_loss: 0.7602
 964/1000 [===========================>..] - ETA: 16s - loss: 3.0727 - regression_loss: 2.3133 - classification_loss: 0.7594
 965/1000 [===========================>..] - ETA: 15s - loss: 3.0695 - regression_loss: 2.3109 - classification_loss: 0.7586
 966/1000 [===========================>..] - ETA: 15s - loss: 3.0706 - regression_loss: 2.3120 - classification_loss: 0.7586
 967/1000 [============================>.] - ETA: 14s - loss: 3.0674 - regression_loss: 2.3096 - classification_loss: 0.7578
 968/1000 [============================>.] - ETA: 14s - loss: 3.0693 - regression_loss: 2.3107 - classification_loss: 0.7586
 969/1000 [============================>.] - ETA: 14s - loss: 3.0695 - regression_loss: 2.3106 - classification_loss: 0.7590
 970/1000 [============================>.] - ETA: 13s - loss: 3.0706 - regression_loss: 2.3117 - classification_loss: 0.7589
 971/1000 [============================>.] - ETA: 13s - loss: 3.0741 - regression_loss: 2.3124 - classification_loss: 0.7617
 972/1000 [============================>.] - ETA: 12s - loss: 3.0768 - regression_loss: 2.3141 - classification_loss: 0.7627
 973/1000 [============================>.] - ETA: 12s - loss: 3.0779 - regression_loss: 2.3141 - classification_loss: 0.7639
 974/1000 [============================>.] - ETA: 11s - loss: 3.0749 - regression_loss: 2.3117 - classification_loss: 0.7632
 975/1000 [============================>.] - ETA: 11s - loss: 3.0717 - regression_loss: 2.3093 - classification_loss: 0.7624
 976/1000 [============================>.] - ETA: 10s - loss: 3.0686 - regression_loss: 2.3070 - classification_loss: 0.7616
 977/1000 [============================>.] - ETA: 10s - loss: 3.0710 - regression_loss: 2.3083 - classification_loss: 0.7627
 978/1000 [============================>.] - ETA: 9s - loss: 3.0679 - regression_loss: 2.3060 - classification_loss: 0.7619 
 979/1000 [============================>.] - ETA: 9s - loss: 3.0648 - regression_loss: 2.3036 - classification_loss: 0.7611
 980/1000 [============================>.] - ETA: 9s - loss: 3.0665 - regression_loss: 2.3041 - classification_loss: 0.7624
 981/1000 [============================>.] - ETA: 8s - loss: 3.0634 - regression_loss: 2.3018 - classification_loss: 0.7616
 982/1000 [============================>.] - ETA: 8s - loss: 3.0667 - regression_loss: 2.3037 - classification_loss: 0.7631
 983/1000 [============================>.] - ETA: 7s - loss: 3.0665 - regression_loss: 2.3034 - classification_loss: 0.7631
 984/1000 [============================>.] - ETA: 7s - loss: 3.0662 - regression_loss: 2.3033 - classification_loss: 0.7629
 985/1000 [============================>.] - ETA: 6s - loss: 3.0691 - regression_loss: 2.3050 - classification_loss: 0.7641
 986/1000 [============================>.] - ETA: 6s - loss: 3.0743 - regression_loss: 2.3089 - classification_loss: 0.7654
 987/1000 [============================>.] - ETA: 5s - loss: 3.0712 - regression_loss: 2.3066 - classification_loss: 0.7646
 988/1000 [============================>.] - ETA: 5s - loss: 3.0721 - regression_loss: 2.3074 - classification_loss: 0.7647
 989/1000 [============================>.] - ETA: 4s - loss: 3.0738 - regression_loss: 2.3081 - classification_loss: 0.7658
 990/1000 [============================>.] - ETA: 4s - loss: 3.0707 - regression_loss: 2.3057 - classification_loss: 0.7650
 991/1000 [============================>.] - ETA: 4s - loss: 3.0728 - regression_loss: 2.3067 - classification_loss: 0.7661
 992/1000 [============================>.] - ETA: 3s - loss: 3.0697 - regression_loss: 2.3043 - classification_loss: 0.7654
 993/1000 [============================>.] - ETA: 3s - loss: 3.0666 - regression_loss: 2.3020 - classification_loss: 0.7646
 994/1000 [============================>.] - ETA: 2s - loss: 3.0679 - regression_loss: 2.3029 - classification_loss: 0.7650
 995/1000 [============================>.] - ETA: 2s - loss: 3.0675 - regression_loss: 2.3030 - classification_loss: 0.7646
 996/1000 [============================>.] - ETA: 1s - loss: 3.0677 - regression_loss: 2.3033 - classification_loss: 0.7644
 997/1000 [============================>.] - ETA: 1s - loss: 3.0703 - regression_loss: 2.3052 - classification_loss: 0.7652
 998/1000 [============================>.] - ETA: 0s - loss: 3.0673 - regression_loss: 2.3029 - classification_loss: 0.7644
 999/1000 [============================>.] - ETA: 0s - loss: 3.0691 - regression_loss: 2.3040 - classification_loss: 0.7651
1000/1000 [==============================] - 453s 453ms/step - loss: 3.0660 - regression_loss: 2.3017 - classification_loss: 0.7643

Epoch 00009: saving model to ./snapshots/resnet50_csv_09.h5
0/2001/2002/2003/2004/2005/2006/2007/2008/2009/20010/20011/20012/20013/20014/20015/20016/20017/20018/20019/20020/20021/20022/20023/20024/20025/20026/20027/20028/20029/20030/20031/20032/20033/20034/20035/20036/20037/20038/20039/20040/20041/20042/20043/20044/20045/20046/20047/20048/20049/20050/20051/20052/20053/20054/20055/20056/20057/20058/20059/20060/20061/20062/20063/20064/20065/20066/20067/20068/20069/20070/20071/20072/20073/20074/20075/20076/20077/20078/20079/20080/20081/20082/20083/20084/20085/20086/20087/20088/20089/20090/20091/20092/20093/20094/20095/20096/20097/20098/20099/200100/200101/200102/200103/200104/200105/200106/200107/200108/200109/200110/200111/200112/200113/200114/200115/200116/200117/200118/200119/200120/200121/200122/200123/200124/200125/200126/200127/200128/200129/200130/200131/200132/200133/200134/200135/200136/200137/200138/200139/200140/200141/200142/200143/200144/200145/200146/200147/200148/200149/200150/200151/200152/200153/200154/200155/200156/200157/200158/200159/200160/200161/200162/200163/200164/200165/200166/200167/200168/200169/200170/200171/200172/200173/200174/200175/200176/200177/200178/200179/200180/200181/200182/200183/200184/200185/200186/200187/200188/200189/200190/200191/200192/200193/200194/200195/200196/200197/200198/200199/2000/2001/2002/2003/2004/2005/2006/2007/2008/2009/20010/20011/20012/20013/20014/20015/20016/20017/20018/20019/20020/20021/20022/20023/20024/20025/20026/20027/20028/20029/20030/20031/20032/20033/20034/20035/20036/20037/20038/20039/20040/20041/20042/20043/20044/20045/20046/20047/20048/20049/20050/20051/20052/20053/20054/20055/20056/20057/20058/20059/20060/20061/20062/20063/20064/20065/20066/20067/20068/20069/20070/20071/20072/20073/20074/20075/20076/20077/20078/20079/20080/20081/20082/20083/20084/20085/20086/20087/20088/20089/20090/20091/20092/20093/20094/20095/20096/20097/20098/20099/200100/200101/200102/200103/200104/200105/200106/200107/200108/200109/200110/200111/200112/200113/200114/200115/200116/200117/200118/200119/200120/200121/200122/200123/200124/200125/200126/200127/200128/200129/200130/200131/200132/200133/200134/200135/200136/200137/200138/200139/200140/200141/200142/200143/200144/200145/200146/200147/200148/200149/200150/200151/200152/200153/200154/200155/200156/200157/200158/200159/200160/200161/200162/200163/200164/200165/200166/200167/200168/200169/200170/200171/200172/200173/200174/200175/200176/200177/200178/200179/200180/200181/200182/200183/200184/200185/200186/200187/200188/200189/200190/200191/200192/200193/200194/200195/200196/200197/200198/200199/200

M 0.1062
N 0.0000
mAP: 0.0531
Epoch 10/30

   1/1000 [..............................] - ETA: 7:20 - loss: 0.0014 - regression_loss: 0.0000e+00 - classification_loss: 0.0014
   2/1000 [..............................] - ETA: 7:23 - loss: 1.8378 - regression_loss: 1.1720 - classification_loss: 0.6658    
   3/1000 [..............................] - ETA: 7:24 - loss: 1.2601 - regression_loss: 0.7814 - classification_loss: 0.4788
   4/1000 [..............................] - ETA: 7:25 - loss: 0.9455 - regression_loss: 0.5860 - classification_loss: 0.3595
   5/1000 [..............................] - ETA: 7:27 - loss: 1.3794 - regression_loss: 0.8313 - classification_loss: 0.5481
   6/1000 [..............................] - ETA: 7:28 - loss: 1.1543 - regression_loss: 0.6927 - classification_loss: 0.4616
   7/1000 [..............................] - ETA: 7:28 - loss: 1.6408 - regression_loss: 1.1130 - classification_loss: 0.5278
   8/1000 [..............................] - ETA: 7:28 - loss: 1.4357 - regression_loss: 0.9739 - classification_loss: 0.4618
   9/1000 [..............................] - ETA: 7:27 - loss: 1.8704 - regression_loss: 1.2713 - classification_loss: 0.5990
  10/1000 [..............................] - ETA: 7:26 - loss: 1.6833 - regression_loss: 1.1442 - classification_loss: 0.5391
  11/1000 [..............................] - ETA: 7:25 - loss: 2.0668 - regression_loss: 1.4060 - classification_loss: 0.6608
  12/1000 [..............................] - ETA: 7:26 - loss: 2.2308 - regression_loss: 1.5561 - classification_loss: 0.6747
  13/1000 [..............................] - ETA: 7:25 - loss: 2.3667 - regression_loss: 1.6716 - classification_loss: 0.6950
  14/1000 [..............................] - ETA: 7:25 - loss: 2.5188 - regression_loss: 1.7450 - classification_loss: 0.7738
  15/1000 [..............................] - ETA: 7:24 - loss: 2.6203 - regression_loss: 1.8317 - classification_loss: 0.7887
  16/1000 [..............................] - ETA: 7:24 - loss: 2.7720 - regression_loss: 1.9223 - classification_loss: 0.8497
  17/1000 [..............................] - ETA: 7:23 - loss: 2.7677 - regression_loss: 1.9481 - classification_loss: 0.8196
  18/1000 [..............................] - ETA: 7:23 - loss: 2.8652 - regression_loss: 2.0142 - classification_loss: 0.8511
  19/1000 [..............................] - ETA: 7:22 - loss: 2.9818 - regression_loss: 2.1458 - classification_loss: 0.8360
  20/1000 [..............................] - ETA: 7:20 - loss: 2.8327 - regression_loss: 2.0385 - classification_loss: 0.7942
  21/1000 [..............................] - ETA: 7:19 - loss: 2.8394 - regression_loss: 2.0583 - classification_loss: 0.7811
  22/1000 [..............................] - ETA: 7:19 - loss: 2.8785 - regression_loss: 2.1083 - classification_loss: 0.7703
  23/1000 [..............................] - ETA: 7:19 - loss: 2.7534 - regression_loss: 2.0166 - classification_loss: 0.7368
  24/1000 [..............................] - ETA: 7:18 - loss: 2.8507 - regression_loss: 2.0703 - classification_loss: 0.7804
  25/1000 [..............................] - ETA: 7:18 - loss: 2.8968 - regression_loss: 2.1119 - classification_loss: 0.7850
  26/1000 [..............................] - ETA: 7:18 - loss: 2.9538 - regression_loss: 2.1347 - classification_loss: 0.8191
  27/1000 [..............................] - ETA: 7:18 - loss: 2.9443 - regression_loss: 2.1283 - classification_loss: 0.8161
  28/1000 [..............................] - ETA: 7:17 - loss: 2.9584 - regression_loss: 2.1425 - classification_loss: 0.8159
  29/1000 [..............................] - ETA: 7:17 - loss: 3.0357 - regression_loss: 2.1841 - classification_loss: 0.8515
  30/1000 [..............................] - ETA: 7:17 - loss: 3.0719 - regression_loss: 2.2189 - classification_loss: 0.8530
  31/1000 [..............................] - ETA: 7:17 - loss: 3.1203 - regression_loss: 2.2588 - classification_loss: 0.8615
  32/1000 [..............................] - ETA: 7:16 - loss: 3.1238 - regression_loss: 2.2670 - classification_loss: 0.8568
  33/1000 [..............................] - ETA: 7:16 - loss: 3.1921 - regression_loss: 2.3113 - classification_loss: 0.8808
  34/1000 [>.............................] - ETA: 7:16 - loss: 3.1850 - regression_loss: 2.3126 - classification_loss: 0.8724
  35/1000 [>.............................] - ETA: 7:15 - loss: 3.2503 - regression_loss: 2.3554 - classification_loss: 0.8949
  36/1000 [>.............................] - ETA: 7:15 - loss: 3.2832 - regression_loss: 2.3860 - classification_loss: 0.8972
  37/1000 [>.............................] - ETA: 7:14 - loss: 3.3274 - regression_loss: 2.4030 - classification_loss: 0.9243
  38/1000 [>.............................] - ETA: 7:13 - loss: 3.3707 - regression_loss: 2.4498 - classification_loss: 0.9209
  39/1000 [>.............................] - ETA: 7:13 - loss: 3.2843 - regression_loss: 2.3870 - classification_loss: 0.8973
  40/1000 [>.............................] - ETA: 7:12 - loss: 3.2022 - regression_loss: 2.3273 - classification_loss: 0.8749
  41/1000 [>.............................] - ETA: 7:12 - loss: 3.2370 - regression_loss: 2.3537 - classification_loss: 0.8832
  42/1000 [>.............................] - ETA: 7:12 - loss: 3.2460 - regression_loss: 2.3497 - classification_loss: 0.8963
  43/1000 [>.............................] - ETA: 7:11 - loss: 3.2277 - regression_loss: 2.3380 - classification_loss: 0.8897
  44/1000 [>.............................] - ETA: 7:11 - loss: 3.2588 - regression_loss: 2.3617 - classification_loss: 0.8971
  45/1000 [>.............................] - ETA: 7:10 - loss: 3.2975 - regression_loss: 2.3911 - classification_loss: 0.9064
  46/1000 [>.............................] - ETA: 7:10 - loss: 3.2736 - regression_loss: 2.3758 - classification_loss: 0.8978
  47/1000 [>.............................] - ETA: 7:10 - loss: 3.2040 - regression_loss: 2.3252 - classification_loss: 0.8787
  48/1000 [>.............................] - ETA: 7:09 - loss: 3.2114 - regression_loss: 2.3251 - classification_loss: 0.8863
  49/1000 [>.............................] - ETA: 7:09 - loss: 3.2351 - regression_loss: 2.3355 - classification_loss: 0.8996
  50/1000 [>.............................] - ETA: 7:08 - loss: 3.2641 - regression_loss: 2.3523 - classification_loss: 0.9118
  51/1000 [>.............................] - ETA: 7:08 - loss: 3.3310 - regression_loss: 2.3062 - classification_loss: 1.0249
  52/1000 [>.............................] - ETA: 7:08 - loss: 3.3233 - regression_loss: 2.3105 - classification_loss: 1.0128
  53/1000 [>.............................] - ETA: 7:07 - loss: 3.3358 - regression_loss: 2.3282 - classification_loss: 1.0076
  54/1000 [>.............................] - ETA: 7:07 - loss: 3.3543 - regression_loss: 2.3487 - classification_loss: 1.0056
  55/1000 [>.............................] - ETA: 7:06 - loss: 3.3518 - regression_loss: 2.3553 - classification_loss: 0.9965
  56/1000 [>.............................] - ETA: 7:06 - loss: 3.3470 - regression_loss: 2.3552 - classification_loss: 0.9918
  57/1000 [>.............................] - ETA: 7:05 - loss: 3.3439 - regression_loss: 2.3551 - classification_loss: 0.9887
  58/1000 [>.............................] - ETA: 7:05 - loss: 3.3674 - regression_loss: 2.3777 - classification_loss: 0.9896
  59/1000 [>.............................] - ETA: 7:05 - loss: 3.3873 - regression_loss: 2.3897 - classification_loss: 0.9977
  60/1000 [>.............................] - ETA: 7:04 - loss: 3.3825 - regression_loss: 2.3956 - classification_loss: 0.9869
  61/1000 [>.............................] - ETA: 7:04 - loss: 3.3271 - regression_loss: 2.3563 - classification_loss: 0.9708
  62/1000 [>.............................] - ETA: 7:03 - loss: 3.3433 - regression_loss: 2.3743 - classification_loss: 0.9690
  63/1000 [>.............................] - ETA: 7:03 - loss: 3.3438 - regression_loss: 2.3821 - classification_loss: 0.9617
  64/1000 [>.............................] - ETA: 7:02 - loss: 3.3842 - regression_loss: 2.4297 - classification_loss: 0.9545
  65/1000 [>.............................] - ETA: 7:02 - loss: 3.3951 - regression_loss: 2.4426 - classification_loss: 0.9525
  66/1000 [>.............................] - ETA: 7:01 - loss: 3.4133 - regression_loss: 2.4575 - classification_loss: 0.9558
  67/1000 [=>............................] - ETA: 7:01 - loss: 3.3623 - regression_loss: 2.4208 - classification_loss: 0.9416
  68/1000 [=>............................] - ETA: 7:01 - loss: 3.3623 - regression_loss: 2.4233 - classification_loss: 0.9390
  69/1000 [=>............................] - ETA: 7:00 - loss: 3.3602 - regression_loss: 2.4294 - classification_loss: 0.9308
  70/1000 [=>............................] - ETA: 7:00 - loss: 3.3636 - regression_loss: 2.4277 - classification_loss: 0.9359
  71/1000 [=>............................] - ETA: 6:59 - loss: 3.3168 - regression_loss: 2.3935 - classification_loss: 0.9232
  72/1000 [=>............................] - ETA: 6:59 - loss: 3.3077 - regression_loss: 2.3900 - classification_loss: 0.9177
  73/1000 [=>............................] - ETA: 6:59 - loss: 3.3101 - regression_loss: 2.3988 - classification_loss: 0.9113
  74/1000 [=>............................] - ETA: 6:58 - loss: 3.2654 - regression_loss: 2.3664 - classification_loss: 0.8990
  75/1000 [=>............................] - ETA: 6:58 - loss: 3.2761 - regression_loss: 2.3709 - classification_loss: 0.9052
  76/1000 [=>............................] - ETA: 6:57 - loss: 3.2816 - regression_loss: 2.3777 - classification_loss: 0.9039
  77/1000 [=>............................] - ETA: 6:57 - loss: 3.2390 - regression_loss: 2.3468 - classification_loss: 0.8922
  78/1000 [=>............................] - ETA: 6:56 - loss: 3.2565 - regression_loss: 2.3676 - classification_loss: 0.8889
  79/1000 [=>............................] - ETA: 6:56 - loss: 3.2694 - regression_loss: 2.3751 - classification_loss: 0.8944
  80/1000 [=>............................] - ETA: 6:55 - loss: 3.2700 - regression_loss: 2.3797 - classification_loss: 0.8903
  81/1000 [=>............................] - ETA: 6:55 - loss: 3.2687 - regression_loss: 2.3832 - classification_loss: 0.8855
  82/1000 [=>............................] - ETA: 6:54 - loss: 3.2288 - regression_loss: 2.3541 - classification_loss: 0.8747
  83/1000 [=>............................] - ETA: 6:54 - loss: 3.2305 - regression_loss: 2.3573 - classification_loss: 0.8732
  84/1000 [=>............................] - ETA: 6:53 - loss: 3.1921 - regression_loss: 2.3292 - classification_loss: 0.8628
  85/1000 [=>............................] - ETA: 6:53 - loss: 3.1895 - regression_loss: 2.3203 - classification_loss: 0.8692
  86/1000 [=>............................] - ETA: 6:53 - loss: 3.1797 - regression_loss: 2.3165 - classification_loss: 0.8632
  87/1000 [=>............................] - ETA: 6:52 - loss: 3.1969 - regression_loss: 2.3310 - classification_loss: 0.8659
  88/1000 [=>............................] - ETA: 6:52 - loss: 3.2083 - regression_loss: 2.3323 - classification_loss: 0.8760
  89/1000 [=>............................] - ETA: 6:51 - loss: 3.2320 - regression_loss: 2.3450 - classification_loss: 0.8870
  90/1000 [=>............................] - ETA: 6:51 - loss: 3.2496 - regression_loss: 2.3516 - classification_loss: 0.8980
  91/1000 [=>............................] - ETA: 6:51 - loss: 3.2599 - regression_loss: 2.3605 - classification_loss: 0.8993
  92/1000 [=>............................] - ETA: 6:50 - loss: 3.2244 - regression_loss: 2.3349 - classification_loss: 0.8895
  93/1000 [=>............................] - ETA: 6:50 - loss: 3.2049 - regression_loss: 2.3098 - classification_loss: 0.8951
  94/1000 [=>............................] - ETA: 6:49 - loss: 3.2092 - regression_loss: 2.3179 - classification_loss: 0.8913
  95/1000 [=>............................] - ETA: 6:49 - loss: 3.2315 - regression_loss: 2.3310 - classification_loss: 0.9005
  96/1000 [=>............................] - ETA: 6:48 - loss: 3.2237 - regression_loss: 2.3287 - classification_loss: 0.8950
  97/1000 [=>............................] - ETA: 6:48 - loss: 3.1905 - regression_loss: 2.3047 - classification_loss: 0.8858
  98/1000 [=>............................] - ETA: 6:47 - loss: 3.1579 - regression_loss: 2.2811 - classification_loss: 0.8768
  99/1000 [=>............................] - ETA: 6:47 - loss: 3.1661 - regression_loss: 2.2851 - classification_loss: 0.8810
 100/1000 [==>...........................] - ETA: 6:46 - loss: 3.1646 - regression_loss: 2.2860 - classification_loss: 0.8785
 101/1000 [==>...........................] - ETA: 6:46 - loss: 3.1914 - regression_loss: 2.3055 - classification_loss: 0.8859
 102/1000 [==>...........................] - ETA: 6:45 - loss: 3.2005 - regression_loss: 2.3133 - classification_loss: 0.8872
 103/1000 [==>...........................] - ETA: 6:45 - loss: 3.2013 - regression_loss: 2.3076 - classification_loss: 0.8937
 104/1000 [==>...........................] - ETA: 6:45 - loss: 3.2067 - regression_loss: 2.3082 - classification_loss: 0.8986
 105/1000 [==>...........................] - ETA: 6:44 - loss: 3.2174 - regression_loss: 2.3220 - classification_loss: 0.8954
 106/1000 [==>...........................] - ETA: 6:44 - loss: 3.1871 - regression_loss: 2.3001 - classification_loss: 0.8870
 107/1000 [==>...........................] - ETA: 6:43 - loss: 3.1942 - regression_loss: 2.3014 - classification_loss: 0.8928
 108/1000 [==>...........................] - ETA: 6:43 - loss: 3.2001 - regression_loss: 2.3041 - classification_loss: 0.8960
 109/1000 [==>...........................] - ETA: 6:43 - loss: 3.2055 - regression_loss: 2.3065 - classification_loss: 0.8990
 110/1000 [==>...........................] - ETA: 6:42 - loss: 3.2051 - regression_loss: 2.3080 - classification_loss: 0.8971
 111/1000 [==>...........................] - ETA: 6:42 - loss: 3.1763 - regression_loss: 2.2872 - classification_loss: 0.8891
 112/1000 [==>...........................] - ETA: 6:41 - loss: 3.1881 - regression_loss: 2.3000 - classification_loss: 0.8881
 113/1000 [==>...........................] - ETA: 6:41 - loss: 3.1912 - regression_loss: 2.3060 - classification_loss: 0.8853
 114/1000 [==>...........................] - ETA: 6:40 - loss: 3.1913 - regression_loss: 2.3042 - classification_loss: 0.8871
 115/1000 [==>...........................] - ETA: 6:40 - loss: 3.1963 - regression_loss: 2.3105 - classification_loss: 0.8858
 116/1000 [==>...........................] - ETA: 6:39 - loss: 3.2166 - regression_loss: 2.3245 - classification_loss: 0.8922
 117/1000 [==>...........................] - ETA: 6:39 - loss: 3.1896 - regression_loss: 2.3046 - classification_loss: 0.8850
 118/1000 [==>...........................] - ETA: 6:39 - loss: 3.1950 - regression_loss: 2.3089 - classification_loss: 0.8861
 119/1000 [==>...........................] - ETA: 6:38 - loss: 3.1847 - regression_loss: 2.3036 - classification_loss: 0.8812
 120/1000 [==>...........................] - ETA: 6:38 - loss: 3.1853 - regression_loss: 2.3057 - classification_loss: 0.8795
 121/1000 [==>...........................] - ETA: 6:37 - loss: 3.1589 - regression_loss: 2.2867 - classification_loss: 0.8723
 122/1000 [==>...........................] - ETA: 6:37 - loss: 3.1666 - regression_loss: 2.2901 - classification_loss: 0.8765
 123/1000 [==>...........................] - ETA: 6:36 - loss: 3.1825 - regression_loss: 2.3012 - classification_loss: 0.8813
 124/1000 [==>...........................] - ETA: 6:36 - loss: 3.1886 - regression_loss: 2.3044 - classification_loss: 0.8841
 125/1000 [==>...........................] - ETA: 6:36 - loss: 3.1918 - regression_loss: 2.3084 - classification_loss: 0.8834
 126/1000 [==>...........................] - ETA: 6:35 - loss: 3.1665 - regression_loss: 2.2901 - classification_loss: 0.8764
 127/1000 [==>...........................] - ETA: 6:35 - loss: 3.1809 - regression_loss: 2.3014 - classification_loss: 0.8795
 128/1000 [==>...........................] - ETA: 6:34 - loss: 3.1789 - regression_loss: 2.3033 - classification_loss: 0.8756
 129/1000 [==>...........................] - ETA: 6:34 - loss: 3.1829 - regression_loss: 2.3110 - classification_loss: 0.8718
 130/1000 [==>...........................] - ETA: 6:33 - loss: 3.1584 - regression_loss: 2.2932 - classification_loss: 0.8651
 131/1000 [==>...........................] - ETA: 6:33 - loss: 3.1555 - regression_loss: 2.2934 - classification_loss: 0.8621
 132/1000 [==>...........................] - ETA: 6:32 - loss: 3.1803 - regression_loss: 2.3128 - classification_loss: 0.8676
 133/1000 [==>...........................] - ETA: 6:32 - loss: 3.1848 - regression_loss: 2.3160 - classification_loss: 0.8689
 134/1000 [===>..........................] - ETA: 6:31 - loss: 3.1881 - regression_loss: 2.3182 - classification_loss: 0.8699
 135/1000 [===>..........................] - ETA: 6:31 - loss: 3.1909 - regression_loss: 2.3174 - classification_loss: 0.8736
 136/1000 [===>..........................] - ETA: 6:31 - loss: 3.2093 - regression_loss: 2.3276 - classification_loss: 0.8817
 137/1000 [===>..........................] - ETA: 6:30 - loss: 3.2040 - regression_loss: 2.3242 - classification_loss: 0.8798
 138/1000 [===>..........................] - ETA: 6:30 - loss: 3.1807 - regression_loss: 2.3073 - classification_loss: 0.8734
 139/1000 [===>..........................] - ETA: 6:29 - loss: 3.1817 - regression_loss: 2.3118 - classification_loss: 0.8700
 140/1000 [===>..........................] - ETA: 6:29 - loss: 3.1884 - regression_loss: 2.3148 - classification_loss: 0.8736
 141/1000 [===>..........................] - ETA: 6:28 - loss: 3.1661 - regression_loss: 2.2984 - classification_loss: 0.8677
 142/1000 [===>..........................] - ETA: 6:28 - loss: 3.1459 - regression_loss: 2.2822 - classification_loss: 0.8636
 143/1000 [===>..........................] - ETA: 6:27 - loss: 3.1505 - regression_loss: 2.2823 - classification_loss: 0.8682
 144/1000 [===>..........................] - ETA: 6:27 - loss: 3.1527 - regression_loss: 2.2863 - classification_loss: 0.8665
 145/1000 [===>..........................] - ETA: 6:27 - loss: 3.1592 - regression_loss: 2.2950 - classification_loss: 0.8641
 146/1000 [===>..........................] - ETA: 6:26 - loss: 3.1608 - regression_loss: 2.2987 - classification_loss: 0.8620
 147/1000 [===>..........................] - ETA: 6:26 - loss: 3.1640 - regression_loss: 2.3032 - classification_loss: 0.8608
 148/1000 [===>..........................] - ETA: 6:25 - loss: 3.1552 - regression_loss: 2.2981 - classification_loss: 0.8572
 149/1000 [===>..........................] - ETA: 6:25 - loss: 3.1620 - regression_loss: 2.3025 - classification_loss: 0.8595
 150/1000 [===>..........................] - ETA: 6:24 - loss: 3.1412 - regression_loss: 2.2872 - classification_loss: 0.8540
 151/1000 [===>..........................] - ETA: 6:24 - loss: 3.1424 - regression_loss: 2.2890 - classification_loss: 0.8534
 152/1000 [===>..........................] - ETA: 6:23 - loss: 3.1392 - regression_loss: 2.2865 - classification_loss: 0.8527
 153/1000 [===>..........................] - ETA: 6:23 - loss: 3.1498 - regression_loss: 2.2957 - classification_loss: 0.8541
 154/1000 [===>..........................] - ETA: 6:23 - loss: 3.1655 - regression_loss: 2.3072 - classification_loss: 0.8583
 155/1000 [===>..........................] - ETA: 6:22 - loss: 3.1698 - regression_loss: 2.3109 - classification_loss: 0.8589
 156/1000 [===>..........................] - ETA: 6:22 - loss: 3.1499 - regression_loss: 2.2961 - classification_loss: 0.8537
 157/1000 [===>..........................] - ETA: 6:21 - loss: 3.1478 - regression_loss: 2.2933 - classification_loss: 0.8545
 158/1000 [===>..........................] - ETA: 6:21 - loss: 3.1546 - regression_loss: 2.2978 - classification_loss: 0.8568
 159/1000 [===>..........................] - ETA: 6:20 - loss: 3.1643 - regression_loss: 2.3015 - classification_loss: 0.8628
 160/1000 [===>..........................] - ETA: 6:20 - loss: 3.1679 - regression_loss: 2.3047 - classification_loss: 0.8633
 161/1000 [===>..........................] - ETA: 6:19 - loss: 3.1808 - regression_loss: 2.3158 - classification_loss: 0.8651
 162/1000 [===>..........................] - ETA: 6:19 - loss: 3.1612 - regression_loss: 2.3015 - classification_loss: 0.8597
 163/1000 [===>..........................] - ETA: 6:19 - loss: 3.1418 - regression_loss: 2.2874 - classification_loss: 0.8545
 164/1000 [===>..........................] - ETA: 6:18 - loss: 3.1423 - regression_loss: 2.2863 - classification_loss: 0.8560
 165/1000 [===>..........................] - ETA: 6:18 - loss: 3.1418 - regression_loss: 2.2851 - classification_loss: 0.8567
 166/1000 [===>..........................] - ETA: 6:17 - loss: 3.1564 - regression_loss: 2.2962 - classification_loss: 0.8602
 167/1000 [====>.........................] - ETA: 6:17 - loss: 3.1604 - regression_loss: 2.3006 - classification_loss: 0.8599
 168/1000 [====>.........................] - ETA: 6:16 - loss: 3.1667 - regression_loss: 2.3074 - classification_loss: 0.8593
 169/1000 [====>.........................] - ETA: 6:16 - loss: 3.1659 - regression_loss: 2.3061 - classification_loss: 0.8598
 170/1000 [====>.........................] - ETA: 6:15 - loss: 3.1617 - regression_loss: 2.3041 - classification_loss: 0.8576
 171/1000 [====>.........................] - ETA: 6:15 - loss: 3.1633 - regression_loss: 2.3025 - classification_loss: 0.8607
 172/1000 [====>.........................] - ETA: 6:15 - loss: 3.1804 - regression_loss: 2.3227 - classification_loss: 0.8577
 173/1000 [====>.........................] - ETA: 6:14 - loss: 3.1812 - regression_loss: 2.3210 - classification_loss: 0.8602
 174/1000 [====>.........................] - ETA: 6:14 - loss: 3.1857 - regression_loss: 2.3266 - classification_loss: 0.8590
 175/1000 [====>.........................] - ETA: 6:13 - loss: 3.1675 - regression_loss: 2.3133 - classification_loss: 0.8541
 176/1000 [====>.........................] - ETA: 6:13 - loss: 3.1647 - regression_loss: 2.3137 - classification_loss: 0.8510
 177/1000 [====>.........................] - ETA: 6:12 - loss: 3.1468 - regression_loss: 2.3006 - classification_loss: 0.8462
 178/1000 [====>.........................] - ETA: 6:12 - loss: 3.1291 - regression_loss: 2.2877 - classification_loss: 0.8415
 179/1000 [====>.........................] - ETA: 6:11 - loss: 3.1363 - regression_loss: 2.2961 - classification_loss: 0.8402
 180/1000 [====>.........................] - ETA: 6:11 - loss: 3.1539 - regression_loss: 2.3087 - classification_loss: 0.8452
 181/1000 [====>.........................] - ETA: 6:11 - loss: 3.1540 - regression_loss: 2.3060 - classification_loss: 0.8479
 182/1000 [====>.........................] - ETA: 6:10 - loss: 3.1575 - regression_loss: 2.3115 - classification_loss: 0.8460
 183/1000 [====>.........................] - ETA: 6:10 - loss: 3.1605 - regression_loss: 2.3163 - classification_loss: 0.8442
 184/1000 [====>.........................] - ETA: 6:09 - loss: 3.1433 - regression_loss: 2.3037 - classification_loss: 0.8396
 185/1000 [====>.........................] - ETA: 6:09 - loss: 3.1405 - regression_loss: 2.3026 - classification_loss: 0.8380
 186/1000 [====>.........................] - ETA: 6:08 - loss: 3.1237 - regression_loss: 2.2902 - classification_loss: 0.8334
 187/1000 [====>.........................] - ETA: 6:08 - loss: 3.1070 - regression_loss: 2.2780 - classification_loss: 0.8290
 188/1000 [====>.........................] - ETA: 6:07 - loss: 3.1044 - regression_loss: 2.2761 - classification_loss: 0.8283
 189/1000 [====>.........................] - ETA: 6:07 - loss: 3.0880 - regression_loss: 2.2640 - classification_loss: 0.8239
 190/1000 [====>.........................] - ETA: 6:07 - loss: 3.0961 - regression_loss: 2.2662 - classification_loss: 0.8298
 191/1000 [====>.........................] - ETA: 6:06 - loss: 3.1063 - regression_loss: 2.2723 - classification_loss: 0.8340
 192/1000 [====>.........................] - ETA: 6:06 - loss: 3.1143 - regression_loss: 2.2778 - classification_loss: 0.8364
 193/1000 [====>.........................] - ETA: 6:05 - loss: 3.1132 - regression_loss: 2.2797 - classification_loss: 0.8335
 194/1000 [====>.........................] - ETA: 6:05 - loss: 3.1107 - regression_loss: 2.2795 - classification_loss: 0.8312
 195/1000 [====>.........................] - ETA: 6:04 - loss: 3.1201 - regression_loss: 2.2854 - classification_loss: 0.8347
 196/1000 [====>.........................] - ETA: 6:04 - loss: 3.1042 - regression_loss: 2.2738 - classification_loss: 0.8304
 197/1000 [====>.........................] - ETA: 6:03 - loss: 3.1011 - regression_loss: 2.2727 - classification_loss: 0.8284
 198/1000 [====>.........................] - ETA: 6:03 - loss: 3.1099 - regression_loss: 2.2776 - classification_loss: 0.8323
 199/1000 [====>.........................] - ETA: 6:02 - loss: 3.1118 - regression_loss: 2.2784 - classification_loss: 0.8334
 200/1000 [=====>........................] - ETA: 6:02 - loss: 3.1228 - regression_loss: 2.2850 - classification_loss: 0.8378
 201/1000 [=====>........................] - ETA: 6:02 - loss: 3.1316 - regression_loss: 2.2910 - classification_loss: 0.8406
 202/1000 [=====>........................] - ETA: 6:01 - loss: 3.1314 - regression_loss: 2.2917 - classification_loss: 0.8397
 203/1000 [=====>........................] - ETA: 6:01 - loss: 3.1348 - regression_loss: 2.2962 - classification_loss: 0.8386
 204/1000 [=====>........................] - ETA: 6:00 - loss: 3.1194 - regression_loss: 2.2849 - classification_loss: 0.8345
 205/1000 [=====>........................] - ETA: 6:00 - loss: 3.1291 - regression_loss: 2.2918 - classification_loss: 0.8373
 206/1000 [=====>........................] - ETA: 5:59 - loss: 3.1362 - regression_loss: 2.2966 - classification_loss: 0.8396
 207/1000 [=====>........................] - ETA: 5:59 - loss: 3.1449 - regression_loss: 2.3023 - classification_loss: 0.8426
 208/1000 [=====>........................] - ETA: 5:58 - loss: 3.1493 - regression_loss: 2.3033 - classification_loss: 0.8459
 209/1000 [=====>........................] - ETA: 5:58 - loss: 3.1575 - regression_loss: 2.3084 - classification_loss: 0.8492
 210/1000 [=====>........................] - ETA: 5:57 - loss: 3.1531 - regression_loss: 2.3062 - classification_loss: 0.8469
 211/1000 [=====>........................] - ETA: 5:57 - loss: 3.1544 - regression_loss: 2.3085 - classification_loss: 0.8459
 212/1000 [=====>........................] - ETA: 5:57 - loss: 3.1675 - regression_loss: 2.3185 - classification_loss: 0.8490
 213/1000 [=====>........................] - ETA: 5:56 - loss: 3.1527 - regression_loss: 2.3077 - classification_loss: 0.8450
 214/1000 [=====>........................] - ETA: 5:56 - loss: 3.1379 - regression_loss: 2.2969 - classification_loss: 0.8411
 215/1000 [=====>........................] - ETA: 5:55 - loss: 3.1315 - regression_loss: 2.2930 - classification_loss: 0.8385
 216/1000 [=====>........................] - ETA: 5:55 - loss: 3.1425 - regression_loss: 2.3053 - classification_loss: 0.8372
 217/1000 [=====>........................] - ETA: 5:54 - loss: 3.1417 - regression_loss: 2.3062 - classification_loss: 0.8355
 218/1000 [=====>........................] - ETA: 5:54 - loss: 3.1413 - regression_loss: 2.3079 - classification_loss: 0.8334
 219/1000 [=====>........................] - ETA: 5:53 - loss: 3.1457 - regression_loss: 2.3133 - classification_loss: 0.8324
 220/1000 [=====>........................] - ETA: 5:53 - loss: 3.1439 - regression_loss: 2.3133 - classification_loss: 0.8306
 221/1000 [=====>........................] - ETA: 5:52 - loss: 3.1431 - regression_loss: 2.3114 - classification_loss: 0.8316
 222/1000 [=====>........................] - ETA: 5:52 - loss: 3.1423 - regression_loss: 2.3126 - classification_loss: 0.8297
 223/1000 [=====>........................] - ETA: 5:51 - loss: 3.1282 - regression_loss: 2.3023 - classification_loss: 0.8260
 224/1000 [=====>........................] - ETA: 5:51 - loss: 3.1266 - regression_loss: 2.3029 - classification_loss: 0.8238
 225/1000 [=====>........................] - ETA: 5:51 - loss: 3.1414 - regression_loss: 2.3096 - classification_loss: 0.8318
 226/1000 [=====>........................] - ETA: 5:50 - loss: 3.1280 - regression_loss: 2.2994 - classification_loss: 0.8286
 227/1000 [=====>........................] - ETA: 5:50 - loss: 3.1272 - regression_loss: 2.3008 - classification_loss: 0.8264
 228/1000 [=====>........................] - ETA: 5:49 - loss: 3.1268 - regression_loss: 2.2999 - classification_loss: 0.8269
 229/1000 [=====>........................] - ETA: 5:49 - loss: 3.1323 - regression_loss: 2.3047 - classification_loss: 0.8275
 230/1000 [=====>........................] - ETA: 5:48 - loss: 3.1187 - regression_loss: 2.2947 - classification_loss: 0.8239
 231/1000 [=====>........................] - ETA: 5:48 - loss: 3.1252 - regression_loss: 2.3010 - classification_loss: 0.8242
 232/1000 [=====>........................] - ETA: 5:47 - loss: 3.1226 - regression_loss: 2.3006 - classification_loss: 0.8220
 233/1000 [=====>........................] - ETA: 5:47 - loss: 3.1335 - regression_loss: 2.3070 - classification_loss: 0.8266
 234/1000 [======>.......................] - ETA: 5:47 - loss: 3.1363 - regression_loss: 2.3083 - classification_loss: 0.8280
 235/1000 [======>.......................] - ETA: 5:46 - loss: 3.1414 - regression_loss: 2.3099 - classification_loss: 0.8315
 236/1000 [======>.......................] - ETA: 5:46 - loss: 3.1447 - regression_loss: 2.3145 - classification_loss: 0.8302
 237/1000 [======>.......................] - ETA: 5:45 - loss: 3.1460 - regression_loss: 2.3171 - classification_loss: 0.8289
 238/1000 [======>.......................] - ETA: 5:45 - loss: 3.1436 - regression_loss: 2.3165 - classification_loss: 0.8271
 239/1000 [======>.......................] - ETA: 5:44 - loss: 3.1458 - regression_loss: 2.3180 - classification_loss: 0.8278
 240/1000 [======>.......................] - ETA: 5:44 - loss: 3.1327 - regression_loss: 2.3083 - classification_loss: 0.8244
 241/1000 [======>.......................] - ETA: 5:43 - loss: 3.1211 - regression_loss: 2.2987 - classification_loss: 0.8224
 242/1000 [======>.......................] - ETA: 5:43 - loss: 3.1230 - regression_loss: 2.2997 - classification_loss: 0.8233
 243/1000 [======>.......................] - ETA: 5:43 - loss: 3.1102 - regression_loss: 2.2902 - classification_loss: 0.8199
 244/1000 [======>.......................] - ETA: 5:42 - loss: 3.1103 - regression_loss: 2.2914 - classification_loss: 0.8189
 245/1000 [======>.......................] - ETA: 5:42 - loss: 3.1241 - regression_loss: 2.3003 - classification_loss: 0.8238
 246/1000 [======>.......................] - ETA: 5:41 - loss: 3.1264 - regression_loss: 2.3044 - classification_loss: 0.8220
 247/1000 [======>.......................] - ETA: 5:41 - loss: 3.1137 - regression_loss: 2.2951 - classification_loss: 0.8186
 248/1000 [======>.......................] - ETA: 5:40 - loss: 3.1134 - regression_loss: 2.2956 - classification_loss: 0.8177
 249/1000 [======>.......................] - ETA: 5:40 - loss: 3.1109 - regression_loss: 2.2950 - classification_loss: 0.8160
 250/1000 [======>.......................] - ETA: 5:39 - loss: 3.1139 - regression_loss: 2.2962 - classification_loss: 0.8176
 251/1000 [======>.......................] - ETA: 5:39 - loss: 3.1221 - regression_loss: 2.3044 - classification_loss: 0.8177
 252/1000 [======>.......................] - ETA: 5:38 - loss: 3.1288 - regression_loss: 2.3090 - classification_loss: 0.8198
 253/1000 [======>.......................] - ETA: 5:38 - loss: 3.1250 - regression_loss: 2.3069 - classification_loss: 0.8181
 254/1000 [======>.......................] - ETA: 5:38 - loss: 3.1236 - regression_loss: 2.3054 - classification_loss: 0.8182
 255/1000 [======>.......................] - ETA: 5:37 - loss: 3.1306 - regression_loss: 2.3109 - classification_loss: 0.8197
 256/1000 [======>.......................] - ETA: 5:37 - loss: 3.1422 - regression_loss: 2.3173 - classification_loss: 0.8249
 257/1000 [======>.......................] - ETA: 5:36 - loss: 3.1485 - regression_loss: 2.3224 - classification_loss: 0.8260
 258/1000 [======>.......................] - ETA: 5:36 - loss: 3.1520 - regression_loss: 2.3257 - classification_loss: 0.8263
 259/1000 [======>.......................] - ETA: 5:35 - loss: 3.1493 - regression_loss: 2.3249 - classification_loss: 0.8244
 260/1000 [======>.......................] - ETA: 5:35 - loss: 3.1505 - regression_loss: 2.3264 - classification_loss: 0.8240
 261/1000 [======>.......................] - ETA: 5:35 - loss: 3.1389 - regression_loss: 2.3175 - classification_loss: 0.8214
 262/1000 [======>.......................] - ETA: 5:34 - loss: 3.1370 - regression_loss: 2.3178 - classification_loss: 0.8192
 263/1000 [======>.......................] - ETA: 5:34 - loss: 3.1451 - regression_loss: 2.3228 - classification_loss: 0.8224
 264/1000 [======>.......................] - ETA: 5:33 - loss: 3.1467 - regression_loss: 2.3262 - classification_loss: 0.8205
 265/1000 [======>.......................] - ETA: 5:33 - loss: 3.1498 - regression_loss: 2.3284 - classification_loss: 0.8214
 266/1000 [======>.......................] - ETA: 5:32 - loss: 3.1530 - regression_loss: 2.3320 - classification_loss: 0.8210
 267/1000 [=======>......................] - ETA: 5:32 - loss: 3.1538 - regression_loss: 2.3318 - classification_loss: 0.8220
 268/1000 [=======>......................] - ETA: 5:31 - loss: 3.1421 - regression_loss: 2.3231 - classification_loss: 0.8190
 269/1000 [=======>......................] - ETA: 5:31 - loss: 3.1406 - regression_loss: 2.3231 - classification_loss: 0.8175
 270/1000 [=======>......................] - ETA: 5:30 - loss: 3.1290 - regression_loss: 2.3145 - classification_loss: 0.8145
 271/1000 [=======>......................] - ETA: 5:30 - loss: 3.1174 - regression_loss: 2.3059 - classification_loss: 0.8115
 272/1000 [=======>......................] - ETA: 5:30 - loss: 3.1060 - regression_loss: 2.2975 - classification_loss: 0.8085
 273/1000 [=======>......................] - ETA: 5:29 - loss: 3.1083 - regression_loss: 2.2980 - classification_loss: 0.8103
 274/1000 [=======>......................] - ETA: 5:29 - loss: 3.1053 - regression_loss: 2.2962 - classification_loss: 0.8090
 275/1000 [=======>......................] - ETA: 5:28 - loss: 3.1094 - regression_loss: 2.3002 - classification_loss: 0.8093
 276/1000 [=======>......................] - ETA: 5:28 - loss: 3.1192 - regression_loss: 2.3099 - classification_loss: 0.8094
 277/1000 [=======>......................] - ETA: 5:27 - loss: 3.1229 - regression_loss: 2.3121 - classification_loss: 0.8108
 278/1000 [=======>......................] - ETA: 5:27 - loss: 3.1243 - regression_loss: 2.3118 - classification_loss: 0.8125
 279/1000 [=======>......................] - ETA: 5:26 - loss: 3.1131 - regression_loss: 2.3035 - classification_loss: 0.8096
 280/1000 [=======>......................] - ETA: 5:26 - loss: 3.1175 - regression_loss: 2.3052 - classification_loss: 0.8124
 281/1000 [=======>......................] - ETA: 5:25 - loss: 3.1162 - regression_loss: 2.3046 - classification_loss: 0.8115
 282/1000 [=======>......................] - ETA: 5:25 - loss: 3.1232 - regression_loss: 2.3095 - classification_loss: 0.8137
 283/1000 [=======>......................] - ETA: 5:24 - loss: 3.1263 - regression_loss: 2.3134 - classification_loss: 0.8129
 284/1000 [=======>......................] - ETA: 5:24 - loss: 3.1294 - regression_loss: 2.3156 - classification_loss: 0.8138
 285/1000 [=======>......................] - ETA: 5:24 - loss: 3.1372 - regression_loss: 2.3215 - classification_loss: 0.8158
 286/1000 [=======>......................] - ETA: 5:23 - loss: 3.1384 - regression_loss: 2.3230 - classification_loss: 0.8154
 287/1000 [=======>......................] - ETA: 5:23 - loss: 3.1424 - regression_loss: 2.3270 - classification_loss: 0.8154
 288/1000 [=======>......................] - ETA: 5:22 - loss: 3.1435 - regression_loss: 2.3281 - classification_loss: 0.8154
 289/1000 [=======>......................] - ETA: 5:22 - loss: 3.1437 - regression_loss: 2.3289 - classification_loss: 0.8149
 290/1000 [=======>......................] - ETA: 5:21 - loss: 3.1493 - regression_loss: 2.3335 - classification_loss: 0.8158
 291/1000 [=======>......................] - ETA: 5:21 - loss: 3.1576 - regression_loss: 2.3382 - classification_loss: 0.8194
 292/1000 [=======>......................] - ETA: 5:20 - loss: 3.1540 - regression_loss: 2.3353 - classification_loss: 0.8188
 293/1000 [=======>......................] - ETA: 5:20 - loss: 3.1640 - regression_loss: 2.3434 - classification_loss: 0.8207
 294/1000 [=======>......................] - ETA: 5:19 - loss: 3.1661 - regression_loss: 2.3450 - classification_loss: 0.8212
 295/1000 [=======>......................] - ETA: 5:19 - loss: 3.1723 - regression_loss: 2.3500 - classification_loss: 0.8223
 296/1000 [=======>......................] - ETA: 5:19 - loss: 3.1692 - regression_loss: 2.3481 - classification_loss: 0.8211
 297/1000 [=======>......................] - ETA: 5:18 - loss: 3.1741 - regression_loss: 2.3510 - classification_loss: 0.8232
 298/1000 [=======>......................] - ETA: 5:18 - loss: 3.1731 - regression_loss: 2.3489 - classification_loss: 0.8242
 299/1000 [=======>......................] - ETA: 5:17 - loss: 3.1730 - regression_loss: 2.3493 - classification_loss: 0.8237
 300/1000 [========>.....................] - ETA: 5:17 - loss: 3.1748 - regression_loss: 2.3513 - classification_loss: 0.8235
 301/1000 [========>.....................] - ETA: 5:16 - loss: 3.1704 - regression_loss: 2.3483 - classification_loss: 0.8221
 302/1000 [========>.....................] - ETA: 5:16 - loss: 3.1703 - regression_loss: 2.3484 - classification_loss: 0.8220
 303/1000 [========>.....................] - ETA: 5:15 - loss: 3.1647 - regression_loss: 2.3406 - classification_loss: 0.8240
 304/1000 [========>.....................] - ETA: 5:15 - loss: 3.1658 - regression_loss: 2.3427 - classification_loss: 0.8231
 305/1000 [========>.....................] - ETA: 5:14 - loss: 3.1554 - regression_loss: 2.3350 - classification_loss: 0.8204
 306/1000 [========>.....................] - ETA: 5:14 - loss: 3.1561 - regression_loss: 2.3352 - classification_loss: 0.8209
 307/1000 [========>.....................] - ETA: 5:13 - loss: 3.1586 - regression_loss: 2.3381 - classification_loss: 0.8205
 308/1000 [========>.....................] - ETA: 5:13 - loss: 3.1564 - regression_loss: 2.3372 - classification_loss: 0.8192
 309/1000 [========>.....................] - ETA: 5:13 - loss: 3.1470 - regression_loss: 2.3296 - classification_loss: 0.8173
 310/1000 [========>.....................] - ETA: 5:12 - loss: 3.1515 - regression_loss: 2.3329 - classification_loss: 0.8186
 311/1000 [========>.....................] - ETA: 5:12 - loss: 3.1414 - regression_loss: 2.3254 - classification_loss: 0.8160
 312/1000 [========>.....................] - ETA: 5:11 - loss: 3.1430 - regression_loss: 2.3282 - classification_loss: 0.8149
 313/1000 [========>.....................] - ETA: 5:11 - loss: 3.1426 - regression_loss: 2.3295 - classification_loss: 0.8132
 314/1000 [========>.....................] - ETA: 5:10 - loss: 3.1395 - regression_loss: 2.3271 - classification_loss: 0.8124
 315/1000 [========>.....................] - ETA: 5:10 - loss: 3.1460 - regression_loss: 2.3328 - classification_loss: 0.8133
 316/1000 [========>.....................] - ETA: 5:09 - loss: 3.1377 - regression_loss: 2.3254 - classification_loss: 0.8123
 317/1000 [========>.....................] - ETA: 5:09 - loss: 3.1381 - regression_loss: 2.3249 - classification_loss: 0.8132
 318/1000 [========>.....................] - ETA: 5:08 - loss: 3.1400 - regression_loss: 2.3263 - classification_loss: 0.8137
 319/1000 [========>.....................] - ETA: 5:08 - loss: 3.1352 - regression_loss: 2.3234 - classification_loss: 0.8119
 320/1000 [========>.....................] - ETA: 5:08 - loss: 3.1255 - regression_loss: 2.3161 - classification_loss: 0.8094
 321/1000 [========>.....................] - ETA: 5:07 - loss: 3.1253 - regression_loss: 2.3171 - classification_loss: 0.8083
 322/1000 [========>.....................] - ETA: 5:07 - loss: 3.1156 - regression_loss: 2.3099 - classification_loss: 0.8058
 323/1000 [========>.....................] - ETA: 5:06 - loss: 3.1169 - regression_loss: 2.3102 - classification_loss: 0.8067
 324/1000 [========>.....................] - ETA: 5:06 - loss: 3.1175 - regression_loss: 2.3107 - classification_loss: 0.8068
 325/1000 [========>.....................] - ETA: 5:05 - loss: 3.1237 - regression_loss: 2.3147 - classification_loss: 0.8090
 326/1000 [========>.....................] - ETA: 5:05 - loss: 3.1232 - regression_loss: 2.3141 - classification_loss: 0.8091
 327/1000 [========>.....................] - ETA: 5:04 - loss: 3.1136 - regression_loss: 2.3070 - classification_loss: 0.8066
 328/1000 [========>.....................] - ETA: 5:04 - loss: 3.1132 - regression_loss: 2.3072 - classification_loss: 0.8059
 329/1000 [========>.....................] - ETA: 5:03 - loss: 3.1182 - regression_loss: 2.3116 - classification_loss: 0.8066
 330/1000 [========>.....................] - ETA: 5:03 - loss: 3.1087 - regression_loss: 2.3046 - classification_loss: 0.8042
 331/1000 [========>.....................] - ETA: 5:03 - loss: 3.1064 - regression_loss: 2.3027 - classification_loss: 0.8036
 332/1000 [========>.....................] - ETA: 5:02 - loss: 3.1080 - regression_loss: 2.3050 - classification_loss: 0.8031
 333/1000 [========>.....................] - ETA: 5:02 - loss: 3.0995 - regression_loss: 2.2980 - classification_loss: 0.8015
 334/1000 [=========>....................] - ETA: 5:01 - loss: 3.0981 - regression_loss: 2.2970 - classification_loss: 0.8011
 335/1000 [=========>....................] - ETA: 5:01 - loss: 3.0985 - regression_loss: 2.2979 - classification_loss: 0.8005
 336/1000 [=========>....................] - ETA: 5:00 - loss: 3.0987 - regression_loss: 2.2981 - classification_loss: 0.8006
 337/1000 [=========>....................] - ETA: 5:00 - loss: 3.1029 - regression_loss: 2.3011 - classification_loss: 0.8018
 338/1000 [=========>....................] - ETA: 4:59 - loss: 3.0937 - regression_loss: 2.2943 - classification_loss: 0.7994
 339/1000 [=========>....................] - ETA: 4:59 - loss: 3.0941 - regression_loss: 2.2942 - classification_loss: 0.7999
 340/1000 [=========>....................] - ETA: 4:59 - loss: 3.0850 - regression_loss: 2.2874 - classification_loss: 0.7976
 341/1000 [=========>....................] - ETA: 4:58 - loss: 3.0862 - regression_loss: 2.2891 - classification_loss: 0.7971
 342/1000 [=========>....................] - ETA: 4:58 - loss: 3.0919 - regression_loss: 2.2924 - classification_loss: 0.7995
 343/1000 [=========>....................] - ETA: 4:57 - loss: 3.0930 - regression_loss: 2.2921 - classification_loss: 0.8009
 344/1000 [=========>....................] - ETA: 4:57 - loss: 3.0987 - regression_loss: 2.2971 - classification_loss: 0.8015
 345/1000 [=========>....................] - ETA: 4:56 - loss: 3.0951 - regression_loss: 2.2948 - classification_loss: 0.8003
 346/1000 [=========>....................] - ETA: 4:56 - loss: 3.0862 - regression_loss: 2.2882 - classification_loss: 0.7980
 347/1000 [=========>....................] - ETA: 4:55 - loss: 3.0859 - regression_loss: 2.2886 - classification_loss: 0.7973
 348/1000 [=========>....................] - ETA: 4:55 - loss: 3.0859 - regression_loss: 2.2897 - classification_loss: 0.7962
 349/1000 [=========>....................] - ETA: 4:54 - loss: 3.0891 - regression_loss: 2.2924 - classification_loss: 0.7967
 350/1000 [=========>....................] - ETA: 4:54 - loss: 3.0911 - regression_loss: 2.2938 - classification_loss: 0.7973
 351/1000 [=========>....................] - ETA: 4:53 - loss: 3.0939 - regression_loss: 2.2961 - classification_loss: 0.7979
 352/1000 [=========>....................] - ETA: 4:53 - loss: 3.0973 - regression_loss: 2.2991 - classification_loss: 0.7982
 353/1000 [=========>....................] - ETA: 4:53 - loss: 3.0998 - regression_loss: 2.2998 - classification_loss: 0.8000
 354/1000 [=========>....................] - ETA: 4:52 - loss: 3.1030 - regression_loss: 2.3002 - classification_loss: 0.8028
 355/1000 [=========>....................] - ETA: 4:52 - loss: 3.1102 - regression_loss: 2.3040 - classification_loss: 0.8062
 356/1000 [=========>....................] - ETA: 4:51 - loss: 3.1156 - regression_loss: 2.3078 - classification_loss: 0.8077
 357/1000 [=========>....................] - ETA: 4:51 - loss: 3.1209 - regression_loss: 2.3131 - classification_loss: 0.8078
 358/1000 [=========>....................] - ETA: 4:50 - loss: 3.1202 - regression_loss: 2.3128 - classification_loss: 0.8073
 359/1000 [=========>....................] - ETA: 4:50 - loss: 3.1115 - regression_loss: 2.3064 - classification_loss: 0.8051
 360/1000 [=========>....................] - ETA: 4:49 - loss: 3.1096 - regression_loss: 2.3053 - classification_loss: 0.8043
 361/1000 [=========>....................] - ETA: 4:49 - loss: 3.1014 - regression_loss: 2.2990 - classification_loss: 0.8024
 362/1000 [=========>....................] - ETA: 4:49 - loss: 3.0974 - regression_loss: 2.2966 - classification_loss: 0.8008
 363/1000 [=========>....................] - ETA: 4:48 - loss: 3.0987 - regression_loss: 2.2979 - classification_loss: 0.8008
 364/1000 [=========>....................] - ETA: 4:48 - loss: 3.0902 - regression_loss: 2.2916 - classification_loss: 0.7986
 365/1000 [=========>....................] - ETA: 4:47 - loss: 3.0817 - regression_loss: 2.2853 - classification_loss: 0.7964
 366/1000 [=========>....................] - ETA: 4:47 - loss: 3.0832 - regression_loss: 2.2874 - classification_loss: 0.7958
 367/1000 [==========>...................] - ETA: 4:46 - loss: 3.0810 - regression_loss: 2.2865 - classification_loss: 0.7945
 368/1000 [==========>...................] - ETA: 4:46 - loss: 3.0843 - regression_loss: 2.2898 - classification_loss: 0.7945
 369/1000 [==========>...................] - ETA: 4:45 - loss: 3.0941 - regression_loss: 2.2926 - classification_loss: 0.8015
 370/1000 [==========>...................] - ETA: 4:45 - loss: 3.0857 - regression_loss: 2.2864 - classification_loss: 0.7993
 371/1000 [==========>...................] - ETA: 4:44 - loss: 3.0774 - regression_loss: 2.2802 - classification_loss: 0.7972
 372/1000 [==========>...................] - ETA: 4:44 - loss: 3.0804 - regression_loss: 2.2839 - classification_loss: 0.7964
 373/1000 [==========>...................] - ETA: 4:44 - loss: 3.0824 - regression_loss: 2.2842 - classification_loss: 0.7982
 374/1000 [==========>...................] - ETA: 4:43 - loss: 3.0890 - regression_loss: 2.2891 - classification_loss: 0.8000
 375/1000 [==========>...................] - ETA: 4:43 - loss: 3.0808 - regression_loss: 2.2830 - classification_loss: 0.7978
 376/1000 [==========>...................] - ETA: 4:42 - loss: 3.0726 - regression_loss: 2.2769 - classification_loss: 0.7957
 377/1000 [==========>...................] - ETA: 4:42 - loss: 3.0735 - regression_loss: 2.2787 - classification_loss: 0.7948
 378/1000 [==========>...................] - ETA: 4:41 - loss: 3.0731 - regression_loss: 2.2793 - classification_loss: 0.7938
 379/1000 [==========>...................] - ETA: 4:41 - loss: 3.0767 - regression_loss: 2.2811 - classification_loss: 0.7956
 380/1000 [==========>...................] - ETA: 4:40 - loss: 3.0792 - regression_loss: 2.2841 - classification_loss: 0.7951
 381/1000 [==========>...................] - ETA: 4:40 - loss: 3.0850 - regression_loss: 2.2874 - classification_loss: 0.7977
 382/1000 [==========>...................] - ETA: 4:40 - loss: 3.0837 - regression_loss: 2.2865 - classification_loss: 0.7972
 383/1000 [==========>...................] - ETA: 4:39 - loss: 3.0852 - regression_loss: 2.2883 - classification_loss: 0.7969
 384/1000 [==========>...................] - ETA: 4:39 - loss: 3.0925 - regression_loss: 2.2933 - classification_loss: 0.7991
 385/1000 [==========>...................] - ETA: 4:38 - loss: 3.0844 - regression_loss: 2.2874 - classification_loss: 0.7971
 386/1000 [==========>...................] - ETA: 4:38 - loss: 3.0867 - regression_loss: 2.2880 - classification_loss: 0.7987
 387/1000 [==========>...................] - ETA: 4:37 - loss: 3.0914 - regression_loss: 2.2897 - classification_loss: 0.8017
 388/1000 [==========>...................] - ETA: 4:37 - loss: 3.0834 - regression_loss: 2.2838 - classification_loss: 0.7997
 389/1000 [==========>...................] - ETA: 4:36 - loss: 3.0831 - regression_loss: 2.2835 - classification_loss: 0.7997
 390/1000 [==========>...................] - ETA: 4:36 - loss: 3.0850 - regression_loss: 2.2840 - classification_loss: 0.8011
 391/1000 [==========>...................] - ETA: 4:35 - loss: 3.0862 - regression_loss: 2.2833 - classification_loss: 0.8029
 392/1000 [==========>...................] - ETA: 4:35 - loss: 3.0869 - regression_loss: 2.2831 - classification_loss: 0.8039
 393/1000 [==========>...................] - ETA: 4:35 - loss: 3.0911 - regression_loss: 2.2850 - classification_loss: 0.8061
 394/1000 [==========>...................] - ETA: 4:34 - loss: 3.0956 - regression_loss: 2.2881 - classification_loss: 0.8074
 395/1000 [==========>...................] - ETA: 4:34 - loss: 3.0963 - regression_loss: 2.2881 - classification_loss: 0.8082
 396/1000 [==========>...................] - ETA: 4:33 - loss: 3.0885 - regression_loss: 2.2824 - classification_loss: 0.8061
 397/1000 [==========>...................] - ETA: 4:33 - loss: 3.0919 - regression_loss: 2.2830 - classification_loss: 0.8089
 398/1000 [==========>...................] - ETA: 4:32 - loss: 3.0926 - regression_loss: 2.2848 - classification_loss: 0.8079
 399/1000 [==========>...................] - ETA: 4:32 - loss: 3.0849 - regression_loss: 2.2791 - classification_loss: 0.8058
 400/1000 [===========>..................] - ETA: 4:31 - loss: 3.0772 - regression_loss: 2.2734 - classification_loss: 0.8039
 401/1000 [===========>..................] - ETA: 4:31 - loss: 3.0822 - regression_loss: 2.2769 - classification_loss: 0.8054
 402/1000 [===========>..................] - ETA: 4:30 - loss: 3.0865 - regression_loss: 2.2810 - classification_loss: 0.8055
 403/1000 [===========>..................] - ETA: 4:30 - loss: 3.0913 - regression_loss: 2.2840 - classification_loss: 0.8073
 404/1000 [===========>..................] - ETA: 4:30 - loss: 3.0836 - regression_loss: 2.2783 - classification_loss: 0.8053
 405/1000 [===========>..................] - ETA: 4:29 - loss: 3.0894 - regression_loss: 2.2824 - classification_loss: 0.8070
 406/1000 [===========>..................] - ETA: 4:29 - loss: 3.0893 - regression_loss: 2.2828 - classification_loss: 0.8066
 407/1000 [===========>..................] - ETA: 4:28 - loss: 3.0929 - regression_loss: 2.2867 - classification_loss: 0.8062
 408/1000 [===========>..................] - ETA: 4:28 - loss: 3.0957 - regression_loss: 2.2881 - classification_loss: 0.8076
 409/1000 [===========>..................] - ETA: 4:27 - loss: 3.0881 - regression_loss: 2.2825 - classification_loss: 0.8056
 410/1000 [===========>..................] - ETA: 4:27 - loss: 3.0806 - regression_loss: 2.2769 - classification_loss: 0.8037
 411/1000 [===========>..................] - ETA: 4:26 - loss: 3.0834 - regression_loss: 2.2783 - classification_loss: 0.8051
 412/1000 [===========>..................] - ETA: 4:26 - loss: 3.0759 - regression_loss: 2.2727 - classification_loss: 0.8031
 413/1000 [===========>..................] - ETA: 4:25 - loss: 3.0761 - regression_loss: 2.2724 - classification_loss: 0.8036
 414/1000 [===========>..................] - ETA: 4:25 - loss: 3.0775 - regression_loss: 2.2723 - classification_loss: 0.8052
 415/1000 [===========>..................] - ETA: 4:25 - loss: 3.0807 - regression_loss: 2.2737 - classification_loss: 0.8070
 416/1000 [===========>..................] - ETA: 4:24 - loss: 3.0846 - regression_loss: 2.2757 - classification_loss: 0.8088
 417/1000 [===========>..................] - ETA: 4:24 - loss: 3.0891 - regression_loss: 2.2790 - classification_loss: 0.8101
 418/1000 [===========>..................] - ETA: 4:23 - loss: 3.0907 - regression_loss: 2.2798 - classification_loss: 0.8109
 419/1000 [===========>..................] - ETA: 4:23 - loss: 3.0912 - regression_loss: 2.2806 - classification_loss: 0.8106
 420/1000 [===========>..................] - ETA: 4:22 - loss: 3.0952 - regression_loss: 2.2826 - classification_loss: 0.8126
 421/1000 [===========>..................] - ETA: 4:22 - loss: 3.0967 - regression_loss: 2.2838 - classification_loss: 0.8129
 422/1000 [===========>..................] - ETA: 4:21 - loss: 3.0962 - regression_loss: 2.2840 - classification_loss: 0.8121
 423/1000 [===========>..................] - ETA: 4:21 - loss: 3.0889 - regression_loss: 2.2786 - classification_loss: 0.8102
 424/1000 [===========>..................] - ETA: 4:20 - loss: 3.0938 - regression_loss: 2.2840 - classification_loss: 0.8098
 425/1000 [===========>..................] - ETA: 4:20 - loss: 3.0990 - regression_loss: 2.2878 - classification_loss: 0.8113
 426/1000 [===========>..................] - ETA: 4:20 - loss: 3.1028 - regression_loss: 2.2907 - classification_loss: 0.8121
 427/1000 [===========>..................] - ETA: 4:19 - loss: 3.0956 - regression_loss: 2.2853 - classification_loss: 0.8102
 428/1000 [===========>..................] - ETA: 4:19 - loss: 3.0957 - regression_loss: 2.2856 - classification_loss: 0.8100
 429/1000 [===========>..................] - ETA: 4:18 - loss: 3.0885 - regression_loss: 2.2803 - classification_loss: 0.8081
 430/1000 [===========>..................] - ETA: 4:18 - loss: 3.0923 - regression_loss: 2.2835 - classification_loss: 0.8088
 431/1000 [===========>..................] - ETA: 4:17 - loss: 3.0945 - regression_loss: 2.2850 - classification_loss: 0.8095
 432/1000 [===========>..................] - ETA: 4:17 - loss: 3.0986 - regression_loss: 2.2886 - classification_loss: 0.8100
 433/1000 [===========>..................] - ETA: 4:16 - loss: 3.0992 - regression_loss: 2.2893 - classification_loss: 0.8099
 434/1000 [============>.................] - ETA: 4:16 - loss: 3.0924 - regression_loss: 2.2841 - classification_loss: 0.8083
 435/1000 [============>.................] - ETA: 4:16 - loss: 3.0926 - regression_loss: 2.2833 - classification_loss: 0.8093
 436/1000 [============>.................] - ETA: 4:15 - loss: 3.0919 - regression_loss: 2.2836 - classification_loss: 0.8083
 437/1000 [============>.................] - ETA: 4:15 - loss: 3.0987 - regression_loss: 2.2883 - classification_loss: 0.8104
 438/1000 [============>.................] - ETA: 4:14 - loss: 3.0977 - regression_loss: 2.2884 - classification_loss: 0.8093
 439/1000 [============>.................] - ETA: 4:14 - loss: 3.1020 - regression_loss: 2.2911 - classification_loss: 0.8109
 440/1000 [============>.................] - ETA: 4:13 - loss: 3.1069 - regression_loss: 2.2945 - classification_loss: 0.8124
 441/1000 [============>.................] - ETA: 4:13 - loss: 3.1079 - regression_loss: 2.2948 - classification_loss: 0.8131
 442/1000 [============>.................] - ETA: 4:12 - loss: 3.1106 - regression_loss: 2.2960 - classification_loss: 0.8146
 443/1000 [============>.................] - ETA: 4:12 - loss: 3.1036 - regression_loss: 2.2909 - classification_loss: 0.8127
 444/1000 [============>.................] - ETA: 4:11 - loss: 3.0966 - regression_loss: 2.2857 - classification_loss: 0.8109
 445/1000 [============>.................] - ETA: 4:11 - loss: 3.0974 - regression_loss: 2.2864 - classification_loss: 0.8110
 446/1000 [============>.................] - ETA: 4:11 - loss: 3.0904 - regression_loss: 2.2813 - classification_loss: 0.8092
 447/1000 [============>.................] - ETA: 4:10 - loss: 3.0906 - regression_loss: 2.2822 - classification_loss: 0.8083
 448/1000 [============>.................] - ETA: 4:10 - loss: 3.0936 - regression_loss: 2.2845 - classification_loss: 0.8091
 449/1000 [============>.................] - ETA: 4:09 - loss: 3.0940 - regression_loss: 2.2853 - classification_loss: 0.8087
 450/1000 [============>.................] - ETA: 4:09 - loss: 3.0960 - regression_loss: 2.2869 - classification_loss: 0.8090
 451/1000 [============>.................] - ETA: 4:08 - loss: 3.1003 - regression_loss: 2.2901 - classification_loss: 0.8102
 452/1000 [============>.................] - ETA: 4:08 - loss: 3.1053 - regression_loss: 2.2941 - classification_loss: 0.8113
 453/1000 [============>.................] - ETA: 4:07 - loss: 3.1070 - regression_loss: 2.2960 - classification_loss: 0.8110
 454/1000 [============>.................] - ETA: 4:07 - loss: 3.1069 - regression_loss: 2.2964 - classification_loss: 0.8105
 455/1000 [============>.................] - ETA: 4:06 - loss: 3.1092 - regression_loss: 2.2980 - classification_loss: 0.8112
 456/1000 [============>.................] - ETA: 4:06 - loss: 3.1135 - regression_loss: 2.3008 - classification_loss: 0.8127
 457/1000 [============>.................] - ETA: 4:06 - loss: 3.1141 - regression_loss: 2.3011 - classification_loss: 0.8130
 458/1000 [============>.................] - ETA: 4:05 - loss: 3.1177 - regression_loss: 2.3045 - classification_loss: 0.8132
 459/1000 [============>.................] - ETA: 4:05 - loss: 3.1109 - regression_loss: 2.2995 - classification_loss: 0.8114
 460/1000 [============>.................] - ETA: 4:04 - loss: 3.1132 - regression_loss: 2.3010 - classification_loss: 0.8122
 461/1000 [============>.................] - ETA: 4:04 - loss: 3.1168 - regression_loss: 2.3041 - classification_loss: 0.8127
 462/1000 [============>.................] - ETA: 4:03 - loss: 3.1212 - regression_loss: 2.3083 - classification_loss: 0.8129
 463/1000 [============>.................] - ETA: 4:03 - loss: 3.1145 - regression_loss: 2.3033 - classification_loss: 0.8112
 464/1000 [============>.................] - ETA: 4:02 - loss: 3.1148 - regression_loss: 2.3039 - classification_loss: 0.8110
 465/1000 [============>.................] - ETA: 4:02 - loss: 3.1144 - regression_loss: 2.3033 - classification_loss: 0.8111
 466/1000 [============>.................] - ETA: 4:01 - loss: 3.1077 - regression_loss: 2.2984 - classification_loss: 0.8094
 467/1000 [=============>................] - ETA: 4:01 - loss: 3.1011 - regression_loss: 2.2934 - classification_loss: 0.8077
 468/1000 [=============>................] - ETA: 4:01 - loss: 3.1008 - regression_loss: 2.2939 - classification_loss: 0.8069
 469/1000 [=============>................] - ETA: 4:00 - loss: 3.1030 - regression_loss: 2.2946 - classification_loss: 0.8084
 470/1000 [=============>................] - ETA: 4:00 - loss: 3.1042 - regression_loss: 2.2956 - classification_loss: 0.8086
 471/1000 [=============>................] - ETA: 3:59 - loss: 3.1099 - regression_loss: 2.3004 - classification_loss: 0.8095
 472/1000 [=============>................] - ETA: 3:59 - loss: 3.1095 - regression_loss: 2.3006 - classification_loss: 0.8089
 473/1000 [=============>................] - ETA: 3:58 - loss: 3.1030 - regression_loss: 2.2958 - classification_loss: 0.8072
 474/1000 [=============>................] - ETA: 3:58 - loss: 3.1096 - regression_loss: 2.3006 - classification_loss: 0.8090
 475/1000 [=============>................] - ETA: 3:57 - loss: 3.1113 - regression_loss: 2.3014 - classification_loss: 0.8099
 476/1000 [=============>................] - ETA: 3:57 - loss: 3.1122 - regression_loss: 2.3027 - classification_loss: 0.8095
 477/1000 [=============>................] - ETA: 3:56 - loss: 3.1136 - regression_loss: 2.3047 - classification_loss: 0.8089
 478/1000 [=============>................] - ETA: 3:56 - loss: 3.1071 - regression_loss: 2.2999 - classification_loss: 0.8072
 479/1000 [=============>................] - ETA: 3:56 - loss: 3.1006 - regression_loss: 2.2951 - classification_loss: 0.8055
 480/1000 [=============>................] - ETA: 3:55 - loss: 3.1000 - regression_loss: 2.2952 - classification_loss: 0.8048
 481/1000 [=============>................] - ETA: 3:55 - loss: 3.1004 - regression_loss: 2.2964 - classification_loss: 0.8040
 482/1000 [=============>................] - ETA: 3:54 - loss: 3.1021 - regression_loss: 2.2975 - classification_loss: 0.8046
 483/1000 [=============>................] - ETA: 3:54 - loss: 3.1070 - regression_loss: 2.3008 - classification_loss: 0.8061
 484/1000 [=============>................] - ETA: 3:53 - loss: 3.1121 - regression_loss: 2.3036 - classification_loss: 0.8084
 485/1000 [=============>................] - ETA: 3:53 - loss: 3.1106 - regression_loss: 2.3033 - classification_loss: 0.8073
 486/1000 [=============>................] - ETA: 3:52 - loss: 3.1100 - regression_loss: 2.3032 - classification_loss: 0.8068
 487/1000 [=============>................] - ETA: 3:52 - loss: 3.1037 - regression_loss: 2.2985 - classification_loss: 0.8052
 488/1000 [=============>................] - ETA: 3:51 - loss: 3.1040 - regression_loss: 2.2984 - classification_loss: 0.8057
 489/1000 [=============>................] - ETA: 3:51 - loss: 3.1059 - regression_loss: 2.3004 - classification_loss: 0.8055
 490/1000 [=============>................] - ETA: 3:50 - loss: 3.1084 - regression_loss: 2.3019 - classification_loss: 0.8065
 491/1000 [=============>................] - ETA: 3:50 - loss: 3.1103 - regression_loss: 2.3042 - classification_loss: 0.8061
 492/1000 [=============>................] - ETA: 3:50 - loss: 3.1040 - regression_loss: 2.2996 - classification_loss: 0.8045
 493/1000 [=============>................] - ETA: 3:49 - loss: 3.0977 - regression_loss: 2.2949 - classification_loss: 0.8028
 494/1000 [=============>................] - ETA: 3:49 - loss: 3.0915 - regression_loss: 2.2902 - classification_loss: 0.8012
 495/1000 [=============>................] - ETA: 3:48 - loss: 3.0932 - regression_loss: 2.2912 - classification_loss: 0.8020
 496/1000 [=============>................] - ETA: 3:48 - loss: 3.0870 - regression_loss: 2.2866 - classification_loss: 0.8004
 497/1000 [=============>................] - ETA: 3:47 - loss: 3.0812 - regression_loss: 2.2820 - classification_loss: 0.7993
 498/1000 [=============>................] - ETA: 3:47 - loss: 3.0819 - regression_loss: 2.2828 - classification_loss: 0.7991
 499/1000 [=============>................] - ETA: 3:46 - loss: 3.0847 - regression_loss: 2.2839 - classification_loss: 0.8008
 500/1000 [==============>...............] - ETA: 3:46 - loss: 3.0892 - regression_loss: 2.2864 - classification_loss: 0.8027
 501/1000 [==============>...............] - ETA: 3:46 - loss: 3.0830 - regression_loss: 2.2819 - classification_loss: 0.8011
 502/1000 [==============>...............] - ETA: 3:45 - loss: 3.0768 - regression_loss: 2.2773 - classification_loss: 0.7995
 503/1000 [==============>...............] - ETA: 3:45 - loss: 3.0798 - regression_loss: 2.2790 - classification_loss: 0.8008
 504/1000 [==============>...............] - ETA: 3:44 - loss: 3.0819 - regression_loss: 2.2791 - classification_loss: 0.8028
 505/1000 [==============>...............] - ETA: 3:44 - loss: 3.0820 - regression_loss: 2.2797 - classification_loss: 0.8024
 506/1000 [==============>...............] - ETA: 3:43 - loss: 3.0842 - regression_loss: 2.2801 - classification_loss: 0.8041
 507/1000 [==============>...............] - ETA: 3:43 - loss: 3.0866 - regression_loss: 2.2814 - classification_loss: 0.8052
 508/1000 [==============>...............] - ETA: 3:42 - loss: 3.0920 - regression_loss: 2.2843 - classification_loss: 0.8077
 509/1000 [==============>...............] - ETA: 3:42 - loss: 3.0942 - regression_loss: 2.2851 - classification_loss: 0.8091
 510/1000 [==============>...............] - ETA: 3:41 - loss: 3.0930 - regression_loss: 2.2844 - classification_loss: 0.8086
 511/1000 [==============>...............] - ETA: 3:41 - loss: 3.0968 - regression_loss: 2.2864 - classification_loss: 0.8105
 512/1000 [==============>...............] - ETA: 3:41 - loss: 3.0908 - regression_loss: 2.2819 - classification_loss: 0.8089
 513/1000 [==============>...............] - ETA: 3:40 - loss: 3.0945 - regression_loss: 2.2859 - classification_loss: 0.8085
 514/1000 [==============>...............] - ETA: 3:40 - loss: 3.0965 - regression_loss: 2.2867 - classification_loss: 0.8098
 515/1000 [==============>...............] - ETA: 3:39 - loss: 3.0978 - regression_loss: 2.2888 - classification_loss: 0.8091
 516/1000 [==============>...............] - ETA: 3:39 - loss: 3.1009 - regression_loss: 2.2903 - classification_loss: 0.8107
 517/1000 [==============>...............] - ETA: 3:38 - loss: 3.1064 - regression_loss: 2.2938 - classification_loss: 0.8126
 518/1000 [==============>...............] - ETA: 3:38 - loss: 3.1106 - regression_loss: 2.2973 - classification_loss: 0.8133
 519/1000 [==============>...............] - ETA: 3:37 - loss: 3.1117 - regression_loss: 2.2992 - classification_loss: 0.8125
 520/1000 [==============>...............] - ETA: 3:37 - loss: 3.1116 - regression_loss: 2.2990 - classification_loss: 0.8126
 521/1000 [==============>...............] - ETA: 3:36 - loss: 3.1149 - regression_loss: 2.3007 - classification_loss: 0.8143
 522/1000 [==============>...............] - ETA: 3:36 - loss: 3.1178 - regression_loss: 2.3030 - classification_loss: 0.8149
 523/1000 [==============>...............] - ETA: 3:36 - loss: 3.1197 - regression_loss: 2.3037 - classification_loss: 0.8160
 524/1000 [==============>...............] - ETA: 3:35 - loss: 3.1209 - regression_loss: 2.3041 - classification_loss: 0.8169
 525/1000 [==============>...............] - ETA: 3:35 - loss: 3.1241 - regression_loss: 2.3066 - classification_loss: 0.8175
 526/1000 [==============>...............] - ETA: 3:34 - loss: 3.1245 - regression_loss: 2.3073 - classification_loss: 0.8172
 527/1000 [==============>...............] - ETA: 3:34 - loss: 3.1186 - regression_loss: 2.3029 - classification_loss: 0.8157
 528/1000 [==============>...............] - ETA: 3:33 - loss: 3.1178 - regression_loss: 2.3021 - classification_loss: 0.8157
 529/1000 [==============>...............] - ETA: 3:33 - loss: 3.1180 - regression_loss: 2.3031 - classification_loss: 0.8149
 530/1000 [==============>...............] - ETA: 3:32 - loss: 3.1193 - regression_loss: 2.3046 - classification_loss: 0.8146
 531/1000 [==============>...............] - ETA: 3:32 - loss: 3.1178 - regression_loss: 2.3041 - classification_loss: 0.8137
 532/1000 [==============>...............] - ETA: 3:32 - loss: 3.1219 - regression_loss: 2.3071 - classification_loss: 0.8147
 533/1000 [==============>...............] - ETA: 3:31 - loss: 3.1252 - regression_loss: 2.3097 - classification_loss: 0.8155
 534/1000 [===============>..............] - ETA: 3:31 - loss: 3.1260 - regression_loss: 2.3101 - classification_loss: 0.8159
 535/1000 [===============>..............] - ETA: 3:30 - loss: 3.1251 - regression_loss: 2.3095 - classification_loss: 0.8156
 536/1000 [===============>..............] - ETA: 3:30 - loss: 3.1249 - regression_loss: 2.3096 - classification_loss: 0.8154
 537/1000 [===============>..............] - ETA: 3:29 - loss: 3.1273 - regression_loss: 2.3115 - classification_loss: 0.8158
 538/1000 [===============>..............] - ETA: 3:29 - loss: 3.1292 - regression_loss: 2.3135 - classification_loss: 0.8157
 539/1000 [===============>..............] - ETA: 3:28 - loss: 3.1234 - regression_loss: 2.3093 - classification_loss: 0.8142
 540/1000 [===============>..............] - ETA: 3:28 - loss: 3.1224 - regression_loss: 2.3089 - classification_loss: 0.8135
 541/1000 [===============>..............] - ETA: 3:27 - loss: 3.1167 - regression_loss: 2.3046 - classification_loss: 0.8120
 542/1000 [===============>..............] - ETA: 3:27 - loss: 3.1161 - regression_loss: 2.3048 - classification_loss: 0.8113
 543/1000 [===============>..............] - ETA: 3:27 - loss: 3.1153 - regression_loss: 2.3044 - classification_loss: 0.8109
 544/1000 [===============>..............] - ETA: 3:26 - loss: 3.1206 - regression_loss: 2.3087 - classification_loss: 0.8119
 545/1000 [===============>..............] - ETA: 3:26 - loss: 3.1222 - regression_loss: 2.3099 - classification_loss: 0.8123
 546/1000 [===============>..............] - ETA: 3:25 - loss: 3.1165 - regression_loss: 2.3057 - classification_loss: 0.8108
 547/1000 [===============>..............] - ETA: 3:25 - loss: 3.1180 - regression_loss: 2.3069 - classification_loss: 0.8111
 548/1000 [===============>..............] - ETA: 3:24 - loss: 3.1203 - regression_loss: 2.3086 - classification_loss: 0.8117
 549/1000 [===============>..............] - ETA: 3:24 - loss: 3.1204 - regression_loss: 2.3095 - classification_loss: 0.8109
 550/1000 [===============>..............] - ETA: 3:23 - loss: 3.1222 - regression_loss: 2.3113 - classification_loss: 0.8109
 551/1000 [===============>..............] - ETA: 3:23 - loss: 3.1169 - regression_loss: 2.3071 - classification_loss: 0.8098
 552/1000 [===============>..............] - ETA: 3:22 - loss: 3.1191 - regression_loss: 2.3090 - classification_loss: 0.8101
 553/1000 [===============>..............] - ETA: 3:22 - loss: 3.1183 - regression_loss: 2.3089 - classification_loss: 0.8094
 554/1000 [===============>..............] - ETA: 3:22 - loss: 3.1174 - regression_loss: 2.3084 - classification_loss: 0.8090
 555/1000 [===============>..............] - ETA: 3:21 - loss: 3.1173 - regression_loss: 2.3086 - classification_loss: 0.8086
 556/1000 [===============>..............] - ETA: 3:21 - loss: 3.1162 - regression_loss: 2.3077 - classification_loss: 0.8085
 557/1000 [===============>..............] - ETA: 3:20 - loss: 3.1198 - regression_loss: 2.3111 - classification_loss: 0.8087
 558/1000 [===============>..............] - ETA: 3:20 - loss: 3.1210 - regression_loss: 2.3123 - classification_loss: 0.8087
 559/1000 [===============>..............] - ETA: 3:19 - loss: 3.1223 - regression_loss: 2.3130 - classification_loss: 0.8094
 560/1000 [===============>..............] - ETA: 3:19 - loss: 3.1244 - regression_loss: 2.3157 - classification_loss: 0.8087
 561/1000 [===============>..............] - ETA: 3:18 - loss: 3.1248 - regression_loss: 2.3160 - classification_loss: 0.8089
 562/1000 [===============>..............] - ETA: 3:18 - loss: 3.1243 - regression_loss: 2.3158 - classification_loss: 0.8085
 563/1000 [===============>..............] - ETA: 3:17 - loss: 3.1188 - regression_loss: 2.3117 - classification_loss: 0.8071
 564/1000 [===============>..............] - ETA: 3:17 - loss: 3.1134 - regression_loss: 2.3076 - classification_loss: 0.8058
 565/1000 [===============>..............] - ETA: 3:17 - loss: 3.1150 - regression_loss: 2.3096 - classification_loss: 0.8054
 566/1000 [===============>..............] - ETA: 3:16 - loss: 3.1140 - regression_loss: 2.3089 - classification_loss: 0.8051
 567/1000 [================>.............] - ETA: 3:16 - loss: 3.1138 - regression_loss: 2.3093 - classification_loss: 0.8045
 568/1000 [================>.............] - ETA: 3:15 - loss: 3.1167 - regression_loss: 2.3115 - classification_loss: 0.8052
 569/1000 [================>.............] - ETA: 3:15 - loss: 3.1153 - regression_loss: 2.3098 - classification_loss: 0.8055
 570/1000 [================>.............] - ETA: 3:14 - loss: 3.1178 - regression_loss: 2.3119 - classification_loss: 0.8059
 571/1000 [================>.............] - ETA: 3:14 - loss: 3.1124 - regression_loss: 2.3078 - classification_loss: 0.8045
 572/1000 [================>.............] - ETA: 3:13 - loss: 3.1069 - regression_loss: 2.3038 - classification_loss: 0.8031
 573/1000 [================>.............] - ETA: 3:13 - loss: 3.1061 - regression_loss: 2.3029 - classification_loss: 0.8032
 574/1000 [================>.............] - ETA: 3:12 - loss: 3.1082 - regression_loss: 2.3056 - classification_loss: 0.8026
 575/1000 [================>.............] - ETA: 3:12 - loss: 3.1146 - regression_loss: 2.3106 - classification_loss: 0.8040
 576/1000 [================>.............] - ETA: 3:12 - loss: 3.1135 - regression_loss: 2.3103 - classification_loss: 0.8031
 577/1000 [================>.............] - ETA: 3:11 - loss: 3.1122 - regression_loss: 2.3098 - classification_loss: 0.8025
 578/1000 [================>.............] - ETA: 3:11 - loss: 3.1135 - regression_loss: 2.3104 - classification_loss: 0.8031
 579/1000 [================>.............] - ETA: 3:10 - loss: 3.1144 - regression_loss: 2.3107 - classification_loss: 0.8037
 580/1000 [================>.............] - ETA: 3:10 - loss: 3.1180 - regression_loss: 2.3127 - classification_loss: 0.8053
 581/1000 [================>.............] - ETA: 3:09 - loss: 3.1184 - regression_loss: 2.3132 - classification_loss: 0.8052
 582/1000 [================>.............] - ETA: 3:09 - loss: 3.1189 - regression_loss: 2.3141 - classification_loss: 0.8048
 583/1000 [================>.............] - ETA: 3:08 - loss: 3.1136 - regression_loss: 2.3102 - classification_loss: 0.8034
 584/1000 [================>.............] - ETA: 3:08 - loss: 3.1124 - regression_loss: 2.3100 - classification_loss: 0.8024
 585/1000 [================>.............] - ETA: 3:08 - loss: 3.1138 - regression_loss: 2.3061 - classification_loss: 0.8077
 586/1000 [================>.............] - ETA: 3:07 - loss: 3.1170 - regression_loss: 2.3080 - classification_loss: 0.8090
 587/1000 [================>.............] - ETA: 3:07 - loss: 3.1154 - regression_loss: 2.3066 - classification_loss: 0.8088
 588/1000 [================>.............] - ETA: 3:06 - loss: 3.1162 - regression_loss: 2.3065 - classification_loss: 0.8097
 589/1000 [================>.............] - ETA: 3:06 - loss: 3.1162 - regression_loss: 2.3068 - classification_loss: 0.8094
 590/1000 [================>.............] - ETA: 3:05 - loss: 3.1186 - regression_loss: 2.3081 - classification_loss: 0.8105
 591/1000 [================>.............] - ETA: 3:05 - loss: 3.1182 - regression_loss: 2.3085 - classification_loss: 0.8096
 592/1000 [================>.............] - ETA: 3:04 - loss: 3.1190 - regression_loss: 2.3096 - classification_loss: 0.8093
 593/1000 [================>.............] - ETA: 3:04 - loss: 3.1184 - regression_loss: 2.3097 - classification_loss: 0.8088
 594/1000 [================>.............] - ETA: 3:03 - loss: 3.1208 - regression_loss: 2.3113 - classification_loss: 0.8094
 595/1000 [================>.............] - ETA: 3:03 - loss: 3.1223 - regression_loss: 2.3128 - classification_loss: 0.8094
 596/1000 [================>.............] - ETA: 3:03 - loss: 3.1171 - regression_loss: 2.3090 - classification_loss: 0.8081
 597/1000 [================>.............] - ETA: 3:02 - loss: 3.1174 - regression_loss: 2.3096 - classification_loss: 0.8078
 598/1000 [================>.............] - ETA: 3:02 - loss: 3.1194 - regression_loss: 2.3110 - classification_loss: 0.8083
 599/1000 [================>.............] - ETA: 3:01 - loss: 3.1189 - regression_loss: 2.3108 - classification_loss: 0.8081
 600/1000 [=================>............] - ETA: 3:01 - loss: 3.1175 - regression_loss: 2.3101 - classification_loss: 0.8074
 601/1000 [=================>............] - ETA: 3:00 - loss: 3.1203 - regression_loss: 2.3131 - classification_loss: 0.8072
 602/1000 [=================>............] - ETA: 3:00 - loss: 3.1231 - regression_loss: 2.3154 - classification_loss: 0.8077
 603/1000 [=================>............] - ETA: 2:59 - loss: 3.1257 - regression_loss: 2.3171 - classification_loss: 0.8085
 604/1000 [=================>............] - ETA: 2:59 - loss: 3.1205 - regression_loss: 2.3133 - classification_loss: 0.8072
 605/1000 [=================>............] - ETA: 2:58 - loss: 3.1228 - regression_loss: 2.3151 - classification_loss: 0.8077
 606/1000 [=================>............] - ETA: 2:58 - loss: 3.1255 - regression_loss: 2.3173 - classification_loss: 0.8082
 607/1000 [=================>............] - ETA: 2:58 - loss: 3.1234 - regression_loss: 2.3160 - classification_loss: 0.8074
 608/1000 [=================>............] - ETA: 2:57 - loss: 3.1253 - regression_loss: 2.3176 - classification_loss: 0.8078
 609/1000 [=================>............] - ETA: 2:57 - loss: 3.1240 - regression_loss: 2.3173 - classification_loss: 0.8067
 610/1000 [=================>............] - ETA: 2:56 - loss: 3.1247 - regression_loss: 2.3183 - classification_loss: 0.8064
 611/1000 [=================>............] - ETA: 2:56 - loss: 3.1196 - regression_loss: 2.3145 - classification_loss: 0.8051
 612/1000 [=================>............] - ETA: 2:55 - loss: 3.1197 - regression_loss: 2.3153 - classification_loss: 0.8043
 613/1000 [=================>............] - ETA: 2:55 - loss: 3.1146 - regression_loss: 2.3116 - classification_loss: 0.8030
 614/1000 [=================>............] - ETA: 2:54 - loss: 3.1095 - regression_loss: 2.3078 - classification_loss: 0.8017
 615/1000 [=================>............] - ETA: 2:54 - loss: 3.1081 - regression_loss: 2.3073 - classification_loss: 0.8009
 616/1000 [=================>............] - ETA: 2:53 - loss: 3.1031 - regression_loss: 2.3035 - classification_loss: 0.7996
 617/1000 [=================>............] - ETA: 2:53 - loss: 3.1066 - regression_loss: 2.3067 - classification_loss: 0.7999
 618/1000 [=================>............] - ETA: 2:53 - loss: 3.1101 - regression_loss: 2.3094 - classification_loss: 0.8007
 619/1000 [=================>............] - ETA: 2:52 - loss: 3.1122 - regression_loss: 2.3112 - classification_loss: 0.8010
 620/1000 [=================>............] - ETA: 2:52 - loss: 3.1072 - regression_loss: 2.3075 - classification_loss: 0.7997
 621/1000 [=================>............] - ETA: 2:51 - loss: 3.1072 - regression_loss: 2.3080 - classification_loss: 0.7993
 622/1000 [=================>............] - ETA: 2:51 - loss: 3.1060 - regression_loss: 2.3075 - classification_loss: 0.7985
 623/1000 [=================>............] - ETA: 2:50 - loss: 3.1010 - regression_loss: 2.3038 - classification_loss: 0.7972
 624/1000 [=================>............] - ETA: 2:50 - loss: 3.1020 - regression_loss: 2.3035 - classification_loss: 0.7984
 625/1000 [=================>............] - ETA: 2:49 - loss: 3.1044 - regression_loss: 2.3046 - classification_loss: 0.7998
 626/1000 [=================>............] - ETA: 2:49 - loss: 3.1079 - regression_loss: 2.3070 - classification_loss: 0.8009
 627/1000 [=================>............] - ETA: 2:48 - loss: 3.1116 - regression_loss: 2.3089 - classification_loss: 0.8027
 628/1000 [=================>............] - ETA: 2:48 - loss: 3.1134 - regression_loss: 2.3089 - classification_loss: 0.8045
 629/1000 [=================>............] - ETA: 2:48 - loss: 3.1127 - regression_loss: 2.3086 - classification_loss: 0.8041
 630/1000 [=================>............] - ETA: 2:47 - loss: 3.1159 - regression_loss: 2.3117 - classification_loss: 0.8042
 631/1000 [=================>............] - ETA: 2:47 - loss: 3.1156 - regression_loss: 2.3115 - classification_loss: 0.8042
 632/1000 [=================>............] - ETA: 2:46 - loss: 3.1152 - regression_loss: 2.3115 - classification_loss: 0.8037
 633/1000 [=================>............] - ETA: 2:46 - loss: 3.1103 - regression_loss: 2.3079 - classification_loss: 0.8024
 634/1000 [==================>...........] - ETA: 2:45 - loss: 3.1112 - regression_loss: 2.3085 - classification_loss: 0.8028
 635/1000 [==================>...........] - ETA: 2:45 - loss: 3.1136 - regression_loss: 2.3094 - classification_loss: 0.8042
 636/1000 [==================>...........] - ETA: 2:44 - loss: 3.1167 - regression_loss: 2.3117 - classification_loss: 0.8050
 637/1000 [==================>...........] - ETA: 2:44 - loss: 3.1159 - regression_loss: 2.3117 - classification_loss: 0.8042
 638/1000 [==================>...........] - ETA: 2:43 - loss: 3.1186 - regression_loss: 2.3142 - classification_loss: 0.8044
 639/1000 [==================>...........] - ETA: 2:43 - loss: 3.1182 - regression_loss: 2.3145 - classification_loss: 0.8037
 640/1000 [==================>...........] - ETA: 2:43 - loss: 3.1184 - regression_loss: 2.3143 - classification_loss: 0.8041
 641/1000 [==================>...........] - ETA: 2:42 - loss: 3.1195 - regression_loss: 2.3159 - classification_loss: 0.8036
 642/1000 [==================>...........] - ETA: 2:42 - loss: 3.1146 - regression_loss: 2.3123 - classification_loss: 0.8024
 643/1000 [==================>...........] - ETA: 2:41 - loss: 3.1159 - regression_loss: 2.3131 - classification_loss: 0.8028
 644/1000 [==================>...........] - ETA: 2:41 - loss: 3.1111 - regression_loss: 2.3095 - classification_loss: 0.8015
 645/1000 [==================>...........] - ETA: 2:40 - loss: 3.1092 - regression_loss: 2.3085 - classification_loss: 0.8007
 646/1000 [==================>...........] - ETA: 2:40 - loss: 3.1093 - regression_loss: 2.3088 - classification_loss: 0.8005
 647/1000 [==================>...........] - ETA: 2:39 - loss: 3.1096 - regression_loss: 2.3095 - classification_loss: 0.8001
 648/1000 [==================>...........] - ETA: 2:39 - loss: 3.1048 - regression_loss: 2.3059 - classification_loss: 0.7988
 649/1000 [==================>...........] - ETA: 2:38 - loss: 3.1068 - regression_loss: 2.3075 - classification_loss: 0.7992
 650/1000 [==================>...........] - ETA: 2:38 - loss: 3.1020 - regression_loss: 2.3040 - classification_loss: 0.7980
 651/1000 [==================>...........] - ETA: 2:38 - loss: 3.0972 - regression_loss: 2.3004 - classification_loss: 0.7968
 652/1000 [==================>...........] - ETA: 2:37 - loss: 3.0997 - regression_loss: 2.3022 - classification_loss: 0.7975
 653/1000 [==================>...........] - ETA: 2:37 - loss: 3.0997 - regression_loss: 2.3025 - classification_loss: 0.7973
 654/1000 [==================>...........] - ETA: 2:36 - loss: 3.0991 - regression_loss: 2.3024 - classification_loss: 0.7967
 655/1000 [==================>...........] - ETA: 2:36 - loss: 3.0988 - regression_loss: 2.3027 - classification_loss: 0.7961
 656/1000 [==================>...........] - ETA: 2:35 - loss: 3.1004 - regression_loss: 2.3049 - classification_loss: 0.7955
 657/1000 [==================>...........] - ETA: 2:35 - loss: 3.1017 - regression_loss: 2.3047 - classification_loss: 0.7969
 658/1000 [==================>...........] - ETA: 2:34 - loss: 3.1018 - regression_loss: 2.3053 - classification_loss: 0.7965
 659/1000 [==================>...........] - ETA: 2:34 - loss: 3.0971 - regression_loss: 2.3018 - classification_loss: 0.7953
 660/1000 [==================>...........] - ETA: 2:34 - loss: 3.0980 - regression_loss: 2.3018 - classification_loss: 0.7962
 661/1000 [==================>...........] - ETA: 2:33 - loss: 3.1026 - regression_loss: 2.3047 - classification_loss: 0.7979
 662/1000 [==================>...........] - ETA: 2:33 - loss: 3.0979 - regression_loss: 2.3012 - classification_loss: 0.7967
 663/1000 [==================>...........] - ETA: 2:32 - loss: 3.0981 - regression_loss: 2.3016 - classification_loss: 0.7965
 664/1000 [==================>...........] - ETA: 2:32 - loss: 3.0990 - regression_loss: 2.3026 - classification_loss: 0.7964
 665/1000 [==================>...........] - ETA: 2:31 - loss: 3.0987 - regression_loss: 2.3028 - classification_loss: 0.7959
 666/1000 [==================>...........] - ETA: 2:31 - loss: 3.1003 - regression_loss: 2.3048 - classification_loss: 0.7955
 667/1000 [===================>..........] - ETA: 2:30 - loss: 3.1037 - regression_loss: 2.3065 - classification_loss: 0.7972
 668/1000 [===================>..........] - ETA: 2:30 - loss: 3.1051 - regression_loss: 2.3082 - classification_loss: 0.7969
 669/1000 [===================>..........] - ETA: 2:29 - loss: 3.1076 - regression_loss: 2.3092 - classification_loss: 0.7984
 670/1000 [===================>..........] - ETA: 2:29 - loss: 3.1104 - regression_loss: 2.3122 - classification_loss: 0.7982
 671/1000 [===================>..........] - ETA: 2:29 - loss: 3.1126 - regression_loss: 2.3146 - classification_loss: 0.7980
 672/1000 [===================>..........] - ETA: 2:28 - loss: 3.1153 - regression_loss: 2.3169 - classification_loss: 0.7984
 673/1000 [===================>..........] - ETA: 2:28 - loss: 3.1151 - regression_loss: 2.3166 - classification_loss: 0.7985
 674/1000 [===================>..........] - ETA: 2:27 - loss: 3.1193 - regression_loss: 2.3191 - classification_loss: 0.8002
 675/1000 [===================>..........] - ETA: 2:27 - loss: 3.1218 - regression_loss: 2.3208 - classification_loss: 0.8011
 676/1000 [===================>..........] - ETA: 2:26 - loss: 3.1208 - regression_loss: 2.3203 - classification_loss: 0.8005
 677/1000 [===================>..........] - ETA: 2:26 - loss: 3.1202 - regression_loss: 2.3205 - classification_loss: 0.7998
 678/1000 [===================>..........] - ETA: 2:25 - loss: 3.1206 - regression_loss: 2.3214 - classification_loss: 0.7992
 679/1000 [===================>..........] - ETA: 2:25 - loss: 3.1214 - regression_loss: 2.3218 - classification_loss: 0.7996
 680/1000 [===================>..........] - ETA: 2:24 - loss: 3.1234 - regression_loss: 2.3230 - classification_loss: 0.8004
 681/1000 [===================>..........] - ETA: 2:24 - loss: 3.1260 - regression_loss: 2.3258 - classification_loss: 0.8002
 682/1000 [===================>..........] - ETA: 2:24 - loss: 3.1214 - regression_loss: 2.3224 - classification_loss: 0.7991
 683/1000 [===================>..........] - ETA: 2:23 - loss: 3.1168 - regression_loss: 2.3190 - classification_loss: 0.7979
 684/1000 [===================>..........] - ETA: 2:23 - loss: 3.1188 - regression_loss: 2.3208 - classification_loss: 0.7981
 685/1000 [===================>..........] - ETA: 2:22 - loss: 3.1189 - regression_loss: 2.3208 - classification_loss: 0.7980
 686/1000 [===================>..........] - ETA: 2:22 - loss: 3.1188 - regression_loss: 2.3205 - classification_loss: 0.7983
 687/1000 [===================>..........] - ETA: 2:21 - loss: 3.1201 - regression_loss: 2.3218 - classification_loss: 0.7984
 688/1000 [===================>..........] - ETA: 2:21 - loss: 3.1207 - regression_loss: 2.3229 - classification_loss: 0.7979
 689/1000 [===================>..........] - ETA: 2:20 - loss: 3.1211 - regression_loss: 2.3231 - classification_loss: 0.7980
 690/1000 [===================>..........] - ETA: 2:20 - loss: 3.1215 - regression_loss: 2.3236 - classification_loss: 0.7979
 691/1000 [===================>..........] - ETA: 2:19 - loss: 3.1226 - regression_loss: 2.3238 - classification_loss: 0.7988
 692/1000 [===================>..........] - ETA: 2:19 - loss: 3.1224 - regression_loss: 2.3241 - classification_loss: 0.7983
 693/1000 [===================>..........] - ETA: 2:19 - loss: 3.1222 - regression_loss: 2.3244 - classification_loss: 0.7978
 694/1000 [===================>..........] - ETA: 2:18 - loss: 3.1237 - regression_loss: 2.3262 - classification_loss: 0.7975
 695/1000 [===================>..........] - ETA: 2:18 - loss: 3.1192 - regression_loss: 2.3229 - classification_loss: 0.7964
 696/1000 [===================>..........] - ETA: 2:17 - loss: 3.1222 - regression_loss: 2.3253 - classification_loss: 0.7969
 697/1000 [===================>..........] - ETA: 2:17 - loss: 3.1177 - regression_loss: 2.3219 - classification_loss: 0.7958
 698/1000 [===================>..........] - ETA: 2:16 - loss: 3.1204 - regression_loss: 2.3230 - classification_loss: 0.7975
 699/1000 [===================>..........] - ETA: 2:16 - loss: 3.1192 - regression_loss: 2.3225 - classification_loss: 0.7967
 700/1000 [====================>.........] - ETA: 2:15 - loss: 3.1147 - regression_loss: 2.3192 - classification_loss: 0.7955
 701/1000 [====================>.........] - ETA: 2:15 - loss: 3.1141 - regression_loss: 2.3188 - classification_loss: 0.7953
 702/1000 [====================>.........] - ETA: 2:14 - loss: 3.1140 - regression_loss: 2.3190 - classification_loss: 0.7950
 703/1000 [====================>.........] - ETA: 2:14 - loss: 3.1126 - regression_loss: 2.3185 - classification_loss: 0.7941
 704/1000 [====================>.........] - ETA: 2:14 - loss: 3.1096 - regression_loss: 2.3152 - classification_loss: 0.7944
 705/1000 [====================>.........] - ETA: 2:13 - loss: 3.1139 - regression_loss: 2.3184 - classification_loss: 0.7955
 706/1000 [====================>.........] - ETA: 2:13 - loss: 3.1141 - regression_loss: 2.3186 - classification_loss: 0.7956
 707/1000 [====================>.........] - ETA: 2:12 - loss: 3.1141 - regression_loss: 2.3191 - classification_loss: 0.7950
 708/1000 [====================>.........] - ETA: 2:12 - loss: 3.1147 - regression_loss: 2.3198 - classification_loss: 0.7948
 709/1000 [====================>.........] - ETA: 2:11 - loss: 3.1134 - regression_loss: 2.3182 - classification_loss: 0.7953
 710/1000 [====================>.........] - ETA: 2:11 - loss: 3.1143 - regression_loss: 2.3194 - classification_loss: 0.7948
 711/1000 [====================>.........] - ETA: 2:10 - loss: 3.1099 - regression_loss: 2.3162 - classification_loss: 0.7937
 712/1000 [====================>.........] - ETA: 2:10 - loss: 3.1109 - regression_loss: 2.3170 - classification_loss: 0.7938
 713/1000 [====================>.........] - ETA: 2:10 - loss: 3.1126 - regression_loss: 2.3178 - classification_loss: 0.7948
 714/1000 [====================>.........] - ETA: 2:09 - loss: 3.1148 - regression_loss: 2.3195 - classification_loss: 0.7953
 715/1000 [====================>.........] - ETA: 2:09 - loss: 3.1155 - regression_loss: 2.3204 - classification_loss: 0.7951
 716/1000 [====================>.........] - ETA: 2:08 - loss: 3.1112 - regression_loss: 2.3171 - classification_loss: 0.7940
 717/1000 [====================>.........] - ETA: 2:08 - loss: 3.1068 - regression_loss: 2.3139 - classification_loss: 0.7929
 718/1000 [====================>.........] - ETA: 2:07 - loss: 3.1073 - regression_loss: 2.3148 - classification_loss: 0.7925
 719/1000 [====================>.........] - ETA: 2:07 - loss: 3.1062 - regression_loss: 2.3144 - classification_loss: 0.7918
 720/1000 [====================>.........] - ETA: 2:06 - loss: 3.1044 - regression_loss: 2.3135 - classification_loss: 0.7910
 721/1000 [====================>.........] - ETA: 2:06 - loss: 3.1039 - regression_loss: 2.3134 - classification_loss: 0.7905
 722/1000 [====================>.........] - ETA: 2:05 - loss: 3.1041 - regression_loss: 2.3133 - classification_loss: 0.7908
 723/1000 [====================>.........] - ETA: 2:05 - loss: 3.1054 - regression_loss: 2.3153 - classification_loss: 0.7902
 724/1000 [====================>.........] - ETA: 2:05 - loss: 3.1061 - regression_loss: 2.3150 - classification_loss: 0.7911
 725/1000 [====================>.........] - ETA: 2:04 - loss: 3.1069 - regression_loss: 2.3148 - classification_loss: 0.7922
 726/1000 [====================>.........] - ETA: 2:04 - loss: 3.1067 - regression_loss: 2.3152 - classification_loss: 0.7916
 727/1000 [====================>.........] - ETA: 2:03 - loss: 3.1094 - regression_loss: 2.3181 - classification_loss: 0.7912
 728/1000 [====================>.........] - ETA: 2:03 - loss: 3.1093 - regression_loss: 2.3170 - classification_loss: 0.7923
 729/1000 [====================>.........] - ETA: 2:02 - loss: 3.1101 - regression_loss: 2.3179 - classification_loss: 0.7922
 730/1000 [====================>.........] - ETA: 2:02 - loss: 3.1058 - regression_loss: 2.3147 - classification_loss: 0.7912
 731/1000 [====================>.........] - ETA: 2:01 - loss: 3.1016 - regression_loss: 2.3115 - classification_loss: 0.7901
 732/1000 [====================>.........] - ETA: 2:01 - loss: 3.1013 - regression_loss: 2.3112 - classification_loss: 0.7901
 733/1000 [====================>.........] - ETA: 2:00 - loss: 3.1002 - regression_loss: 2.3098 - classification_loss: 0.7903
 734/1000 [=====================>........] - ETA: 2:00 - loss: 3.1026 - regression_loss: 2.3121 - classification_loss: 0.7905
 735/1000 [=====================>........] - ETA: 2:00 - loss: 3.1047 - regression_loss: 2.3126 - classification_loss: 0.7921
 736/1000 [=====================>........] - ETA: 1:59 - loss: 3.1065 - regression_loss: 2.3139 - classification_loss: 0.7926
 737/1000 [=====================>........] - ETA: 1:59 - loss: 3.1077 - regression_loss: 2.3140 - classification_loss: 0.7937
 738/1000 [=====================>........] - ETA: 1:58 - loss: 3.1035 - regression_loss: 2.3108 - classification_loss: 0.7927
 739/1000 [=====================>........] - ETA: 1:58 - loss: 3.0993 - regression_loss: 2.3077 - classification_loss: 0.7916
 740/1000 [=====================>........] - ETA: 1:57 - loss: 3.1011 - regression_loss: 2.3095 - classification_loss: 0.7917
 741/1000 [=====================>........] - ETA: 1:57 - loss: 3.0970 - regression_loss: 2.3064 - classification_loss: 0.7906
 742/1000 [=====================>........] - ETA: 1:56 - loss: 3.0984 - regression_loss: 2.3075 - classification_loss: 0.7909
 743/1000 [=====================>........] - ETA: 1:56 - loss: 3.0943 - regression_loss: 2.3044 - classification_loss: 0.7898
 744/1000 [=====================>........] - ETA: 1:55 - loss: 3.0971 - regression_loss: 2.3068 - classification_loss: 0.7903
 745/1000 [=====================>........] - ETA: 1:55 - loss: 3.0971 - regression_loss: 2.3072 - classification_loss: 0.7899
 746/1000 [=====================>........] - ETA: 1:55 - loss: 3.0982 - regression_loss: 2.3086 - classification_loss: 0.7897
 747/1000 [=====================>........] - ETA: 1:54 - loss: 3.0941 - regression_loss: 2.3055 - classification_loss: 0.7886
 748/1000 [=====================>........] - ETA: 1:54 - loss: 3.0899 - regression_loss: 2.3024 - classification_loss: 0.7876
 749/1000 [=====================>........] - ETA: 1:53 - loss: 3.0899 - regression_loss: 2.3029 - classification_loss: 0.7870
 750/1000 [=====================>........] - ETA: 1:53 - loss: 3.0914 - regression_loss: 2.3046 - classification_loss: 0.7868
 751/1000 [=====================>........] - ETA: 1:52 - loss: 3.0907 - regression_loss: 2.3036 - classification_loss: 0.7871
 752/1000 [=====================>........] - ETA: 1:52 - loss: 3.0902 - regression_loss: 2.3036 - classification_loss: 0.7867
 753/1000 [=====================>........] - ETA: 1:51 - loss: 3.0861 - regression_loss: 2.3005 - classification_loss: 0.7856
 754/1000 [=====================>........] - ETA: 1:51 - loss: 3.0889 - regression_loss: 2.3021 - classification_loss: 0.7868
 755/1000 [=====================>........] - ETA: 1:50 - loss: 3.0893 - regression_loss: 2.3030 - classification_loss: 0.7863
 756/1000 [=====================>........] - ETA: 1:50 - loss: 3.0887 - regression_loss: 2.3027 - classification_loss: 0.7860
 757/1000 [=====================>........] - ETA: 1:50 - loss: 3.0914 - regression_loss: 2.3043 - classification_loss: 0.7871
 758/1000 [=====================>........] - ETA: 1:49 - loss: 3.0873 - regression_loss: 2.3012 - classification_loss: 0.7860
 759/1000 [=====================>........] - ETA: 1:49 - loss: 3.0935 - regression_loss: 2.3042 - classification_loss: 0.7893
 760/1000 [=====================>........] - ETA: 1:48 - loss: 3.0948 - regression_loss: 2.3046 - classification_loss: 0.7902
 761/1000 [=====================>........] - ETA: 1:48 - loss: 3.0907 - regression_loss: 2.3016 - classification_loss: 0.7891
 762/1000 [=====================>........] - ETA: 1:47 - loss: 3.0904 - regression_loss: 2.3016 - classification_loss: 0.7888
 763/1000 [=====================>........] - ETA: 1:47 - loss: 3.0921 - regression_loss: 2.3025 - classification_loss: 0.7896
 764/1000 [=====================>........] - ETA: 1:46 - loss: 3.0947 - regression_loss: 2.3043 - classification_loss: 0.7904
 765/1000 [=====================>........] - ETA: 1:46 - loss: 3.0950 - regression_loss: 2.3047 - classification_loss: 0.7903
 766/1000 [=====================>........] - ETA: 1:45 - loss: 3.0964 - regression_loss: 2.3067 - classification_loss: 0.7898
 767/1000 [======================>.......] - ETA: 1:45 - loss: 3.0968 - regression_loss: 2.3067 - classification_loss: 0.7900
 768/1000 [======================>.......] - ETA: 1:45 - loss: 3.0927 - regression_loss: 2.3037 - classification_loss: 0.7890
 769/1000 [======================>.......] - ETA: 1:44 - loss: 3.0931 - regression_loss: 2.3044 - classification_loss: 0.7886
 770/1000 [======================>.......] - ETA: 1:44 - loss: 3.0942 - regression_loss: 2.3057 - classification_loss: 0.7884
 771/1000 [======================>.......] - ETA: 1:43 - loss: 3.0977 - regression_loss: 2.3076 - classification_loss: 0.7901
 772/1000 [======================>.......] - ETA: 1:43 - loss: 3.0975 - regression_loss: 2.3080 - classification_loss: 0.7894
 773/1000 [======================>.......] - ETA: 1:42 - loss: 3.0988 - regression_loss: 2.3086 - classification_loss: 0.7902
 774/1000 [======================>.......] - ETA: 1:42 - loss: 3.0993 - regression_loss: 2.3093 - classification_loss: 0.7899
 775/1000 [======================>.......] - ETA: 1:41 - loss: 3.0997 - regression_loss: 2.3095 - classification_loss: 0.7902
 776/1000 [======================>.......] - ETA: 1:41 - loss: 3.1023 - regression_loss: 2.3110 - classification_loss: 0.7912
 777/1000 [======================>.......] - ETA: 1:41 - loss: 3.1039 - regression_loss: 2.3119 - classification_loss: 0.7920
 778/1000 [======================>.......] - ETA: 1:40 - loss: 3.1035 - regression_loss: 2.3119 - classification_loss: 0.7916
 779/1000 [======================>.......] - ETA: 1:40 - loss: 3.1039 - regression_loss: 2.3121 - classification_loss: 0.7918
 780/1000 [======================>.......] - ETA: 1:39 - loss: 3.1057 - regression_loss: 2.3137 - classification_loss: 0.7921
 781/1000 [======================>.......] - ETA: 1:39 - loss: 3.1049 - regression_loss: 2.3134 - classification_loss: 0.7915
 782/1000 [======================>.......] - ETA: 1:38 - loss: 3.1069 - regression_loss: 2.3153 - classification_loss: 0.7916
 783/1000 [======================>.......] - ETA: 1:38 - loss: 3.1081 - regression_loss: 2.3123 - classification_loss: 0.7958
 784/1000 [======================>.......] - ETA: 1:37 - loss: 3.1087 - regression_loss: 2.3133 - classification_loss: 0.7954
 785/1000 [======================>.......] - ETA: 1:37 - loss: 3.1096 - regression_loss: 2.3143 - classification_loss: 0.7954
 786/1000 [======================>.......] - ETA: 1:36 - loss: 3.1100 - regression_loss: 2.3151 - classification_loss: 0.7949
 787/1000 [======================>.......] - ETA: 1:36 - loss: 3.1108 - regression_loss: 2.3160 - classification_loss: 0.7948
 788/1000 [======================>.......] - ETA: 1:36 - loss: 3.1131 - regression_loss: 2.3176 - classification_loss: 0.7955
 789/1000 [======================>.......] - ETA: 1:35 - loss: 3.1132 - regression_loss: 2.3178 - classification_loss: 0.7954
 790/1000 [======================>.......] - ETA: 1:35 - loss: 3.1141 - regression_loss: 2.3183 - classification_loss: 0.7958
 791/1000 [======================>.......] - ETA: 1:34 - loss: 3.1102 - regression_loss: 2.3154 - classification_loss: 0.7948
 792/1000 [======================>.......] - ETA: 1:34 - loss: 3.1108 - regression_loss: 2.3158 - classification_loss: 0.7950
 793/1000 [======================>.......] - ETA: 1:33 - loss: 3.1113 - regression_loss: 2.3161 - classification_loss: 0.7951
 794/1000 [======================>.......] - ETA: 1:33 - loss: 3.1128 - regression_loss: 2.3179 - classification_loss: 0.7949
 795/1000 [======================>.......] - ETA: 1:32 - loss: 3.1151 - regression_loss: 2.3192 - classification_loss: 0.7959
 796/1000 [======================>.......] - ETA: 1:32 - loss: 3.1164 - regression_loss: 2.3163 - classification_loss: 0.8001
 797/1000 [======================>.......] - ETA: 1:31 - loss: 3.1125 - regression_loss: 2.3134 - classification_loss: 0.7991
 798/1000 [======================>.......] - ETA: 1:31 - loss: 3.1131 - regression_loss: 2.3143 - classification_loss: 0.7988
 799/1000 [======================>.......] - ETA: 1:31 - loss: 3.1134 - regression_loss: 2.3149 - classification_loss: 0.7985
 800/1000 [=======================>......] - ETA: 1:30 - loss: 3.1142 - regression_loss: 2.3154 - classification_loss: 0.7988
 801/1000 [=======================>......] - ETA: 1:30 - loss: 3.1153 - regression_loss: 2.3167 - classification_loss: 0.7986
 802/1000 [=======================>......] - ETA: 1:29 - loss: 3.1114 - regression_loss: 2.3138 - classification_loss: 0.7976
 803/1000 [=======================>......] - ETA: 1:29 - loss: 3.1118 - regression_loss: 2.3147 - classification_loss: 0.7972
 804/1000 [=======================>......] - ETA: 1:28 - loss: 3.1100 - regression_loss: 2.3118 - classification_loss: 0.7983
 805/1000 [=======================>......] - ETA: 1:28 - loss: 3.1062 - regression_loss: 2.3089 - classification_loss: 0.7973
 806/1000 [=======================>......] - ETA: 1:27 - loss: 3.1063 - regression_loss: 2.3094 - classification_loss: 0.7970
 807/1000 [=======================>......] - ETA: 1:27 - loss: 3.1025 - regression_loss: 2.3065 - classification_loss: 0.7960
 808/1000 [=======================>......] - ETA: 1:26 - loss: 3.0986 - regression_loss: 2.3036 - classification_loss: 0.7950
 809/1000 [=======================>......] - ETA: 1:26 - loss: 3.1001 - regression_loss: 2.3045 - classification_loss: 0.7956
 810/1000 [=======================>......] - ETA: 1:26 - loss: 3.1005 - regression_loss: 2.3050 - classification_loss: 0.7955
 811/1000 [=======================>......] - ETA: 1:25 - loss: 3.0967 - regression_loss: 2.3022 - classification_loss: 0.7945
 812/1000 [=======================>......] - ETA: 1:25 - loss: 3.0980 - regression_loss: 2.3033 - classification_loss: 0.7947
 813/1000 [=======================>......] - ETA: 1:24 - loss: 3.1008 - regression_loss: 2.3050 - classification_loss: 0.7958
 814/1000 [=======================>......] - ETA: 1:24 - loss: 3.1000 - regression_loss: 2.3048 - classification_loss: 0.7952
 815/1000 [=======================>......] - ETA: 1:23 - loss: 3.0992 - regression_loss: 2.3043 - classification_loss: 0.7949
 816/1000 [=======================>......] - ETA: 1:23 - loss: 3.0993 - regression_loss: 2.3048 - classification_loss: 0.7945
 817/1000 [=======================>......] - ETA: 1:22 - loss: 3.1023 - regression_loss: 2.3073 - classification_loss: 0.7950
 818/1000 [=======================>......] - ETA: 1:22 - loss: 3.1051 - regression_loss: 2.3097 - classification_loss: 0.7954
 819/1000 [=======================>......] - ETA: 1:21 - loss: 3.1107 - regression_loss: 2.3133 - classification_loss: 0.7974
 820/1000 [=======================>......] - ETA: 1:21 - loss: 3.1137 - regression_loss: 2.3152 - classification_loss: 0.7986
 821/1000 [=======================>......] - ETA: 1:21 - loss: 3.1139 - regression_loss: 2.3152 - classification_loss: 0.7986
 822/1000 [=======================>......] - ETA: 1:20 - loss: 3.1157 - regression_loss: 2.3162 - classification_loss: 0.7995
 823/1000 [=======================>......] - ETA: 1:20 - loss: 3.1164 - regression_loss: 2.3165 - classification_loss: 0.7998
 824/1000 [=======================>......] - ETA: 1:19 - loss: 3.1182 - regression_loss: 2.3173 - classification_loss: 0.8009
 825/1000 [=======================>......] - ETA: 1:19 - loss: 3.1182 - regression_loss: 2.3174 - classification_loss: 0.8008
 826/1000 [=======================>......] - ETA: 1:18 - loss: 3.1184 - regression_loss: 2.3173 - classification_loss: 0.8012
 827/1000 [=======================>......] - ETA: 1:18 - loss: 3.1147 - regression_loss: 2.3145 - classification_loss: 0.8003
 828/1000 [=======================>......] - ETA: 1:17 - loss: 3.1140 - regression_loss: 2.3139 - classification_loss: 0.8001
 829/1000 [=======================>......] - ETA: 1:17 - loss: 3.1129 - regression_loss: 2.3133 - classification_loss: 0.7996
 830/1000 [=======================>......] - ETA: 1:17 - loss: 3.1118 - regression_loss: 2.3127 - classification_loss: 0.7991
 831/1000 [=======================>......] - ETA: 1:16 - loss: 3.1081 - regression_loss: 2.3099 - classification_loss: 0.7981
 832/1000 [=======================>......] - ETA: 1:16 - loss: 3.1093 - regression_loss: 2.3107 - classification_loss: 0.7986
 833/1000 [=======================>......] - ETA: 1:15 - loss: 3.1093 - regression_loss: 2.3102 - classification_loss: 0.7991
 834/1000 [========================>.....] - ETA: 1:15 - loss: 3.1056 - regression_loss: 2.3074 - classification_loss: 0.7982
 835/1000 [========================>.....] - ETA: 1:14 - loss: 3.1079 - regression_loss: 2.3088 - classification_loss: 0.7990
 836/1000 [========================>.....] - ETA: 1:14 - loss: 3.1101 - regression_loss: 2.3101 - classification_loss: 0.7999
 837/1000 [========================>.....] - ETA: 1:13 - loss: 3.1064 - regression_loss: 2.3074 - classification_loss: 0.7990
 838/1000 [========================>.....] - ETA: 1:13 - loss: 3.1027 - regression_loss: 2.3046 - classification_loss: 0.7980
 839/1000 [========================>.....] - ETA: 1:12 - loss: 3.1026 - regression_loss: 2.3049 - classification_loss: 0.7977
 840/1000 [========================>.....] - ETA: 1:12 - loss: 3.0990 - regression_loss: 2.3022 - classification_loss: 0.7968
 841/1000 [========================>.....] - ETA: 1:12 - loss: 3.0973 - regression_loss: 2.3011 - classification_loss: 0.7962
 842/1000 [========================>.....] - ETA: 1:11 - loss: 3.0955 - regression_loss: 2.2999 - classification_loss: 0.7955
 843/1000 [========================>.....] - ETA: 1:11 - loss: 3.0918 - regression_loss: 2.2972 - classification_loss: 0.7946
 844/1000 [========================>.....] - ETA: 1:10 - loss: 3.0921 - regression_loss: 2.2974 - classification_loss: 0.7947
 845/1000 [========================>.....] - ETA: 1:10 - loss: 3.0927 - regression_loss: 2.2980 - classification_loss: 0.7946
 846/1000 [========================>.....] - ETA: 1:09 - loss: 3.0896 - regression_loss: 2.2953 - classification_loss: 0.7943
 847/1000 [========================>.....] - ETA: 1:09 - loss: 3.0911 - regression_loss: 2.2958 - classification_loss: 0.7953
 848/1000 [========================>.....] - ETA: 1:08 - loss: 3.0915 - regression_loss: 2.2964 - classification_loss: 0.7951
 849/1000 [========================>.....] - ETA: 1:08 - loss: 3.0932 - regression_loss: 2.2982 - classification_loss: 0.7950
 850/1000 [========================>.....] - ETA: 1:07 - loss: 3.0895 - regression_loss: 2.2955 - classification_loss: 0.7941
 851/1000 [========================>.....] - ETA: 1:07 - loss: 3.0911 - regression_loss: 2.2962 - classification_loss: 0.7949
 852/1000 [========================>.....] - ETA: 1:07 - loss: 3.0875 - regression_loss: 2.2935 - classification_loss: 0.7939
 853/1000 [========================>.....] - ETA: 1:06 - loss: 3.0895 - regression_loss: 2.2953 - classification_loss: 0.7942
 854/1000 [========================>.....] - ETA: 1:06 - loss: 3.0910 - regression_loss: 2.2960 - classification_loss: 0.7950
 855/1000 [========================>.....] - ETA: 1:05 - loss: 3.0924 - regression_loss: 2.2978 - classification_loss: 0.7946
 856/1000 [========================>.....] - ETA: 1:05 - loss: 3.0942 - regression_loss: 2.2989 - classification_loss: 0.7953
 857/1000 [========================>.....] - ETA: 1:04 - loss: 3.0942 - regression_loss: 2.2990 - classification_loss: 0.7952
 858/1000 [========================>.....] - ETA: 1:04 - loss: 3.0979 - regression_loss: 2.3015 - classification_loss: 0.7963
 859/1000 [========================>.....] - ETA: 1:03 - loss: 3.0943 - regression_loss: 2.2989 - classification_loss: 0.7954
 860/1000 [========================>.....] - ETA: 1:03 - loss: 3.0939 - regression_loss: 2.2983 - classification_loss: 0.7957
 861/1000 [========================>.....] - ETA: 1:02 - loss: 3.0954 - regression_loss: 2.2990 - classification_loss: 0.7965
 862/1000 [========================>.....] - ETA: 1:02 - loss: 3.0966 - regression_loss: 2.2995 - classification_loss: 0.7971
 863/1000 [========================>.....] - ETA: 1:02 - loss: 3.0990 - regression_loss: 2.3010 - classification_loss: 0.7980
 864/1000 [========================>.....] - ETA: 1:01 - loss: 3.1004 - regression_loss: 2.3024 - classification_loss: 0.7980
 865/1000 [========================>.....] - ETA: 1:01 - loss: 3.1014 - regression_loss: 2.3024 - classification_loss: 0.7989
 866/1000 [========================>.....] - ETA: 1:00 - loss: 3.0978 - regression_loss: 2.2998 - classification_loss: 0.7980
 867/1000 [=========================>....] - ETA: 1:00 - loss: 3.0967 - regression_loss: 2.2992 - classification_loss: 0.7974
 868/1000 [=========================>....] - ETA: 59s - loss: 3.0969 - regression_loss: 2.2989 - classification_loss: 0.7980 
 869/1000 [=========================>....] - ETA: 59s - loss: 3.0967 - regression_loss: 2.2988 - classification_loss: 0.7979
 870/1000 [=========================>....] - ETA: 58s - loss: 3.0931 - regression_loss: 2.2961 - classification_loss: 0.7970
 871/1000 [=========================>....] - ETA: 58s - loss: 3.0937 - regression_loss: 2.2969 - classification_loss: 0.7968
 872/1000 [=========================>....] - ETA: 57s - loss: 3.0902 - regression_loss: 2.2943 - classification_loss: 0.7959
 873/1000 [=========================>....] - ETA: 57s - loss: 3.0907 - regression_loss: 2.2952 - classification_loss: 0.7955
 874/1000 [=========================>....] - ETA: 57s - loss: 3.0913 - regression_loss: 2.2955 - classification_loss: 0.7958
 875/1000 [=========================>....] - ETA: 56s - loss: 3.0937 - regression_loss: 2.2981 - classification_loss: 0.7956
 876/1000 [=========================>....] - ETA: 56s - loss: 3.0902 - regression_loss: 2.2955 - classification_loss: 0.7947
 877/1000 [=========================>....] - ETA: 55s - loss: 3.0918 - regression_loss: 2.2969 - classification_loss: 0.7949
 878/1000 [=========================>....] - ETA: 55s - loss: 3.0909 - regression_loss: 2.2967 - classification_loss: 0.7943
 879/1000 [=========================>....] - ETA: 54s - loss: 3.0925 - regression_loss: 2.2968 - classification_loss: 0.7957
 880/1000 [=========================>....] - ETA: 54s - loss: 3.0925 - regression_loss: 2.2971 - classification_loss: 0.7954
 881/1000 [=========================>....] - ETA: 53s - loss: 3.0945 - regression_loss: 2.2980 - classification_loss: 0.7965
 882/1000 [=========================>....] - ETA: 53s - loss: 3.0938 - regression_loss: 2.2978 - classification_loss: 0.7960
 883/1000 [=========================>....] - ETA: 52s - loss: 3.0947 - regression_loss: 2.2989 - classification_loss: 0.7958
 884/1000 [=========================>....] - ETA: 52s - loss: 3.0950 - regression_loss: 2.2990 - classification_loss: 0.7960
 885/1000 [=========================>....] - ETA: 52s - loss: 3.0915 - regression_loss: 2.2964 - classification_loss: 0.7951
 886/1000 [=========================>....] - ETA: 51s - loss: 3.0881 - regression_loss: 2.2938 - classification_loss: 0.7943
 887/1000 [=========================>....] - ETA: 51s - loss: 3.0898 - regression_loss: 2.2952 - classification_loss: 0.7946
 888/1000 [=========================>....] - ETA: 50s - loss: 3.0906 - regression_loss: 2.2950 - classification_loss: 0.7956
 889/1000 [=========================>....] - ETA: 50s - loss: 3.0900 - regression_loss: 2.2947 - classification_loss: 0.7953
 890/1000 [=========================>....] - ETA: 49s - loss: 3.0893 - regression_loss: 2.2944 - classification_loss: 0.7948
 891/1000 [=========================>....] - ETA: 49s - loss: 3.0917 - regression_loss: 2.2959 - classification_loss: 0.7959
 892/1000 [=========================>....] - ETA: 48s - loss: 3.0921 - regression_loss: 2.2967 - classification_loss: 0.7954
 893/1000 [=========================>....] - ETA: 48s - loss: 3.0909 - regression_loss: 2.2942 - classification_loss: 0.7968
 894/1000 [=========================>....] - ETA: 48s - loss: 3.0927 - regression_loss: 2.2958 - classification_loss: 0.7968
 895/1000 [=========================>....] - ETA: 47s - loss: 3.0934 - regression_loss: 2.2960 - classification_loss: 0.7973
 896/1000 [=========================>....] - ETA: 47s - loss: 3.0937 - regression_loss: 2.2970 - classification_loss: 0.7967
 897/1000 [=========================>....] - ETA: 46s - loss: 3.0903 - regression_loss: 2.2944 - classification_loss: 0.7959
 898/1000 [=========================>....] - ETA: 46s - loss: 3.0868 - regression_loss: 2.2919 - classification_loss: 0.7950
 899/1000 [=========================>....] - ETA: 45s - loss: 3.0890 - regression_loss: 2.2937 - classification_loss: 0.7953
 900/1000 [==========================>...] - ETA: 45s - loss: 3.0856 - regression_loss: 2.2911 - classification_loss: 0.7944
 901/1000 [==========================>...] - ETA: 44s - loss: 3.0821 - regression_loss: 2.2886 - classification_loss: 0.7935
 902/1000 [==========================>...] - ETA: 44s - loss: 3.0810 - regression_loss: 2.2879 - classification_loss: 0.7931
 903/1000 [==========================>...] - ETA: 43s - loss: 3.0776 - regression_loss: 2.2854 - classification_loss: 0.7922
 904/1000 [==========================>...] - ETA: 43s - loss: 3.0788 - regression_loss: 2.2861 - classification_loss: 0.7928
 905/1000 [==========================>...] - ETA: 43s - loss: 3.0754 - regression_loss: 2.2836 - classification_loss: 0.7919
 906/1000 [==========================>...] - ETA: 42s - loss: 3.0774 - regression_loss: 2.2842 - classification_loss: 0.7932
 907/1000 [==========================>...] - ETA: 42s - loss: 3.0786 - regression_loss: 2.2847 - classification_loss: 0.7939
 908/1000 [==========================>...] - ETA: 41s - loss: 3.0787 - regression_loss: 2.2852 - classification_loss: 0.7935
 909/1000 [==========================>...] - ETA: 41s - loss: 3.0785 - regression_loss: 2.2850 - classification_loss: 0.7935
 910/1000 [==========================>...] - ETA: 40s - loss: 3.0798 - regression_loss: 2.2862 - classification_loss: 0.7937
 911/1000 [==========================>...] - ETA: 40s - loss: 3.0814 - regression_loss: 2.2865 - classification_loss: 0.7949
 912/1000 [==========================>...] - ETA: 39s - loss: 3.0780 - regression_loss: 2.2840 - classification_loss: 0.7940
 913/1000 [==========================>...] - ETA: 39s - loss: 3.0775 - regression_loss: 2.2838 - classification_loss: 0.7938
 914/1000 [==========================>...] - ETA: 38s - loss: 3.0782 - regression_loss: 2.2844 - classification_loss: 0.7938
 915/1000 [==========================>...] - ETA: 38s - loss: 3.0801 - regression_loss: 2.2852 - classification_loss: 0.7949
 916/1000 [==========================>...] - ETA: 38s - loss: 3.0814 - regression_loss: 2.2862 - classification_loss: 0.7952
 917/1000 [==========================>...] - ETA: 37s - loss: 3.0848 - regression_loss: 2.2874 - classification_loss: 0.7974
 918/1000 [==========================>...] - ETA: 37s - loss: 3.0814 - regression_loss: 2.2849 - classification_loss: 0.7965
 919/1000 [==========================>...] - ETA: 36s - loss: 3.0823 - regression_loss: 2.2857 - classification_loss: 0.7966
 920/1000 [==========================>...] - ETA: 36s - loss: 3.0827 - regression_loss: 2.2862 - classification_loss: 0.7964
 921/1000 [==========================>...] - ETA: 35s - loss: 3.0841 - regression_loss: 2.2869 - classification_loss: 0.7972
 922/1000 [==========================>...] - ETA: 35s - loss: 3.0808 - regression_loss: 2.2844 - classification_loss: 0.7964
 923/1000 [==========================>...] - ETA: 34s - loss: 3.0774 - regression_loss: 2.2819 - classification_loss: 0.7955
 924/1000 [==========================>...] - ETA: 34s - loss: 3.0797 - regression_loss: 2.2827 - classification_loss: 0.7970
 925/1000 [==========================>...] - ETA: 33s - loss: 3.0800 - regression_loss: 2.2833 - classification_loss: 0.7967
 926/1000 [==========================>...] - ETA: 33s - loss: 3.0766 - regression_loss: 2.2808 - classification_loss: 0.7958
 927/1000 [==========================>...] - ETA: 33s - loss: 3.0757 - regression_loss: 2.2799 - classification_loss: 0.7958
 928/1000 [==========================>...] - ETA: 32s - loss: 3.0724 - regression_loss: 2.2774 - classification_loss: 0.7950
 929/1000 [==========================>...] - ETA: 32s - loss: 3.0733 - regression_loss: 2.2785 - classification_loss: 0.7948
 930/1000 [==========================>...] - ETA: 31s - loss: 3.0700 - regression_loss: 2.2761 - classification_loss: 0.7940
 931/1000 [==========================>...] - ETA: 31s - loss: 3.0747 - regression_loss: 2.2800 - classification_loss: 0.7946
 932/1000 [==========================>...] - ETA: 30s - loss: 3.0770 - regression_loss: 2.2812 - classification_loss: 0.7957
 933/1000 [==========================>...] - ETA: 30s - loss: 3.0792 - regression_loss: 2.2823 - classification_loss: 0.7968
 934/1000 [===========================>..] - ETA: 29s - loss: 3.0799 - regression_loss: 2.2834 - classification_loss: 0.7965
 935/1000 [===========================>..] - ETA: 29s - loss: 3.0820 - regression_loss: 2.2857 - classification_loss: 0.7963
 936/1000 [===========================>..] - ETA: 28s - loss: 3.0826 - regression_loss: 2.2861 - classification_loss: 0.7965
 937/1000 [===========================>..] - ETA: 28s - loss: 3.0824 - regression_loss: 2.2863 - classification_loss: 0.7961
 938/1000 [===========================>..] - ETA: 28s - loss: 3.0813 - regression_loss: 2.2857 - classification_loss: 0.7955
 939/1000 [===========================>..] - ETA: 27s - loss: 3.0829 - regression_loss: 2.2871 - classification_loss: 0.7957
 940/1000 [===========================>..] - ETA: 27s - loss: 3.0847 - regression_loss: 2.2881 - classification_loss: 0.7965
 941/1000 [===========================>..] - ETA: 26s - loss: 3.0863 - regression_loss: 2.2894 - classification_loss: 0.7969
 942/1000 [===========================>..] - ETA: 26s - loss: 3.0884 - regression_loss: 2.2908 - classification_loss: 0.7977
 943/1000 [===========================>..] - ETA: 25s - loss: 3.0893 - regression_loss: 2.2918 - classification_loss: 0.7975
 944/1000 [===========================>..] - ETA: 25s - loss: 3.0902 - regression_loss: 2.2928 - classification_loss: 0.7974
 945/1000 [===========================>..] - ETA: 24s - loss: 3.0869 - regression_loss: 2.2904 - classification_loss: 0.7965
 946/1000 [===========================>..] - ETA: 24s - loss: 3.0836 - regression_loss: 2.2880 - classification_loss: 0.7957
 947/1000 [===========================>..] - ETA: 24s - loss: 3.0845 - regression_loss: 2.2892 - classification_loss: 0.7953
 948/1000 [===========================>..] - ETA: 23s - loss: 3.0847 - regression_loss: 2.2896 - classification_loss: 0.7951
 949/1000 [===========================>..] - ETA: 23s - loss: 3.0851 - regression_loss: 2.2903 - classification_loss: 0.7948
 950/1000 [===========================>..] - ETA: 22s - loss: 3.0842 - regression_loss: 2.2897 - classification_loss: 0.7945
 951/1000 [===========================>..] - ETA: 22s - loss: 3.0846 - regression_loss: 2.2905 - classification_loss: 0.7941
 952/1000 [===========================>..] - ETA: 21s - loss: 3.0844 - regression_loss: 2.2908 - classification_loss: 0.7936
 953/1000 [===========================>..] - ETA: 21s - loss: 3.0860 - regression_loss: 2.2916 - classification_loss: 0.7944
 954/1000 [===========================>..] - ETA: 20s - loss: 3.0828 - regression_loss: 2.2892 - classification_loss: 0.7936
 955/1000 [===========================>..] - ETA: 20s - loss: 3.0834 - regression_loss: 2.2902 - classification_loss: 0.7933
 956/1000 [===========================>..] - ETA: 19s - loss: 3.0845 - regression_loss: 2.2906 - classification_loss: 0.7939
 957/1000 [===========================>..] - ETA: 19s - loss: 3.0867 - regression_loss: 2.2923 - classification_loss: 0.7944
 958/1000 [===========================>..] - ETA: 19s - loss: 3.0835 - regression_loss: 2.2899 - classification_loss: 0.7936
 959/1000 [===========================>..] - ETA: 18s - loss: 3.0845 - regression_loss: 2.2904 - classification_loss: 0.7941
 960/1000 [===========================>..] - ETA: 18s - loss: 3.0852 - regression_loss: 2.2906 - classification_loss: 0.7946
 961/1000 [===========================>..] - ETA: 17s - loss: 3.0820 - regression_loss: 2.2882 - classification_loss: 0.7938
 962/1000 [===========================>..] - ETA: 17s - loss: 3.0823 - regression_loss: 2.2885 - classification_loss: 0.7938
 963/1000 [===========================>..] - ETA: 16s - loss: 3.0791 - regression_loss: 2.2861 - classification_loss: 0.7930
 964/1000 [===========================>..] - ETA: 16s - loss: 3.0808 - regression_loss: 2.2873 - classification_loss: 0.7935
 965/1000 [===========================>..] - ETA: 15s - loss: 3.0826 - regression_loss: 2.2890 - classification_loss: 0.7936
 966/1000 [===========================>..] - ETA: 15s - loss: 3.0794 - regression_loss: 2.2866 - classification_loss: 0.7928
 967/1000 [============================>.] - ETA: 14s - loss: 3.0762 - regression_loss: 2.2843 - classification_loss: 0.7920
 968/1000 [============================>.] - ETA: 14s - loss: 3.0754 - regression_loss: 2.2837 - classification_loss: 0.7917
 969/1000 [============================>.] - ETA: 14s - loss: 3.0775 - regression_loss: 2.2848 - classification_loss: 0.7927
 970/1000 [============================>.] - ETA: 13s - loss: 3.0786 - regression_loss: 2.2861 - classification_loss: 0.7925
 971/1000 [============================>.] - ETA: 13s - loss: 3.0788 - regression_loss: 2.2857 - classification_loss: 0.7932
 972/1000 [============================>.] - ETA: 12s - loss: 3.0801 - regression_loss: 2.2873 - classification_loss: 0.7928
 973/1000 [============================>.] - ETA: 12s - loss: 3.0805 - regression_loss: 2.2870 - classification_loss: 0.7936
 974/1000 [============================>.] - ETA: 11s - loss: 3.0803 - regression_loss: 2.2870 - classification_loss: 0.7934
 975/1000 [============================>.] - ETA: 11s - loss: 3.0843 - regression_loss: 2.2894 - classification_loss: 0.7948
 976/1000 [============================>.] - ETA: 10s - loss: 3.0811 - regression_loss: 2.2871 - classification_loss: 0.7940
 977/1000 [============================>.] - ETA: 10s - loss: 3.0780 - regression_loss: 2.2848 - classification_loss: 0.7932
 978/1000 [============================>.] - ETA: 9s - loss: 3.0774 - regression_loss: 2.2847 - classification_loss: 0.7928 
 979/1000 [============================>.] - ETA: 9s - loss: 3.0786 - regression_loss: 2.2860 - classification_loss: 0.7926
 980/1000 [============================>.] - ETA: 9s - loss: 3.0788 - regression_loss: 2.2866 - classification_loss: 0.7922
 981/1000 [============================>.] - ETA: 8s - loss: 3.0776 - regression_loss: 2.2859 - classification_loss: 0.7917
 982/1000 [============================>.] - ETA: 8s - loss: 3.0765 - regression_loss: 2.2854 - classification_loss: 0.7912
 983/1000 [============================>.] - ETA: 7s - loss: 3.0734 - regression_loss: 2.2830 - classification_loss: 0.7904
 984/1000 [============================>.] - ETA: 7s - loss: 3.0740 - regression_loss: 2.2834 - classification_loss: 0.7906
 985/1000 [============================>.] - ETA: 6s - loss: 3.0747 - regression_loss: 2.2836 - classification_loss: 0.7911
 986/1000 [============================>.] - ETA: 6s - loss: 3.0716 - regression_loss: 2.2813 - classification_loss: 0.7903
 987/1000 [============================>.] - ETA: 5s - loss: 3.0713 - regression_loss: 2.2815 - classification_loss: 0.7898
 988/1000 [============================>.] - ETA: 5s - loss: 3.0682 - regression_loss: 2.2792 - classification_loss: 0.7890
 989/1000 [============================>.] - ETA: 4s - loss: 3.0689 - regression_loss: 2.2798 - classification_loss: 0.7891
 990/1000 [============================>.] - ETA: 4s - loss: 3.0680 - regression_loss: 2.2793 - classification_loss: 0.7888
 991/1000 [============================>.] - ETA: 4s - loss: 3.0650 - regression_loss: 2.2770 - classification_loss: 0.7880
 992/1000 [============================>.] - ETA: 3s - loss: 3.0645 - regression_loss: 2.2768 - classification_loss: 0.7878
 993/1000 [============================>.] - ETA: 3s - loss: 3.0668 - regression_loss: 2.2780 - classification_loss: 0.7888
 994/1000 [============================>.] - ETA: 2s - loss: 3.0667 - regression_loss: 2.2777 - classification_loss: 0.7890
 995/1000 [============================>.] - ETA: 2s - loss: 3.0671 - regression_loss: 2.2775 - classification_loss: 0.7895
 996/1000 [============================>.] - ETA: 1s - loss: 3.0674 - regression_loss: 2.2780 - classification_loss: 0.7893
 997/1000 [============================>.] - ETA: 1s - loss: 3.0643 - regression_loss: 2.2758 - classification_loss: 0.7885
 998/1000 [============================>.] - ETA: 0s - loss: 3.0673 - regression_loss: 2.2787 - classification_loss: 0.7886
 999/1000 [============================>.] - ETA: 0s - loss: 3.0668 - regression_loss: 2.2782 - classification_loss: 0.7886
1000/1000 [==============================] - 453s 453ms/step - loss: 3.0688 - regression_loss: 2.2795 - classification_loss: 0.7893

Epoch 00010: saving model to ./snapshots/resnet50_csv_10.h5
0/2001/2002/2003/2004/2005/2006/2007/2008/2009/20010/20011/20012/20013/20014/20015/20016/20017/20018/20019/20020/20021/20022/20023/20024/20025/20026/20027/20028/20029/20030/20031/20032/20033/20034/20035/20036/20037/20038/20039/20040/20041/20042/20043/20044/20045/20046/20047/20048/20049/20050/20051/20052/20053/20054/20055/20056/20057/20058/20059/20060/20061/20062/20063/20064/20065/20066/20067/20068/20069/20070/20071/20072/20073/20074/20075/20076/20077/20078/20079/20080/20081/20082/20083/20084/20085/20086/20087/20088/20089/20090/20091/20092/20093/20094/20095/20096/20097/20098/20099/200100/200101/200102/200103/200104/200105/200106/200107/200108/200109/200110/200111/200112/200113/200114/200115/200116/200117/200118/200119/200120/200121/200122/200123/200124/200125/200126/200127/200128/200129/200130/200131/200132/200133/200134/200135/200136/200137/200138/200139/200140/200141/200142/200143/200144/200145/200146/200147/200148/200149/200150/200151/200152/200153/200154/200155/200156/200157/200158/200159/200160/200161/200162/200163/200164/200165/200166/200167/200168/200169/200170/200171/200172/200173/200174/200175/200176/200177/200178/200179/200180/200181/200182/200183/200184/200185/200186/200187/200188/200189/200190/200191/200192/200193/200194/200195/200196/200197/200198/200199/2000/2001/2002/2003/2004/2005/2006/2007/2008/2009/20010/20011/20012/20013/20014/20015/20016/20017/20018/20019/20020/20021/20022/20023/20024/20025/20026/20027/20028/20029/20030/20031/20032/20033/20034/20035/20036/20037/20038/20039/20040/20041/20042/20043/20044/20045/20046/20047/20048/20049/20050/20051/20052/20053/20054/20055/20056/20057/20058/20059/20060/20061/20062/20063/20064/20065/20066/20067/20068/20069/20070/20071/20072/20073/20074/20075/20076/20077/20078/20079/20080/20081/20082/20083/20084/20085/20086/20087/20088/20089/20090/20091/20092/20093/20094/20095/20096/20097/20098/20099/200100/200101/200102/200103/200104/200105/200106/200107/200108/200109/200110/200111/200112/200113/200114/200115/200116/200117/200118/200119/200120/200121/200122/200123/200124/200125/200126/200127/200128/200129/200130/200131/200132/200133/200134/200135/200136/200137/200138/200139/200140/200141/200142/200143/200144/200145/200146/200147/200148/200149/200150/200151/200152/200153/200154/200155/200156/200157/200158/200159/200160/200161/200162/200163/200164/200165/200166/200167/200168/200169/200170/200171/200172/200173/200174/200175/200176/200177/200178/200179/200180/200181/200182/200183/200184/200185/200186/200187/200188/200189/200190/200191/200192/200193/200194/200195/200196/200197/200198/200199/200

M 0.1076
N 0.0045
mAP: 0.0560
Epoch 11/30

   1/1000 [..............................] - ETA: 7:18 - loss: 3.2574 - regression_loss: 2.5113 - classification_loss: 0.7460
   2/1000 [..............................] - ETA: 7:24 - loss: 1.6287 - regression_loss: 1.2557 - classification_loss: 0.3730
   3/1000 [..............................] - ETA: 7:26 - loss: 1.3836 - regression_loss: 0.8371 - classification_loss: 0.5465
   4/1000 [..............................] - ETA: 7:25 - loss: 1.7526 - regression_loss: 1.1169 - classification_loss: 0.6357
   5/1000 [..............................] - ETA: 7:25 - loss: 2.1297 - regression_loss: 1.2836 - classification_loss: 0.8461
   6/1000 [..............................] - ETA: 7:25 - loss: 2.2404 - regression_loss: 1.4400 - classification_loss: 0.8004
   7/1000 [..............................] - ETA: 7:24 - loss: 2.6102 - regression_loss: 1.7332 - classification_loss: 0.8769
   8/1000 [..............................] - ETA: 7:24 - loss: 2.7642 - regression_loss: 1.8404 - classification_loss: 0.9237
   9/1000 [..............................] - ETA: 7:24 - loss: 2.4571 - regression_loss: 1.6360 - classification_loss: 0.8212
  10/1000 [..............................] - ETA: 7:23 - loss: 2.6696 - regression_loss: 1.8172 - classification_loss: 0.8524
  11/1000 [..............................] - ETA: 7:24 - loss: 2.4269 - regression_loss: 1.6520 - classification_loss: 0.7750
  12/1000 [..............................] - ETA: 7:25 - loss: 2.4600 - regression_loss: 1.6911 - classification_loss: 0.7689
  13/1000 [..............................] - ETA: 7:24 - loss: 2.6586 - regression_loss: 1.8120 - classification_loss: 0.8466
  14/1000 [..............................] - ETA: 7:24 - loss: 2.7017 - regression_loss: 1.8487 - classification_loss: 0.8530
  15/1000 [..............................] - ETA: 7:24 - loss: 2.8971 - regression_loss: 1.9649 - classification_loss: 0.9321
  16/1000 [..............................] - ETA: 7:23 - loss: 2.9605 - regression_loss: 2.0374 - classification_loss: 0.9231
  17/1000 [..............................] - ETA: 7:23 - loss: 2.7864 - regression_loss: 1.9176 - classification_loss: 0.8688
  18/1000 [..............................] - ETA: 7:21 - loss: 2.6489 - regression_loss: 1.8110 - classification_loss: 0.8379
  19/1000 [..............................] - ETA: 7:19 - loss: 2.6499 - regression_loss: 1.8176 - classification_loss: 0.8322
  20/1000 [..............................] - ETA: 7:19 - loss: 2.5209 - regression_loss: 1.7267 - classification_loss: 0.7942
  21/1000 [..............................] - ETA: 7:19 - loss: 2.4009 - regression_loss: 1.6445 - classification_loss: 0.7563
  22/1000 [..............................] - ETA: 7:18 - loss: 2.4258 - regression_loss: 1.6768 - classification_loss: 0.7490
  23/1000 [..............................] - ETA: 7:18 - loss: 2.4897 - regression_loss: 1.7380 - classification_loss: 0.7517
  24/1000 [..............................] - ETA: 7:18 - loss: 2.5551 - regression_loss: 1.7658 - classification_loss: 0.7893
  25/1000 [..............................] - ETA: 7:18 - loss: 2.4550 - regression_loss: 1.6951 - classification_loss: 0.7599
  26/1000 [..............................] - ETA: 7:17 - loss: 2.3609 - regression_loss: 1.6299 - classification_loss: 0.7309
  27/1000 [..............................] - ETA: 7:17 - loss: 2.2735 - regression_loss: 1.5696 - classification_loss: 0.7039
  28/1000 [..............................] - ETA: 7:17 - loss: 2.3639 - regression_loss: 1.6350 - classification_loss: 0.7289
  29/1000 [..............................] - ETA: 7:16 - loss: 2.4470 - regression_loss: 1.7022 - classification_loss: 0.7448
  30/1000 [..............................] - ETA: 7:16 - loss: 2.5057 - regression_loss: 1.7240 - classification_loss: 0.7817
  31/1000 [..............................] - ETA: 7:15 - loss: 2.4249 - regression_loss: 1.6684 - classification_loss: 0.7565
  32/1000 [..............................] - ETA: 7:15 - loss: 2.4523 - regression_loss: 1.6965 - classification_loss: 0.7558
  33/1000 [..............................] - ETA: 7:15 - loss: 2.4875 - regression_loss: 1.7193 - classification_loss: 0.7682
  34/1000 [>.............................] - ETA: 7:14 - loss: 2.5592 - regression_loss: 1.7681 - classification_loss: 0.7911
  35/1000 [>.............................] - ETA: 7:14 - loss: 2.6191 - regression_loss: 1.8072 - classification_loss: 0.8119
  36/1000 [>.............................] - ETA: 7:13 - loss: 2.6349 - regression_loss: 1.8231 - classification_loss: 0.8118
  37/1000 [>.............................] - ETA: 7:12 - loss: 2.5640 - regression_loss: 1.7738 - classification_loss: 0.7902
  38/1000 [>.............................] - ETA: 7:12 - loss: 2.6014 - regression_loss: 1.8006 - classification_loss: 0.8007
  39/1000 [>.............................] - ETA: 7:11 - loss: 2.6204 - regression_loss: 1.8201 - classification_loss: 0.8003
  40/1000 [>.............................] - ETA: 7:11 - loss: 2.6408 - regression_loss: 1.8153 - classification_loss: 0.8255
  41/1000 [>.............................] - ETA: 7:11 - loss: 2.6896 - regression_loss: 1.8537 - classification_loss: 0.8360
  42/1000 [>.............................] - ETA: 7:10 - loss: 2.6256 - regression_loss: 1.8095 - classification_loss: 0.8161
  43/1000 [>.............................] - ETA: 7:10 - loss: 2.7079 - regression_loss: 1.8668 - classification_loss: 0.8412
  44/1000 [>.............................] - ETA: 7:10 - loss: 2.7286 - regression_loss: 1.8790 - classification_loss: 0.8496
  45/1000 [>.............................] - ETA: 7:10 - loss: 2.6680 - regression_loss: 1.8372 - classification_loss: 0.8307
  46/1000 [>.............................] - ETA: 7:09 - loss: 2.6100 - regression_loss: 1.7973 - classification_loss: 0.8127
  47/1000 [>.............................] - ETA: 7:09 - loss: 2.5545 - regression_loss: 1.7591 - classification_loss: 0.7954
  48/1000 [>.............................] - ETA: 7:08 - loss: 2.5715 - regression_loss: 1.7745 - classification_loss: 0.7969
  49/1000 [>.............................] - ETA: 7:08 - loss: 2.5190 - regression_loss: 1.7383 - classification_loss: 0.7806
  50/1000 [>.............................] - ETA: 7:07 - loss: 2.4686 - regression_loss: 1.7036 - classification_loss: 0.7650
  51/1000 [>.............................] - ETA: 7:07 - loss: 2.4202 - regression_loss: 1.6702 - classification_loss: 0.7500
  52/1000 [>.............................] - ETA: 7:07 - loss: 2.4320 - regression_loss: 1.6789 - classification_loss: 0.7531
  53/1000 [>.............................] - ETA: 7:06 - loss: 2.4701 - regression_loss: 1.7115 - classification_loss: 0.7586
  54/1000 [>.............................] - ETA: 7:06 - loss: 2.5493 - regression_loss: 1.7627 - classification_loss: 0.7866
  55/1000 [>.............................] - ETA: 7:05 - loss: 2.6153 - regression_loss: 1.8084 - classification_loss: 0.8069
  56/1000 [>.............................] - ETA: 7:05 - loss: 2.6343 - regression_loss: 1.8277 - classification_loss: 0.8065
  57/1000 [>.............................] - ETA: 7:05 - loss: 2.6701 - regression_loss: 1.8576 - classification_loss: 0.8125
  58/1000 [>.............................] - ETA: 7:04 - loss: 2.7241 - regression_loss: 1.8912 - classification_loss: 0.8330
  59/1000 [>.............................] - ETA: 7:04 - loss: 2.7636 - regression_loss: 1.9080 - classification_loss: 0.8555
  60/1000 [>.............................] - ETA: 7:03 - loss: 2.7176 - regression_loss: 1.8762 - classification_loss: 0.8413
  61/1000 [>.............................] - ETA: 7:03 - loss: 2.7406 - regression_loss: 1.8933 - classification_loss: 0.8473
  62/1000 [>.............................] - ETA: 7:02 - loss: 2.7313 - regression_loss: 1.8852 - classification_loss: 0.8461
  63/1000 [>.............................] - ETA: 7:02 - loss: 2.6879 - regression_loss: 1.8552 - classification_loss: 0.8327
  64/1000 [>.............................] - ETA: 7:02 - loss: 2.7425 - regression_loss: 1.9003 - classification_loss: 0.8422
  65/1000 [>.............................] - ETA: 7:01 - loss: 2.7544 - regression_loss: 1.8990 - classification_loss: 0.8554
  66/1000 [>.............................] - ETA: 7:01 - loss: 2.7902 - regression_loss: 1.9241 - classification_loss: 0.8661
  67/1000 [=>............................] - ETA: 7:00 - loss: 2.7486 - regression_loss: 1.8954 - classification_loss: 0.8532
  68/1000 [=>............................] - ETA: 7:00 - loss: 2.7082 - regression_loss: 1.8675 - classification_loss: 0.8406
  69/1000 [=>............................] - ETA: 6:59 - loss: 2.7339 - regression_loss: 1.8885 - classification_loss: 0.8453
  70/1000 [=>............................] - ETA: 6:59 - loss: 2.7706 - regression_loss: 1.9129 - classification_loss: 0.8577
  71/1000 [=>............................] - ETA: 6:59 - loss: 2.7908 - regression_loss: 1.9361 - classification_loss: 0.8547
  72/1000 [=>............................] - ETA: 6:58 - loss: 2.8214 - regression_loss: 1.9605 - classification_loss: 0.8609
  73/1000 [=>............................] - ETA: 6:58 - loss: 2.8533 - regression_loss: 1.9793 - classification_loss: 0.8741
  74/1000 [=>............................] - ETA: 6:57 - loss: 2.8690 - regression_loss: 1.9901 - classification_loss: 0.8789
  75/1000 [=>............................] - ETA: 6:57 - loss: 2.8830 - regression_loss: 1.9974 - classification_loss: 0.8856
  76/1000 [=>............................] - ETA: 6:56 - loss: 2.8957 - regression_loss: 2.0124 - classification_loss: 0.8834
  77/1000 [=>............................] - ETA: 6:56 - loss: 2.9104 - regression_loss: 2.0227 - classification_loss: 0.8877
  78/1000 [=>............................] - ETA: 6:55 - loss: 2.9129 - regression_loss: 2.0281 - classification_loss: 0.8849
  79/1000 [=>............................] - ETA: 6:55 - loss: 2.8761 - regression_loss: 2.0024 - classification_loss: 0.8737
  80/1000 [=>............................] - ETA: 6:55 - loss: 2.8784 - regression_loss: 2.0095 - classification_loss: 0.8689
  81/1000 [=>............................] - ETA: 6:54 - loss: 2.8951 - regression_loss: 2.0204 - classification_loss: 0.8747
  82/1000 [=>............................] - ETA: 6:54 - loss: 2.8598 - regression_loss: 1.9958 - classification_loss: 0.8640
  83/1000 [=>............................] - ETA: 6:54 - loss: 2.8689 - regression_loss: 2.0027 - classification_loss: 0.8662
  84/1000 [=>............................] - ETA: 6:53 - loss: 2.8876 - regression_loss: 2.0174 - classification_loss: 0.8702
  85/1000 [=>............................] - ETA: 6:53 - loss: 2.8915 - regression_loss: 2.0236 - classification_loss: 0.8679
  86/1000 [=>............................] - ETA: 6:52 - loss: 2.9018 - regression_loss: 2.0375 - classification_loss: 0.8643
  87/1000 [=>............................] - ETA: 6:52 - loss: 2.9116 - regression_loss: 2.0481 - classification_loss: 0.8635
  88/1000 [=>............................] - ETA: 6:51 - loss: 2.8802 - regression_loss: 2.0248 - classification_loss: 0.8554
  89/1000 [=>............................] - ETA: 6:51 - loss: 2.8889 - regression_loss: 2.0311 - classification_loss: 0.8577
  90/1000 [=>............................] - ETA: 6:51 - loss: 2.8568 - regression_loss: 2.0086 - classification_loss: 0.8482
  91/1000 [=>............................] - ETA: 6:50 - loss: 2.8254 - regression_loss: 1.9865 - classification_loss: 0.8389
  92/1000 [=>............................] - ETA: 6:50 - loss: 2.7948 - regression_loss: 1.9649 - classification_loss: 0.8299
  93/1000 [=>............................] - ETA: 6:49 - loss: 2.7647 - regression_loss: 1.9438 - classification_loss: 0.8209
  94/1000 [=>............................] - ETA: 6:48 - loss: 2.7796 - regression_loss: 1.9617 - classification_loss: 0.8180
  95/1000 [=>............................] - ETA: 6:48 - loss: 2.8132 - regression_loss: 1.9866 - classification_loss: 0.8266
  96/1000 [=>............................] - ETA: 6:48 - loss: 2.8163 - regression_loss: 1.9932 - classification_loss: 0.8231
  97/1000 [=>............................] - ETA: 6:47 - loss: 2.8194 - regression_loss: 1.9946 - classification_loss: 0.8248
  98/1000 [=>............................] - ETA: 6:47 - loss: 2.8402 - regression_loss: 2.0067 - classification_loss: 0.8335
  99/1000 [=>............................] - ETA: 6:46 - loss: 2.8357 - regression_loss: 2.0057 - classification_loss: 0.8300
 100/1000 [==>...........................] - ETA: 6:46 - loss: 2.8074 - regression_loss: 1.9857 - classification_loss: 0.8217
 101/1000 [==>...........................] - ETA: 6:45 - loss: 2.8049 - regression_loss: 1.9852 - classification_loss: 0.8198
 102/1000 [==>...........................] - ETA: 6:45 - loss: 2.8047 - regression_loss: 1.9849 - classification_loss: 0.8198
 103/1000 [==>...........................] - ETA: 6:45 - loss: 2.8146 - regression_loss: 1.9903 - classification_loss: 0.8243
 104/1000 [==>...........................] - ETA: 6:44 - loss: 2.8182 - regression_loss: 1.9952 - classification_loss: 0.8230
 105/1000 [==>...........................] - ETA: 6:44 - loss: 2.8259 - regression_loss: 2.0027 - classification_loss: 0.8232
 106/1000 [==>...........................] - ETA: 6:43 - loss: 2.8260 - regression_loss: 2.0026 - classification_loss: 0.8234
 107/1000 [==>...........................] - ETA: 6:43 - loss: 2.8492 - regression_loss: 2.0230 - classification_loss: 0.8263
 108/1000 [==>...........................] - ETA: 6:43 - loss: 2.8701 - regression_loss: 2.0377 - classification_loss: 0.8324
 109/1000 [==>...........................] - ETA: 6:42 - loss: 2.8663 - regression_loss: 2.0367 - classification_loss: 0.8297
 110/1000 [==>...........................] - ETA: 6:42 - loss: 2.8402 - regression_loss: 2.0181 - classification_loss: 0.8221
 111/1000 [==>...........................] - ETA: 6:41 - loss: 2.8442 - regression_loss: 2.0159 - classification_loss: 0.8282
 112/1000 [==>...........................] - ETA: 6:41 - loss: 2.8508 - regression_loss: 2.0226 - classification_loss: 0.8282
 113/1000 [==>...........................] - ETA: 6:40 - loss: 2.8769 - regression_loss: 2.0378 - classification_loss: 0.8391
 114/1000 [==>...........................] - ETA: 6:40 - loss: 2.8517 - regression_loss: 2.0199 - classification_loss: 0.8317
 115/1000 [==>...........................] - ETA: 6:40 - loss: 2.8530 - regression_loss: 2.0249 - classification_loss: 0.8280
 116/1000 [==>...........................] - ETA: 6:39 - loss: 2.8284 - regression_loss: 2.0075 - classification_loss: 0.8209
 117/1000 [==>...........................] - ETA: 6:39 - loss: 2.8333 - regression_loss: 2.0094 - classification_loss: 0.8239
 118/1000 [==>...........................] - ETA: 6:38 - loss: 2.8395 - regression_loss: 2.0154 - classification_loss: 0.8241
 119/1000 [==>...........................] - ETA: 6:38 - loss: 2.8435 - regression_loss: 2.0167 - classification_loss: 0.8268
 120/1000 [==>...........................] - ETA: 6:37 - loss: 2.8542 - regression_loss: 2.0297 - classification_loss: 0.8244
 121/1000 [==>...........................] - ETA: 6:37 - loss: 2.8641 - regression_loss: 2.0359 - classification_loss: 0.8281
 122/1000 [==>...........................] - ETA: 6:37 - loss: 2.8670 - regression_loss: 2.0400 - classification_loss: 0.8270
 123/1000 [==>...........................] - ETA: 6:36 - loss: 2.8439 - regression_loss: 2.0234 - classification_loss: 0.8205
 124/1000 [==>...........................] - ETA: 6:36 - loss: 2.8615 - regression_loss: 2.0366 - classification_loss: 0.8250
 125/1000 [==>...........................] - ETA: 6:35 - loss: 2.8751 - regression_loss: 2.0508 - classification_loss: 0.8243
 126/1000 [==>...........................] - ETA: 6:35 - loss: 2.8943 - regression_loss: 2.0664 - classification_loss: 0.8279
 127/1000 [==>...........................] - ETA: 6:34 - loss: 2.8720 - regression_loss: 2.0501 - classification_loss: 0.8219
 128/1000 [==>...........................] - ETA: 6:34 - loss: 2.8998 - regression_loss: 2.0780 - classification_loss: 0.8219
 129/1000 [==>...........................] - ETA: 6:33 - loss: 2.8980 - regression_loss: 2.0786 - classification_loss: 0.8194
 130/1000 [==>...........................] - ETA: 6:33 - loss: 2.8962 - regression_loss: 2.0784 - classification_loss: 0.8178
 131/1000 [==>...........................] - ETA: 6:33 - loss: 2.8990 - regression_loss: 2.0847 - classification_loss: 0.8142
 132/1000 [==>...........................] - ETA: 6:32 - loss: 2.9049 - regression_loss: 2.0850 - classification_loss: 0.8199
 133/1000 [==>...........................] - ETA: 6:32 - loss: 2.9066 - regression_loss: 2.0885 - classification_loss: 0.8181
 134/1000 [===>..........................] - ETA: 6:31 - loss: 2.8849 - regression_loss: 2.0729 - classification_loss: 0.8120
 135/1000 [===>..........................] - ETA: 6:31 - loss: 2.8970 - regression_loss: 2.0846 - classification_loss: 0.8124
 136/1000 [===>..........................] - ETA: 6:30 - loss: 2.9222 - regression_loss: 2.1032 - classification_loss: 0.8190
 137/1000 [===>..........................] - ETA: 6:30 - loss: 2.9334 - regression_loss: 2.1066 - classification_loss: 0.8267
 138/1000 [===>..........................] - ETA: 6:29 - loss: 2.9441 - regression_loss: 2.1164 - classification_loss: 0.8276
 139/1000 [===>..........................] - ETA: 6:29 - loss: 2.9555 - regression_loss: 2.1261 - classification_loss: 0.8294
 140/1000 [===>..........................] - ETA: 6:29 - loss: 2.9636 - regression_loss: 2.1300 - classification_loss: 0.8336
 141/1000 [===>..........................] - ETA: 6:28 - loss: 2.9744 - regression_loss: 2.1384 - classification_loss: 0.8359
 142/1000 [===>..........................] - ETA: 6:28 - loss: 2.9842 - regression_loss: 2.1492 - classification_loss: 0.8351
 143/1000 [===>..........................] - ETA: 6:27 - loss: 2.9908 - regression_loss: 2.1568 - classification_loss: 0.8340
 144/1000 [===>..........................] - ETA: 6:27 - loss: 2.9701 - regression_loss: 2.1418 - classification_loss: 0.8283
 145/1000 [===>..........................] - ETA: 6:26 - loss: 2.9496 - regression_loss: 2.1270 - classification_loss: 0.8226
 146/1000 [===>..........................] - ETA: 6:26 - loss: 2.9673 - regression_loss: 2.1394 - classification_loss: 0.8280
 147/1000 [===>..........................] - ETA: 6:25 - loss: 2.9471 - regression_loss: 2.1248 - classification_loss: 0.8223
 148/1000 [===>..........................] - ETA: 6:25 - loss: 2.9272 - regression_loss: 2.1104 - classification_loss: 0.8168
 149/1000 [===>..........................] - ETA: 6:25 - loss: 2.9370 - regression_loss: 2.1170 - classification_loss: 0.8200
 150/1000 [===>..........................] - ETA: 6:24 - loss: 2.9437 - regression_loss: 2.1227 - classification_loss: 0.8210
 151/1000 [===>..........................] - ETA: 6:24 - loss: 2.9557 - regression_loss: 2.1301 - classification_loss: 0.8257
 152/1000 [===>..........................] - ETA: 6:23 - loss: 2.9686 - regression_loss: 2.1389 - classification_loss: 0.8297
 153/1000 [===>..........................] - ETA: 6:23 - loss: 2.9691 - regression_loss: 2.1394 - classification_loss: 0.8297
 154/1000 [===>..........................] - ETA: 6:22 - loss: 2.9749 - regression_loss: 2.1463 - classification_loss: 0.8285
 155/1000 [===>..........................] - ETA: 6:22 - loss: 2.9833 - regression_loss: 2.1547 - classification_loss: 0.8286
 156/1000 [===>..........................] - ETA: 6:21 - loss: 2.9895 - regression_loss: 2.1572 - classification_loss: 0.8323
 157/1000 [===>..........................] - ETA: 6:21 - loss: 2.9916 - regression_loss: 2.1586 - classification_loss: 0.8331
 158/1000 [===>..........................] - ETA: 6:20 - loss: 2.9908 - regression_loss: 2.1592 - classification_loss: 0.8316
 159/1000 [===>..........................] - ETA: 6:20 - loss: 2.9939 - regression_loss: 2.1576 - classification_loss: 0.8362
 160/1000 [===>..........................] - ETA: 6:20 - loss: 2.9908 - regression_loss: 2.1573 - classification_loss: 0.8335
 161/1000 [===>..........................] - ETA: 6:19 - loss: 2.9723 - regression_loss: 2.1439 - classification_loss: 0.8284
 162/1000 [===>..........................] - ETA: 6:19 - loss: 2.9539 - regression_loss: 2.1306 - classification_loss: 0.8233
 163/1000 [===>..........................] - ETA: 6:18 - loss: 2.9656 - regression_loss: 2.1393 - classification_loss: 0.8262
 164/1000 [===>..........................] - ETA: 6:18 - loss: 2.9714 - regression_loss: 2.1452 - classification_loss: 0.8262
 165/1000 [===>..........................] - ETA: 6:17 - loss: 2.9834 - regression_loss: 2.1578 - classification_loss: 0.8255
 166/1000 [===>..........................] - ETA: 6:17 - loss: 2.9801 - regression_loss: 2.1569 - classification_loss: 0.8233
 167/1000 [====>.........................] - ETA: 6:16 - loss: 2.9887 - regression_loss: 2.1631 - classification_loss: 0.8256
 168/1000 [====>.........................] - ETA: 6:16 - loss: 2.9907 - regression_loss: 2.1658 - classification_loss: 0.8248
 169/1000 [====>.........................] - ETA: 6:16 - loss: 2.9932 - regression_loss: 2.1700 - classification_loss: 0.8232
 170/1000 [====>.........................] - ETA: 6:15 - loss: 3.0032 - regression_loss: 2.1766 - classification_loss: 0.8265
 171/1000 [====>.........................] - ETA: 6:15 - loss: 3.0102 - regression_loss: 2.1824 - classification_loss: 0.8277
 172/1000 [====>.........................] - ETA: 6:14 - loss: 3.0089 - regression_loss: 2.1841 - classification_loss: 0.8248
 173/1000 [====>.........................] - ETA: 6:14 - loss: 3.0106 - regression_loss: 2.1862 - classification_loss: 0.8245
 174/1000 [====>.........................] - ETA: 6:13 - loss: 3.0221 - regression_loss: 2.1919 - classification_loss: 0.8302
 175/1000 [====>.........................] - ETA: 6:13 - loss: 3.0271 - regression_loss: 2.1970 - classification_loss: 0.8302
 176/1000 [====>.........................] - ETA: 6:12 - loss: 3.0099 - regression_loss: 2.1845 - classification_loss: 0.8255
 177/1000 [====>.........................] - ETA: 6:12 - loss: 2.9930 - regression_loss: 2.1721 - classification_loss: 0.8208
 178/1000 [====>.........................] - ETA: 6:12 - loss: 2.9896 - regression_loss: 2.1716 - classification_loss: 0.8179
 179/1000 [====>.........................] - ETA: 6:11 - loss: 2.9729 - regression_loss: 2.1595 - classification_loss: 0.8134
 180/1000 [====>.........................] - ETA: 6:11 - loss: 2.9769 - regression_loss: 2.1631 - classification_loss: 0.8138
 181/1000 [====>.........................] - ETA: 6:10 - loss: 2.9879 - regression_loss: 2.1744 - classification_loss: 0.8134
 182/1000 [====>.........................] - ETA: 6:10 - loss: 2.9947 - regression_loss: 2.1789 - classification_loss: 0.8158
 183/1000 [====>.........................] - ETA: 6:09 - loss: 3.0006 - regression_loss: 2.1833 - classification_loss: 0.8173
 184/1000 [====>.........................] - ETA: 6:09 - loss: 2.9998 - regression_loss: 2.1815 - classification_loss: 0.8183
 185/1000 [====>.........................] - ETA: 6:08 - loss: 2.9836 - regression_loss: 2.1697 - classification_loss: 0.8139
 186/1000 [====>.........................] - ETA: 6:08 - loss: 2.9864 - regression_loss: 2.1747 - classification_loss: 0.8117
 187/1000 [====>.........................] - ETA: 6:07 - loss: 2.9783 - regression_loss: 2.1696 - classification_loss: 0.8087
 188/1000 [====>.........................] - ETA: 6:07 - loss: 2.9844 - regression_loss: 2.1742 - classification_loss: 0.8102
 189/1000 [====>.........................] - ETA: 6:07 - loss: 2.9894 - regression_loss: 2.1784 - classification_loss: 0.8110
 190/1000 [====>.........................] - ETA: 6:06 - loss: 2.9906 - regression_loss: 2.1815 - classification_loss: 0.8091
 191/1000 [====>.........................] - ETA: 6:06 - loss: 2.9903 - regression_loss: 2.1817 - classification_loss: 0.8087
 192/1000 [====>.........................] - ETA: 6:05 - loss: 2.9953 - regression_loss: 2.1860 - classification_loss: 0.8093
 193/1000 [====>.........................] - ETA: 6:05 - loss: 2.9935 - regression_loss: 2.1855 - classification_loss: 0.8080
 194/1000 [====>.........................] - ETA: 6:04 - loss: 2.9960 - regression_loss: 2.1890 - classification_loss: 0.8070
 195/1000 [====>.........................] - ETA: 6:04 - loss: 2.9812 - regression_loss: 2.1778 - classification_loss: 0.8035
 196/1000 [====>.........................] - ETA: 6:03 - loss: 2.9661 - regression_loss: 2.1666 - classification_loss: 0.7994
 197/1000 [====>.........................] - ETA: 6:03 - loss: 2.9647 - regression_loss: 2.1677 - classification_loss: 0.7970
 198/1000 [====>.........................] - ETA: 6:02 - loss: 2.9795 - regression_loss: 2.1783 - classification_loss: 0.8012
 199/1000 [====>.........................] - ETA: 6:02 - loss: 2.9878 - regression_loss: 2.1846 - classification_loss: 0.8031
 200/1000 [=====>........................] - ETA: 6:01 - loss: 2.9751 - regression_loss: 2.1737 - classification_loss: 0.8014
 201/1000 [=====>........................] - ETA: 6:01 - loss: 2.9828 - regression_loss: 2.1839 - classification_loss: 0.7989
 202/1000 [=====>........................] - ETA: 6:01 - loss: 2.9925 - regression_loss: 2.1935 - classification_loss: 0.7990
 203/1000 [=====>........................] - ETA: 6:00 - loss: 2.9778 - regression_loss: 2.1827 - classification_loss: 0.7950
 204/1000 [=====>........................] - ETA: 5:59 - loss: 2.9632 - regression_loss: 2.1720 - classification_loss: 0.7911
 205/1000 [=====>........................] - ETA: 5:59 - loss: 2.9487 - regression_loss: 2.1614 - classification_loss: 0.7873
 206/1000 [=====>........................] - ETA: 5:59 - loss: 2.9576 - regression_loss: 2.1664 - classification_loss: 0.7912
 207/1000 [=====>........................] - ETA: 5:58 - loss: 2.9671 - regression_loss: 2.1730 - classification_loss: 0.7941
 208/1000 [=====>........................] - ETA: 5:58 - loss: 2.9695 - regression_loss: 2.1742 - classification_loss: 0.7953
 209/1000 [=====>........................] - ETA: 5:57 - loss: 2.9749 - regression_loss: 2.1809 - classification_loss: 0.7940
 210/1000 [=====>........................] - ETA: 5:57 - loss: 2.9771 - regression_loss: 2.1835 - classification_loss: 0.7937
 211/1000 [=====>........................] - ETA: 5:56 - loss: 2.9755 - regression_loss: 2.1817 - classification_loss: 0.7938
 212/1000 [=====>........................] - ETA: 5:56 - loss: 2.9750 - regression_loss: 2.1805 - classification_loss: 0.7945
 213/1000 [=====>........................] - ETA: 5:55 - loss: 2.9883 - regression_loss: 2.1878 - classification_loss: 0.8005
 214/1000 [=====>........................] - ETA: 5:55 - loss: 2.9744 - regression_loss: 2.1776 - classification_loss: 0.7968
 215/1000 [=====>........................] - ETA: 5:55 - loss: 2.9776 - regression_loss: 2.1801 - classification_loss: 0.7976
 216/1000 [=====>........................] - ETA: 5:54 - loss: 2.9844 - regression_loss: 2.1870 - classification_loss: 0.7974
 217/1000 [=====>........................] - ETA: 5:54 - loss: 2.9916 - regression_loss: 2.1891 - classification_loss: 0.8025
 218/1000 [=====>........................] - ETA: 5:53 - loss: 3.0021 - regression_loss: 2.1996 - classification_loss: 0.8025
 219/1000 [=====>........................] - ETA: 5:53 - loss: 3.0139 - regression_loss: 2.2068 - classification_loss: 0.8071
 220/1000 [=====>........................] - ETA: 5:52 - loss: 3.0002 - regression_loss: 2.1967 - classification_loss: 0.8035
 221/1000 [=====>........................] - ETA: 5:52 - loss: 3.0032 - regression_loss: 2.2006 - classification_loss: 0.8026
 222/1000 [=====>........................] - ETA: 5:51 - loss: 3.0138 - regression_loss: 2.2066 - classification_loss: 0.8072
 223/1000 [=====>........................] - ETA: 5:51 - loss: 3.0150 - regression_loss: 2.2097 - classification_loss: 0.8053
 224/1000 [=====>........................] - ETA: 5:51 - loss: 3.0117 - regression_loss: 2.2066 - classification_loss: 0.8051
 225/1000 [=====>........................] - ETA: 5:50 - loss: 3.0161 - regression_loss: 2.2092 - classification_loss: 0.8069
 226/1000 [=====>........................] - ETA: 5:50 - loss: 3.0191 - regression_loss: 2.2130 - classification_loss: 0.8061
 227/1000 [=====>........................] - ETA: 5:49 - loss: 3.0243 - regression_loss: 2.2173 - classification_loss: 0.8070
 228/1000 [=====>........................] - ETA: 5:49 - loss: 3.0110 - regression_loss: 2.2076 - classification_loss: 0.8034
 229/1000 [=====>........................] - ETA: 5:48 - loss: 3.0175 - regression_loss: 2.2122 - classification_loss: 0.8053
 230/1000 [=====>........................] - ETA: 5:48 - loss: 3.0044 - regression_loss: 2.2026 - classification_loss: 0.8018
 231/1000 [=====>........................] - ETA: 5:47 - loss: 2.9914 - regression_loss: 2.1931 - classification_loss: 0.7983
 232/1000 [=====>........................] - ETA: 5:47 - loss: 2.9785 - regression_loss: 2.1836 - classification_loss: 0.7949
 233/1000 [=====>........................] - ETA: 5:46 - loss: 2.9865 - regression_loss: 2.1893 - classification_loss: 0.7972
 234/1000 [======>.......................] - ETA: 5:46 - loss: 2.9838 - regression_loss: 2.1887 - classification_loss: 0.7950
 235/1000 [======>.......................] - ETA: 5:45 - loss: 2.9712 - regression_loss: 2.1794 - classification_loss: 0.7918
 236/1000 [======>.......................] - ETA: 5:45 - loss: 2.9586 - regression_loss: 2.1702 - classification_loss: 0.7884
 237/1000 [======>.......................] - ETA: 5:44 - loss: 2.9634 - regression_loss: 2.1716 - classification_loss: 0.7919
 238/1000 [======>.......................] - ETA: 5:44 - loss: 2.9651 - regression_loss: 2.1722 - classification_loss: 0.7929
 239/1000 [======>.......................] - ETA: 5:44 - loss: 2.9694 - regression_loss: 2.1746 - classification_loss: 0.7948
 240/1000 [======>.......................] - ETA: 5:43 - loss: 2.9760 - regression_loss: 2.1773 - classification_loss: 0.7987
 241/1000 [======>.......................] - ETA: 5:43 - loss: 2.9779 - regression_loss: 2.1792 - classification_loss: 0.7987
 242/1000 [======>.......................] - ETA: 5:42 - loss: 2.9796 - regression_loss: 2.1827 - classification_loss: 0.7969
 243/1000 [======>.......................] - ETA: 5:42 - loss: 2.9820 - regression_loss: 2.1846 - classification_loss: 0.7974
 244/1000 [======>.......................] - ETA: 5:41 - loss: 2.9698 - regression_loss: 2.1757 - classification_loss: 0.7941
 245/1000 [======>.......................] - ETA: 5:41 - loss: 2.9684 - regression_loss: 2.1745 - classification_loss: 0.7940
 246/1000 [======>.......................] - ETA: 5:40 - loss: 2.9848 - regression_loss: 2.1868 - classification_loss: 0.7981
 247/1000 [======>.......................] - ETA: 5:40 - loss: 2.9964 - regression_loss: 2.1950 - classification_loss: 0.8014
 248/1000 [======>.......................] - ETA: 5:39 - loss: 3.0026 - regression_loss: 2.2005 - classification_loss: 0.8021
 249/1000 [======>.......................] - ETA: 5:39 - loss: 3.0039 - regression_loss: 2.2029 - classification_loss: 0.8011
 250/1000 [======>.......................] - ETA: 5:38 - loss: 3.0106 - regression_loss: 2.2082 - classification_loss: 0.8024
 251/1000 [======>.......................] - ETA: 5:38 - loss: 3.0128 - regression_loss: 2.2096 - classification_loss: 0.8032
 252/1000 [======>.......................] - ETA: 5:38 - loss: 3.0228 - regression_loss: 2.2189 - classification_loss: 0.8038
 253/1000 [======>.......................] - ETA: 5:37 - loss: 3.0223 - regression_loss: 2.2190 - classification_loss: 0.8033
 254/1000 [======>.......................] - ETA: 5:37 - loss: 3.0303 - regression_loss: 2.2240 - classification_loss: 0.8063
 255/1000 [======>.......................] - ETA: 5:36 - loss: 3.0334 - regression_loss: 2.2256 - classification_loss: 0.8078
 256/1000 [======>.......................] - ETA: 5:36 - loss: 3.0363 - regression_loss: 2.2260 - classification_loss: 0.8102
 257/1000 [======>.......................] - ETA: 5:35 - loss: 3.0376 - regression_loss: 2.2280 - classification_loss: 0.8096
 258/1000 [======>.......................] - ETA: 5:35 - loss: 3.0348 - regression_loss: 2.2265 - classification_loss: 0.8083
 259/1000 [======>.......................] - ETA: 5:34 - loss: 3.0336 - regression_loss: 2.2268 - classification_loss: 0.8068
 260/1000 [======>.......................] - ETA: 5:34 - loss: 3.0351 - regression_loss: 2.2285 - classification_loss: 0.8066
 261/1000 [======>.......................] - ETA: 5:33 - loss: 3.0409 - regression_loss: 2.2340 - classification_loss: 0.8069
 262/1000 [======>.......................] - ETA: 5:33 - loss: 3.0592 - regression_loss: 2.2255 - classification_loss: 0.8337
 263/1000 [======>.......................] - ETA: 5:33 - loss: 3.0493 - regression_loss: 2.2170 - classification_loss: 0.8323
 264/1000 [======>.......................] - ETA: 5:32 - loss: 3.0567 - regression_loss: 2.2241 - classification_loss: 0.8326
 265/1000 [======>.......................] - ETA: 5:32 - loss: 3.0475 - regression_loss: 2.2157 - classification_loss: 0.8318
 266/1000 [======>.......................] - ETA: 5:31 - loss: 3.0517 - regression_loss: 2.2208 - classification_loss: 0.8309
 267/1000 [=======>......................] - ETA: 5:31 - loss: 3.0469 - regression_loss: 2.2179 - classification_loss: 0.8289
 268/1000 [=======>......................] - ETA: 5:30 - loss: 3.0530 - regression_loss: 2.2234 - classification_loss: 0.8296
 269/1000 [=======>......................] - ETA: 5:30 - loss: 3.0553 - regression_loss: 2.2247 - classification_loss: 0.8306
 270/1000 [=======>......................] - ETA: 5:29 - loss: 3.0611 - regression_loss: 2.2289 - classification_loss: 0.8322
 271/1000 [=======>......................] - ETA: 5:29 - loss: 3.0675 - regression_loss: 2.2344 - classification_loss: 0.8330
 272/1000 [=======>......................] - ETA: 5:29 - loss: 3.0562 - regression_loss: 2.2262 - classification_loss: 0.8300
 273/1000 [=======>......................] - ETA: 5:28 - loss: 3.0451 - regression_loss: 2.2181 - classification_loss: 0.8270
 274/1000 [=======>......................] - ETA: 5:28 - loss: 3.0415 - regression_loss: 2.2154 - classification_loss: 0.8261
 275/1000 [=======>......................] - ETA: 5:27 - loss: 3.0351 - regression_loss: 2.2073 - classification_loss: 0.8278
 276/1000 [=======>......................] - ETA: 5:27 - loss: 3.0241 - regression_loss: 2.1993 - classification_loss: 0.8248
 277/1000 [=======>......................] - ETA: 5:26 - loss: 3.0246 - regression_loss: 2.2009 - classification_loss: 0.8237
 278/1000 [=======>......................] - ETA: 5:26 - loss: 3.0272 - regression_loss: 2.2038 - classification_loss: 0.8234
 279/1000 [=======>......................] - ETA: 5:25 - loss: 3.0258 - regression_loss: 2.2037 - classification_loss: 0.8221
 280/1000 [=======>......................] - ETA: 5:25 - loss: 3.0295 - regression_loss: 2.2073 - classification_loss: 0.8222
 281/1000 [=======>......................] - ETA: 5:24 - loss: 3.0362 - regression_loss: 2.2126 - classification_loss: 0.8235
 282/1000 [=======>......................] - ETA: 5:24 - loss: 3.0254 - regression_loss: 2.2048 - classification_loss: 0.8206
 283/1000 [=======>......................] - ETA: 5:23 - loss: 3.0310 - regression_loss: 2.2079 - classification_loss: 0.8231
 284/1000 [=======>......................] - ETA: 5:23 - loss: 3.0325 - regression_loss: 2.2100 - classification_loss: 0.8224
 285/1000 [=======>......................] - ETA: 5:23 - loss: 3.0350 - regression_loss: 2.2103 - classification_loss: 0.8246
 286/1000 [=======>......................] - ETA: 5:22 - loss: 3.0244 - regression_loss: 2.2026 - classification_loss: 0.8218
 287/1000 [=======>......................] - ETA: 5:22 - loss: 3.0146 - regression_loss: 2.1949 - classification_loss: 0.8197
 288/1000 [=======>......................] - ETA: 5:21 - loss: 3.0181 - regression_loss: 2.1978 - classification_loss: 0.8203
 289/1000 [=======>......................] - ETA: 5:21 - loss: 3.0180 - regression_loss: 2.1978 - classification_loss: 0.8202
 290/1000 [=======>......................] - ETA: 5:20 - loss: 3.0244 - regression_loss: 2.2033 - classification_loss: 0.8211
 291/1000 [=======>......................] - ETA: 5:20 - loss: 3.0284 - regression_loss: 2.2054 - classification_loss: 0.8230
 292/1000 [=======>......................] - ETA: 5:19 - loss: 3.0340 - regression_loss: 2.2080 - classification_loss: 0.8260
 293/1000 [=======>......................] - ETA: 5:19 - loss: 3.0424 - regression_loss: 2.2133 - classification_loss: 0.8290
 294/1000 [=======>......................] - ETA: 5:18 - loss: 3.0401 - regression_loss: 2.2116 - classification_loss: 0.8284
 295/1000 [=======>......................] - ETA: 5:18 - loss: 3.0298 - regression_loss: 2.2041 - classification_loss: 0.8256
 296/1000 [=======>......................] - ETA: 5:18 - loss: 3.0285 - regression_loss: 2.2035 - classification_loss: 0.8250
 297/1000 [=======>......................] - ETA: 5:17 - loss: 3.0289 - regression_loss: 2.2038 - classification_loss: 0.8252
 298/1000 [=======>......................] - ETA: 5:17 - loss: 3.0189 - regression_loss: 2.1964 - classification_loss: 0.8225
 299/1000 [=======>......................] - ETA: 5:16 - loss: 3.0227 - regression_loss: 2.2000 - classification_loss: 0.8227
 300/1000 [========>.....................] - ETA: 5:16 - loss: 3.0291 - regression_loss: 2.2044 - classification_loss: 0.8247
 301/1000 [========>.....................] - ETA: 5:15 - loss: 3.0334 - regression_loss: 2.2094 - classification_loss: 0.8241
 302/1000 [========>.....................] - ETA: 5:15 - loss: 3.0346 - regression_loss: 2.2107 - classification_loss: 0.8238
 303/1000 [========>.....................] - ETA: 5:14 - loss: 3.0335 - regression_loss: 2.2109 - classification_loss: 0.8227
 304/1000 [========>.....................] - ETA: 5:14 - loss: 3.0368 - regression_loss: 2.2140 - classification_loss: 0.8228
 305/1000 [========>.....................] - ETA: 5:13 - loss: 3.0268 - regression_loss: 2.2068 - classification_loss: 0.8201
 306/1000 [========>.....................] - ETA: 5:13 - loss: 3.0275 - regression_loss: 2.2063 - classification_loss: 0.8213
 307/1000 [========>.....................] - ETA: 5:12 - loss: 3.0275 - regression_loss: 2.2068 - classification_loss: 0.8208
 308/1000 [========>.....................] - ETA: 5:12 - loss: 3.0333 - regression_loss: 2.2128 - classification_loss: 0.8205
 309/1000 [========>.....................] - ETA: 5:12 - loss: 3.0353 - regression_loss: 2.2153 - classification_loss: 0.8201
 310/1000 [========>.....................] - ETA: 5:11 - loss: 3.0400 - regression_loss: 2.2184 - classification_loss: 0.8216
 311/1000 [========>.....................] - ETA: 5:11 - loss: 3.0459 - regression_loss: 2.2225 - classification_loss: 0.8235
 312/1000 [========>.....................] - ETA: 5:10 - loss: 3.0437 - regression_loss: 2.2203 - classification_loss: 0.8233
 313/1000 [========>.....................] - ETA: 5:10 - loss: 3.0429 - regression_loss: 2.2198 - classification_loss: 0.8231
 314/1000 [========>.....................] - ETA: 5:09 - loss: 3.0476 - regression_loss: 2.2233 - classification_loss: 0.8244
 315/1000 [========>.....................] - ETA: 5:09 - loss: 3.0379 - regression_loss: 2.2162 - classification_loss: 0.8217
 316/1000 [========>.....................] - ETA: 5:08 - loss: 3.0400 - regression_loss: 2.2177 - classification_loss: 0.8224
 317/1000 [========>.....................] - ETA: 5:08 - loss: 3.0406 - regression_loss: 2.2173 - classification_loss: 0.8233
 318/1000 [========>.....................] - ETA: 5:08 - loss: 3.0435 - regression_loss: 2.2196 - classification_loss: 0.8239
 319/1000 [========>.....................] - ETA: 5:07 - loss: 3.0469 - regression_loss: 2.2236 - classification_loss: 0.8232
 320/1000 [========>.....................] - ETA: 5:07 - loss: 3.0515 - regression_loss: 2.2267 - classification_loss: 0.8248
 321/1000 [========>.....................] - ETA: 5:06 - loss: 3.0560 - regression_loss: 2.2309 - classification_loss: 0.8252
 322/1000 [========>.....................] - ETA: 5:06 - loss: 3.0591 - regression_loss: 2.2337 - classification_loss: 0.8254
 323/1000 [========>.....................] - ETA: 5:05 - loss: 3.0630 - regression_loss: 2.2380 - classification_loss: 0.8250
 324/1000 [========>.....................] - ETA: 5:05 - loss: 3.0535 - regression_loss: 2.2311 - classification_loss: 0.8224
 325/1000 [========>.....................] - ETA: 5:04 - loss: 3.0559 - regression_loss: 2.2327 - classification_loss: 0.8232
 326/1000 [========>.....................] - ETA: 5:04 - loss: 3.0574 - regression_loss: 2.2349 - classification_loss: 0.8225
 327/1000 [========>.....................] - ETA: 5:04 - loss: 3.0489 - regression_loss: 2.2281 - classification_loss: 0.8209
 328/1000 [========>.....................] - ETA: 5:03 - loss: 3.0492 - regression_loss: 2.2279 - classification_loss: 0.8214
 329/1000 [========>.....................] - ETA: 5:03 - loss: 3.0400 - regression_loss: 2.2211 - classification_loss: 0.8189
 330/1000 [========>.....................] - ETA: 5:02 - loss: 3.0308 - regression_loss: 2.2144 - classification_loss: 0.8164
 331/1000 [========>.....................] - ETA: 5:02 - loss: 3.0294 - regression_loss: 2.2140 - classification_loss: 0.8153
 332/1000 [========>.....................] - ETA: 5:01 - loss: 3.0305 - regression_loss: 2.2160 - classification_loss: 0.8145
 333/1000 [========>.....................] - ETA: 5:01 - loss: 3.0347 - regression_loss: 2.2190 - classification_loss: 0.8157
 334/1000 [=========>....................] - ETA: 5:00 - loss: 3.0268 - regression_loss: 2.2123 - classification_loss: 0.8145
 335/1000 [=========>....................] - ETA: 5:00 - loss: 3.0180 - regression_loss: 2.2057 - classification_loss: 0.8123
 336/1000 [=========>....................] - ETA: 5:00 - loss: 3.0160 - regression_loss: 2.2036 - classification_loss: 0.8124
 337/1000 [=========>....................] - ETA: 4:59 - loss: 3.0201 - regression_loss: 2.2049 - classification_loss: 0.8152
 338/1000 [=========>....................] - ETA: 4:59 - loss: 3.0209 - regression_loss: 2.2066 - classification_loss: 0.8143
 339/1000 [=========>....................] - ETA: 4:58 - loss: 3.0207 - regression_loss: 2.2066 - classification_loss: 0.8141
 340/1000 [=========>....................] - ETA: 4:58 - loss: 3.0119 - regression_loss: 2.2001 - classification_loss: 0.8117
 341/1000 [=========>....................] - ETA: 4:57 - loss: 3.0142 - regression_loss: 2.2012 - classification_loss: 0.8130
 342/1000 [=========>....................] - ETA: 4:57 - loss: 3.0187 - regression_loss: 2.2059 - classification_loss: 0.8128
 343/1000 [=========>....................] - ETA: 4:56 - loss: 3.0234 - regression_loss: 2.2089 - classification_loss: 0.8145
 344/1000 [=========>....................] - ETA: 4:56 - loss: 3.0241 - regression_loss: 2.2088 - classification_loss: 0.8153
 345/1000 [=========>....................] - ETA: 4:55 - loss: 3.0255 - regression_loss: 2.2107 - classification_loss: 0.8148
 346/1000 [=========>....................] - ETA: 4:55 - loss: 3.0238 - regression_loss: 2.2084 - classification_loss: 0.8154
 347/1000 [=========>....................] - ETA: 4:55 - loss: 3.0293 - regression_loss: 2.2111 - classification_loss: 0.8182
 348/1000 [=========>....................] - ETA: 4:54 - loss: 3.0306 - regression_loss: 2.2122 - classification_loss: 0.8184
 349/1000 [=========>....................] - ETA: 4:54 - loss: 3.0318 - regression_loss: 2.2138 - classification_loss: 0.8180
 350/1000 [=========>....................] - ETA: 4:53 - loss: 3.0232 - regression_loss: 2.2075 - classification_loss: 0.8157
 351/1000 [=========>....................] - ETA: 4:53 - loss: 3.0228 - regression_loss: 2.2078 - classification_loss: 0.8151
 352/1000 [=========>....................] - ETA: 4:52 - loss: 3.0142 - regression_loss: 2.2015 - classification_loss: 0.8127
 353/1000 [=========>....................] - ETA: 4:52 - loss: 3.0153 - regression_loss: 2.2025 - classification_loss: 0.8129
 354/1000 [=========>....................] - ETA: 4:51 - loss: 3.0179 - regression_loss: 2.2046 - classification_loss: 0.8133
 355/1000 [=========>....................] - ETA: 4:51 - loss: 3.0274 - regression_loss: 2.2088 - classification_loss: 0.8186
 356/1000 [=========>....................] - ETA: 4:51 - loss: 3.0281 - regression_loss: 2.2102 - classification_loss: 0.8179
 357/1000 [=========>....................] - ETA: 4:50 - loss: 3.0280 - regression_loss: 2.2099 - classification_loss: 0.8181
 358/1000 [=========>....................] - ETA: 4:50 - loss: 3.0319 - regression_loss: 2.2117 - classification_loss: 0.8202
 359/1000 [=========>....................] - ETA: 4:49 - loss: 3.0411 - regression_loss: 2.2189 - classification_loss: 0.8222
 360/1000 [=========>....................] - ETA: 4:49 - loss: 3.0410 - regression_loss: 2.2198 - classification_loss: 0.8212
 361/1000 [=========>....................] - ETA: 4:48 - loss: 3.0410 - regression_loss: 2.2195 - classification_loss: 0.8216
 362/1000 [=========>....................] - ETA: 4:48 - loss: 3.0326 - regression_loss: 2.2133 - classification_loss: 0.8193
 363/1000 [=========>....................] - ETA: 4:47 - loss: 3.0362 - regression_loss: 2.2151 - classification_loss: 0.8211
 364/1000 [=========>....................] - ETA: 4:47 - loss: 3.0355 - regression_loss: 2.2149 - classification_loss: 0.8206
 365/1000 [=========>....................] - ETA: 4:47 - loss: 3.0386 - regression_loss: 2.2166 - classification_loss: 0.8219
 366/1000 [=========>....................] - ETA: 4:46 - loss: 3.0450 - regression_loss: 2.2207 - classification_loss: 0.8243
 367/1000 [==========>...................] - ETA: 4:46 - loss: 3.0490 - regression_loss: 2.2227 - classification_loss: 0.8263
 368/1000 [==========>...................] - ETA: 4:45 - loss: 3.0407 - regression_loss: 2.2167 - classification_loss: 0.8241
 369/1000 [==========>...................] - ETA: 4:45 - loss: 3.0326 - regression_loss: 2.2106 - classification_loss: 0.8220
 370/1000 [==========>...................] - ETA: 4:44 - loss: 3.0245 - regression_loss: 2.2047 - classification_loss: 0.8198
 371/1000 [==========>...................] - ETA: 4:44 - loss: 3.0232 - regression_loss: 2.2046 - classification_loss: 0.8186
 372/1000 [==========>...................] - ETA: 4:43 - loss: 3.0259 - regression_loss: 2.2084 - classification_loss: 0.8175
 373/1000 [==========>...................] - ETA: 4:43 - loss: 3.0265 - regression_loss: 2.2093 - classification_loss: 0.8172
 374/1000 [==========>...................] - ETA: 4:42 - loss: 3.0185 - regression_loss: 2.2034 - classification_loss: 0.8151
 375/1000 [==========>...................] - ETA: 4:42 - loss: 3.0196 - regression_loss: 2.2040 - classification_loss: 0.8157
 376/1000 [==========>...................] - ETA: 4:42 - loss: 3.0216 - regression_loss: 2.2057 - classification_loss: 0.8159
 377/1000 [==========>...................] - ETA: 4:41 - loss: 3.0143 - regression_loss: 2.1999 - classification_loss: 0.8144
 378/1000 [==========>...................] - ETA: 4:41 - loss: 3.0064 - regression_loss: 2.1941 - classification_loss: 0.8123
 379/1000 [==========>...................] - ETA: 4:40 - loss: 3.0076 - regression_loss: 2.1943 - classification_loss: 0.8133
 380/1000 [==========>...................] - ETA: 4:40 - loss: 2.9997 - regression_loss: 2.1886 - classification_loss: 0.8111
 381/1000 [==========>...................] - ETA: 4:39 - loss: 2.9972 - regression_loss: 2.1873 - classification_loss: 0.8099
 382/1000 [==========>...................] - ETA: 4:39 - loss: 2.9987 - regression_loss: 2.1889 - classification_loss: 0.8098
 383/1000 [==========>...................] - ETA: 4:38 - loss: 2.9970 - regression_loss: 2.1884 - classification_loss: 0.8086
 384/1000 [==========>...................] - ETA: 4:38 - loss: 2.9892 - regression_loss: 2.1827 - classification_loss: 0.8065
 385/1000 [==========>...................] - ETA: 4:38 - loss: 2.9815 - regression_loss: 2.1771 - classification_loss: 0.8044
 386/1000 [==========>...................] - ETA: 4:37 - loss: 2.9742 - regression_loss: 2.1714 - classification_loss: 0.8028
 387/1000 [==========>...................] - ETA: 4:37 - loss: 2.9759 - regression_loss: 2.1738 - classification_loss: 0.8022
 388/1000 [==========>...................] - ETA: 4:36 - loss: 2.9683 - regression_loss: 2.1682 - classification_loss: 0.8001
 389/1000 [==========>...................] - ETA: 4:36 - loss: 2.9606 - regression_loss: 2.1626 - classification_loss: 0.7981
 390/1000 [==========>...................] - ETA: 4:35 - loss: 2.9670 - regression_loss: 2.1668 - classification_loss: 0.8003
 391/1000 [==========>...................] - ETA: 4:35 - loss: 2.9664 - regression_loss: 2.1664 - classification_loss: 0.8000
 392/1000 [==========>...................] - ETA: 4:34 - loss: 2.9732 - regression_loss: 2.1712 - classification_loss: 0.8020
 393/1000 [==========>...................] - ETA: 4:34 - loss: 2.9656 - regression_loss: 2.1657 - classification_loss: 0.7999
 394/1000 [==========>...................] - ETA: 4:33 - loss: 2.9581 - regression_loss: 2.1602 - classification_loss: 0.7979
 395/1000 [==========>...................] - ETA: 4:33 - loss: 2.9629 - regression_loss: 2.1625 - classification_loss: 0.8005
 396/1000 [==========>...................] - ETA: 4:33 - loss: 2.9555 - regression_loss: 2.1570 - classification_loss: 0.7985
 397/1000 [==========>...................] - ETA: 4:32 - loss: 2.9581 - regression_loss: 2.1577 - classification_loss: 0.8005
 398/1000 [==========>...................] - ETA: 4:32 - loss: 2.9624 - regression_loss: 2.1591 - classification_loss: 0.8032
 399/1000 [==========>...................] - ETA: 4:31 - loss: 2.9665 - regression_loss: 2.1599 - classification_loss: 0.8066
 400/1000 [===========>..................] - ETA: 4:31 - loss: 2.9685 - regression_loss: 2.1594 - classification_loss: 0.8091
 401/1000 [===========>..................] - ETA: 4:30 - loss: 2.9611 - regression_loss: 2.1540 - classification_loss: 0.8071
 402/1000 [===========>..................] - ETA: 4:30 - loss: 2.9538 - regression_loss: 2.1487 - classification_loss: 0.8051
 403/1000 [===========>..................] - ETA: 4:29 - loss: 2.9582 - regression_loss: 2.1500 - classification_loss: 0.8081
 404/1000 [===========>..................] - ETA: 4:29 - loss: 2.9508 - regression_loss: 2.1447 - classification_loss: 0.8061
 405/1000 [===========>..................] - ETA: 4:28 - loss: 2.9599 - regression_loss: 2.1492 - classification_loss: 0.8107
 406/1000 [===========>..................] - ETA: 4:28 - loss: 2.9620 - regression_loss: 2.1513 - classification_loss: 0.8107
 407/1000 [===========>..................] - ETA: 4:27 - loss: 2.9693 - regression_loss: 2.1561 - classification_loss: 0.8132
 408/1000 [===========>..................] - ETA: 4:27 - loss: 2.9620 - regression_loss: 2.1508 - classification_loss: 0.8112
 409/1000 [===========>..................] - ETA: 4:27 - loss: 2.9661 - regression_loss: 2.1528 - classification_loss: 0.8134
 410/1000 [===========>..................] - ETA: 4:26 - loss: 2.9589 - regression_loss: 2.1475 - classification_loss: 0.8114
 411/1000 [===========>..................] - ETA: 4:26 - loss: 2.9630 - regression_loss: 2.1487 - classification_loss: 0.8143
 412/1000 [===========>..................] - ETA: 4:25 - loss: 2.9624 - regression_loss: 2.1479 - classification_loss: 0.8145
 413/1000 [===========>..................] - ETA: 4:25 - loss: 2.9676 - regression_loss: 2.1510 - classification_loss: 0.8167
 414/1000 [===========>..................] - ETA: 4:24 - loss: 2.9707 - regression_loss: 2.1540 - classification_loss: 0.8167
 415/1000 [===========>..................] - ETA: 4:24 - loss: 2.9722 - regression_loss: 2.1555 - classification_loss: 0.8166
 416/1000 [===========>..................] - ETA: 4:23 - loss: 2.9650 - regression_loss: 2.1503 - classification_loss: 0.8147
 417/1000 [===========>..................] - ETA: 4:23 - loss: 2.9701 - regression_loss: 2.1554 - classification_loss: 0.8147
 418/1000 [===========>..................] - ETA: 4:23 - loss: 2.9763 - regression_loss: 2.1597 - classification_loss: 0.8166
 419/1000 [===========>..................] - ETA: 4:22 - loss: 2.9692 - regression_loss: 2.1545 - classification_loss: 0.8146
 420/1000 [===========>..................] - ETA: 4:22 - loss: 2.9733 - regression_loss: 2.1559 - classification_loss: 0.8174
 421/1000 [===========>..................] - ETA: 4:21 - loss: 2.9730 - regression_loss: 2.1562 - classification_loss: 0.8168
 422/1000 [===========>..................] - ETA: 4:21 - loss: 2.9660 - regression_loss: 2.1511 - classification_loss: 0.8149
 423/1000 [===========>..................] - ETA: 4:20 - loss: 2.9699 - regression_loss: 2.1534 - classification_loss: 0.8165
 424/1000 [===========>..................] - ETA: 4:20 - loss: 2.9719 - regression_loss: 2.1542 - classification_loss: 0.8177
 425/1000 [===========>..................] - ETA: 4:19 - loss: 2.9735 - regression_loss: 2.1550 - classification_loss: 0.8185
 426/1000 [===========>..................] - ETA: 4:19 - loss: 2.9745 - regression_loss: 2.1562 - classification_loss: 0.8183
 427/1000 [===========>..................] - ETA: 4:18 - loss: 2.9779 - regression_loss: 2.1600 - classification_loss: 0.8179
 428/1000 [===========>..................] - ETA: 4:18 - loss: 2.9814 - regression_loss: 2.1632 - classification_loss: 0.8181
 429/1000 [===========>..................] - ETA: 4:18 - loss: 2.9744 - regression_loss: 2.1582 - classification_loss: 0.8162
 430/1000 [===========>..................] - ETA: 4:17 - loss: 2.9675 - regression_loss: 2.1532 - classification_loss: 0.8143
 431/1000 [===========>..................] - ETA: 4:17 - loss: 2.9718 - regression_loss: 2.1575 - classification_loss: 0.8143
 432/1000 [===========>..................] - ETA: 4:16 - loss: 2.9738 - regression_loss: 2.1586 - classification_loss: 0.8152
 433/1000 [===========>..................] - ETA: 4:16 - loss: 2.9723 - regression_loss: 2.1577 - classification_loss: 0.8146
 434/1000 [============>.................] - ETA: 4:15 - loss: 2.9711 - regression_loss: 2.1573 - classification_loss: 0.8138
 435/1000 [============>.................] - ETA: 4:15 - loss: 2.9757 - regression_loss: 2.1589 - classification_loss: 0.8168
 436/1000 [============>.................] - ETA: 4:14 - loss: 2.9788 - regression_loss: 2.1625 - classification_loss: 0.8163
 437/1000 [============>.................] - ETA: 4:14 - loss: 2.9854 - regression_loss: 2.1677 - classification_loss: 0.8177
 438/1000 [============>.................] - ETA: 4:13 - loss: 2.9890 - regression_loss: 2.1700 - classification_loss: 0.8191
 439/1000 [============>.................] - ETA: 4:13 - loss: 2.9900 - regression_loss: 2.1712 - classification_loss: 0.8188
 440/1000 [============>.................] - ETA: 4:13 - loss: 2.9928 - regression_loss: 2.1735 - classification_loss: 0.8193
 441/1000 [============>.................] - ETA: 4:12 - loss: 2.9863 - regression_loss: 2.1686 - classification_loss: 0.8176
 442/1000 [============>.................] - ETA: 4:12 - loss: 2.9895 - regression_loss: 2.1724 - classification_loss: 0.8171
 443/1000 [============>.................] - ETA: 4:11 - loss: 2.9827 - regression_loss: 2.1675 - classification_loss: 0.8152
 444/1000 [============>.................] - ETA: 4:11 - loss: 2.9820 - regression_loss: 2.1676 - classification_loss: 0.8144
 445/1000 [============>.................] - ETA: 4:10 - loss: 2.9753 - regression_loss: 2.1627 - classification_loss: 0.8126
 446/1000 [============>.................] - ETA: 4:10 - loss: 2.9796 - regression_loss: 2.1669 - classification_loss: 0.8127
 447/1000 [============>.................] - ETA: 4:09 - loss: 2.9802 - regression_loss: 2.1677 - classification_loss: 0.8125
 448/1000 [============>.................] - ETA: 4:09 - loss: 2.9824 - regression_loss: 2.1692 - classification_loss: 0.8132
 449/1000 [============>.................] - ETA: 4:09 - loss: 2.9758 - regression_loss: 2.1644 - classification_loss: 0.8114
 450/1000 [============>.................] - ETA: 4:08 - loss: 2.9770 - regression_loss: 2.1659 - classification_loss: 0.8111
 451/1000 [============>.................] - ETA: 4:08 - loss: 2.9704 - regression_loss: 2.1611 - classification_loss: 0.8093
 452/1000 [============>.................] - ETA: 4:07 - loss: 2.9745 - regression_loss: 2.1639 - classification_loss: 0.8107
 453/1000 [============>.................] - ETA: 4:07 - loss: 2.9750 - regression_loss: 2.1639 - classification_loss: 0.8111
 454/1000 [============>.................] - ETA: 4:06 - loss: 2.9684 - regression_loss: 2.1591 - classification_loss: 0.8093
 455/1000 [============>.................] - ETA: 4:06 - loss: 2.9670 - regression_loss: 2.1587 - classification_loss: 0.8083
 456/1000 [============>.................] - ETA: 4:05 - loss: 2.9605 - regression_loss: 2.1539 - classification_loss: 0.8066
 457/1000 [============>.................] - ETA: 4:05 - loss: 2.9540 - regression_loss: 2.1492 - classification_loss: 0.8048
 458/1000 [============>.................] - ETA: 4:04 - loss: 2.9561 - regression_loss: 2.1512 - classification_loss: 0.8050
 459/1000 [============>.................] - ETA: 4:04 - loss: 2.9497 - regression_loss: 2.1465 - classification_loss: 0.8032
 460/1000 [============>.................] - ETA: 4:03 - loss: 2.9529 - regression_loss: 2.1483 - classification_loss: 0.8046
 461/1000 [============>.................] - ETA: 4:03 - loss: 2.9567 - regression_loss: 2.1519 - classification_loss: 0.8048
 462/1000 [============>.................] - ETA: 4:03 - loss: 2.9618 - regression_loss: 2.1568 - classification_loss: 0.8050
 463/1000 [============>.................] - ETA: 4:02 - loss: 2.9649 - regression_loss: 2.1577 - classification_loss: 0.8072
 464/1000 [============>.................] - ETA: 4:02 - loss: 2.9585 - regression_loss: 2.1530 - classification_loss: 0.8055
 465/1000 [============>.................] - ETA: 4:01 - loss: 2.9572 - regression_loss: 2.1527 - classification_loss: 0.8045
 466/1000 [============>.................] - ETA: 4:01 - loss: 2.9508 - regression_loss: 2.1481 - classification_loss: 0.8028
 467/1000 [=============>................] - ETA: 4:00 - loss: 2.9575 - regression_loss: 2.1528 - classification_loss: 0.8047
 468/1000 [=============>................] - ETA: 4:00 - loss: 2.9612 - regression_loss: 2.1552 - classification_loss: 0.8060
 469/1000 [=============>................] - ETA: 3:59 - loss: 2.9549 - regression_loss: 2.1506 - classification_loss: 0.8043
 470/1000 [=============>................] - ETA: 3:59 - loss: 2.9554 - regression_loss: 2.1503 - classification_loss: 0.8051
 471/1000 [=============>................] - ETA: 3:59 - loss: 2.9491 - regression_loss: 2.1457 - classification_loss: 0.8034
 472/1000 [=============>................] - ETA: 3:58 - loss: 2.9522 - regression_loss: 2.1490 - classification_loss: 0.8032
 473/1000 [=============>................] - ETA: 3:58 - loss: 2.9571 - regression_loss: 2.1533 - classification_loss: 0.8038
 474/1000 [=============>................] - ETA: 3:57 - loss: 2.9555 - regression_loss: 2.1523 - classification_loss: 0.8032
 475/1000 [=============>................] - ETA: 3:57 - loss: 2.9493 - regression_loss: 2.1478 - classification_loss: 0.8015
 476/1000 [=============>................] - ETA: 3:56 - loss: 2.9543 - regression_loss: 2.1512 - classification_loss: 0.8031
 477/1000 [=============>................] - ETA: 3:56 - loss: 2.9562 - regression_loss: 2.1519 - classification_loss: 0.8043
 478/1000 [=============>................] - ETA: 3:55 - loss: 2.9576 - regression_loss: 2.1540 - classification_loss: 0.8036
 479/1000 [=============>................] - ETA: 3:55 - loss: 2.9600 - regression_loss: 2.1561 - classification_loss: 0.8040
 480/1000 [=============>................] - ETA: 3:54 - loss: 2.9622 - regression_loss: 2.1576 - classification_loss: 0.8046
 481/1000 [=============>................] - ETA: 3:54 - loss: 2.9657 - regression_loss: 2.1602 - classification_loss: 0.8055
 482/1000 [=============>................] - ETA: 3:54 - loss: 2.9644 - regression_loss: 2.1598 - classification_loss: 0.8046
 483/1000 [=============>................] - ETA: 3:53 - loss: 2.9670 - regression_loss: 2.1610 - classification_loss: 0.8060
 484/1000 [=============>................] - ETA: 3:53 - loss: 2.9735 - regression_loss: 2.1654 - classification_loss: 0.8081
 485/1000 [=============>................] - ETA: 3:52 - loss: 2.9818 - regression_loss: 2.1719 - classification_loss: 0.8100
 486/1000 [=============>................] - ETA: 3:52 - loss: 2.9870 - regression_loss: 2.1752 - classification_loss: 0.8118
 487/1000 [=============>................] - ETA: 3:51 - loss: 2.9894 - regression_loss: 2.1760 - classification_loss: 0.8134
 488/1000 [=============>................] - ETA: 3:51 - loss: 2.9832 - regression_loss: 2.1715 - classification_loss: 0.8117
 489/1000 [=============>................] - ETA: 3:50 - loss: 2.9772 - regression_loss: 2.1671 - classification_loss: 0.8101
 490/1000 [=============>................] - ETA: 3:50 - loss: 2.9775 - regression_loss: 2.1682 - classification_loss: 0.8094
 491/1000 [=============>................] - ETA: 3:49 - loss: 2.9715 - regression_loss: 2.1637 - classification_loss: 0.8077
 492/1000 [=============>................] - ETA: 3:49 - loss: 2.9713 - regression_loss: 2.1639 - classification_loss: 0.8073
 493/1000 [=============>................] - ETA: 3:49 - loss: 2.9741 - regression_loss: 2.1651 - classification_loss: 0.8090
 494/1000 [=============>................] - ETA: 3:48 - loss: 2.9749 - regression_loss: 2.1655 - classification_loss: 0.8094
 495/1000 [=============>................] - ETA: 3:48 - loss: 2.9743 - regression_loss: 2.1649 - classification_loss: 0.8094
 496/1000 [=============>................] - ETA: 3:47 - loss: 2.9736 - regression_loss: 2.1652 - classification_loss: 0.8085
 497/1000 [=============>................] - ETA: 3:47 - loss: 2.9736 - regression_loss: 2.1662 - classification_loss: 0.8074
 498/1000 [=============>................] - ETA: 3:46 - loss: 2.9702 - regression_loss: 2.1619 - classification_loss: 0.8083
 499/1000 [=============>................] - ETA: 3:46 - loss: 2.9697 - regression_loss: 2.1612 - classification_loss: 0.8085
 500/1000 [==============>...............] - ETA: 3:45 - loss: 2.9675 - regression_loss: 2.1603 - classification_loss: 0.8073
 501/1000 [==============>...............] - ETA: 3:45 - loss: 2.9671 - regression_loss: 2.1607 - classification_loss: 0.8063
 502/1000 [==============>...............] - ETA: 3:45 - loss: 2.9668 - regression_loss: 2.1612 - classification_loss: 0.8056
 503/1000 [==============>...............] - ETA: 3:44 - loss: 2.9688 - regression_loss: 2.1622 - classification_loss: 0.8067
 504/1000 [==============>...............] - ETA: 3:44 - loss: 2.9718 - regression_loss: 2.1642 - classification_loss: 0.8076
 505/1000 [==============>...............] - ETA: 3:43 - loss: 2.9659 - regression_loss: 2.1599 - classification_loss: 0.8060
 506/1000 [==============>...............] - ETA: 3:43 - loss: 2.9601 - regression_loss: 2.1557 - classification_loss: 0.8044
 507/1000 [==============>...............] - ETA: 3:42 - loss: 2.9643 - regression_loss: 2.1602 - classification_loss: 0.8041
 508/1000 [==============>...............] - ETA: 3:42 - loss: 2.9646 - regression_loss: 2.1610 - classification_loss: 0.8036
 509/1000 [==============>...............] - ETA: 3:41 - loss: 2.9649 - regression_loss: 2.1619 - classification_loss: 0.8030
 510/1000 [==============>...............] - ETA: 3:41 - loss: 2.9591 - regression_loss: 2.1577 - classification_loss: 0.8014
 511/1000 [==============>...............] - ETA: 3:40 - loss: 2.9593 - regression_loss: 2.1579 - classification_loss: 0.8013
 512/1000 [==============>...............] - ETA: 3:40 - loss: 2.9584 - regression_loss: 2.1579 - classification_loss: 0.8006
 513/1000 [==============>...............] - ETA: 3:40 - loss: 2.9611 - regression_loss: 2.1593 - classification_loss: 0.8018
 514/1000 [==============>...............] - ETA: 3:39 - loss: 2.9610 - regression_loss: 2.1595 - classification_loss: 0.8015
 515/1000 [==============>...............] - ETA: 3:39 - loss: 2.9553 - regression_loss: 2.1553 - classification_loss: 0.8000
 516/1000 [==============>...............] - ETA: 3:38 - loss: 2.9582 - regression_loss: 2.1575 - classification_loss: 0.8006
 517/1000 [==============>...............] - ETA: 3:38 - loss: 2.9524 - regression_loss: 2.1534 - classification_loss: 0.7991
 518/1000 [==============>...............] - ETA: 3:37 - loss: 2.9566 - regression_loss: 2.1569 - classification_loss: 0.7997
 519/1000 [==============>...............] - ETA: 3:37 - loss: 2.9562 - regression_loss: 2.1564 - classification_loss: 0.7998
 520/1000 [==============>...............] - ETA: 3:36 - loss: 2.9561 - regression_loss: 2.1569 - classification_loss: 0.7992
 521/1000 [==============>...............] - ETA: 3:36 - loss: 2.9567 - regression_loss: 2.1577 - classification_loss: 0.7989
 522/1000 [==============>...............] - ETA: 3:35 - loss: 2.9584 - regression_loss: 2.1600 - classification_loss: 0.7984
 523/1000 [==============>...............] - ETA: 3:35 - loss: 2.9527 - regression_loss: 2.1559 - classification_loss: 0.7968
 524/1000 [==============>...............] - ETA: 3:35 - loss: 2.9471 - regression_loss: 2.1518 - classification_loss: 0.7953
 525/1000 [==============>...............] - ETA: 3:34 - loss: 2.9482 - regression_loss: 2.1533 - classification_loss: 0.7950
 526/1000 [==============>...............] - ETA: 3:34 - loss: 2.9524 - regression_loss: 2.1556 - classification_loss: 0.7967
 527/1000 [==============>...............] - ETA: 3:33 - loss: 2.9509 - regression_loss: 2.1550 - classification_loss: 0.7959
 528/1000 [==============>...............] - ETA: 3:33 - loss: 2.9518 - regression_loss: 2.1557 - classification_loss: 0.7961
 529/1000 [==============>...............] - ETA: 3:32 - loss: 2.9512 - regression_loss: 2.1553 - classification_loss: 0.7959
 530/1000 [==============>...............] - ETA: 3:32 - loss: 2.9557 - regression_loss: 2.1582 - classification_loss: 0.7975
 531/1000 [==============>...............] - ETA: 3:31 - loss: 2.9501 - regression_loss: 2.1541 - classification_loss: 0.7960
 532/1000 [==============>...............] - ETA: 3:31 - loss: 2.9497 - regression_loss: 2.1535 - classification_loss: 0.7962
 533/1000 [==============>...............] - ETA: 3:30 - loss: 2.9514 - regression_loss: 2.1557 - classification_loss: 0.7957
 534/1000 [===============>..............] - ETA: 3:30 - loss: 2.9507 - regression_loss: 2.1561 - classification_loss: 0.7946
 535/1000 [===============>..............] - ETA: 3:30 - loss: 2.9520 - regression_loss: 2.1567 - classification_loss: 0.7953
 536/1000 [===============>..............] - ETA: 3:29 - loss: 2.9532 - regression_loss: 2.1581 - classification_loss: 0.7951
 537/1000 [===============>..............] - ETA: 3:29 - loss: 2.9547 - regression_loss: 2.1581 - classification_loss: 0.7966
 538/1000 [===============>..............] - ETA: 3:28 - loss: 2.9577 - regression_loss: 2.1606 - classification_loss: 0.7971
 539/1000 [===============>..............] - ETA: 3:28 - loss: 2.9594 - regression_loss: 2.1623 - classification_loss: 0.7970
 540/1000 [===============>..............] - ETA: 3:27 - loss: 2.9645 - regression_loss: 2.1653 - classification_loss: 0.7991
 541/1000 [===============>..............] - ETA: 3:27 - loss: 2.9645 - regression_loss: 2.1660 - classification_loss: 0.7985
 542/1000 [===============>..............] - ETA: 3:26 - loss: 2.9686 - regression_loss: 2.1687 - classification_loss: 0.7999
 543/1000 [===============>..............] - ETA: 3:26 - loss: 2.9686 - regression_loss: 2.1689 - classification_loss: 0.7997
 544/1000 [===============>..............] - ETA: 3:26 - loss: 2.9682 - regression_loss: 2.1694 - classification_loss: 0.7988
 545/1000 [===============>..............] - ETA: 3:25 - loss: 2.9711 - regression_loss: 2.1713 - classification_loss: 0.7998
 546/1000 [===============>..............] - ETA: 3:25 - loss: 2.9712 - regression_loss: 2.1715 - classification_loss: 0.7997
 547/1000 [===============>..............] - ETA: 3:24 - loss: 2.9720 - regression_loss: 2.1720 - classification_loss: 0.8000
 548/1000 [===============>..............] - ETA: 3:24 - loss: 2.9745 - regression_loss: 2.1745 - classification_loss: 0.8000
 549/1000 [===============>..............] - ETA: 3:23 - loss: 2.9753 - regression_loss: 2.1750 - classification_loss: 0.8004
 550/1000 [===============>..............] - ETA: 3:23 - loss: 2.9699 - regression_loss: 2.1710 - classification_loss: 0.7989
 551/1000 [===============>..............] - ETA: 3:22 - loss: 2.9662 - regression_loss: 2.1671 - classification_loss: 0.7991
 552/1000 [===============>..............] - ETA: 3:22 - loss: 2.9608 - regression_loss: 2.1631 - classification_loss: 0.7977
 553/1000 [===============>..............] - ETA: 3:21 - loss: 2.9592 - regression_loss: 2.1625 - classification_loss: 0.7967
 554/1000 [===============>..............] - ETA: 3:21 - loss: 2.9538 - regression_loss: 2.1586 - classification_loss: 0.7952
 555/1000 [===============>..............] - ETA: 3:21 - loss: 2.9545 - regression_loss: 2.1599 - classification_loss: 0.7946
 556/1000 [===============>..............] - ETA: 3:20 - loss: 2.9523 - regression_loss: 2.1587 - classification_loss: 0.7937
 557/1000 [===============>..............] - ETA: 3:20 - loss: 2.9548 - regression_loss: 2.1612 - classification_loss: 0.7936
 558/1000 [===============>..............] - ETA: 3:19 - loss: 2.9495 - regression_loss: 2.1573 - classification_loss: 0.7922
 559/1000 [===============>..............] - ETA: 3:19 - loss: 2.9532 - regression_loss: 2.1600 - classification_loss: 0.7932
 560/1000 [===============>..............] - ETA: 3:18 - loss: 2.9559 - regression_loss: 2.1625 - classification_loss: 0.7934
 561/1000 [===============>..............] - ETA: 3:18 - loss: 2.9557 - regression_loss: 2.1624 - classification_loss: 0.7933
 562/1000 [===============>..............] - ETA: 3:17 - loss: 2.9556 - regression_loss: 2.1630 - classification_loss: 0.7925
 563/1000 [===============>..............] - ETA: 3:17 - loss: 2.9504 - regression_loss: 2.1592 - classification_loss: 0.7912
 564/1000 [===============>..............] - ETA: 3:17 - loss: 2.9518 - regression_loss: 2.1609 - classification_loss: 0.7909
 565/1000 [===============>..............] - ETA: 3:16 - loss: 2.9525 - regression_loss: 2.1617 - classification_loss: 0.7907
 566/1000 [===============>..............] - ETA: 3:16 - loss: 2.9540 - regression_loss: 2.1640 - classification_loss: 0.7900
 567/1000 [================>.............] - ETA: 3:15 - loss: 2.9570 - regression_loss: 2.1667 - classification_loss: 0.7903
 568/1000 [================>.............] - ETA: 3:15 - loss: 2.9590 - regression_loss: 2.1687 - classification_loss: 0.7903
 569/1000 [================>.............] - ETA: 3:14 - loss: 2.9598 - regression_loss: 2.1701 - classification_loss: 0.7897
 570/1000 [================>.............] - ETA: 3:14 - loss: 2.9599 - regression_loss: 2.1692 - classification_loss: 0.7907
 571/1000 [================>.............] - ETA: 3:13 - loss: 2.9635 - regression_loss: 2.1726 - classification_loss: 0.7909
 572/1000 [================>.............] - ETA: 3:13 - loss: 2.9644 - regression_loss: 2.1728 - classification_loss: 0.7916
 573/1000 [================>.............] - ETA: 3:12 - loss: 2.9659 - regression_loss: 2.1742 - classification_loss: 0.7917
 574/1000 [================>.............] - ETA: 3:12 - loss: 2.9665 - regression_loss: 2.1743 - classification_loss: 0.7922
 575/1000 [================>.............] - ETA: 3:12 - loss: 2.9614 - regression_loss: 2.1705 - classification_loss: 0.7909
 576/1000 [================>.............] - ETA: 3:11 - loss: 2.9610 - regression_loss: 2.1701 - classification_loss: 0.7909
 577/1000 [================>.............] - ETA: 3:11 - loss: 2.9611 - regression_loss: 2.1700 - classification_loss: 0.7910
 578/1000 [================>.............] - ETA: 3:10 - loss: 2.9648 - regression_loss: 2.1735 - classification_loss: 0.7913
 579/1000 [================>.............] - ETA: 3:10 - loss: 2.9648 - regression_loss: 2.1737 - classification_loss: 0.7911
 580/1000 [================>.............] - ETA: 3:09 - loss: 2.9646 - regression_loss: 2.1742 - classification_loss: 0.7903
 581/1000 [================>.............] - ETA: 3:09 - loss: 2.9681 - regression_loss: 2.1771 - classification_loss: 0.7910
 582/1000 [================>.............] - ETA: 3:08 - loss: 2.9667 - regression_loss: 2.1755 - classification_loss: 0.7912
 583/1000 [================>.............] - ETA: 3:08 - loss: 2.9698 - regression_loss: 2.1775 - classification_loss: 0.7923
 584/1000 [================>.............] - ETA: 3:07 - loss: 2.9647 - regression_loss: 2.1738 - classification_loss: 0.7909
 585/1000 [================>.............] - ETA: 3:07 - loss: 2.9609 - regression_loss: 2.1701 - classification_loss: 0.7908
 586/1000 [================>.............] - ETA: 3:07 - loss: 2.9620 - regression_loss: 2.1715 - classification_loss: 0.7905
 587/1000 [================>.............] - ETA: 3:06 - loss: 2.9570 - regression_loss: 2.1678 - classification_loss: 0.7892
 588/1000 [================>.............] - ETA: 3:06 - loss: 2.9520 - regression_loss: 2.1641 - classification_loss: 0.7879
 589/1000 [================>.............] - ETA: 3:05 - loss: 2.9508 - regression_loss: 2.1638 - classification_loss: 0.7870
 590/1000 [================>.............] - ETA: 3:05 - loss: 2.9554 - regression_loss: 2.1661 - classification_loss: 0.7893
 591/1000 [================>.............] - ETA: 3:04 - loss: 2.9553 - regression_loss: 2.1660 - classification_loss: 0.7893
 592/1000 [================>.............] - ETA: 3:04 - loss: 2.9503 - regression_loss: 2.1623 - classification_loss: 0.7880
 593/1000 [================>.............] - ETA: 3:03 - loss: 2.9525 - regression_loss: 2.1649 - classification_loss: 0.7876
 594/1000 [================>.............] - ETA: 3:03 - loss: 2.9535 - regression_loss: 2.1653 - classification_loss: 0.7883
 595/1000 [================>.............] - ETA: 3:03 - loss: 2.9528 - regression_loss: 2.1652 - classification_loss: 0.7877
 596/1000 [================>.............] - ETA: 3:02 - loss: 2.9533 - regression_loss: 2.1657 - classification_loss: 0.7876
 597/1000 [================>.............] - ETA: 3:02 - loss: 2.9529 - regression_loss: 2.1652 - classification_loss: 0.7877
 598/1000 [================>.............] - ETA: 3:01 - loss: 2.9513 - regression_loss: 2.1644 - classification_loss: 0.7869
 599/1000 [================>.............] - ETA: 3:01 - loss: 2.9543 - regression_loss: 2.1674 - classification_loss: 0.7869
 600/1000 [=================>............] - ETA: 3:00 - loss: 2.9557 - regression_loss: 2.1692 - classification_loss: 0.7865
 601/1000 [=================>............] - ETA: 3:00 - loss: 2.9558 - regression_loss: 2.1694 - classification_loss: 0.7864
 602/1000 [=================>............] - ETA: 2:59 - loss: 2.9604 - regression_loss: 2.1733 - classification_loss: 0.7872
 603/1000 [=================>............] - ETA: 2:59 - loss: 2.9603 - regression_loss: 2.1731 - classification_loss: 0.7872
 604/1000 [=================>............] - ETA: 2:58 - loss: 2.9593 - regression_loss: 2.1728 - classification_loss: 0.7865
 605/1000 [=================>............] - ETA: 2:58 - loss: 2.9645 - regression_loss: 2.1761 - classification_loss: 0.7884
 606/1000 [=================>............] - ETA: 2:58 - loss: 2.9677 - regression_loss: 2.1789 - classification_loss: 0.7888
 607/1000 [=================>............] - ETA: 2:57 - loss: 2.9689 - regression_loss: 2.1791 - classification_loss: 0.7897
 608/1000 [=================>............] - ETA: 2:57 - loss: 2.9640 - regression_loss: 2.1755 - classification_loss: 0.7884
 609/1000 [=================>............] - ETA: 2:56 - loss: 2.9591 - regression_loss: 2.1720 - classification_loss: 0.7871
 610/1000 [=================>............] - ETA: 2:56 - loss: 2.9619 - regression_loss: 2.1739 - classification_loss: 0.7881
 611/1000 [=================>............] - ETA: 2:55 - loss: 2.9623 - regression_loss: 2.1737 - classification_loss: 0.7886
 612/1000 [=================>............] - ETA: 2:55 - loss: 2.9629 - regression_loss: 2.1751 - classification_loss: 0.7879
 613/1000 [=================>............] - ETA: 2:54 - loss: 2.9641 - regression_loss: 2.1766 - classification_loss: 0.7875
 614/1000 [=================>............] - ETA: 2:54 - loss: 2.9648 - regression_loss: 2.1772 - classification_loss: 0.7876
 615/1000 [=================>............] - ETA: 2:53 - loss: 2.9670 - regression_loss: 2.1784 - classification_loss: 0.7886
 616/1000 [=================>............] - ETA: 2:53 - loss: 2.9695 - regression_loss: 2.1808 - classification_loss: 0.7888
 617/1000 [=================>............] - ETA: 2:53 - loss: 2.9697 - regression_loss: 2.1816 - classification_loss: 0.7881
 618/1000 [=================>............] - ETA: 2:52 - loss: 2.9649 - regression_loss: 2.1781 - classification_loss: 0.7868
 619/1000 [=================>............] - ETA: 2:52 - loss: 2.9689 - regression_loss: 2.1819 - classification_loss: 0.7870
 620/1000 [=================>............] - ETA: 2:51 - loss: 2.9706 - regression_loss: 2.1837 - classification_loss: 0.7868
 621/1000 [=================>............] - ETA: 2:51 - loss: 2.9658 - regression_loss: 2.1802 - classification_loss: 0.7856
 622/1000 [=================>............] - ETA: 2:50 - loss: 2.9671 - regression_loss: 2.1812 - classification_loss: 0.7859
 623/1000 [=================>............] - ETA: 2:50 - loss: 2.9664 - regression_loss: 2.1814 - classification_loss: 0.7850
 624/1000 [=================>............] - ETA: 2:49 - loss: 2.9680 - regression_loss: 2.1833 - classification_loss: 0.7847
 625/1000 [=================>............] - ETA: 2:49 - loss: 2.9700 - regression_loss: 2.1851 - classification_loss: 0.7849
 626/1000 [=================>............] - ETA: 2:49 - loss: 2.9674 - regression_loss: 2.1816 - classification_loss: 0.7858
 627/1000 [=================>............] - ETA: 2:48 - loss: 2.9626 - regression_loss: 2.1781 - classification_loss: 0.7845
 628/1000 [=================>............] - ETA: 2:48 - loss: 2.9589 - regression_loss: 2.1747 - classification_loss: 0.7843
 629/1000 [=================>............] - ETA: 2:47 - loss: 2.9542 - regression_loss: 2.1712 - classification_loss: 0.7830
 630/1000 [=================>............] - ETA: 2:47 - loss: 2.9531 - regression_loss: 2.1707 - classification_loss: 0.7824
 631/1000 [=================>............] - ETA: 2:46 - loss: 2.9554 - regression_loss: 2.1727 - classification_loss: 0.7828
 632/1000 [=================>............] - ETA: 2:46 - loss: 2.9582 - regression_loss: 2.1742 - classification_loss: 0.7841
 633/1000 [=================>............] - ETA: 2:45 - loss: 2.9592 - regression_loss: 2.1758 - classification_loss: 0.7833
 634/1000 [==================>...........] - ETA: 2:45 - loss: 2.9599 - regression_loss: 2.1767 - classification_loss: 0.7832
 635/1000 [==================>...........] - ETA: 2:44 - loss: 2.9615 - regression_loss: 2.1765 - classification_loss: 0.7850
 636/1000 [==================>...........] - ETA: 2:44 - loss: 2.9612 - regression_loss: 2.1770 - classification_loss: 0.7843
 637/1000 [==================>...........] - ETA: 2:44 - loss: 2.9633 - regression_loss: 2.1786 - classification_loss: 0.7846
 638/1000 [==================>...........] - ETA: 2:43 - loss: 2.9650 - regression_loss: 2.1810 - classification_loss: 0.7840
 639/1000 [==================>...........] - ETA: 2:43 - loss: 2.9641 - regression_loss: 2.1798 - classification_loss: 0.7843
 640/1000 [==================>...........] - ETA: 2:42 - loss: 2.9595 - regression_loss: 2.1764 - classification_loss: 0.7831
 641/1000 [==================>...........] - ETA: 2:42 - loss: 2.9613 - regression_loss: 2.1787 - classification_loss: 0.7826
 642/1000 [==================>...........] - ETA: 2:41 - loss: 2.9616 - regression_loss: 2.1787 - classification_loss: 0.7830
 643/1000 [==================>...........] - ETA: 2:41 - loss: 2.9632 - regression_loss: 2.1803 - classification_loss: 0.7829
 644/1000 [==================>...........] - ETA: 2:40 - loss: 2.9637 - regression_loss: 2.1810 - classification_loss: 0.7828
 645/1000 [==================>...........] - ETA: 2:40 - loss: 2.9624 - regression_loss: 2.1804 - classification_loss: 0.7820
 646/1000 [==================>...........] - ETA: 2:40 - loss: 2.9585 - regression_loss: 2.1770 - classification_loss: 0.7816
 647/1000 [==================>...........] - ETA: 2:39 - loss: 2.9613 - regression_loss: 2.1792 - classification_loss: 0.7821
 648/1000 [==================>...........] - ETA: 2:39 - loss: 2.9622 - regression_loss: 2.1796 - classification_loss: 0.7826
 649/1000 [==================>...........] - ETA: 2:38 - loss: 2.9577 - regression_loss: 2.1763 - classification_loss: 0.7814
 650/1000 [==================>...........] - ETA: 2:38 - loss: 2.9536 - regression_loss: 2.1729 - classification_loss: 0.7806
 651/1000 [==================>...........] - ETA: 2:37 - loss: 2.9529 - regression_loss: 2.1731 - classification_loss: 0.7798
 652/1000 [==================>...........] - ETA: 2:37 - loss: 2.9519 - regression_loss: 2.1698 - classification_loss: 0.7821
 653/1000 [==================>...........] - ETA: 2:36 - loss: 2.9530 - regression_loss: 2.1700 - classification_loss: 0.7830
 654/1000 [==================>...........] - ETA: 2:36 - loss: 2.9485 - regression_loss: 2.1667 - classification_loss: 0.7818
 655/1000 [==================>...........] - ETA: 2:35 - loss: 2.9477 - regression_loss: 2.1661 - classification_loss: 0.7816
 656/1000 [==================>...........] - ETA: 2:35 - loss: 2.9460 - regression_loss: 2.1651 - classification_loss: 0.7809
 657/1000 [==================>...........] - ETA: 2:35 - loss: 2.9472 - regression_loss: 2.1652 - classification_loss: 0.7819
 658/1000 [==================>...........] - ETA: 2:34 - loss: 2.9488 - regression_loss: 2.1657 - classification_loss: 0.7831
 659/1000 [==================>...........] - ETA: 2:34 - loss: 2.9443 - regression_loss: 2.1624 - classification_loss: 0.7820
 660/1000 [==================>...........] - ETA: 2:33 - loss: 2.9459 - regression_loss: 2.1630 - classification_loss: 0.7828
 661/1000 [==================>...........] - ETA: 2:33 - loss: 2.9417 - regression_loss: 2.1598 - classification_loss: 0.7819
 662/1000 [==================>...........] - ETA: 2:32 - loss: 2.9424 - regression_loss: 2.1598 - classification_loss: 0.7827
 663/1000 [==================>...........] - ETA: 2:32 - loss: 2.9447 - regression_loss: 2.1610 - classification_loss: 0.7837
 664/1000 [==================>...........] - ETA: 2:31 - loss: 2.9460 - regression_loss: 2.1623 - classification_loss: 0.7837
 665/1000 [==================>...........] - ETA: 2:31 - loss: 2.9472 - regression_loss: 2.1627 - classification_loss: 0.7846
 666/1000 [==================>...........] - ETA: 2:30 - loss: 2.9502 - regression_loss: 2.1654 - classification_loss: 0.7848
 667/1000 [===================>..........] - ETA: 2:30 - loss: 2.9493 - regression_loss: 2.1652 - classification_loss: 0.7841
 668/1000 [===================>..........] - ETA: 2:30 - loss: 2.9501 - regression_loss: 2.1656 - classification_loss: 0.7845
 669/1000 [===================>..........] - ETA: 2:29 - loss: 2.9520 - regression_loss: 2.1669 - classification_loss: 0.7850
 670/1000 [===================>..........] - ETA: 2:29 - loss: 2.9476 - regression_loss: 2.1637 - classification_loss: 0.7839
 671/1000 [===================>..........] - ETA: 2:28 - loss: 2.9510 - regression_loss: 2.1659 - classification_loss: 0.7851
 672/1000 [===================>..........] - ETA: 2:28 - loss: 2.9490 - regression_loss: 2.1644 - classification_loss: 0.7846
 673/1000 [===================>..........] - ETA: 2:27 - loss: 2.9497 - regression_loss: 2.1651 - classification_loss: 0.7846
 674/1000 [===================>..........] - ETA: 2:27 - loss: 2.9506 - regression_loss: 2.1656 - classification_loss: 0.7849
 675/1000 [===================>..........] - ETA: 2:26 - loss: 2.9507 - regression_loss: 2.1661 - classification_loss: 0.7845
 676/1000 [===================>..........] - ETA: 2:26 - loss: 2.9533 - regression_loss: 2.1685 - classification_loss: 0.7848
 677/1000 [===================>..........] - ETA: 2:26 - loss: 2.9490 - regression_loss: 2.1653 - classification_loss: 0.7838
 678/1000 [===================>..........] - ETA: 2:25 - loss: 2.9500 - regression_loss: 2.1657 - classification_loss: 0.7843
 679/1000 [===================>..........] - ETA: 2:25 - loss: 2.9513 - regression_loss: 2.1669 - classification_loss: 0.7845
 680/1000 [===================>..........] - ETA: 2:24 - loss: 2.9506 - regression_loss: 2.1666 - classification_loss: 0.7840
 681/1000 [===================>..........] - ETA: 2:24 - loss: 2.9512 - regression_loss: 2.1672 - classification_loss: 0.7840
 682/1000 [===================>..........] - ETA: 2:23 - loss: 2.9531 - regression_loss: 2.1687 - classification_loss: 0.7844
 683/1000 [===================>..........] - ETA: 2:23 - loss: 2.9539 - regression_loss: 2.1697 - classification_loss: 0.7842
 684/1000 [===================>..........] - ETA: 2:22 - loss: 2.9562 - regression_loss: 2.1718 - classification_loss: 0.7844
 685/1000 [===================>..........] - ETA: 2:22 - loss: 2.9518 - regression_loss: 2.1686 - classification_loss: 0.7832
 686/1000 [===================>..........] - ETA: 2:21 - loss: 2.9505 - regression_loss: 2.1677 - classification_loss: 0.7828
 687/1000 [===================>..........] - ETA: 2:21 - loss: 2.9462 - regression_loss: 2.1646 - classification_loss: 0.7816
 688/1000 [===================>..........] - ETA: 2:21 - loss: 2.9488 - regression_loss: 2.1670 - classification_loss: 0.7818
 689/1000 [===================>..........] - ETA: 2:20 - loss: 2.9487 - regression_loss: 2.1674 - classification_loss: 0.7813
 690/1000 [===================>..........] - ETA: 2:20 - loss: 2.9514 - regression_loss: 2.1693 - classification_loss: 0.7821
 691/1000 [===================>..........] - ETA: 2:19 - loss: 2.9471 - regression_loss: 2.1662 - classification_loss: 0.7810
 692/1000 [===================>..........] - ETA: 2:19 - loss: 2.9490 - regression_loss: 2.1673 - classification_loss: 0.7817
 693/1000 [===================>..........] - ETA: 2:18 - loss: 2.9507 - regression_loss: 2.1687 - classification_loss: 0.7820
 694/1000 [===================>..........] - ETA: 2:18 - loss: 2.9523 - regression_loss: 2.1701 - classification_loss: 0.7822
 695/1000 [===================>..........] - ETA: 2:17 - loss: 2.9511 - regression_loss: 2.1694 - classification_loss: 0.7817
 696/1000 [===================>..........] - ETA: 2:17 - loss: 2.9531 - regression_loss: 2.1710 - classification_loss: 0.7820
 697/1000 [===================>..........] - ETA: 2:16 - loss: 2.9534 - regression_loss: 2.1714 - classification_loss: 0.7820
 698/1000 [===================>..........] - ETA: 2:16 - loss: 2.9528 - regression_loss: 2.1715 - classification_loss: 0.7814
 699/1000 [===================>..........] - ETA: 2:16 - loss: 2.9514 - regression_loss: 2.1704 - classification_loss: 0.7810
 700/1000 [====================>.........] - ETA: 2:15 - loss: 2.9471 - regression_loss: 2.1673 - classification_loss: 0.7798
 701/1000 [====================>.........] - ETA: 2:15 - loss: 2.9507 - regression_loss: 2.1709 - classification_loss: 0.7797
 702/1000 [====================>.........] - ETA: 2:14 - loss: 2.9548 - regression_loss: 2.1749 - classification_loss: 0.7799
 703/1000 [====================>.........] - ETA: 2:14 - loss: 2.9506 - regression_loss: 2.1718 - classification_loss: 0.7788
 704/1000 [====================>.........] - ETA: 2:13 - loss: 2.9538 - regression_loss: 2.1740 - classification_loss: 0.7799
 705/1000 [====================>.........] - ETA: 2:13 - loss: 2.9560 - regression_loss: 2.1753 - classification_loss: 0.7807
 706/1000 [====================>.........] - ETA: 2:12 - loss: 2.9518 - regression_loss: 2.1722 - classification_loss: 0.7796
 707/1000 [====================>.........] - ETA: 2:12 - loss: 2.9526 - regression_loss: 2.1713 - classification_loss: 0.7813
 708/1000 [====================>.........] - ETA: 2:11 - loss: 2.9518 - regression_loss: 2.1705 - classification_loss: 0.7813
 709/1000 [====================>.........] - ETA: 2:11 - loss: 2.9543 - regression_loss: 2.1717 - classification_loss: 0.7826
 710/1000 [====================>.........] - ETA: 2:11 - loss: 2.9547 - regression_loss: 2.1718 - classification_loss: 0.7830
 711/1000 [====================>.........] - ETA: 2:10 - loss: 2.9552 - regression_loss: 2.1715 - classification_loss: 0.7837
 712/1000 [====================>.........] - ETA: 2:10 - loss: 2.9559 - regression_loss: 2.1718 - classification_loss: 0.7840
 713/1000 [====================>.........] - ETA: 2:09 - loss: 2.9583 - regression_loss: 2.1737 - classification_loss: 0.7846
 714/1000 [====================>.........] - ETA: 2:09 - loss: 2.9571 - regression_loss: 2.1733 - classification_loss: 0.7839
 715/1000 [====================>.........] - ETA: 2:08 - loss: 2.9579 - regression_loss: 2.1743 - classification_loss: 0.7836
 716/1000 [====================>.........] - ETA: 2:08 - loss: 2.9571 - regression_loss: 2.1743 - classification_loss: 0.7828
 717/1000 [====================>.........] - ETA: 2:07 - loss: 2.9570 - regression_loss: 2.1747 - classification_loss: 0.7822
 718/1000 [====================>.........] - ETA: 2:07 - loss: 2.9595 - regression_loss: 2.1771 - classification_loss: 0.7825
 719/1000 [====================>.........] - ETA: 2:07 - loss: 2.9555 - regression_loss: 2.1740 - classification_loss: 0.7814
 720/1000 [====================>.........] - ETA: 2:06 - loss: 2.9549 - regression_loss: 2.1742 - classification_loss: 0.7807
 721/1000 [====================>.........] - ETA: 2:06 - loss: 2.9508 - regression_loss: 2.1712 - classification_loss: 0.7796
 722/1000 [====================>.........] - ETA: 2:05 - loss: 2.9515 - regression_loss: 2.1717 - classification_loss: 0.7798
 723/1000 [====================>.........] - ETA: 2:05 - loss: 2.9474 - regression_loss: 2.1687 - classification_loss: 0.7787
 724/1000 [====================>.........] - ETA: 2:04 - loss: 2.9484 - regression_loss: 2.1697 - classification_loss: 0.7787
 725/1000 [====================>.........] - ETA: 2:04 - loss: 2.9517 - regression_loss: 2.1720 - classification_loss: 0.7797
 726/1000 [====================>.........] - ETA: 2:03 - loss: 2.9545 - regression_loss: 2.1736 - classification_loss: 0.7808
 727/1000 [====================>.........] - ETA: 2:03 - loss: 2.9567 - regression_loss: 2.1746 - classification_loss: 0.7821
 728/1000 [====================>.........] - ETA: 2:02 - loss: 2.9573 - regression_loss: 2.1749 - classification_loss: 0.7824
 729/1000 [====================>.........] - ETA: 2:02 - loss: 2.9575 - regression_loss: 2.1753 - classification_loss: 0.7822
 730/1000 [====================>.........] - ETA: 2:02 - loss: 2.9584 - regression_loss: 2.1764 - classification_loss: 0.7820
 731/1000 [====================>.........] - ETA: 2:01 - loss: 2.9612 - regression_loss: 2.1780 - classification_loss: 0.7832
 732/1000 [====================>.........] - ETA: 2:01 - loss: 2.9571 - regression_loss: 2.1750 - classification_loss: 0.7821
 733/1000 [====================>.........] - ETA: 2:00 - loss: 2.9596 - regression_loss: 2.1765 - classification_loss: 0.7831
 734/1000 [=====================>........] - ETA: 2:00 - loss: 2.9624 - regression_loss: 2.1792 - classification_loss: 0.7832
 735/1000 [=====================>........] - ETA: 1:59 - loss: 2.9625 - regression_loss: 2.1793 - classification_loss: 0.7832
 736/1000 [=====================>........] - ETA: 1:59 - loss: 2.9648 - regression_loss: 2.1814 - classification_loss: 0.7834
 737/1000 [=====================>........] - ETA: 1:58 - loss: 2.9676 - regression_loss: 2.1842 - classification_loss: 0.7834
 738/1000 [=====================>........] - ETA: 1:58 - loss: 2.9704 - regression_loss: 2.1865 - classification_loss: 0.7839
 739/1000 [=====================>........] - ETA: 1:57 - loss: 2.9734 - regression_loss: 2.1893 - classification_loss: 0.7841
 740/1000 [=====================>........] - ETA: 1:57 - loss: 2.9736 - regression_loss: 2.1901 - classification_loss: 0.7835
 741/1000 [=====================>........] - ETA: 1:57 - loss: 2.9696 - regression_loss: 2.1871 - classification_loss: 0.7825
 742/1000 [=====================>........] - ETA: 1:56 - loss: 2.9689 - regression_loss: 2.1870 - classification_loss: 0.7819
 743/1000 [=====================>........] - ETA: 1:56 - loss: 2.9676 - regression_loss: 2.1863 - classification_loss: 0.7813
 744/1000 [=====================>........] - ETA: 1:55 - loss: 2.9636 - regression_loss: 2.1834 - classification_loss: 0.7802
 745/1000 [=====================>........] - ETA: 1:55 - loss: 2.9675 - regression_loss: 2.1857 - classification_loss: 0.7818
 746/1000 [=====================>........] - ETA: 1:54 - loss: 2.9635 - regression_loss: 2.1828 - classification_loss: 0.7808
 747/1000 [=====================>........] - ETA: 1:54 - loss: 2.9597 - regression_loss: 2.1798 - classification_loss: 0.7799
 748/1000 [=====================>........] - ETA: 1:53 - loss: 2.9594 - regression_loss: 2.1799 - classification_loss: 0.7795
 749/1000 [=====================>........] - ETA: 1:53 - loss: 2.9617 - regression_loss: 2.1811 - classification_loss: 0.7807
 750/1000 [=====================>........] - ETA: 1:52 - loss: 2.9625 - regression_loss: 2.1820 - classification_loss: 0.7805
 751/1000 [=====================>........] - ETA: 1:52 - loss: 2.9620 - regression_loss: 2.1819 - classification_loss: 0.7800
 752/1000 [=====================>........] - ETA: 1:52 - loss: 2.9580 - regression_loss: 2.1790 - classification_loss: 0.7790
 753/1000 [=====================>........] - ETA: 1:51 - loss: 2.9541 - regression_loss: 2.1761 - classification_loss: 0.7780
 754/1000 [=====================>........] - ETA: 1:51 - loss: 2.9502 - regression_loss: 2.1732 - classification_loss: 0.7769
 755/1000 [=====================>........] - ETA: 1:50 - loss: 2.9507 - regression_loss: 2.1730 - classification_loss: 0.7777
 756/1000 [=====================>........] - ETA: 1:50 - loss: 2.9532 - regression_loss: 2.1748 - classification_loss: 0.7784
 757/1000 [=====================>........] - ETA: 1:49 - loss: 2.9530 - regression_loss: 2.1746 - classification_loss: 0.7784
 758/1000 [=====================>........] - ETA: 1:49 - loss: 2.9563 - regression_loss: 2.1765 - classification_loss: 0.7798
 759/1000 [=====================>........] - ETA: 1:48 - loss: 2.9579 - regression_loss: 2.1781 - classification_loss: 0.7797
 760/1000 [=====================>........] - ETA: 1:48 - loss: 2.9583 - regression_loss: 2.1783 - classification_loss: 0.7799
 761/1000 [=====================>........] - ETA: 1:48 - loss: 2.9589 - regression_loss: 2.1788 - classification_loss: 0.7800
 762/1000 [=====================>........] - ETA: 1:47 - loss: 2.9622 - regression_loss: 2.1811 - classification_loss: 0.7811
 763/1000 [=====================>........] - ETA: 1:47 - loss: 2.9660 - regression_loss: 2.1835 - classification_loss: 0.7825
 764/1000 [=====================>........] - ETA: 1:46 - loss: 2.9652 - regression_loss: 2.1831 - classification_loss: 0.7821
 765/1000 [=====================>........] - ETA: 1:46 - loss: 2.9670 - regression_loss: 2.1839 - classification_loss: 0.7831
 766/1000 [=====================>........] - ETA: 1:45 - loss: 2.9656 - regression_loss: 2.1830 - classification_loss: 0.7827
 767/1000 [======================>.......] - ETA: 1:45 - loss: 2.9618 - regression_loss: 2.1801 - classification_loss: 0.7817
 768/1000 [======================>.......] - ETA: 1:44 - loss: 2.9579 - regression_loss: 2.1773 - classification_loss: 0.7806
 769/1000 [======================>.......] - ETA: 1:44 - loss: 2.9594 - regression_loss: 2.1786 - classification_loss: 0.7808
 770/1000 [======================>.......] - ETA: 1:43 - loss: 2.9556 - regression_loss: 2.1758 - classification_loss: 0.7798
 771/1000 [======================>.......] - ETA: 1:43 - loss: 2.9575 - regression_loss: 2.1772 - classification_loss: 0.7803
 772/1000 [======================>.......] - ETA: 1:43 - loss: 2.9595 - regression_loss: 2.1785 - classification_loss: 0.7810
 773/1000 [======================>.......] - ETA: 1:42 - loss: 2.9602 - regression_loss: 2.1791 - classification_loss: 0.7811
 774/1000 [======================>.......] - ETA: 1:42 - loss: 2.9617 - regression_loss: 2.1809 - classification_loss: 0.7809
 775/1000 [======================>.......] - ETA: 1:41 - loss: 2.9579 - regression_loss: 2.1781 - classification_loss: 0.7799
 776/1000 [======================>.......] - ETA: 1:41 - loss: 2.9604 - regression_loss: 2.1804 - classification_loss: 0.7800
 777/1000 [======================>.......] - ETA: 1:40 - loss: 2.9626 - regression_loss: 2.1813 - classification_loss: 0.7813
 778/1000 [======================>.......] - ETA: 1:40 - loss: 2.9646 - regression_loss: 2.1824 - classification_loss: 0.7822
 779/1000 [======================>.......] - ETA: 1:39 - loss: 2.9635 - regression_loss: 2.1820 - classification_loss: 0.7815
 780/1000 [======================>.......] - ETA: 1:39 - loss: 2.9667 - regression_loss: 2.1849 - classification_loss: 0.7818
 781/1000 [======================>.......] - ETA: 1:38 - loss: 2.9674 - regression_loss: 2.1857 - classification_loss: 0.7817
 782/1000 [======================>.......] - ETA: 1:38 - loss: 2.9678 - regression_loss: 2.1868 - classification_loss: 0.7810
 783/1000 [======================>.......] - ETA: 1:38 - loss: 2.9679 - regression_loss: 2.1871 - classification_loss: 0.7808
 784/1000 [======================>.......] - ETA: 1:37 - loss: 2.9676 - regression_loss: 2.1873 - classification_loss: 0.7803
 785/1000 [======================>.......] - ETA: 1:37 - loss: 2.9638 - regression_loss: 2.1845 - classification_loss: 0.7793
 786/1000 [======================>.......] - ETA: 1:36 - loss: 2.9638 - regression_loss: 2.1850 - classification_loss: 0.7788
 787/1000 [======================>.......] - ETA: 1:36 - loss: 2.9654 - regression_loss: 2.1861 - classification_loss: 0.7793
 788/1000 [======================>.......] - ETA: 1:35 - loss: 2.9687 - regression_loss: 2.1880 - classification_loss: 0.7807
 789/1000 [======================>.......] - ETA: 1:35 - loss: 2.9702 - regression_loss: 2.1900 - classification_loss: 0.7802
 790/1000 [======================>.......] - ETA: 1:34 - loss: 2.9696 - regression_loss: 2.1900 - classification_loss: 0.7795
 791/1000 [======================>.......] - ETA: 1:34 - loss: 2.9683 - regression_loss: 2.1894 - classification_loss: 0.7789
 792/1000 [======================>.......] - ETA: 1:33 - loss: 2.9645 - regression_loss: 2.1866 - classification_loss: 0.7779
 793/1000 [======================>.......] - ETA: 1:33 - loss: 2.9651 - regression_loss: 2.1875 - classification_loss: 0.7775
 794/1000 [======================>.......] - ETA: 1:33 - loss: 2.9675 - regression_loss: 2.1893 - classification_loss: 0.7782
 795/1000 [======================>.......] - ETA: 1:32 - loss: 2.9681 - regression_loss: 2.1903 - classification_loss: 0.7779
 796/1000 [======================>.......] - ETA: 1:32 - loss: 2.9644 - regression_loss: 2.1875 - classification_loss: 0.7769
 797/1000 [======================>.......] - ETA: 1:31 - loss: 2.9650 - regression_loss: 2.1883 - classification_loss: 0.7767
 798/1000 [======================>.......] - ETA: 1:31 - loss: 2.9654 - regression_loss: 2.1890 - classification_loss: 0.7764
 799/1000 [======================>.......] - ETA: 1:30 - loss: 2.9654 - regression_loss: 2.1894 - classification_loss: 0.7760
 800/1000 [=======================>......] - ETA: 1:30 - loss: 2.9672 - regression_loss: 2.1914 - classification_loss: 0.7757
 801/1000 [=======================>......] - ETA: 1:29 - loss: 2.9687 - regression_loss: 2.1921 - classification_loss: 0.7767
 802/1000 [=======================>......] - ETA: 1:29 - loss: 2.9688 - regression_loss: 2.1925 - classification_loss: 0.7762
 803/1000 [=======================>......] - ETA: 1:29 - loss: 2.9694 - regression_loss: 2.1937 - classification_loss: 0.7756
 804/1000 [=======================>......] - ETA: 1:28 - loss: 2.9701 - regression_loss: 2.1948 - classification_loss: 0.7753
 805/1000 [=======================>......] - ETA: 1:28 - loss: 2.9722 - regression_loss: 2.1969 - classification_loss: 0.7753
 806/1000 [=======================>......] - ETA: 1:27 - loss: 2.9726 - regression_loss: 2.1978 - classification_loss: 0.7748
 807/1000 [=======================>......] - ETA: 1:27 - loss: 2.9754 - regression_loss: 2.1997 - classification_loss: 0.7758
 808/1000 [=======================>......] - ETA: 1:26 - loss: 2.9775 - regression_loss: 2.2010 - classification_loss: 0.7766
 809/1000 [=======================>......] - ETA: 1:26 - loss: 2.9774 - regression_loss: 2.2013 - classification_loss: 0.7762
 810/1000 [=======================>......] - ETA: 1:25 - loss: 2.9797 - regression_loss: 2.2030 - classification_loss: 0.7766
 811/1000 [=======================>......] - ETA: 1:25 - loss: 2.9814 - regression_loss: 2.2043 - classification_loss: 0.7770
 812/1000 [=======================>......] - ETA: 1:24 - loss: 2.9824 - regression_loss: 2.2056 - classification_loss: 0.7767
 813/1000 [=======================>......] - ETA: 1:24 - loss: 2.9824 - regression_loss: 2.2060 - classification_loss: 0.7765
 814/1000 [=======================>......] - ETA: 1:24 - loss: 2.9816 - regression_loss: 2.2033 - classification_loss: 0.7783
 815/1000 [=======================>......] - ETA: 1:23 - loss: 2.9779 - regression_loss: 2.2005 - classification_loss: 0.7774
 816/1000 [=======================>......] - ETA: 1:23 - loss: 2.9798 - regression_loss: 2.2019 - classification_loss: 0.7779
 817/1000 [=======================>......] - ETA: 1:22 - loss: 2.9815 - regression_loss: 2.2031 - classification_loss: 0.7784
 818/1000 [=======================>......] - ETA: 1:22 - loss: 2.9819 - regression_loss: 2.2040 - classification_loss: 0.7779
 819/1000 [=======================>......] - ETA: 1:21 - loss: 2.9809 - regression_loss: 2.2037 - classification_loss: 0.7772
 820/1000 [=======================>......] - ETA: 1:21 - loss: 2.9827 - regression_loss: 2.2051 - classification_loss: 0.7776
 821/1000 [=======================>......] - ETA: 1:20 - loss: 2.9855 - regression_loss: 2.2066 - classification_loss: 0.7789
 822/1000 [=======================>......] - ETA: 1:20 - loss: 2.9819 - regression_loss: 2.2039 - classification_loss: 0.7780
 823/1000 [=======================>......] - ETA: 1:19 - loss: 2.9842 - regression_loss: 2.2056 - classification_loss: 0.7786
 824/1000 [=======================>......] - ETA: 1:19 - loss: 2.9838 - regression_loss: 2.2057 - classification_loss: 0.7781
 825/1000 [=======================>......] - ETA: 1:19 - loss: 2.9862 - regression_loss: 2.2079 - classification_loss: 0.7783
 826/1000 [=======================>......] - ETA: 1:18 - loss: 2.9873 - regression_loss: 2.2085 - classification_loss: 0.7788
 827/1000 [=======================>......] - ETA: 1:18 - loss: 2.9837 - regression_loss: 2.2059 - classification_loss: 0.7778
 828/1000 [=======================>......] - ETA: 1:17 - loss: 2.9842 - regression_loss: 2.2067 - classification_loss: 0.7776
 829/1000 [=======================>......] - ETA: 1:17 - loss: 2.9867 - regression_loss: 2.2084 - classification_loss: 0.7783
 830/1000 [=======================>......] - ETA: 1:16 - loss: 2.9858 - regression_loss: 2.2082 - classification_loss: 0.7776
 831/1000 [=======================>......] - ETA: 1:16 - loss: 2.9822 - regression_loss: 2.2055 - classification_loss: 0.7767
 832/1000 [=======================>......] - ETA: 1:15 - loss: 2.9824 - regression_loss: 2.2051 - classification_loss: 0.7773
 833/1000 [=======================>......] - ETA: 1:15 - loss: 2.9788 - regression_loss: 2.2025 - classification_loss: 0.7764
 834/1000 [========================>.....] - ETA: 1:15 - loss: 2.9753 - regression_loss: 2.1998 - classification_loss: 0.7754
 835/1000 [========================>.....] - ETA: 1:14 - loss: 2.9763 - regression_loss: 2.2003 - classification_loss: 0.7759
 836/1000 [========================>.....] - ETA: 1:14 - loss: 2.9768 - regression_loss: 2.2010 - classification_loss: 0.7759
 837/1000 [========================>.....] - ETA: 1:13 - loss: 2.9777 - regression_loss: 2.2015 - classification_loss: 0.7762
 838/1000 [========================>.....] - ETA: 1:13 - loss: 2.9773 - regression_loss: 2.2014 - classification_loss: 0.7759
 839/1000 [========================>.....] - ETA: 1:12 - loss: 2.9763 - regression_loss: 2.2011 - classification_loss: 0.7753
 840/1000 [========================>.....] - ETA: 1:12 - loss: 2.9757 - regression_loss: 2.2010 - classification_loss: 0.7747
 841/1000 [========================>.....] - ETA: 1:11 - loss: 2.9760 - regression_loss: 2.2017 - classification_loss: 0.7743
 842/1000 [========================>.....] - ETA: 1:11 - loss: 2.9776 - regression_loss: 2.2028 - classification_loss: 0.7748
 843/1000 [========================>.....] - ETA: 1:10 - loss: 2.9770 - regression_loss: 2.2024 - classification_loss: 0.7745
 844/1000 [========================>.....] - ETA: 1:10 - loss: 2.9791 - regression_loss: 2.2039 - classification_loss: 0.7752
 845/1000 [========================>.....] - ETA: 1:10 - loss: 2.9756 - regression_loss: 2.2013 - classification_loss: 0.7743
 846/1000 [========================>.....] - ETA: 1:09 - loss: 2.9750 - regression_loss: 2.2009 - classification_loss: 0.7741
 847/1000 [========================>.....] - ETA: 1:09 - loss: 2.9774 - regression_loss: 2.2026 - classification_loss: 0.7748
 848/1000 [========================>.....] - ETA: 1:08 - loss: 2.9790 - regression_loss: 2.2042 - classification_loss: 0.7748
 849/1000 [========================>.....] - ETA: 1:08 - loss: 2.9801 - regression_loss: 2.2058 - classification_loss: 0.7743
 850/1000 [========================>.....] - ETA: 1:07 - loss: 2.9766 - regression_loss: 2.2032 - classification_loss: 0.7734
 851/1000 [========================>.....] - ETA: 1:07 - loss: 2.9766 - regression_loss: 2.2033 - classification_loss: 0.7733
 852/1000 [========================>.....] - ETA: 1:06 - loss: 2.9767 - regression_loss: 2.2038 - classification_loss: 0.7728
 853/1000 [========================>.....] - ETA: 1:06 - loss: 2.9769 - regression_loss: 2.2045 - classification_loss: 0.7724
 854/1000 [========================>.....] - ETA: 1:05 - loss: 2.9784 - regression_loss: 2.2051 - classification_loss: 0.7733
 855/1000 [========================>.....] - ETA: 1:05 - loss: 2.9796 - regression_loss: 2.2061 - classification_loss: 0.7736
 856/1000 [========================>.....] - ETA: 1:05 - loss: 2.9809 - regression_loss: 2.2064 - classification_loss: 0.7745
 857/1000 [========================>.....] - ETA: 1:04 - loss: 2.9820 - regression_loss: 2.2076 - classification_loss: 0.7744
 858/1000 [========================>.....] - ETA: 1:04 - loss: 2.9832 - regression_loss: 2.2083 - classification_loss: 0.7748
 859/1000 [========================>.....] - ETA: 1:03 - loss: 2.9856 - regression_loss: 2.2099 - classification_loss: 0.7757
 860/1000 [========================>.....] - ETA: 1:03 - loss: 2.9874 - regression_loss: 2.2112 - classification_loss: 0.7763
 861/1000 [========================>.....] - ETA: 1:02 - loss: 2.9867 - regression_loss: 2.2108 - classification_loss: 0.7759
 862/1000 [========================>.....] - ETA: 1:02 - loss: 2.9833 - regression_loss: 2.2083 - classification_loss: 0.7750
 863/1000 [========================>.....] - ETA: 1:01 - loss: 2.9836 - regression_loss: 2.2089 - classification_loss: 0.7747
 864/1000 [========================>.....] - ETA: 1:01 - loss: 2.9848 - regression_loss: 2.2101 - classification_loss: 0.7747
 865/1000 [========================>.....] - ETA: 1:00 - loss: 2.9838 - regression_loss: 2.2097 - classification_loss: 0.7741
 866/1000 [========================>.....] - ETA: 1:00 - loss: 2.9900 - regression_loss: 2.2135 - classification_loss: 0.7765
 867/1000 [=========================>....] - ETA: 1:00 - loss: 2.9908 - regression_loss: 2.2146 - classification_loss: 0.7762
 868/1000 [=========================>....] - ETA: 59s - loss: 2.9874 - regression_loss: 2.2120 - classification_loss: 0.7754 
 869/1000 [=========================>....] - ETA: 59s - loss: 2.9868 - regression_loss: 2.2118 - classification_loss: 0.7750
 870/1000 [=========================>....] - ETA: 58s - loss: 2.9883 - regression_loss: 2.2135 - classification_loss: 0.7748
 871/1000 [=========================>....] - ETA: 58s - loss: 2.9889 - regression_loss: 2.2142 - classification_loss: 0.7747
 872/1000 [=========================>....] - ETA: 57s - loss: 2.9854 - regression_loss: 2.2116 - classification_loss: 0.7738
 873/1000 [=========================>....] - ETA: 57s - loss: 2.9852 - regression_loss: 2.2117 - classification_loss: 0.7735
 874/1000 [=========================>....] - ETA: 56s - loss: 2.9865 - regression_loss: 2.2129 - classification_loss: 0.7735
 875/1000 [=========================>....] - ETA: 56s - loss: 2.9866 - regression_loss: 2.2126 - classification_loss: 0.7740
 876/1000 [=========================>....] - ETA: 56s - loss: 2.9891 - regression_loss: 2.2142 - classification_loss: 0.7749
 877/1000 [=========================>....] - ETA: 55s - loss: 2.9929 - regression_loss: 2.2179 - classification_loss: 0.7750
 878/1000 [=========================>....] - ETA: 55s - loss: 2.9927 - regression_loss: 2.2181 - classification_loss: 0.7747
 879/1000 [=========================>....] - ETA: 54s - loss: 2.9893 - regression_loss: 2.2155 - classification_loss: 0.7738
 880/1000 [=========================>....] - ETA: 54s - loss: 2.9859 - regression_loss: 2.2130 - classification_loss: 0.7729
 881/1000 [=========================>....] - ETA: 53s - loss: 2.9825 - regression_loss: 2.2105 - classification_loss: 0.7720
 882/1000 [=========================>....] - ETA: 53s - loss: 2.9840 - regression_loss: 2.2120 - classification_loss: 0.7720
 883/1000 [=========================>....] - ETA: 52s - loss: 2.9837 - regression_loss: 2.2120 - classification_loss: 0.7717
 884/1000 [=========================>....] - ETA: 52s - loss: 2.9845 - regression_loss: 2.2131 - classification_loss: 0.7714
 885/1000 [=========================>....] - ETA: 51s - loss: 2.9858 - regression_loss: 2.2139 - classification_loss: 0.7719
 886/1000 [=========================>....] - ETA: 51s - loss: 2.9861 - regression_loss: 2.2144 - classification_loss: 0.7717
 887/1000 [=========================>....] - ETA: 51s - loss: 2.9856 - regression_loss: 2.2141 - classification_loss: 0.7715
 888/1000 [=========================>....] - ETA: 50s - loss: 2.9858 - regression_loss: 2.2146 - classification_loss: 0.7712
 889/1000 [=========================>....] - ETA: 50s - loss: 2.9824 - regression_loss: 2.2121 - classification_loss: 0.7703
 890/1000 [=========================>....] - ETA: 49s - loss: 2.9836 - regression_loss: 2.2127 - classification_loss: 0.7709
 891/1000 [=========================>....] - ETA: 49s - loss: 2.9840 - regression_loss: 2.2134 - classification_loss: 0.7706
 892/1000 [=========================>....] - ETA: 48s - loss: 2.9806 - regression_loss: 2.2109 - classification_loss: 0.7697
 893/1000 [=========================>....] - ETA: 48s - loss: 2.9801 - regression_loss: 2.2108 - classification_loss: 0.7693
 894/1000 [=========================>....] - ETA: 47s - loss: 2.9831 - regression_loss: 2.2130 - classification_loss: 0.7700
 895/1000 [=========================>....] - ETA: 47s - loss: 2.9835 - regression_loss: 2.2132 - classification_loss: 0.7703
 896/1000 [=========================>....] - ETA: 46s - loss: 2.9801 - regression_loss: 2.2107 - classification_loss: 0.7694
 897/1000 [=========================>....] - ETA: 46s - loss: 2.9824 - regression_loss: 2.2124 - classification_loss: 0.7700
 898/1000 [=========================>....] - ETA: 46s - loss: 2.9854 - regression_loss: 2.2154 - classification_loss: 0.7700
 899/1000 [=========================>....] - ETA: 45s - loss: 2.9847 - regression_loss: 2.2150 - classification_loss: 0.7697
 900/1000 [==========================>...] - ETA: 45s - loss: 2.9851 - regression_loss: 2.2150 - classification_loss: 0.7701
 901/1000 [==========================>...] - ETA: 44s - loss: 2.9818 - regression_loss: 2.2125 - classification_loss: 0.7692
 902/1000 [==========================>...] - ETA: 44s - loss: 2.9836 - regression_loss: 2.2137 - classification_loss: 0.7699
 903/1000 [==========================>...] - ETA: 43s - loss: 2.9855 - regression_loss: 2.2150 - classification_loss: 0.7705
 904/1000 [==========================>...] - ETA: 43s - loss: 2.9866 - regression_loss: 2.2150 - classification_loss: 0.7717
 905/1000 [==========================>...] - ETA: 42s - loss: 2.9879 - regression_loss: 2.2154 - classification_loss: 0.7725
 906/1000 [==========================>...] - ETA: 42s - loss: 2.9914 - regression_loss: 2.2181 - classification_loss: 0.7732
 907/1000 [==========================>...] - ETA: 42s - loss: 2.9948 - regression_loss: 2.2218 - classification_loss: 0.7730
 908/1000 [==========================>...] - ETA: 41s - loss: 2.9947 - regression_loss: 2.2218 - classification_loss: 0.7729
 909/1000 [==========================>...] - ETA: 41s - loss: 2.9914 - regression_loss: 2.2194 - classification_loss: 0.7721
 910/1000 [==========================>...] - ETA: 40s - loss: 2.9920 - regression_loss: 2.2203 - classification_loss: 0.7717
 911/1000 [==========================>...] - ETA: 40s - loss: 2.9934 - regression_loss: 2.2214 - classification_loss: 0.7720
 912/1000 [==========================>...] - ETA: 39s - loss: 2.9901 - regression_loss: 2.2189 - classification_loss: 0.7712
 913/1000 [==========================>...] - ETA: 39s - loss: 2.9902 - regression_loss: 2.2188 - classification_loss: 0.7714
 914/1000 [==========================>...] - ETA: 38s - loss: 2.9896 - regression_loss: 2.2186 - classification_loss: 0.7710
 915/1000 [==========================>...] - ETA: 38s - loss: 2.9864 - regression_loss: 2.2162 - classification_loss: 0.7702
 916/1000 [==========================>...] - ETA: 37s - loss: 2.9872 - regression_loss: 2.2170 - classification_loss: 0.7703
 917/1000 [==========================>...] - ETA: 37s - loss: 2.9858 - regression_loss: 2.2161 - classification_loss: 0.7697
 918/1000 [==========================>...] - ETA: 37s - loss: 2.9866 - regression_loss: 2.2160 - classification_loss: 0.7706
 919/1000 [==========================>...] - ETA: 36s - loss: 2.9872 - regression_loss: 2.2168 - classification_loss: 0.7704
 920/1000 [==========================>...] - ETA: 36s - loss: 2.9840 - regression_loss: 2.2144 - classification_loss: 0.7696
 921/1000 [==========================>...] - ETA: 35s - loss: 2.9848 - regression_loss: 2.2153 - classification_loss: 0.7694
 922/1000 [==========================>...] - ETA: 35s - loss: 2.9866 - regression_loss: 2.2162 - classification_loss: 0.7704
 923/1000 [==========================>...] - ETA: 34s - loss: 2.9876 - regression_loss: 2.2170 - classification_loss: 0.7707
 924/1000 [==========================>...] - ETA: 34s - loss: 2.9844 - regression_loss: 2.2146 - classification_loss: 0.7698
 925/1000 [==========================>...] - ETA: 33s - loss: 2.9844 - regression_loss: 2.2149 - classification_loss: 0.7695
 926/1000 [==========================>...] - ETA: 33s - loss: 2.9811 - regression_loss: 2.2125 - classification_loss: 0.7686
 927/1000 [==========================>...] - ETA: 32s - loss: 2.9824 - regression_loss: 2.2127 - classification_loss: 0.7696
 928/1000 [==========================>...] - ETA: 32s - loss: 2.9820 - regression_loss: 2.2127 - classification_loss: 0.7692
 929/1000 [==========================>...] - ETA: 32s - loss: 2.9817 - regression_loss: 2.2127 - classification_loss: 0.7690
 930/1000 [==========================>...] - ETA: 31s - loss: 2.9814 - regression_loss: 2.2123 - classification_loss: 0.7691
 931/1000 [==========================>...] - ETA: 31s - loss: 2.9834 - regression_loss: 2.2140 - classification_loss: 0.7694
 932/1000 [==========================>...] - ETA: 30s - loss: 2.9847 - regression_loss: 2.2156 - classification_loss: 0.7691
 933/1000 [==========================>...] - ETA: 30s - loss: 2.9848 - regression_loss: 2.2158 - classification_loss: 0.7690
 934/1000 [===========================>..] - ETA: 29s - loss: 2.9860 - regression_loss: 2.2166 - classification_loss: 0.7694
 935/1000 [===========================>..] - ETA: 29s - loss: 2.9828 - regression_loss: 2.2142 - classification_loss: 0.7686
 936/1000 [===========================>..] - ETA: 28s - loss: 2.9796 - regression_loss: 2.2118 - classification_loss: 0.7678
 937/1000 [===========================>..] - ETA: 28s - loss: 2.9804 - regression_loss: 2.2130 - classification_loss: 0.7674
 938/1000 [===========================>..] - ETA: 28s - loss: 2.9816 - regression_loss: 2.2137 - classification_loss: 0.7679
 939/1000 [===========================>..] - ETA: 27s - loss: 2.9809 - regression_loss: 2.2135 - classification_loss: 0.7674
 940/1000 [===========================>..] - ETA: 27s - loss: 2.9777 - regression_loss: 2.2112 - classification_loss: 0.7666
 941/1000 [===========================>..] - ETA: 26s - loss: 2.9785 - regression_loss: 2.2119 - classification_loss: 0.7666
 942/1000 [===========================>..] - ETA: 26s - loss: 2.9753 - regression_loss: 2.2095 - classification_loss: 0.7658
 943/1000 [===========================>..] - ETA: 25s - loss: 2.9758 - regression_loss: 2.2101 - classification_loss: 0.7657
 944/1000 [===========================>..] - ETA: 25s - loss: 2.9755 - regression_loss: 2.2100 - classification_loss: 0.7656
 945/1000 [===========================>..] - ETA: 24s - loss: 2.9752 - regression_loss: 2.2099 - classification_loss: 0.7653
 946/1000 [===========================>..] - ETA: 24s - loss: 2.9764 - regression_loss: 2.2113 - classification_loss: 0.7650
 947/1000 [===========================>..] - ETA: 23s - loss: 2.9782 - regression_loss: 2.2132 - classification_loss: 0.7650
 948/1000 [===========================>..] - ETA: 23s - loss: 2.9774 - regression_loss: 2.2126 - classification_loss: 0.7647
 949/1000 [===========================>..] - ETA: 23s - loss: 2.9767 - regression_loss: 2.2123 - classification_loss: 0.7644
 950/1000 [===========================>..] - ETA: 22s - loss: 2.9792 - regression_loss: 2.2138 - classification_loss: 0.7654
 951/1000 [===========================>..] - ETA: 22s - loss: 2.9811 - regression_loss: 2.2143 - classification_loss: 0.7668
 952/1000 [===========================>..] - ETA: 21s - loss: 2.9825 - regression_loss: 2.2156 - classification_loss: 0.7669
 953/1000 [===========================>..] - ETA: 21s - loss: 2.9846 - regression_loss: 2.2179 - classification_loss: 0.7666
 954/1000 [===========================>..] - ETA: 20s - loss: 2.9814 - regression_loss: 2.2156 - classification_loss: 0.7658
 955/1000 [===========================>..] - ETA: 20s - loss: 2.9813 - regression_loss: 2.2159 - classification_loss: 0.7654
 956/1000 [===========================>..] - ETA: 19s - loss: 2.9811 - regression_loss: 2.2159 - classification_loss: 0.7652
 957/1000 [===========================>..] - ETA: 19s - loss: 2.9780 - regression_loss: 2.2136 - classification_loss: 0.7644
 958/1000 [===========================>..] - ETA: 18s - loss: 2.9772 - regression_loss: 2.2132 - classification_loss: 0.7640
 959/1000 [===========================>..] - ETA: 18s - loss: 2.9770 - regression_loss: 2.2132 - classification_loss: 0.7638
 960/1000 [===========================>..] - ETA: 18s - loss: 2.9785 - regression_loss: 2.2140 - classification_loss: 0.7645
 961/1000 [===========================>..] - ETA: 17s - loss: 2.9806 - regression_loss: 2.2148 - classification_loss: 0.7658
 962/1000 [===========================>..] - ETA: 17s - loss: 2.9834 - regression_loss: 2.2169 - classification_loss: 0.7666
 963/1000 [===========================>..] - ETA: 16s - loss: 2.9849 - regression_loss: 2.2177 - classification_loss: 0.7672
 964/1000 [===========================>..] - ETA: 16s - loss: 2.9846 - regression_loss: 2.2176 - classification_loss: 0.7670
 965/1000 [===========================>..] - ETA: 15s - loss: 2.9871 - regression_loss: 2.2192 - classification_loss: 0.7678
 966/1000 [===========================>..] - ETA: 15s - loss: 2.9862 - regression_loss: 2.2189 - classification_loss: 0.7674
 967/1000 [============================>.] - ETA: 14s - loss: 2.9832 - regression_loss: 2.2166 - classification_loss: 0.7666
 968/1000 [============================>.] - ETA: 14s - loss: 2.9848 - regression_loss: 2.2182 - classification_loss: 0.7666
 969/1000 [============================>.] - ETA: 14s - loss: 2.9817 - regression_loss: 2.2160 - classification_loss: 0.7658
 970/1000 [============================>.] - ETA: 13s - loss: 2.9787 - regression_loss: 2.2137 - classification_loss: 0.7650
 971/1000 [============================>.] - ETA: 13s - loss: 2.9795 - regression_loss: 2.2147 - classification_loss: 0.7648
 972/1000 [============================>.] - ETA: 12s - loss: 2.9801 - regression_loss: 2.2143 - classification_loss: 0.7658
 973/1000 [============================>.] - ETA: 12s - loss: 2.9794 - regression_loss: 2.2137 - classification_loss: 0.7657
 974/1000 [============================>.] - ETA: 11s - loss: 2.9764 - regression_loss: 2.2114 - classification_loss: 0.7650
 975/1000 [============================>.] - ETA: 11s - loss: 2.9790 - regression_loss: 2.2131 - classification_loss: 0.7659
 976/1000 [============================>.] - ETA: 10s - loss: 2.9772 - regression_loss: 2.2120 - classification_loss: 0.7653
 977/1000 [============================>.] - ETA: 10s - loss: 2.9790 - regression_loss: 2.2129 - classification_loss: 0.7661
 978/1000 [============================>.] - ETA: 9s - loss: 2.9760 - regression_loss: 2.2106 - classification_loss: 0.7653 
 979/1000 [============================>.] - ETA: 9s - loss: 2.9729 - regression_loss: 2.2084 - classification_loss: 0.7646
 980/1000 [============================>.] - ETA: 9s - loss: 2.9742 - regression_loss: 2.2099 - classification_loss: 0.7643
 981/1000 [============================>.] - ETA: 8s - loss: 2.9744 - regression_loss: 2.2100 - classification_loss: 0.7644
 982/1000 [============================>.] - ETA: 8s - loss: 2.9714 - regression_loss: 2.2078 - classification_loss: 0.7636
 983/1000 [============================>.] - ETA: 7s - loss: 2.9735 - regression_loss: 2.2101 - classification_loss: 0.7634
 984/1000 [============================>.] - ETA: 7s - loss: 2.9763 - regression_loss: 2.2118 - classification_loss: 0.7645
 985/1000 [============================>.] - ETA: 6s - loss: 2.9767 - regression_loss: 2.2120 - classification_loss: 0.7648
 986/1000 [============================>.] - ETA: 6s - loss: 2.9737 - regression_loss: 2.2097 - classification_loss: 0.7640
 987/1000 [============================>.] - ETA: 5s - loss: 2.9734 - regression_loss: 2.2099 - classification_loss: 0.7635
 988/1000 [============================>.] - ETA: 5s - loss: 2.9763 - regression_loss: 2.2127 - classification_loss: 0.7636
 989/1000 [============================>.] - ETA: 4s - loss: 2.9758 - regression_loss: 2.2126 - classification_loss: 0.7632
 990/1000 [============================>.] - ETA: 4s - loss: 2.9764 - regression_loss: 2.2135 - classification_loss: 0.7630
 991/1000 [============================>.] - ETA: 4s - loss: 2.9736 - regression_loss: 2.2112 - classification_loss: 0.7624
 992/1000 [============================>.] - ETA: 3s - loss: 2.9735 - regression_loss: 2.2116 - classification_loss: 0.7619
 993/1000 [============================>.] - ETA: 3s - loss: 2.9705 - regression_loss: 2.2093 - classification_loss: 0.7612
 994/1000 [============================>.] - ETA: 2s - loss: 2.9675 - regression_loss: 2.2071 - classification_loss: 0.7604
 995/1000 [============================>.] - ETA: 2s - loss: 2.9669 - regression_loss: 2.2069 - classification_loss: 0.7599
 996/1000 [============================>.] - ETA: 1s - loss: 2.9663 - regression_loss: 2.2067 - classification_loss: 0.7596
 997/1000 [============================>.] - ETA: 1s - loss: 2.9680 - regression_loss: 2.2083 - classification_loss: 0.7597
 998/1000 [============================>.] - ETA: 0s - loss: 2.9691 - regression_loss: 2.2086 - classification_loss: 0.7606
 999/1000 [============================>.] - ETA: 0s - loss: 2.9703 - regression_loss: 2.2096 - classification_loss: 0.7607
1000/1000 [==============================] - 452s 452ms/step - loss: 2.9673 - regression_loss: 2.2074 - classification_loss: 0.7599

Epoch 00011: saving model to ./snapshots/resnet50_csv_11.h5
0/2001/2002/2003/2004/2005/2006/2007/2008/2009/20010/20011/20012/20013/20014/20015/20016/20017/20018/20019/20020/20021/20022/20023/20024/20025/20026/20027/20028/20029/20030/20031/20032/20033/20034/20035/20036/20037/20038/20039/20040/20041/20042/20043/20044/20045/20046/20047/20048/20049/20050/20051/20052/20053/20054/20055/20056/20057/20058/20059/20060/20061/20062/20063/20064/20065/20066/20067/20068/20069/20070/20071/20072/20073/20074/20075/20076/20077/20078/20079/20080/20081/20082/20083/20084/20085/20086/20087/20088/20089/20090/20091/20092/20093/20094/20095/20096/20097/20098/20099/200100/200101/200102/200103/200104/200105/200106/200107/200108/200109/200110/200111/200112/200113/200114/200115/200116/200117/200118/200119/200120/200121/200122/200123/200124/200125/200126/200127/200128/200129/200130/200131/200132/200133/200134/200135/200136/200137/200138/200139/200140/200141/200142/200143/200144/200145/200146/200147/200148/200149/200150/200151/200152/200153/200154/200155/200156/200157/200158/200159/200160/200161/200162/200163/200164/200165/200166/200167/200168/200169/200170/200171/200172/200173/200174/200175/200176/200177/200178/200179/200180/200181/200182/200183/200184/200185/200186/200187/200188/200189/200190/200191/200192/200193/200194/200195/200196/200197/200198/200199/2000/2001/2002/2003/2004/2005/2006/2007/2008/2009/20010/20011/20012/20013/20014/20015/20016/20017/20018/20019/20020/20021/20022/20023/20024/20025/20026/20027/20028/20029/20030/20031/20032/20033/20034/20035/20036/20037/20038/20039/20040/20041/20042/20043/20044/20045/20046/20047/20048/20049/20050/20051/20052/20053/20054/20055/20056/20057/20058/20059/20060/20061/20062/20063/20064/20065/20066/20067/20068/20069/20070/20071/20072/20073/20074/20075/20076/20077/20078/20079/20080/20081/20082/20083/20084/20085/20086/20087/20088/20089/20090/20091/20092/20093/20094/20095/20096/20097/20098/20099/200100/200101/200102/200103/200104/200105/200106/200107/200108/200109/200110/200111/200112/200113/200114/200115/200116/200117/200118/200119/200120/200121/200122/200123/200124/200125/200126/200127/200128/200129/200130/200131/200132/200133/200134/200135/200136/200137/200138/200139/200140/200141/200142/200143/200144/200145/200146/200147/200148/200149/200150/200151/200152/200153/200154/200155/200156/200157/200158/200159/200160/200161/200162/200163/200164/200165/200166/200167/200168/200169/200170/200171/200172/200173/200174/200175/200176/200177/200178/200179/200180/200181/200182/200183/200184/200185/200186/200187/200188/200189/200190/200191/200192/200193/200194/200195/200196/200197/200198/200199/200

M 0.1218
N 0.0002
mAP: 0.0610
Epoch 12/30

   1/1000 [..............................] - ETA: 7:28 - loss: 0.0544 - regression_loss: 0.0000e+00 - classification_loss: 0.0544
   2/1000 [..............................] - ETA: 7:28 - loss: 1.7757 - regression_loss: 1.3999 - classification_loss: 0.3758    
   3/1000 [..............................] - ETA: 7:28 - loss: 1.1838 - regression_loss: 0.9333 - classification_loss: 0.2505
   4/1000 [..............................] - ETA: 7:26 - loss: 2.2415 - regression_loss: 1.6246 - classification_loss: 0.6169
   5/1000 [..............................] - ETA: 7:27 - loss: 2.3486 - regression_loss: 1.6845 - classification_loss: 0.6642
   6/1000 [..............................] - ETA: 7:27 - loss: 2.7638 - regression_loss: 2.0794 - classification_loss: 0.6843
   7/1000 [..............................] - ETA: 7:28 - loss: 3.2435 - regression_loss: 2.3718 - classification_loss: 0.8718
   8/1000 [..............................] - ETA: 7:29 - loss: 2.8381 - regression_loss: 2.0753 - classification_loss: 0.7628
   9/1000 [..............................] - ETA: 7:28 - loss: 2.5228 - regression_loss: 1.8447 - classification_loss: 0.6781
  10/1000 [..............................] - ETA: 7:28 - loss: 2.2705 - regression_loss: 1.6602 - classification_loss: 0.6102
  11/1000 [..............................] - ETA: 7:28 - loss: 2.4349 - regression_loss: 1.7339 - classification_loss: 0.7010
  12/1000 [..............................] - ETA: 7:28 - loss: 2.2320 - regression_loss: 1.5894 - classification_loss: 0.6426
  13/1000 [..............................] - ETA: 7:27 - loss: 2.4284 - regression_loss: 1.6783 - classification_loss: 0.7502
  14/1000 [..............................] - ETA: 7:27 - loss: 2.2550 - regression_loss: 1.5584 - classification_loss: 0.6966
  15/1000 [..............................] - ETA: 7:26 - loss: 2.3517 - regression_loss: 1.5882 - classification_loss: 0.7635
  16/1000 [..............................] - ETA: 7:26 - loss: 2.4770 - regression_loss: 1.6531 - classification_loss: 0.8239
  17/1000 [..............................] - ETA: 7:25 - loss: 2.3313 - regression_loss: 1.5558 - classification_loss: 0.7755
  18/1000 [..............................] - ETA: 7:24 - loss: 2.4403 - regression_loss: 1.6547 - classification_loss: 0.7856
  19/1000 [..............................] - ETA: 7:24 - loss: 2.5463 - regression_loss: 1.7099 - classification_loss: 0.8365
  20/1000 [..............................] - ETA: 7:23 - loss: 2.6605 - regression_loss: 1.7648 - classification_loss: 0.8957
  21/1000 [..............................] - ETA: 7:23 - loss: 2.7037 - regression_loss: 1.8234 - classification_loss: 0.8803
  22/1000 [..............................] - ETA: 7:22 - loss: 2.6969 - regression_loss: 1.8348 - classification_loss: 0.8621
  23/1000 [..............................] - ETA: 7:22 - loss: 2.5797 - regression_loss: 1.7551 - classification_loss: 0.8246
  24/1000 [..............................] - ETA: 7:21 - loss: 2.4722 - regression_loss: 1.6819 - classification_loss: 0.7903
  25/1000 [..............................] - ETA: 7:21 - loss: 2.4957 - regression_loss: 1.7041 - classification_loss: 0.7916
  26/1000 [..............................] - ETA: 7:20 - loss: 2.3997 - regression_loss: 1.6385 - classification_loss: 0.7612
  27/1000 [..............................] - ETA: 7:20 - loss: 2.5651 - regression_loss: 1.7285 - classification_loss: 0.8366
  28/1000 [..............................] - ETA: 7:20 - loss: 2.4888 - regression_loss: 1.6667 - classification_loss: 0.8221
  29/1000 [..............................] - ETA: 7:19 - loss: 2.5314 - regression_loss: 1.6834 - classification_loss: 0.8479
  30/1000 [..............................] - ETA: 7:18 - loss: 2.7040 - regression_loss: 1.8562 - classification_loss: 0.8479
  31/1000 [..............................] - ETA: 7:18 - loss: 2.7347 - regression_loss: 1.8627 - classification_loss: 0.8720
  32/1000 [..............................] - ETA: 7:16 - loss: 2.7207 - regression_loss: 1.8639 - classification_loss: 0.8568
  33/1000 [..............................] - ETA: 7:16 - loss: 2.7617 - regression_loss: 1.9030 - classification_loss: 0.8587
  34/1000 [>.............................] - ETA: 7:15 - loss: 2.8218 - regression_loss: 1.9400 - classification_loss: 0.8818
  35/1000 [>.............................] - ETA: 7:15 - loss: 2.8459 - regression_loss: 1.9437 - classification_loss: 0.9022
  36/1000 [>.............................] - ETA: 7:14 - loss: 2.9059 - regression_loss: 1.9755 - classification_loss: 0.9304
  37/1000 [>.............................] - ETA: 7:14 - loss: 2.8274 - regression_loss: 1.9221 - classification_loss: 0.9052
  38/1000 [>.............................] - ETA: 7:14 - loss: 2.8335 - regression_loss: 1.9391 - classification_loss: 0.8944
  39/1000 [>.............................] - ETA: 7:13 - loss: 2.8220 - regression_loss: 1.9400 - classification_loss: 0.8820
  40/1000 [>.............................] - ETA: 7:13 - loss: 2.7537 - regression_loss: 1.8915 - classification_loss: 0.8623
  41/1000 [>.............................] - ETA: 7:13 - loss: 2.7515 - regression_loss: 1.9003 - classification_loss: 0.8512
  42/1000 [>.............................] - ETA: 7:12 - loss: 2.8116 - regression_loss: 1.9369 - classification_loss: 0.8747
  43/1000 [>.............................] - ETA: 7:12 - loss: 2.7463 - regression_loss: 1.8919 - classification_loss: 0.8544
  44/1000 [>.............................] - ETA: 7:11 - loss: 2.7561 - regression_loss: 1.8983 - classification_loss: 0.8578
  45/1000 [>.............................] - ETA: 7:11 - loss: 2.7956 - regression_loss: 1.9161 - classification_loss: 0.8794
  46/1000 [>.............................] - ETA: 7:10 - loss: 2.8072 - regression_loss: 1.9241 - classification_loss: 0.8831
  47/1000 [>.............................] - ETA: 7:10 - loss: 2.8115 - regression_loss: 1.9296 - classification_loss: 0.8819
  48/1000 [>.............................] - ETA: 7:10 - loss: 2.7890 - regression_loss: 1.9196 - classification_loss: 0.8694
  49/1000 [>.............................] - ETA: 7:09 - loss: 2.7990 - regression_loss: 1.9192 - classification_loss: 0.8799
  50/1000 [>.............................] - ETA: 7:09 - loss: 2.8089 - regression_loss: 1.9343 - classification_loss: 0.8746
  51/1000 [>.............................] - ETA: 7:08 - loss: 2.8425 - regression_loss: 1.9646 - classification_loss: 0.8779
  52/1000 [>.............................] - ETA: 7:08 - loss: 2.8575 - regression_loss: 1.9860 - classification_loss: 0.8715
  53/1000 [>.............................] - ETA: 7:07 - loss: 2.8482 - regression_loss: 1.9901 - classification_loss: 0.8581
  54/1000 [>.............................] - ETA: 7:07 - loss: 2.7955 - regression_loss: 1.9532 - classification_loss: 0.8422
  55/1000 [>.............................] - ETA: 7:07 - loss: 2.8395 - regression_loss: 1.9964 - classification_loss: 0.8430
  56/1000 [>.............................] - ETA: 7:06 - loss: 2.8492 - regression_loss: 2.0117 - classification_loss: 0.8375
  57/1000 [>.............................] - ETA: 7:06 - loss: 2.7993 - regression_loss: 1.9765 - classification_loss: 0.8229
  58/1000 [>.............................] - ETA: 7:05 - loss: 2.7511 - regression_loss: 1.9424 - classification_loss: 0.8087
  59/1000 [>.............................] - ETA: 7:05 - loss: 2.7045 - regression_loss: 1.9095 - classification_loss: 0.7950
  60/1000 [>.............................] - ETA: 7:04 - loss: 2.6596 - regression_loss: 1.8776 - classification_loss: 0.7820
  61/1000 [>.............................] - ETA: 7:04 - loss: 2.6619 - regression_loss: 1.8840 - classification_loss: 0.7778
  62/1000 [>.............................] - ETA: 7:03 - loss: 2.6221 - regression_loss: 1.8536 - classification_loss: 0.7685
  63/1000 [>.............................] - ETA: 7:03 - loss: 2.6698 - regression_loss: 1.8842 - classification_loss: 0.7856
  64/1000 [>.............................] - ETA: 7:03 - loss: 2.6767 - regression_loss: 1.8939 - classification_loss: 0.7827
  65/1000 [>.............................] - ETA: 7:02 - loss: 2.7017 - regression_loss: 1.9064 - classification_loss: 0.7953
  66/1000 [>.............................] - ETA: 7:02 - loss: 2.7058 - regression_loss: 1.9109 - classification_loss: 0.7949
  67/1000 [=>............................] - ETA: 7:01 - loss: 2.7108 - regression_loss: 1.9216 - classification_loss: 0.7892
  68/1000 [=>............................] - ETA: 7:01 - loss: 2.7232 - regression_loss: 1.9344 - classification_loss: 0.7887
  69/1000 [=>............................] - ETA: 7:00 - loss: 2.7571 - regression_loss: 1.9560 - classification_loss: 0.8011
  70/1000 [=>............................] - ETA: 7:00 - loss: 2.7177 - regression_loss: 1.9280 - classification_loss: 0.7897
  71/1000 [=>............................] - ETA: 6:59 - loss: 2.6794 - regression_loss: 1.9009 - classification_loss: 0.7786
  72/1000 [=>............................] - ETA: 6:59 - loss: 2.7037 - regression_loss: 1.9130 - classification_loss: 0.7908
  73/1000 [=>............................] - ETA: 6:58 - loss: 2.7131 - regression_loss: 1.9244 - classification_loss: 0.7887
  74/1000 [=>............................] - ETA: 6:58 - loss: 2.7413 - regression_loss: 1.9377 - classification_loss: 0.8036
  75/1000 [=>............................] - ETA: 6:58 - loss: 2.7548 - regression_loss: 1.9391 - classification_loss: 0.8157
  76/1000 [=>............................] - ETA: 6:57 - loss: 2.7675 - regression_loss: 1.9418 - classification_loss: 0.8257
  77/1000 [=>............................] - ETA: 6:57 - loss: 2.7957 - regression_loss: 1.9624 - classification_loss: 0.8333
  78/1000 [=>............................] - ETA: 6:56 - loss: 2.8106 - regression_loss: 1.9709 - classification_loss: 0.8397
  79/1000 [=>............................] - ETA: 6:56 - loss: 2.8386 - regression_loss: 1.9991 - classification_loss: 0.8395
  80/1000 [=>............................] - ETA: 6:56 - loss: 2.8362 - regression_loss: 2.0045 - classification_loss: 0.8317
  81/1000 [=>............................] - ETA: 6:55 - loss: 2.8257 - regression_loss: 2.0010 - classification_loss: 0.8247
  82/1000 [=>............................] - ETA: 6:55 - loss: 2.8367 - regression_loss: 2.0166 - classification_loss: 0.8201
  83/1000 [=>............................] - ETA: 6:54 - loss: 2.8334 - regression_loss: 2.0175 - classification_loss: 0.8159
  84/1000 [=>............................] - ETA: 6:54 - loss: 2.8426 - regression_loss: 2.0221 - classification_loss: 0.8205
  85/1000 [=>............................] - ETA: 6:53 - loss: 2.8091 - regression_loss: 1.9983 - classification_loss: 0.8108
  86/1000 [=>............................] - ETA: 6:53 - loss: 2.8327 - regression_loss: 2.0225 - classification_loss: 0.8103
  87/1000 [=>............................] - ETA: 6:53 - loss: 2.8505 - regression_loss: 2.0351 - classification_loss: 0.8154
  88/1000 [=>............................] - ETA: 6:52 - loss: 2.8711 - regression_loss: 2.0507 - classification_loss: 0.8204
  89/1000 [=>............................] - ETA: 6:52 - loss: 2.8828 - regression_loss: 2.0602 - classification_loss: 0.8226
  90/1000 [=>............................] - ETA: 6:51 - loss: 2.8930 - regression_loss: 2.0702 - classification_loss: 0.8228
  91/1000 [=>............................] - ETA: 6:51 - loss: 2.8843 - regression_loss: 2.0664 - classification_loss: 0.8180
  92/1000 [=>............................] - ETA: 6:50 - loss: 2.8947 - regression_loss: 2.0803 - classification_loss: 0.8144
  93/1000 [=>............................] - ETA: 6:50 - loss: 2.8962 - regression_loss: 2.0866 - classification_loss: 0.8096
  94/1000 [=>............................] - ETA: 6:49 - loss: 2.9172 - regression_loss: 2.0998 - classification_loss: 0.8174
  95/1000 [=>............................] - ETA: 6:49 - loss: 2.9343 - regression_loss: 2.1171 - classification_loss: 0.8171
  96/1000 [=>............................] - ETA: 6:48 - loss: 2.9333 - regression_loss: 2.1174 - classification_loss: 0.8159
  97/1000 [=>............................] - ETA: 6:48 - loss: 2.9261 - regression_loss: 2.1149 - classification_loss: 0.8112
  98/1000 [=>............................] - ETA: 6:47 - loss: 2.9328 - regression_loss: 2.1230 - classification_loss: 0.8098
  99/1000 [=>............................] - ETA: 6:47 - loss: 2.9389 - regression_loss: 2.1326 - classification_loss: 0.8063
 100/1000 [==>...........................] - ETA: 6:46 - loss: 2.9368 - regression_loss: 2.1339 - classification_loss: 0.8029
 101/1000 [==>...........................] - ETA: 6:46 - loss: 2.9079 - regression_loss: 2.1128 - classification_loss: 0.7951
 102/1000 [==>...........................] - ETA: 6:45 - loss: 2.9106 - regression_loss: 2.1179 - classification_loss: 0.7927
 103/1000 [==>...........................] - ETA: 6:45 - loss: 2.9112 - regression_loss: 2.1224 - classification_loss: 0.7887
 104/1000 [==>...........................] - ETA: 6:44 - loss: 2.9162 - regression_loss: 2.1253 - classification_loss: 0.7910
 105/1000 [==>...........................] - ETA: 6:44 - loss: 2.8885 - regression_loss: 2.1050 - classification_loss: 0.7834
 106/1000 [==>...........................] - ETA: 6:44 - loss: 2.8923 - regression_loss: 2.1107 - classification_loss: 0.7816
 107/1000 [==>...........................] - ETA: 6:43 - loss: 2.8994 - regression_loss: 2.1207 - classification_loss: 0.7786
 108/1000 [==>...........................] - ETA: 6:43 - loss: 2.9109 - regression_loss: 2.1310 - classification_loss: 0.7799
 109/1000 [==>...........................] - ETA: 6:42 - loss: 2.8842 - regression_loss: 2.1114 - classification_loss: 0.7728
 110/1000 [==>...........................] - ETA: 6:42 - loss: 2.8898 - regression_loss: 2.1202 - classification_loss: 0.7695
 111/1000 [==>...........................] - ETA: 6:41 - loss: 2.8814 - regression_loss: 2.1168 - classification_loss: 0.7646
 112/1000 [==>...........................] - ETA: 6:41 - loss: 2.8816 - regression_loss: 2.1204 - classification_loss: 0.7611
 113/1000 [==>...........................] - ETA: 6:40 - loss: 2.8561 - regression_loss: 2.1017 - classification_loss: 0.7544
 114/1000 [==>...........................] - ETA: 6:40 - loss: 2.8651 - regression_loss: 2.1092 - classification_loss: 0.7559
 115/1000 [==>...........................] - ETA: 6:40 - loss: 2.8402 - regression_loss: 2.0909 - classification_loss: 0.7493
 116/1000 [==>...........................] - ETA: 6:39 - loss: 2.8452 - regression_loss: 2.0972 - classification_loss: 0.7480
 117/1000 [==>...........................] - ETA: 6:39 - loss: 2.8455 - regression_loss: 2.0992 - classification_loss: 0.7463
 118/1000 [==>...........................] - ETA: 6:38 - loss: 2.8534 - regression_loss: 2.1077 - classification_loss: 0.7457
 119/1000 [==>...........................] - ETA: 6:38 - loss: 2.8761 - regression_loss: 2.1274 - classification_loss: 0.7487
 120/1000 [==>...........................] - ETA: 6:37 - loss: 2.8805 - regression_loss: 2.1340 - classification_loss: 0.7465
 121/1000 [==>...........................] - ETA: 6:37 - loss: 2.8567 - regression_loss: 2.1163 - classification_loss: 0.7403
 122/1000 [==>...........................] - ETA: 6:37 - loss: 2.8722 - regression_loss: 2.1270 - classification_loss: 0.7452
 123/1000 [==>...........................] - ETA: 6:36 - loss: 2.8488 - regression_loss: 2.1097 - classification_loss: 0.7391
 124/1000 [==>...........................] - ETA: 6:36 - loss: 2.8518 - regression_loss: 2.1140 - classification_loss: 0.7378
 125/1000 [==>...........................] - ETA: 6:35 - loss: 2.8569 - regression_loss: 2.1187 - classification_loss: 0.7382
 126/1000 [==>...........................] - ETA: 6:35 - loss: 2.8588 - regression_loss: 2.1175 - classification_loss: 0.7413
 127/1000 [==>...........................] - ETA: 6:34 - loss: 2.8652 - regression_loss: 2.1254 - classification_loss: 0.7398
 128/1000 [==>...........................] - ETA: 6:34 - loss: 2.8816 - regression_loss: 2.1351 - classification_loss: 0.7464
 129/1000 [==>...........................] - ETA: 6:33 - loss: 2.8766 - regression_loss: 2.1328 - classification_loss: 0.7438
 130/1000 [==>...........................] - ETA: 6:33 - loss: 2.8927 - regression_loss: 2.1409 - classification_loss: 0.7518
 131/1000 [==>...........................] - ETA: 6:32 - loss: 2.9089 - regression_loss: 2.1509 - classification_loss: 0.7580
 132/1000 [==>...........................] - ETA: 6:32 - loss: 2.9095 - regression_loss: 2.1525 - classification_loss: 0.7570
 133/1000 [==>...........................] - ETA: 6:32 - loss: 2.9220 - regression_loss: 2.1616 - classification_loss: 0.7604
 134/1000 [===>..........................] - ETA: 6:31 - loss: 2.9327 - regression_loss: 2.1663 - classification_loss: 0.7664
 135/1000 [===>..........................] - ETA: 6:31 - loss: 2.9242 - regression_loss: 2.1614 - classification_loss: 0.7628
 136/1000 [===>..........................] - ETA: 6:30 - loss: 2.9254 - regression_loss: 2.1645 - classification_loss: 0.7609
 137/1000 [===>..........................] - ETA: 6:30 - loss: 2.9322 - regression_loss: 2.1703 - classification_loss: 0.7619
 138/1000 [===>..........................] - ETA: 6:29 - loss: 2.9369 - regression_loss: 2.1776 - classification_loss: 0.7592
 139/1000 [===>..........................] - ETA: 6:29 - loss: 2.9158 - regression_loss: 2.1620 - classification_loss: 0.7538
 140/1000 [===>..........................] - ETA: 6:29 - loss: 2.9262 - regression_loss: 2.1639 - classification_loss: 0.7623
 141/1000 [===>..........................] - ETA: 6:28 - loss: 2.9298 - regression_loss: 2.1696 - classification_loss: 0.7601
 142/1000 [===>..........................] - ETA: 6:28 - loss: 2.9393 - regression_loss: 2.1798 - classification_loss: 0.7595
 143/1000 [===>..........................] - ETA: 6:27 - loss: 2.9187 - regression_loss: 2.1645 - classification_loss: 0.7542
 144/1000 [===>..........................] - ETA: 6:27 - loss: 2.9261 - regression_loss: 2.1698 - classification_loss: 0.7563
 145/1000 [===>..........................] - ETA: 6:26 - loss: 2.9221 - regression_loss: 2.1681 - classification_loss: 0.7539
 146/1000 [===>..........................] - ETA: 6:26 - loss: 2.9268 - regression_loss: 2.1707 - classification_loss: 0.7560
 147/1000 [===>..........................] - ETA: 6:25 - loss: 2.9069 - regression_loss: 2.1560 - classification_loss: 0.7509
 148/1000 [===>..........................] - ETA: 6:25 - loss: 2.9086 - regression_loss: 2.1608 - classification_loss: 0.7478
 149/1000 [===>..........................] - ETA: 6:24 - loss: 2.9105 - regression_loss: 2.1617 - classification_loss: 0.7488
 150/1000 [===>..........................] - ETA: 6:24 - loss: 2.9239 - regression_loss: 2.1654 - classification_loss: 0.7585
 151/1000 [===>..........................] - ETA: 6:24 - loss: 2.9320 - regression_loss: 2.1722 - classification_loss: 0.7598
 152/1000 [===>..........................] - ETA: 6:23 - loss: 2.9480 - regression_loss: 2.1844 - classification_loss: 0.7635
 153/1000 [===>..........................] - ETA: 6:23 - loss: 2.9461 - regression_loss: 2.1831 - classification_loss: 0.7630
 154/1000 [===>..........................] - ETA: 6:22 - loss: 2.9496 - regression_loss: 2.1888 - classification_loss: 0.7608
 155/1000 [===>..........................] - ETA: 6:22 - loss: 2.9518 - regression_loss: 2.1933 - classification_loss: 0.7585
 156/1000 [===>..........................] - ETA: 6:21 - loss: 2.9510 - regression_loss: 2.1920 - classification_loss: 0.7590
 157/1000 [===>..........................] - ETA: 6:21 - loss: 2.9698 - regression_loss: 2.2083 - classification_loss: 0.7615
 158/1000 [===>..........................] - ETA: 6:21 - loss: 2.9803 - regression_loss: 2.2156 - classification_loss: 0.7648
 159/1000 [===>..........................] - ETA: 6:20 - loss: 2.9786 - regression_loss: 2.2154 - classification_loss: 0.7631
 160/1000 [===>..........................] - ETA: 6:20 - loss: 2.9794 - regression_loss: 2.2180 - classification_loss: 0.7614
 161/1000 [===>..........................] - ETA: 6:19 - loss: 2.9834 - regression_loss: 2.2201 - classification_loss: 0.7633
 162/1000 [===>..........................] - ETA: 6:19 - loss: 2.9817 - regression_loss: 2.2213 - classification_loss: 0.7605
 163/1000 [===>..........................] - ETA: 6:18 - loss: 2.9635 - regression_loss: 2.2076 - classification_loss: 0.7558
 164/1000 [===>..........................] - ETA: 6:18 - loss: 2.9647 - regression_loss: 2.2108 - classification_loss: 0.7539
 165/1000 [===>..........................] - ETA: 6:17 - loss: 2.9663 - regression_loss: 2.2142 - classification_loss: 0.7522
 166/1000 [===>..........................] - ETA: 6:17 - loss: 2.9725 - regression_loss: 2.2200 - classification_loss: 0.7526
 167/1000 [====>.........................] - ETA: 6:17 - loss: 2.9667 - regression_loss: 2.2166 - classification_loss: 0.7501
 168/1000 [====>.........................] - ETA: 6:16 - loss: 2.9721 - regression_loss: 2.2225 - classification_loss: 0.7496
 169/1000 [====>.........................] - ETA: 6:16 - loss: 2.9723 - regression_loss: 2.2245 - classification_loss: 0.7478
 170/1000 [====>.........................] - ETA: 6:15 - loss: 2.9773 - regression_loss: 2.2312 - classification_loss: 0.7460
 171/1000 [====>.........................] - ETA: 6:15 - loss: 2.9808 - regression_loss: 2.2362 - classification_loss: 0.7447
 172/1000 [====>.........................] - ETA: 6:14 - loss: 2.9804 - regression_loss: 2.2382 - classification_loss: 0.7422
 173/1000 [====>.........................] - ETA: 6:14 - loss: 2.9902 - regression_loss: 2.2471 - classification_loss: 0.7431
 174/1000 [====>.........................] - ETA: 6:13 - loss: 2.9982 - regression_loss: 2.2538 - classification_loss: 0.7445
 175/1000 [====>.........................] - ETA: 6:13 - loss: 3.0007 - regression_loss: 2.2564 - classification_loss: 0.7444
 176/1000 [====>.........................] - ETA: 6:13 - loss: 3.0170 - regression_loss: 2.2682 - classification_loss: 0.7488
 177/1000 [====>.........................] - ETA: 6:12 - loss: 3.0189 - regression_loss: 2.2701 - classification_loss: 0.7488
 178/1000 [====>.........................] - ETA: 6:12 - loss: 3.0293 - regression_loss: 2.2802 - classification_loss: 0.7491
 179/1000 [====>.........................] - ETA: 6:11 - loss: 3.0124 - regression_loss: 2.2674 - classification_loss: 0.7450
 180/1000 [====>.........................] - ETA: 6:11 - loss: 3.0229 - regression_loss: 2.2787 - classification_loss: 0.7441
 181/1000 [====>.........................] - ETA: 6:10 - loss: 3.0323 - regression_loss: 2.2877 - classification_loss: 0.7446
 182/1000 [====>.........................] - ETA: 6:10 - loss: 3.0164 - regression_loss: 2.2751 - classification_loss: 0.7413
 183/1000 [====>.........................] - ETA: 6:09 - loss: 3.0153 - regression_loss: 2.2749 - classification_loss: 0.7404
 184/1000 [====>.........................] - ETA: 6:09 - loss: 3.0147 - regression_loss: 2.2755 - classification_loss: 0.7392
 185/1000 [====>.........................] - ETA: 6:08 - loss: 3.0213 - regression_loss: 2.2804 - classification_loss: 0.7409
 186/1000 [====>.........................] - ETA: 6:08 - loss: 3.0293 - regression_loss: 2.2864 - classification_loss: 0.7429
 187/1000 [====>.........................] - ETA: 6:08 - loss: 3.0307 - regression_loss: 2.2890 - classification_loss: 0.7416
 188/1000 [====>.........................] - ETA: 6:07 - loss: 3.0370 - regression_loss: 2.2960 - classification_loss: 0.7410
 189/1000 [====>.........................] - ETA: 6:07 - loss: 3.0401 - regression_loss: 2.2986 - classification_loss: 0.7415
 190/1000 [====>.........................] - ETA: 6:06 - loss: 3.0432 - regression_loss: 2.3002 - classification_loss: 0.7430
 191/1000 [====>.........................] - ETA: 6:06 - loss: 3.0528 - regression_loss: 2.3066 - classification_loss: 0.7462
 192/1000 [====>.........................] - ETA: 6:05 - loss: 3.0527 - regression_loss: 2.3073 - classification_loss: 0.7454
 193/1000 [====>.........................] - ETA: 6:05 - loss: 3.0369 - regression_loss: 2.2954 - classification_loss: 0.7415
 194/1000 [====>.........................] - ETA: 6:04 - loss: 3.0432 - regression_loss: 2.3008 - classification_loss: 0.7425
 195/1000 [====>.........................] - ETA: 6:04 - loss: 3.0420 - regression_loss: 2.3014 - classification_loss: 0.7406
 196/1000 [====>.........................] - ETA: 6:04 - loss: 3.0265 - regression_loss: 2.2896 - classification_loss: 0.7368
 197/1000 [====>.........................] - ETA: 6:03 - loss: 3.0261 - regression_loss: 2.2906 - classification_loss: 0.7355
 198/1000 [====>.........................] - ETA: 6:03 - loss: 3.0230 - regression_loss: 2.2882 - classification_loss: 0.7348
 199/1000 [====>.........................] - ETA: 6:02 - loss: 3.0078 - regression_loss: 2.2767 - classification_loss: 0.7311
 200/1000 [=====>........................] - ETA: 6:02 - loss: 2.9928 - regression_loss: 2.2653 - classification_loss: 0.7275
 201/1000 [=====>........................] - ETA: 6:01 - loss: 2.9903 - regression_loss: 2.2620 - classification_loss: 0.7283
 202/1000 [=====>........................] - ETA: 6:01 - loss: 2.9755 - regression_loss: 2.2508 - classification_loss: 0.7247
 203/1000 [=====>........................] - ETA: 6:00 - loss: 2.9624 - regression_loss: 2.2397 - classification_loss: 0.7227
 204/1000 [=====>........................] - ETA: 6:00 - loss: 2.9664 - regression_loss: 2.2429 - classification_loss: 0.7235
 205/1000 [=====>........................] - ETA: 6:00 - loss: 2.9654 - regression_loss: 2.2436 - classification_loss: 0.7218
 206/1000 [=====>........................] - ETA: 5:59 - loss: 2.9735 - regression_loss: 2.2488 - classification_loss: 0.7246
 207/1000 [=====>........................] - ETA: 5:59 - loss: 2.9591 - regression_loss: 2.2380 - classification_loss: 0.7211
 208/1000 [=====>........................] - ETA: 5:58 - loss: 2.9449 - regression_loss: 2.2272 - classification_loss: 0.7176
 209/1000 [=====>........................] - ETA: 5:58 - loss: 2.9557 - regression_loss: 2.2338 - classification_loss: 0.7219
 210/1000 [=====>........................] - ETA: 5:57 - loss: 2.9416 - regression_loss: 2.2232 - classification_loss: 0.7184
 211/1000 [=====>........................] - ETA: 5:57 - loss: 2.9440 - regression_loss: 2.2253 - classification_loss: 0.7186
 212/1000 [=====>........................] - ETA: 5:56 - loss: 2.9494 - regression_loss: 2.2265 - classification_loss: 0.7229
 213/1000 [=====>........................] - ETA: 5:56 - loss: 2.9518 - regression_loss: 2.2280 - classification_loss: 0.7238
 214/1000 [=====>........................] - ETA: 5:55 - loss: 2.9533 - regression_loss: 2.2309 - classification_loss: 0.7224
 215/1000 [=====>........................] - ETA: 5:55 - loss: 2.9635 - regression_loss: 2.2358 - classification_loss: 0.7276
 216/1000 [=====>........................] - ETA: 5:55 - loss: 2.9639 - regression_loss: 2.2334 - classification_loss: 0.7305
 217/1000 [=====>........................] - ETA: 5:54 - loss: 2.9503 - regression_loss: 2.2231 - classification_loss: 0.7272
 218/1000 [=====>........................] - ETA: 5:54 - loss: 2.9540 - regression_loss: 2.2235 - classification_loss: 0.7304
 219/1000 [=====>........................] - ETA: 5:53 - loss: 2.9602 - regression_loss: 2.2254 - classification_loss: 0.7348
 220/1000 [=====>........................] - ETA: 5:53 - loss: 2.9735 - regression_loss: 2.2340 - classification_loss: 0.7395
 221/1000 [=====>........................] - ETA: 5:52 - loss: 2.9799 - regression_loss: 2.2398 - classification_loss: 0.7401
 222/1000 [=====>........................] - ETA: 5:52 - loss: 2.9825 - regression_loss: 2.2426 - classification_loss: 0.7399
 223/1000 [=====>........................] - ETA: 5:51 - loss: 2.9954 - regression_loss: 2.2522 - classification_loss: 0.7432
 224/1000 [=====>........................] - ETA: 5:51 - loss: 3.0001 - regression_loss: 2.2524 - classification_loss: 0.7476
 225/1000 [=====>........................] - ETA: 5:51 - loss: 2.9867 - regression_loss: 2.2424 - classification_loss: 0.7443
 226/1000 [=====>........................] - ETA: 5:50 - loss: 2.9890 - regression_loss: 2.2462 - classification_loss: 0.7429
 227/1000 [=====>........................] - ETA: 5:50 - loss: 3.0069 - regression_loss: 2.2595 - classification_loss: 0.7475
 228/1000 [=====>........................] - ETA: 5:49 - loss: 2.9937 - regression_loss: 2.2496 - classification_loss: 0.7442
 229/1000 [=====>........................] - ETA: 5:49 - loss: 2.9981 - regression_loss: 2.2512 - classification_loss: 0.7469
 230/1000 [=====>........................] - ETA: 5:48 - loss: 2.9991 - regression_loss: 2.2538 - classification_loss: 0.7453
 231/1000 [=====>........................] - ETA: 5:48 - loss: 3.0020 - regression_loss: 2.2562 - classification_loss: 0.7458
 232/1000 [=====>........................] - ETA: 5:47 - loss: 2.9951 - regression_loss: 2.2514 - classification_loss: 0.7438
 233/1000 [=====>........................] - ETA: 5:47 - loss: 2.9960 - regression_loss: 2.2532 - classification_loss: 0.7429
 234/1000 [======>.......................] - ETA: 5:46 - loss: 3.0059 - regression_loss: 2.2617 - classification_loss: 0.7441
 235/1000 [======>.......................] - ETA: 5:46 - loss: 2.9934 - regression_loss: 2.2521 - classification_loss: 0.7413
 236/1000 [======>.......................] - ETA: 5:46 - loss: 2.9807 - regression_loss: 2.2426 - classification_loss: 0.7382
 237/1000 [======>.......................] - ETA: 5:45 - loss: 2.9745 - regression_loss: 2.2384 - classification_loss: 0.7361
 238/1000 [======>.......................] - ETA: 5:45 - loss: 2.9737 - regression_loss: 2.2392 - classification_loss: 0.7345
 239/1000 [======>.......................] - ETA: 5:44 - loss: 2.9675 - regression_loss: 2.2341 - classification_loss: 0.7334
 240/1000 [======>.......................] - ETA: 5:44 - loss: 2.9727 - regression_loss: 2.2352 - classification_loss: 0.7375
 241/1000 [======>.......................] - ETA: 5:43 - loss: 2.9771 - regression_loss: 2.2378 - classification_loss: 0.7393
 242/1000 [======>.......................] - ETA: 5:43 - loss: 2.9726 - regression_loss: 2.2353 - classification_loss: 0.7373
 243/1000 [======>.......................] - ETA: 5:42 - loss: 2.9833 - regression_loss: 2.2419 - classification_loss: 0.7414
 244/1000 [======>.......................] - ETA: 5:42 - loss: 2.9836 - regression_loss: 2.2424 - classification_loss: 0.7412
 245/1000 [======>.......................] - ETA: 5:41 - loss: 2.9714 - regression_loss: 2.2332 - classification_loss: 0.7382
 246/1000 [======>.......................] - ETA: 5:41 - loss: 2.9730 - regression_loss: 2.2349 - classification_loss: 0.7381
 247/1000 [======>.......................] - ETA: 5:41 - loss: 2.9781 - regression_loss: 2.2368 - classification_loss: 0.7413
 248/1000 [======>.......................] - ETA: 5:40 - loss: 2.9880 - regression_loss: 2.2420 - classification_loss: 0.7460
 249/1000 [======>.......................] - ETA: 5:40 - loss: 2.9902 - regression_loss: 2.2456 - classification_loss: 0.7445
 250/1000 [======>.......................] - ETA: 5:39 - loss: 2.9783 - regression_loss: 2.2367 - classification_loss: 0.7416
 251/1000 [======>.......................] - ETA: 5:39 - loss: 2.9823 - regression_loss: 2.2392 - classification_loss: 0.7431
 252/1000 [======>.......................] - ETA: 5:38 - loss: 2.9733 - regression_loss: 2.2303 - classification_loss: 0.7430
 253/1000 [======>.......................] - ETA: 5:38 - loss: 2.9709 - regression_loss: 2.2291 - classification_loss: 0.7418
 254/1000 [======>.......................] - ETA: 5:37 - loss: 2.9593 - regression_loss: 2.2203 - classification_loss: 0.7390
 255/1000 [======>.......................] - ETA: 5:37 - loss: 2.9563 - regression_loss: 2.2190 - classification_loss: 0.7373
 256/1000 [======>.......................] - ETA: 5:36 - loss: 2.9635 - regression_loss: 2.2229 - classification_loss: 0.7406
 257/1000 [======>.......................] - ETA: 5:36 - loss: 2.9533 - regression_loss: 2.2142 - classification_loss: 0.7391
 258/1000 [======>.......................] - ETA: 5:36 - loss: 2.9589 - regression_loss: 2.2191 - classification_loss: 0.7398
 259/1000 [======>.......................] - ETA: 5:35 - loss: 2.9620 - regression_loss: 2.2213 - classification_loss: 0.7407
 260/1000 [======>.......................] - ETA: 5:35 - loss: 2.9621 - regression_loss: 2.2202 - classification_loss: 0.7418
 261/1000 [======>.......................] - ETA: 5:34 - loss: 2.9657 - regression_loss: 2.2222 - classification_loss: 0.7435
 262/1000 [======>.......................] - ETA: 5:34 - loss: 2.9668 - regression_loss: 2.2212 - classification_loss: 0.7456
 263/1000 [======>.......................] - ETA: 5:33 - loss: 2.9762 - regression_loss: 2.2296 - classification_loss: 0.7466
 264/1000 [======>.......................] - ETA: 5:33 - loss: 2.9799 - regression_loss: 2.2329 - classification_loss: 0.7470
 265/1000 [======>.......................] - ETA: 5:32 - loss: 2.9686 - regression_loss: 2.2245 - classification_loss: 0.7442
 266/1000 [======>.......................] - ETA: 5:32 - loss: 2.9706 - regression_loss: 2.2261 - classification_loss: 0.7445
 267/1000 [=======>......................] - ETA: 5:31 - loss: 2.9738 - regression_loss: 2.2288 - classification_loss: 0.7450
 268/1000 [=======>......................] - ETA: 5:31 - loss: 2.9796 - regression_loss: 2.2330 - classification_loss: 0.7466
 269/1000 [=======>......................] - ETA: 5:31 - loss: 2.9807 - regression_loss: 2.2339 - classification_loss: 0.7467
 270/1000 [=======>......................] - ETA: 5:30 - loss: 2.9853 - regression_loss: 2.2384 - classification_loss: 0.7468
 271/1000 [=======>......................] - ETA: 5:30 - loss: 2.9875 - regression_loss: 2.2398 - classification_loss: 0.7477
 272/1000 [=======>......................] - ETA: 5:29 - loss: 2.9931 - regression_loss: 2.2445 - classification_loss: 0.7486
 273/1000 [=======>......................] - ETA: 5:29 - loss: 2.9918 - regression_loss: 2.2441 - classification_loss: 0.7477
 274/1000 [=======>......................] - ETA: 5:28 - loss: 2.9923 - regression_loss: 2.2461 - classification_loss: 0.7463
 275/1000 [=======>......................] - ETA: 5:28 - loss: 2.9973 - regression_loss: 2.2504 - classification_loss: 0.7468
 276/1000 [=======>......................] - ETA: 5:27 - loss: 3.0005 - regression_loss: 2.2543 - classification_loss: 0.7462
 277/1000 [=======>......................] - ETA: 5:27 - loss: 3.0023 - regression_loss: 2.2570 - classification_loss: 0.7452
 278/1000 [=======>......................] - ETA: 5:26 - loss: 2.9924 - regression_loss: 2.2489 - classification_loss: 0.7435
 279/1000 [=======>......................] - ETA: 5:26 - loss: 2.9942 - regression_loss: 2.2513 - classification_loss: 0.7428
 280/1000 [=======>......................] - ETA: 5:26 - loss: 2.9951 - regression_loss: 2.2527 - classification_loss: 0.7424
 281/1000 [=======>......................] - ETA: 5:25 - loss: 2.9978 - regression_loss: 2.2548 - classification_loss: 0.7430
 282/1000 [=======>......................] - ETA: 5:25 - loss: 2.9877 - regression_loss: 2.2468 - classification_loss: 0.7409
 283/1000 [=======>......................] - ETA: 5:24 - loss: 2.9907 - regression_loss: 2.2484 - classification_loss: 0.7423
 284/1000 [=======>......................] - ETA: 5:24 - loss: 2.9802 - regression_loss: 2.2405 - classification_loss: 0.7397
 285/1000 [=======>......................] - ETA: 5:23 - loss: 2.9797 - regression_loss: 2.2407 - classification_loss: 0.7389
 286/1000 [=======>......................] - ETA: 5:23 - loss: 2.9692 - regression_loss: 2.2329 - classification_loss: 0.7364
 287/1000 [=======>......................] - ETA: 5:22 - loss: 2.9590 - regression_loss: 2.2251 - classification_loss: 0.7339
 288/1000 [=======>......................] - ETA: 5:22 - loss: 2.9604 - regression_loss: 2.2256 - classification_loss: 0.7348
 289/1000 [=======>......................] - ETA: 5:21 - loss: 2.9664 - regression_loss: 2.2304 - classification_loss: 0.7360
 290/1000 [=======>......................] - ETA: 5:21 - loss: 2.9741 - regression_loss: 2.2383 - classification_loss: 0.7358
 291/1000 [=======>......................] - ETA: 5:21 - loss: 2.9638 - regression_loss: 2.2306 - classification_loss: 0.7333
 292/1000 [=======>......................] - ETA: 5:20 - loss: 2.9537 - regression_loss: 2.2229 - classification_loss: 0.7308
 293/1000 [=======>......................] - ETA: 5:20 - loss: 2.9436 - regression_loss: 2.2153 - classification_loss: 0.7283
 294/1000 [=======>......................] - ETA: 5:19 - loss: 2.9401 - regression_loss: 2.2130 - classification_loss: 0.7272
 295/1000 [=======>......................] - ETA: 5:19 - loss: 2.9464 - regression_loss: 2.2167 - classification_loss: 0.7297
 296/1000 [=======>......................] - ETA: 5:18 - loss: 2.9470 - regression_loss: 2.2176 - classification_loss: 0.7294
 297/1000 [=======>......................] - ETA: 5:18 - loss: 2.9489 - regression_loss: 2.2180 - classification_loss: 0.7309
 298/1000 [=======>......................] - ETA: 5:17 - loss: 2.9512 - regression_loss: 2.2196 - classification_loss: 0.7316
 299/1000 [=======>......................] - ETA: 5:17 - loss: 2.9578 - regression_loss: 2.2257 - classification_loss: 0.7321
 300/1000 [========>.....................] - ETA: 5:16 - loss: 2.9625 - regression_loss: 2.2265 - classification_loss: 0.7360
 301/1000 [========>.....................] - ETA: 5:16 - loss: 2.9687 - regression_loss: 2.2311 - classification_loss: 0.7376
 302/1000 [========>.....................] - ETA: 5:16 - loss: 2.9590 - regression_loss: 2.2237 - classification_loss: 0.7353
 303/1000 [========>.....................] - ETA: 5:15 - loss: 2.9643 - regression_loss: 2.2269 - classification_loss: 0.7374
 304/1000 [========>.....................] - ETA: 5:15 - loss: 2.9546 - regression_loss: 2.2196 - classification_loss: 0.7350
 305/1000 [========>.....................] - ETA: 5:14 - loss: 2.9449 - regression_loss: 2.2123 - classification_loss: 0.7326
 306/1000 [========>.....................] - ETA: 5:14 - loss: 2.9434 - regression_loss: 2.2108 - classification_loss: 0.7325
 307/1000 [========>.....................] - ETA: 5:13 - loss: 2.9517 - regression_loss: 2.2162 - classification_loss: 0.7355
 308/1000 [========>.....................] - ETA: 5:13 - loss: 2.9421 - regression_loss: 2.2090 - classification_loss: 0.7331
 309/1000 [========>.....................] - ETA: 5:12 - loss: 2.9439 - regression_loss: 2.2087 - classification_loss: 0.7352
 310/1000 [========>.....................] - ETA: 5:12 - loss: 2.9472 - regression_loss: 2.2091 - classification_loss: 0.7381
 311/1000 [========>.....................] - ETA: 5:12 - loss: 2.9481 - regression_loss: 2.2112 - classification_loss: 0.7369
 312/1000 [========>.....................] - ETA: 5:11 - loss: 2.9542 - regression_loss: 2.2149 - classification_loss: 0.7393
 313/1000 [========>.....................] - ETA: 5:11 - loss: 2.9553 - regression_loss: 2.2151 - classification_loss: 0.7402
 314/1000 [========>.....................] - ETA: 5:10 - loss: 2.9612 - regression_loss: 2.2198 - classification_loss: 0.7414
 315/1000 [========>.....................] - ETA: 5:10 - loss: 2.9633 - regression_loss: 2.2227 - classification_loss: 0.7406
 316/1000 [========>.....................] - ETA: 5:09 - loss: 2.9539 - regression_loss: 2.2157 - classification_loss: 0.7382
 317/1000 [========>.....................] - ETA: 5:09 - loss: 2.9446 - regression_loss: 2.2087 - classification_loss: 0.7359
 318/1000 [========>.....................] - ETA: 5:08 - loss: 2.9495 - regression_loss: 2.2117 - classification_loss: 0.7378
 319/1000 [========>.....................] - ETA: 5:08 - loss: 2.9532 - regression_loss: 2.2152 - classification_loss: 0.7380
 320/1000 [========>.....................] - ETA: 5:08 - loss: 2.9585 - regression_loss: 2.2178 - classification_loss: 0.7407
 321/1000 [========>.....................] - ETA: 5:07 - loss: 2.9662 - regression_loss: 2.2223 - classification_loss: 0.7439
 322/1000 [========>.....................] - ETA: 5:07 - loss: 2.9718 - regression_loss: 2.2245 - classification_loss: 0.7473
 323/1000 [========>.....................] - ETA: 5:06 - loss: 2.9834 - regression_loss: 2.2332 - classification_loss: 0.7502
 324/1000 [========>.....................] - ETA: 5:06 - loss: 2.9846 - regression_loss: 2.2333 - classification_loss: 0.7513
 325/1000 [========>.....................] - ETA: 5:05 - loss: 2.9922 - regression_loss: 2.2395 - classification_loss: 0.7527
 326/1000 [========>.....................] - ETA: 5:05 - loss: 2.9982 - regression_loss: 2.2425 - classification_loss: 0.7557
 327/1000 [========>.....................] - ETA: 5:04 - loss: 2.9973 - regression_loss: 2.2426 - classification_loss: 0.7547
 328/1000 [========>.....................] - ETA: 5:04 - loss: 2.9985 - regression_loss: 2.2429 - classification_loss: 0.7555
 329/1000 [========>.....................] - ETA: 5:03 - loss: 3.0029 - regression_loss: 2.2463 - classification_loss: 0.7566
 330/1000 [========>.....................] - ETA: 5:03 - loss: 2.9938 - regression_loss: 2.2395 - classification_loss: 0.7543
 331/1000 [========>.....................] - ETA: 5:03 - loss: 2.9981 - regression_loss: 2.2421 - classification_loss: 0.7560
 332/1000 [========>.....................] - ETA: 5:02 - loss: 3.0066 - regression_loss: 2.2503 - classification_loss: 0.7564
 333/1000 [========>.....................] - ETA: 5:02 - loss: 2.9979 - regression_loss: 2.2435 - classification_loss: 0.7544
 334/1000 [=========>....................] - ETA: 5:01 - loss: 3.0031 - regression_loss: 2.2476 - classification_loss: 0.7555
 335/1000 [=========>....................] - ETA: 5:01 - loss: 3.0082 - regression_loss: 2.2514 - classification_loss: 0.7567
 336/1000 [=========>....................] - ETA: 5:00 - loss: 3.0098 - regression_loss: 2.2536 - classification_loss: 0.7562
 337/1000 [=========>....................] - ETA: 5:00 - loss: 3.0134 - regression_loss: 2.2550 - classification_loss: 0.7584
 338/1000 [=========>....................] - ETA: 4:59 - loss: 3.0197 - regression_loss: 2.2597 - classification_loss: 0.7600
 339/1000 [=========>....................] - ETA: 4:59 - loss: 3.0109 - regression_loss: 2.2530 - classification_loss: 0.7579
 340/1000 [=========>....................] - ETA: 4:58 - loss: 3.0113 - regression_loss: 2.2538 - classification_loss: 0.7575
 341/1000 [=========>....................] - ETA: 4:58 - loss: 3.0159 - regression_loss: 2.2570 - classification_loss: 0.7589
 342/1000 [=========>....................] - ETA: 4:58 - loss: 3.0197 - regression_loss: 2.2606 - classification_loss: 0.7591
 343/1000 [=========>....................] - ETA: 4:57 - loss: 3.0174 - regression_loss: 2.2598 - classification_loss: 0.7576
 344/1000 [=========>....................] - ETA: 4:57 - loss: 3.0198 - regression_loss: 2.2633 - classification_loss: 0.7565
 345/1000 [=========>....................] - ETA: 4:56 - loss: 3.0205 - regression_loss: 2.2634 - classification_loss: 0.7572
 346/1000 [=========>....................] - ETA: 4:56 - loss: 3.0119 - regression_loss: 2.2568 - classification_loss: 0.7551
 347/1000 [=========>....................] - ETA: 4:55 - loss: 3.0125 - regression_loss: 2.2568 - classification_loss: 0.7557
 348/1000 [=========>....................] - ETA: 4:55 - loss: 3.0150 - regression_loss: 2.2588 - classification_loss: 0.7561
 349/1000 [=========>....................] - ETA: 4:54 - loss: 3.0063 - regression_loss: 2.2523 - classification_loss: 0.7540
 350/1000 [=========>....................] - ETA: 4:54 - loss: 3.0096 - regression_loss: 2.2551 - classification_loss: 0.7545
 351/1000 [=========>....................] - ETA: 4:53 - loss: 3.0011 - regression_loss: 2.2487 - classification_loss: 0.7524
 352/1000 [=========>....................] - ETA: 4:53 - loss: 3.0015 - regression_loss: 2.2501 - classification_loss: 0.7514
 353/1000 [=========>....................] - ETA: 4:53 - loss: 3.0023 - regression_loss: 2.2518 - classification_loss: 0.7505
 354/1000 [=========>....................] - ETA: 4:52 - loss: 3.0068 - regression_loss: 2.2564 - classification_loss: 0.7504
 355/1000 [=========>....................] - ETA: 4:52 - loss: 2.9984 - regression_loss: 2.2501 - classification_loss: 0.7483
 356/1000 [=========>....................] - ETA: 4:51 - loss: 2.9977 - regression_loss: 2.2501 - classification_loss: 0.7476
 357/1000 [=========>....................] - ETA: 4:51 - loss: 2.9893 - regression_loss: 2.2438 - classification_loss: 0.7455
 358/1000 [=========>....................] - ETA: 4:50 - loss: 2.9930 - regression_loss: 2.2452 - classification_loss: 0.7477
 359/1000 [=========>....................] - ETA: 4:50 - loss: 2.9930 - regression_loss: 2.2454 - classification_loss: 0.7476
 360/1000 [=========>....................] - ETA: 4:49 - loss: 2.9914 - regression_loss: 2.2448 - classification_loss: 0.7466
 361/1000 [=========>....................] - ETA: 4:49 - loss: 2.9831 - regression_loss: 2.2386 - classification_loss: 0.7446
 362/1000 [=========>....................] - ETA: 4:48 - loss: 2.9807 - regression_loss: 2.2374 - classification_loss: 0.7433
 363/1000 [=========>....................] - ETA: 4:48 - loss: 2.9816 - regression_loss: 2.2379 - classification_loss: 0.7436
 364/1000 [=========>....................] - ETA: 4:48 - loss: 2.9734 - regression_loss: 2.2318 - classification_loss: 0.7416
 365/1000 [=========>....................] - ETA: 4:47 - loss: 2.9788 - regression_loss: 2.2356 - classification_loss: 0.7431
 366/1000 [=========>....................] - ETA: 4:47 - loss: 2.9706 - regression_loss: 2.2295 - classification_loss: 0.7411
 367/1000 [==========>...................] - ETA: 4:46 - loss: 2.9739 - regression_loss: 2.2317 - classification_loss: 0.7422
 368/1000 [==========>...................] - ETA: 4:46 - loss: 2.9658 - regression_loss: 2.2257 - classification_loss: 0.7402
 369/1000 [==========>...................] - ETA: 4:45 - loss: 2.9707 - regression_loss: 2.2277 - classification_loss: 0.7430
 370/1000 [==========>...................] - ETA: 4:45 - loss: 2.9627 - regression_loss: 2.2217 - classification_loss: 0.7410
 371/1000 [==========>...................] - ETA: 4:44 - loss: 2.9698 - regression_loss: 2.2257 - classification_loss: 0.7441
 372/1000 [==========>...................] - ETA: 4:44 - loss: 2.9784 - regression_loss: 2.2323 - classification_loss: 0.7461
 373/1000 [==========>...................] - ETA: 4:44 - loss: 2.9804 - regression_loss: 2.2343 - classification_loss: 0.7461
 374/1000 [==========>...................] - ETA: 4:43 - loss: 2.9781 - regression_loss: 2.2327 - classification_loss: 0.7454
 375/1000 [==========>...................] - ETA: 4:43 - loss: 2.9848 - regression_loss: 2.2375 - classification_loss: 0.7473
 376/1000 [==========>...................] - ETA: 4:42 - loss: 2.9914 - regression_loss: 2.2394 - classification_loss: 0.7520
 377/1000 [==========>...................] - ETA: 4:42 - loss: 2.9943 - regression_loss: 2.2413 - classification_loss: 0.7530
 378/1000 [==========>...................] - ETA: 4:41 - loss: 2.9953 - regression_loss: 2.2416 - classification_loss: 0.7536
 379/1000 [==========>...................] - ETA: 4:41 - loss: 2.9874 - regression_loss: 2.2357 - classification_loss: 0.7517
 380/1000 [==========>...................] - ETA: 4:40 - loss: 2.9879 - regression_loss: 2.2361 - classification_loss: 0.7518
 381/1000 [==========>...................] - ETA: 4:40 - loss: 2.9938 - regression_loss: 2.2401 - classification_loss: 0.7537
 382/1000 [==========>...................] - ETA: 4:39 - loss: 2.9860 - regression_loss: 2.2343 - classification_loss: 0.7517
 383/1000 [==========>...................] - ETA: 4:39 - loss: 2.9897 - regression_loss: 2.2360 - classification_loss: 0.7537
 384/1000 [==========>...................] - ETA: 4:39 - loss: 2.9929 - regression_loss: 2.2382 - classification_loss: 0.7547
 385/1000 [==========>...................] - ETA: 4:38 - loss: 2.9851 - regression_loss: 2.2324 - classification_loss: 0.7527
 386/1000 [==========>...................] - ETA: 4:38 - loss: 2.9847 - regression_loss: 2.2322 - classification_loss: 0.7525
 387/1000 [==========>...................] - ETA: 4:37 - loss: 2.9770 - regression_loss: 2.2264 - classification_loss: 0.7505
 388/1000 [==========>...................] - ETA: 4:37 - loss: 2.9799 - regression_loss: 2.2299 - classification_loss: 0.7499
 389/1000 [==========>...................] - ETA: 4:36 - loss: 2.9778 - regression_loss: 2.2287 - classification_loss: 0.7491
 390/1000 [==========>...................] - ETA: 4:36 - loss: 2.9790 - regression_loss: 2.2301 - classification_loss: 0.7489
 391/1000 [==========>...................] - ETA: 4:35 - loss: 2.9767 - regression_loss: 2.2282 - classification_loss: 0.7485
 392/1000 [==========>...................] - ETA: 4:35 - loss: 2.9803 - regression_loss: 2.2291 - classification_loss: 0.7512
 393/1000 [==========>...................] - ETA: 4:34 - loss: 2.9727 - regression_loss: 2.2234 - classification_loss: 0.7493
 394/1000 [==========>...................] - ETA: 4:34 - loss: 2.9747 - regression_loss: 2.2245 - classification_loss: 0.7501
 395/1000 [==========>...................] - ETA: 4:34 - loss: 2.9671 - regression_loss: 2.2189 - classification_loss: 0.7483
 396/1000 [==========>...................] - ETA: 4:33 - loss: 2.9597 - regression_loss: 2.2133 - classification_loss: 0.7464
 397/1000 [==========>...................] - ETA: 4:33 - loss: 2.9600 - regression_loss: 2.2143 - classification_loss: 0.7457
 398/1000 [==========>...................] - ETA: 4:32 - loss: 2.9676 - regression_loss: 2.2188 - classification_loss: 0.7488
 399/1000 [==========>...................] - ETA: 4:32 - loss: 2.9717 - regression_loss: 2.2225 - classification_loss: 0.7492
 400/1000 [===========>..................] - ETA: 4:31 - loss: 2.9719 - regression_loss: 2.2169 - classification_loss: 0.7549
 401/1000 [===========>..................] - ETA: 4:31 - loss: 2.9740 - regression_loss: 2.2184 - classification_loss: 0.7555
 402/1000 [===========>..................] - ETA: 4:30 - loss: 2.9747 - regression_loss: 2.2198 - classification_loss: 0.7550
 403/1000 [===========>..................] - ETA: 4:30 - loss: 2.9690 - regression_loss: 2.2142 - classification_loss: 0.7548
 404/1000 [===========>..................] - ETA: 4:29 - loss: 2.9721 - regression_loss: 2.2151 - classification_loss: 0.7571
 405/1000 [===========>..................] - ETA: 4:29 - loss: 2.9648 - regression_loss: 2.2096 - classification_loss: 0.7552
 406/1000 [===========>..................] - ETA: 4:29 - loss: 2.9654 - regression_loss: 2.2086 - classification_loss: 0.7569
 407/1000 [===========>..................] - ETA: 4:28 - loss: 2.9582 - regression_loss: 2.2032 - classification_loss: 0.7550
 408/1000 [===========>..................] - ETA: 4:28 - loss: 2.9584 - regression_loss: 2.2033 - classification_loss: 0.7551
 409/1000 [===========>..................] - ETA: 4:27 - loss: 2.9567 - regression_loss: 2.2027 - classification_loss: 0.7541
 410/1000 [===========>..................] - ETA: 4:27 - loss: 2.9602 - regression_loss: 2.2050 - classification_loss: 0.7552
 411/1000 [===========>..................] - ETA: 4:26 - loss: 2.9530 - regression_loss: 2.1997 - classification_loss: 0.7533
 412/1000 [===========>..................] - ETA: 4:26 - loss: 2.9525 - regression_loss: 2.1995 - classification_loss: 0.7530
 413/1000 [===========>..................] - ETA: 4:25 - loss: 2.9528 - regression_loss: 2.1990 - classification_loss: 0.7538
 414/1000 [===========>..................] - ETA: 4:25 - loss: 2.9557 - regression_loss: 2.2012 - classification_loss: 0.7545
 415/1000 [===========>..................] - ETA: 4:24 - loss: 2.9554 - regression_loss: 2.2014 - classification_loss: 0.7540
 416/1000 [===========>..................] - ETA: 4:24 - loss: 2.9570 - regression_loss: 2.2028 - classification_loss: 0.7542
 417/1000 [===========>..................] - ETA: 4:24 - loss: 2.9630 - regression_loss: 2.2063 - classification_loss: 0.7567
 418/1000 [===========>..................] - ETA: 4:23 - loss: 2.9559 - regression_loss: 2.2010 - classification_loss: 0.7549
 419/1000 [===========>..................] - ETA: 4:23 - loss: 2.9580 - regression_loss: 2.2031 - classification_loss: 0.7549
 420/1000 [===========>..................] - ETA: 4:22 - loss: 2.9588 - regression_loss: 2.2034 - classification_loss: 0.7554
 421/1000 [===========>..................] - ETA: 4:22 - loss: 2.9600 - regression_loss: 2.2049 - classification_loss: 0.7551
 422/1000 [===========>..................] - ETA: 4:21 - loss: 2.9636 - regression_loss: 2.2072 - classification_loss: 0.7564
 423/1000 [===========>..................] - ETA: 4:21 - loss: 2.9566 - regression_loss: 2.2020 - classification_loss: 0.7546
 424/1000 [===========>..................] - ETA: 4:20 - loss: 2.9496 - regression_loss: 2.1968 - classification_loss: 0.7528
 425/1000 [===========>..................] - ETA: 4:20 - loss: 2.9539 - regression_loss: 2.1997 - classification_loss: 0.7542
 426/1000 [===========>..................] - ETA: 4:20 - loss: 2.9565 - regression_loss: 2.2003 - classification_loss: 0.7562
 427/1000 [===========>..................] - ETA: 4:19 - loss: 2.9496 - regression_loss: 2.1952 - classification_loss: 0.7544
 428/1000 [===========>..................] - ETA: 4:19 - loss: 2.9537 - regression_loss: 2.1974 - classification_loss: 0.7563
 429/1000 [===========>..................] - ETA: 4:18 - loss: 2.9468 - regression_loss: 2.1923 - classification_loss: 0.7545
 430/1000 [===========>..................] - ETA: 4:18 - loss: 2.9400 - regression_loss: 2.1872 - classification_loss: 0.7528
 431/1000 [===========>..................] - ETA: 4:17 - loss: 2.9331 - regression_loss: 2.1821 - classification_loss: 0.7510
 432/1000 [===========>..................] - ETA: 4:17 - loss: 2.9349 - regression_loss: 2.1838 - classification_loss: 0.7511
 433/1000 [===========>..................] - ETA: 4:16 - loss: 2.9379 - regression_loss: 2.1872 - classification_loss: 0.7507
 434/1000 [============>.................] - ETA: 4:16 - loss: 2.9368 - regression_loss: 2.1854 - classification_loss: 0.7514
 435/1000 [============>.................] - ETA: 4:15 - loss: 2.9301 - regression_loss: 2.1804 - classification_loss: 0.7497
 436/1000 [============>.................] - ETA: 4:15 - loss: 2.9312 - regression_loss: 2.1824 - classification_loss: 0.7488
 437/1000 [============>.................] - ETA: 4:15 - loss: 2.9245 - regression_loss: 2.1774 - classification_loss: 0.7471
 438/1000 [============>.................] - ETA: 4:14 - loss: 2.9269 - regression_loss: 2.1780 - classification_loss: 0.7489
 439/1000 [============>.................] - ETA: 4:14 - loss: 2.9203 - regression_loss: 2.1731 - classification_loss: 0.7472
 440/1000 [============>.................] - ETA: 4:13 - loss: 2.9258 - regression_loss: 2.1758 - classification_loss: 0.7500
 441/1000 [============>.................] - ETA: 4:13 - loss: 2.9191 - regression_loss: 2.1709 - classification_loss: 0.7483
 442/1000 [============>.................] - ETA: 4:12 - loss: 2.9235 - regression_loss: 2.1727 - classification_loss: 0.7508
 443/1000 [============>.................] - ETA: 4:12 - loss: 2.9267 - regression_loss: 2.1761 - classification_loss: 0.7507
 444/1000 [============>.................] - ETA: 4:11 - loss: 2.9283 - regression_loss: 2.1773 - classification_loss: 0.7511
 445/1000 [============>.................] - ETA: 4:11 - loss: 2.9282 - regression_loss: 2.1779 - classification_loss: 0.7503
 446/1000 [============>.................] - ETA: 4:10 - loss: 2.9276 - regression_loss: 2.1783 - classification_loss: 0.7493
 447/1000 [============>.................] - ETA: 4:10 - loss: 2.9288 - regression_loss: 2.1800 - classification_loss: 0.7489
 448/1000 [============>.................] - ETA: 4:10 - loss: 2.9223 - regression_loss: 2.1751 - classification_loss: 0.7472
 449/1000 [============>.................] - ETA: 4:09 - loss: 2.9276 - regression_loss: 2.1780 - classification_loss: 0.7496
 450/1000 [============>.................] - ETA: 4:09 - loss: 2.9314 - regression_loss: 2.1794 - classification_loss: 0.7520
 451/1000 [============>.................] - ETA: 4:08 - loss: 2.9319 - regression_loss: 2.1804 - classification_loss: 0.7515
 452/1000 [============>.................] - ETA: 4:08 - loss: 2.9328 - regression_loss: 2.1817 - classification_loss: 0.7512
 453/1000 [============>.................] - ETA: 4:07 - loss: 2.9362 - regression_loss: 2.1832 - classification_loss: 0.7530
 454/1000 [============>.................] - ETA: 4:07 - loss: 2.9357 - regression_loss: 2.1834 - classification_loss: 0.7523
 455/1000 [============>.................] - ETA: 4:06 - loss: 2.9373 - regression_loss: 2.1855 - classification_loss: 0.7518
 456/1000 [============>.................] - ETA: 4:06 - loss: 2.9416 - regression_loss: 2.1875 - classification_loss: 0.7541
 457/1000 [============>.................] - ETA: 4:05 - loss: 2.9425 - regression_loss: 2.1887 - classification_loss: 0.7538
 458/1000 [============>.................] - ETA: 4:05 - loss: 2.9403 - regression_loss: 2.1873 - classification_loss: 0.7531
 459/1000 [============>.................] - ETA: 4:05 - loss: 2.9491 - regression_loss: 2.1931 - classification_loss: 0.7559
 460/1000 [============>.................] - ETA: 4:04 - loss: 2.9497 - regression_loss: 2.1936 - classification_loss: 0.7561
 461/1000 [============>.................] - ETA: 4:04 - loss: 2.9542 - regression_loss: 2.1956 - classification_loss: 0.7586
 462/1000 [============>.................] - ETA: 4:03 - loss: 2.9538 - regression_loss: 2.1948 - classification_loss: 0.7591
 463/1000 [============>.................] - ETA: 4:03 - loss: 2.9568 - regression_loss: 2.1979 - classification_loss: 0.7589
 464/1000 [============>.................] - ETA: 4:02 - loss: 2.9568 - regression_loss: 2.1987 - classification_loss: 0.7581
 465/1000 [============>.................] - ETA: 4:02 - loss: 2.9505 - regression_loss: 2.1940 - classification_loss: 0.7565
 466/1000 [============>.................] - ETA: 4:01 - loss: 2.9543 - regression_loss: 2.1963 - classification_loss: 0.7581
 467/1000 [=============>................] - ETA: 4:01 - loss: 2.9564 - regression_loss: 2.1966 - classification_loss: 0.7598
 468/1000 [=============>................] - ETA: 4:00 - loss: 2.9614 - regression_loss: 2.2002 - classification_loss: 0.7612
 469/1000 [=============>................] - ETA: 4:00 - loss: 2.9638 - regression_loss: 2.2018 - classification_loss: 0.7620
 470/1000 [=============>................] - ETA: 4:00 - loss: 2.9667 - regression_loss: 2.2045 - classification_loss: 0.7621
 471/1000 [=============>................] - ETA: 3:59 - loss: 2.9604 - regression_loss: 2.1998 - classification_loss: 0.7605
 472/1000 [=============>................] - ETA: 3:59 - loss: 2.9541 - regression_loss: 2.1952 - classification_loss: 0.7589
 473/1000 [=============>................] - ETA: 3:58 - loss: 2.9564 - regression_loss: 2.1970 - classification_loss: 0.7594
 474/1000 [=============>................] - ETA: 3:58 - loss: 2.9572 - regression_loss: 2.1979 - classification_loss: 0.7593
 475/1000 [=============>................] - ETA: 3:57 - loss: 2.9611 - regression_loss: 2.2000 - classification_loss: 0.7611
 476/1000 [=============>................] - ETA: 3:57 - loss: 2.9600 - regression_loss: 2.2000 - classification_loss: 0.7601
 477/1000 [=============>................] - ETA: 3:56 - loss: 2.9626 - regression_loss: 2.2018 - classification_loss: 0.7608
 478/1000 [=============>................] - ETA: 3:56 - loss: 2.9653 - regression_loss: 2.2042 - classification_loss: 0.7611
 479/1000 [=============>................] - ETA: 3:55 - loss: 2.9692 - regression_loss: 2.2079 - classification_loss: 0.7613
 480/1000 [=============>................] - ETA: 3:55 - loss: 2.9630 - regression_loss: 2.2033 - classification_loss: 0.7597
 481/1000 [=============>................] - ETA: 3:55 - loss: 2.9638 - regression_loss: 2.2045 - classification_loss: 0.7593
 482/1000 [=============>................] - ETA: 3:54 - loss: 2.9621 - regression_loss: 2.2034 - classification_loss: 0.7587
 483/1000 [=============>................] - ETA: 3:54 - loss: 2.9657 - regression_loss: 2.2066 - classification_loss: 0.7592
 484/1000 [=============>................] - ETA: 3:53 - loss: 2.9661 - regression_loss: 2.2076 - classification_loss: 0.7585
 485/1000 [=============>................] - ETA: 3:53 - loss: 2.9688 - regression_loss: 2.2099 - classification_loss: 0.7589
 486/1000 [=============>................] - ETA: 3:52 - loss: 2.9691 - regression_loss: 2.2107 - classification_loss: 0.7584
 487/1000 [=============>................] - ETA: 3:52 - loss: 2.9630 - regression_loss: 2.2061 - classification_loss: 0.7569
 488/1000 [=============>................] - ETA: 3:51 - loss: 2.9638 - regression_loss: 2.2065 - classification_loss: 0.7573
 489/1000 [=============>................] - ETA: 3:51 - loss: 2.9645 - regression_loss: 2.2077 - classification_loss: 0.7569
 490/1000 [=============>................] - ETA: 3:51 - loss: 2.9585 - regression_loss: 2.2032 - classification_loss: 0.7553
 491/1000 [=============>................] - ETA: 3:50 - loss: 2.9586 - regression_loss: 2.2026 - classification_loss: 0.7560
 492/1000 [=============>................] - ETA: 3:50 - loss: 2.9593 - regression_loss: 2.2036 - classification_loss: 0.7557
 493/1000 [=============>................] - ETA: 3:49 - loss: 2.9620 - regression_loss: 2.2056 - classification_loss: 0.7564
 494/1000 [=============>................] - ETA: 3:49 - loss: 2.9634 - regression_loss: 2.2053 - classification_loss: 0.7581
 495/1000 [=============>................] - ETA: 3:48 - loss: 2.9687 - regression_loss: 2.2078 - classification_loss: 0.7609
 496/1000 [=============>................] - ETA: 3:48 - loss: 2.9676 - regression_loss: 2.2075 - classification_loss: 0.7601
 497/1000 [=============>................] - ETA: 3:47 - loss: 2.9717 - regression_loss: 2.2102 - classification_loss: 0.7614
 498/1000 [=============>................] - ETA: 3:47 - loss: 2.9721 - regression_loss: 2.2111 - classification_loss: 0.7610
 499/1000 [=============>................] - ETA: 3:46 - loss: 2.9749 - regression_loss: 2.2139 - classification_loss: 0.7611
 500/1000 [==============>...............] - ETA: 3:46 - loss: 2.9772 - regression_loss: 2.2167 - classification_loss: 0.7605
 501/1000 [==============>...............] - ETA: 3:46 - loss: 2.9812 - regression_loss: 2.2190 - classification_loss: 0.7623
 502/1000 [==============>...............] - ETA: 3:45 - loss: 2.9753 - regression_loss: 2.2146 - classification_loss: 0.7608
 503/1000 [==============>...............] - ETA: 3:45 - loss: 2.9791 - regression_loss: 2.2176 - classification_loss: 0.7616
 504/1000 [==============>...............] - ETA: 3:44 - loss: 2.9801 - regression_loss: 2.2193 - classification_loss: 0.7608
 505/1000 [==============>...............] - ETA: 3:44 - loss: 2.9806 - regression_loss: 2.2205 - classification_loss: 0.7601
 506/1000 [==============>...............] - ETA: 3:43 - loss: 2.9747 - regression_loss: 2.2161 - classification_loss: 0.7586
 507/1000 [==============>...............] - ETA: 3:43 - loss: 2.9739 - regression_loss: 2.2157 - classification_loss: 0.7582
 508/1000 [==============>...............] - ETA: 3:42 - loss: 2.9756 - regression_loss: 2.2152 - classification_loss: 0.7604
 509/1000 [==============>...............] - ETA: 3:42 - loss: 2.9772 - regression_loss: 2.2158 - classification_loss: 0.7614
 510/1000 [==============>...............] - ETA: 3:41 - loss: 2.9713 - regression_loss: 2.2115 - classification_loss: 0.7599
 511/1000 [==============>...............] - ETA: 3:41 - loss: 2.9724 - regression_loss: 2.2125 - classification_loss: 0.7599
 512/1000 [==============>...............] - ETA: 3:41 - loss: 2.9707 - regression_loss: 2.2117 - classification_loss: 0.7590
 513/1000 [==============>...............] - ETA: 3:40 - loss: 2.9742 - regression_loss: 2.2140 - classification_loss: 0.7602
 514/1000 [==============>...............] - ETA: 3:40 - loss: 2.9763 - regression_loss: 2.2150 - classification_loss: 0.7613
 515/1000 [==============>...............] - ETA: 3:39 - loss: 2.9782 - regression_loss: 2.2160 - classification_loss: 0.7622
 516/1000 [==============>...............] - ETA: 3:39 - loss: 2.9833 - regression_loss: 2.2208 - classification_loss: 0.7624
 517/1000 [==============>...............] - ETA: 3:38 - loss: 2.9828 - regression_loss: 2.2208 - classification_loss: 0.7619
 518/1000 [==============>...............] - ETA: 3:38 - loss: 2.9817 - regression_loss: 2.2202 - classification_loss: 0.7615
 519/1000 [==============>...............] - ETA: 3:37 - loss: 2.9844 - regression_loss: 2.2220 - classification_loss: 0.7624
 520/1000 [==============>...............] - ETA: 3:37 - loss: 2.9840 - regression_loss: 2.2215 - classification_loss: 0.7625
 521/1000 [==============>...............] - ETA: 3:36 - loss: 2.9866 - regression_loss: 2.2235 - classification_loss: 0.7631
 522/1000 [==============>...............] - ETA: 3:36 - loss: 2.9891 - regression_loss: 2.2264 - classification_loss: 0.7627
 523/1000 [==============>...............] - ETA: 3:36 - loss: 2.9937 - regression_loss: 2.2298 - classification_loss: 0.7639
 524/1000 [==============>...............] - ETA: 3:35 - loss: 2.9967 - regression_loss: 2.2324 - classification_loss: 0.7643
 525/1000 [==============>...............] - ETA: 3:35 - loss: 2.9988 - regression_loss: 2.2340 - classification_loss: 0.7648
 526/1000 [==============>...............] - ETA: 3:34 - loss: 2.9988 - regression_loss: 2.2348 - classification_loss: 0.7640
 527/1000 [==============>...............] - ETA: 3:34 - loss: 2.9996 - regression_loss: 2.2360 - classification_loss: 0.7636
 528/1000 [==============>...............] - ETA: 3:33 - loss: 2.9995 - regression_loss: 2.2367 - classification_loss: 0.7628
 529/1000 [==============>...............] - ETA: 3:33 - loss: 3.0017 - regression_loss: 2.2373 - classification_loss: 0.7644
 530/1000 [==============>...............] - ETA: 3:32 - loss: 3.0016 - regression_loss: 2.2378 - classification_loss: 0.7639
 531/1000 [==============>...............] - ETA: 3:32 - loss: 3.0057 - regression_loss: 2.2421 - classification_loss: 0.7637
 532/1000 [==============>...............] - ETA: 3:32 - loss: 3.0067 - regression_loss: 2.2429 - classification_loss: 0.7638
 533/1000 [==============>...............] - ETA: 3:31 - loss: 3.0081 - regression_loss: 2.2440 - classification_loss: 0.7641
 534/1000 [===============>..............] - ETA: 3:31 - loss: 3.0092 - regression_loss: 2.2446 - classification_loss: 0.7645
 535/1000 [===============>..............] - ETA: 3:30 - loss: 3.0095 - regression_loss: 2.2446 - classification_loss: 0.7649
 536/1000 [===============>..............] - ETA: 3:30 - loss: 3.0135 - regression_loss: 2.2477 - classification_loss: 0.7657
 537/1000 [===============>..............] - ETA: 3:29 - loss: 3.0086 - regression_loss: 2.2435 - classification_loss: 0.7651
 538/1000 [===============>..............] - ETA: 3:29 - loss: 3.0104 - regression_loss: 2.2450 - classification_loss: 0.7654
 539/1000 [===============>..............] - ETA: 3:28 - loss: 3.0129 - regression_loss: 2.2473 - classification_loss: 0.7656
 540/1000 [===============>..............] - ETA: 3:28 - loss: 3.0073 - regression_loss: 2.2431 - classification_loss: 0.7642
 541/1000 [===============>..............] - ETA: 3:27 - loss: 3.0087 - regression_loss: 2.2442 - classification_loss: 0.7645
 542/1000 [===============>..............] - ETA: 3:27 - loss: 3.0089 - regression_loss: 2.2443 - classification_loss: 0.7646
 543/1000 [===============>..............] - ETA: 3:27 - loss: 3.0130 - regression_loss: 2.2484 - classification_loss: 0.7647
 544/1000 [===============>..............] - ETA: 3:26 - loss: 3.0138 - regression_loss: 2.2494 - classification_loss: 0.7644
 545/1000 [===============>..............] - ETA: 3:26 - loss: 3.0179 - regression_loss: 2.2526 - classification_loss: 0.7653
 546/1000 [===============>..............] - ETA: 3:25 - loss: 3.0183 - regression_loss: 2.2532 - classification_loss: 0.7650
 547/1000 [===============>..............] - ETA: 3:25 - loss: 3.0201 - regression_loss: 2.2555 - classification_loss: 0.7646
 548/1000 [===============>..............] - ETA: 3:24 - loss: 3.0244 - regression_loss: 2.2589 - classification_loss: 0.7655
 549/1000 [===============>..............] - ETA: 3:24 - loss: 3.0254 - regression_loss: 2.2603 - classification_loss: 0.7652
 550/1000 [===============>..............] - ETA: 3:23 - loss: 3.0310 - regression_loss: 2.2637 - classification_loss: 0.7673
 551/1000 [===============>..............] - ETA: 3:23 - loss: 3.0310 - regression_loss: 2.2642 - classification_loss: 0.7668
 552/1000 [===============>..............] - ETA: 3:22 - loss: 3.0255 - regression_loss: 2.2601 - classification_loss: 0.7654
 553/1000 [===============>..............] - ETA: 3:22 - loss: 3.0259 - regression_loss: 2.2608 - classification_loss: 0.7651
 554/1000 [===============>..............] - ETA: 3:22 - loss: 3.0269 - regression_loss: 2.2622 - classification_loss: 0.7646
 555/1000 [===============>..............] - ETA: 3:21 - loss: 3.0272 - regression_loss: 2.2632 - classification_loss: 0.7640
 556/1000 [===============>..............] - ETA: 3:21 - loss: 3.0219 - regression_loss: 2.2592 - classification_loss: 0.7627
 557/1000 [===============>..............] - ETA: 3:20 - loss: 3.0165 - regression_loss: 2.2551 - classification_loss: 0.7614
 558/1000 [===============>..............] - ETA: 3:20 - loss: 3.0147 - regression_loss: 2.2536 - classification_loss: 0.7611
 559/1000 [===============>..............] - ETA: 3:19 - loss: 3.0169 - regression_loss: 2.2559 - classification_loss: 0.7610
 560/1000 [===============>..............] - ETA: 3:19 - loss: 3.0116 - regression_loss: 2.2519 - classification_loss: 0.7596
 561/1000 [===============>..............] - ETA: 3:18 - loss: 3.0114 - regression_loss: 2.2519 - classification_loss: 0.7595
 562/1000 [===============>..............] - ETA: 3:18 - loss: 3.0110 - regression_loss: 2.2519 - classification_loss: 0.7591
 563/1000 [===============>..............] - ETA: 3:17 - loss: 3.0114 - regression_loss: 2.2528 - classification_loss: 0.7586
 564/1000 [===============>..............] - ETA: 3:17 - loss: 3.0118 - regression_loss: 2.2530 - classification_loss: 0.7588
 565/1000 [===============>..............] - ETA: 3:17 - loss: 3.0065 - regression_loss: 2.2490 - classification_loss: 0.7574
 566/1000 [===============>..............] - ETA: 3:16 - loss: 3.0045 - regression_loss: 2.2479 - classification_loss: 0.7566
 567/1000 [================>.............] - ETA: 3:16 - loss: 3.0056 - regression_loss: 2.2490 - classification_loss: 0.7566
 568/1000 [================>.............] - ETA: 3:15 - loss: 3.0065 - regression_loss: 2.2493 - classification_loss: 0.7572
 569/1000 [================>.............] - ETA: 3:15 - loss: 3.0101 - regression_loss: 2.2520 - classification_loss: 0.7581
 570/1000 [================>.............] - ETA: 3:14 - loss: 3.0147 - regression_loss: 2.2541 - classification_loss: 0.7606
 571/1000 [================>.............] - ETA: 3:14 - loss: 3.0170 - regression_loss: 2.2550 - classification_loss: 0.7620
 572/1000 [================>.............] - ETA: 3:13 - loss: 3.0209 - regression_loss: 2.2582 - classification_loss: 0.7627
 573/1000 [================>.............] - ETA: 3:13 - loss: 3.0210 - regression_loss: 2.2590 - classification_loss: 0.7620
 574/1000 [================>.............] - ETA: 3:12 - loss: 3.0204 - regression_loss: 2.2589 - classification_loss: 0.7616
 575/1000 [================>.............] - ETA: 3:12 - loss: 3.0230 - regression_loss: 2.2608 - classification_loss: 0.7622
 576/1000 [================>.............] - ETA: 3:12 - loss: 3.0178 - regression_loss: 2.2569 - classification_loss: 0.7609
 577/1000 [================>.............] - ETA: 3:11 - loss: 3.0201 - regression_loss: 2.2595 - classification_loss: 0.7605
 578/1000 [================>.............] - ETA: 3:11 - loss: 3.0148 - regression_loss: 2.2556 - classification_loss: 0.7592
 579/1000 [================>.............] - ETA: 3:10 - loss: 3.0128 - regression_loss: 2.2545 - classification_loss: 0.7583
 580/1000 [================>.............] - ETA: 3:10 - loss: 3.0076 - regression_loss: 2.2506 - classification_loss: 0.7570
 581/1000 [================>.............] - ETA: 3:09 - loss: 3.0068 - regression_loss: 2.2504 - classification_loss: 0.7564
 582/1000 [================>.............] - ETA: 3:09 - loss: 3.0080 - regression_loss: 2.2518 - classification_loss: 0.7562
 583/1000 [================>.............] - ETA: 3:08 - loss: 3.0090 - regression_loss: 2.2532 - classification_loss: 0.7557
 584/1000 [================>.............] - ETA: 3:08 - loss: 3.0095 - regression_loss: 2.2534 - classification_loss: 0.7561
 585/1000 [================>.............] - ETA: 3:07 - loss: 3.0082 - regression_loss: 2.2529 - classification_loss: 0.7553
 586/1000 [================>.............] - ETA: 3:07 - loss: 3.0121 - regression_loss: 2.2563 - classification_loss: 0.7559
 587/1000 [================>.............] - ETA: 3:07 - loss: 3.0145 - regression_loss: 2.2564 - classification_loss: 0.7581
 588/1000 [================>.............] - ETA: 3:06 - loss: 3.0170 - regression_loss: 2.2577 - classification_loss: 0.7592
 589/1000 [================>.............] - ETA: 3:06 - loss: 3.0170 - regression_loss: 2.2584 - classification_loss: 0.7586
 590/1000 [================>.............] - ETA: 3:05 - loss: 3.0202 - regression_loss: 2.2598 - classification_loss: 0.7604
 591/1000 [================>.............] - ETA: 3:05 - loss: 3.0200 - regression_loss: 2.2605 - classification_loss: 0.7595
 592/1000 [================>.............] - ETA: 3:04 - loss: 3.0149 - regression_loss: 2.2567 - classification_loss: 0.7582
 593/1000 [================>.............] - ETA: 3:04 - loss: 3.0172 - regression_loss: 2.2583 - classification_loss: 0.7589
 594/1000 [================>.............] - ETA: 3:03 - loss: 3.0189 - regression_loss: 2.2597 - classification_loss: 0.7592
 595/1000 [================>.............] - ETA: 3:03 - loss: 3.0167 - regression_loss: 2.2583 - classification_loss: 0.7584
 596/1000 [================>.............] - ETA: 3:03 - loss: 3.0191 - regression_loss: 2.2594 - classification_loss: 0.7597
 597/1000 [================>.............] - ETA: 3:02 - loss: 3.0194 - regression_loss: 2.2604 - classification_loss: 0.7590
 598/1000 [================>.............] - ETA: 3:02 - loss: 3.0180 - regression_loss: 2.2597 - classification_loss: 0.7582
 599/1000 [================>.............] - ETA: 3:01 - loss: 3.0189 - regression_loss: 2.2612 - classification_loss: 0.7577
 600/1000 [=================>............] - ETA: 3:01 - loss: 3.0229 - regression_loss: 2.2632 - classification_loss: 0.7597
 601/1000 [=================>............] - ETA: 3:00 - loss: 3.0284 - regression_loss: 2.2677 - classification_loss: 0.7607
 602/1000 [=================>............] - ETA: 3:00 - loss: 3.0275 - regression_loss: 2.2672 - classification_loss: 0.7603
 603/1000 [=================>............] - ETA: 2:59 - loss: 3.0327 - regression_loss: 2.2707 - classification_loss: 0.7620
 604/1000 [=================>............] - ETA: 2:59 - loss: 3.0327 - regression_loss: 2.2708 - classification_loss: 0.7620
 605/1000 [=================>............] - ETA: 2:58 - loss: 3.0338 - regression_loss: 2.2714 - classification_loss: 0.7624
 606/1000 [=================>............] - ETA: 2:58 - loss: 3.0288 - regression_loss: 2.2676 - classification_loss: 0.7611
 607/1000 [=================>............] - ETA: 2:58 - loss: 3.0295 - regression_loss: 2.2689 - classification_loss: 0.7606
 608/1000 [=================>............] - ETA: 2:57 - loss: 3.0323 - regression_loss: 2.2708 - classification_loss: 0.7615
 609/1000 [=================>............] - ETA: 2:57 - loss: 3.0319 - regression_loss: 2.2708 - classification_loss: 0.7611
 610/1000 [=================>............] - ETA: 2:56 - loss: 3.0275 - regression_loss: 2.2671 - classification_loss: 0.7604
 611/1000 [=================>............] - ETA: 2:56 - loss: 3.0297 - regression_loss: 2.2690 - classification_loss: 0.7607
 612/1000 [=================>............] - ETA: 2:55 - loss: 3.0248 - regression_loss: 2.2653 - classification_loss: 0.7595
 613/1000 [=================>............] - ETA: 2:55 - loss: 3.0278 - regression_loss: 2.2681 - classification_loss: 0.7597
 614/1000 [=================>............] - ETA: 2:54 - loss: 3.0279 - regression_loss: 2.2682 - classification_loss: 0.7597
 615/1000 [=================>............] - ETA: 2:54 - loss: 3.0285 - regression_loss: 2.2693 - classification_loss: 0.7592
 616/1000 [=================>............] - ETA: 2:53 - loss: 3.0302 - regression_loss: 2.2707 - classification_loss: 0.7596
 617/1000 [=================>............] - ETA: 2:53 - loss: 3.0254 - regression_loss: 2.2670 - classification_loss: 0.7584
 618/1000 [=================>............] - ETA: 2:53 - loss: 3.0255 - regression_loss: 2.2679 - classification_loss: 0.7576
 619/1000 [=================>............] - ETA: 2:52 - loss: 3.0259 - regression_loss: 2.2683 - classification_loss: 0.7576
 620/1000 [=================>............] - ETA: 2:52 - loss: 3.0210 - regression_loss: 2.2646 - classification_loss: 0.7564
 621/1000 [=================>............] - ETA: 2:51 - loss: 3.0224 - regression_loss: 2.2655 - classification_loss: 0.7569
 622/1000 [=================>............] - ETA: 2:51 - loss: 3.0234 - regression_loss: 2.2667 - classification_loss: 0.7567
 623/1000 [=================>............] - ETA: 2:50 - loss: 3.0235 - regression_loss: 2.2671 - classification_loss: 0.7564
 624/1000 [=================>............] - ETA: 2:50 - loss: 3.0249 - regression_loss: 2.2684 - classification_loss: 0.7564
 625/1000 [=================>............] - ETA: 2:49 - loss: 3.0201 - regression_loss: 2.2648 - classification_loss: 0.7552
 626/1000 [=================>............] - ETA: 2:49 - loss: 3.0187 - regression_loss: 2.2640 - classification_loss: 0.7547
 627/1000 [=================>............] - ETA: 2:48 - loss: 3.0139 - regression_loss: 2.2604 - classification_loss: 0.7535
 628/1000 [=================>............] - ETA: 2:48 - loss: 3.0134 - regression_loss: 2.2600 - classification_loss: 0.7534
 629/1000 [=================>............] - ETA: 2:48 - loss: 3.0086 - regression_loss: 2.2564 - classification_loss: 0.7522
 630/1000 [=================>............] - ETA: 2:47 - loss: 3.0111 - regression_loss: 2.2591 - classification_loss: 0.7520
 631/1000 [=================>............] - ETA: 2:47 - loss: 3.0125 - regression_loss: 2.2605 - classification_loss: 0.7520
 632/1000 [=================>............] - ETA: 2:46 - loss: 3.0077 - regression_loss: 2.2569 - classification_loss: 0.7509
 633/1000 [=================>............] - ETA: 2:46 - loss: 3.0094 - regression_loss: 2.2586 - classification_loss: 0.7508
 634/1000 [==================>...........] - ETA: 2:45 - loss: 3.0046 - regression_loss: 2.2550 - classification_loss: 0.7496
 635/1000 [==================>...........] - ETA: 2:45 - loss: 3.0088 - regression_loss: 2.2582 - classification_loss: 0.7506
 636/1000 [==================>...........] - ETA: 2:44 - loss: 3.0078 - regression_loss: 2.2573 - classification_loss: 0.7505
 637/1000 [==================>...........] - ETA: 2:44 - loss: 3.0096 - regression_loss: 2.2579 - classification_loss: 0.7518
 638/1000 [==================>...........] - ETA: 2:43 - loss: 3.0116 - regression_loss: 2.2590 - classification_loss: 0.7526
 639/1000 [==================>...........] - ETA: 2:43 - loss: 3.0145 - regression_loss: 2.2609 - classification_loss: 0.7536
 640/1000 [==================>...........] - ETA: 2:43 - loss: 3.0158 - regression_loss: 2.2611 - classification_loss: 0.7547
 641/1000 [==================>...........] - ETA: 2:42 - loss: 3.0149 - regression_loss: 2.2605 - classification_loss: 0.7544
 642/1000 [==================>...........] - ETA: 2:42 - loss: 3.0153 - regression_loss: 2.2608 - classification_loss: 0.7545
 643/1000 [==================>...........] - ETA: 2:41 - loss: 3.0182 - regression_loss: 2.2632 - classification_loss: 0.7550
 644/1000 [==================>...........] - ETA: 2:41 - loss: 3.0205 - regression_loss: 2.2641 - classification_loss: 0.7564
 645/1000 [==================>...........] - ETA: 2:40 - loss: 3.0205 - regression_loss: 2.2644 - classification_loss: 0.7561
 646/1000 [==================>...........] - ETA: 2:40 - loss: 3.0208 - regression_loss: 2.2645 - classification_loss: 0.7563
 647/1000 [==================>...........] - ETA: 2:39 - loss: 3.0212 - regression_loss: 2.2653 - classification_loss: 0.7558
 648/1000 [==================>...........] - ETA: 2:39 - loss: 3.0197 - regression_loss: 2.2643 - classification_loss: 0.7554
 649/1000 [==================>...........] - ETA: 2:38 - loss: 3.0151 - regression_loss: 2.2608 - classification_loss: 0.7543
 650/1000 [==================>...........] - ETA: 2:38 - loss: 3.0183 - regression_loss: 2.2631 - classification_loss: 0.7551
 651/1000 [==================>...........] - ETA: 2:38 - loss: 3.0184 - regression_loss: 2.2639 - classification_loss: 0.7545
 652/1000 [==================>...........] - ETA: 2:37 - loss: 3.0212 - regression_loss: 2.2666 - classification_loss: 0.7547
 653/1000 [==================>...........] - ETA: 2:37 - loss: 3.0249 - regression_loss: 2.2696 - classification_loss: 0.7553
 654/1000 [==================>...........] - ETA: 2:36 - loss: 3.0248 - regression_loss: 2.2695 - classification_loss: 0.7554
 655/1000 [==================>...........] - ETA: 2:36 - loss: 3.0283 - regression_loss: 2.2713 - classification_loss: 0.7571
 656/1000 [==================>...........] - ETA: 2:35 - loss: 3.0298 - regression_loss: 2.2726 - classification_loss: 0.7571
 657/1000 [==================>...........] - ETA: 2:35 - loss: 3.0289 - regression_loss: 2.2723 - classification_loss: 0.7566
 658/1000 [==================>...........] - ETA: 2:34 - loss: 3.0324 - regression_loss: 2.2746 - classification_loss: 0.7578
 659/1000 [==================>...........] - ETA: 2:34 - loss: 3.0318 - regression_loss: 2.2745 - classification_loss: 0.7573
 660/1000 [==================>...........] - ETA: 2:34 - loss: 3.0272 - regression_loss: 2.2711 - classification_loss: 0.7561
 661/1000 [==================>...........] - ETA: 2:33 - loss: 3.0294 - regression_loss: 2.2732 - classification_loss: 0.7563
 662/1000 [==================>...........] - ETA: 2:33 - loss: 3.0298 - regression_loss: 2.2738 - classification_loss: 0.7560
 663/1000 [==================>...........] - ETA: 2:32 - loss: 3.0253 - regression_loss: 2.2704 - classification_loss: 0.7549
 664/1000 [==================>...........] - ETA: 2:32 - loss: 3.0248 - regression_loss: 2.2704 - classification_loss: 0.7544
 665/1000 [==================>...........] - ETA: 2:31 - loss: 3.0245 - regression_loss: 2.2705 - classification_loss: 0.7541
 666/1000 [==================>...........] - ETA: 2:31 - loss: 3.0254 - regression_loss: 2.2712 - classification_loss: 0.7541
 667/1000 [===================>..........] - ETA: 2:30 - loss: 3.0262 - regression_loss: 2.2716 - classification_loss: 0.7546
 668/1000 [===================>..........] - ETA: 2:30 - loss: 3.0277 - regression_loss: 2.2737 - classification_loss: 0.7540
 669/1000 [===================>..........] - ETA: 2:29 - loss: 3.0315 - regression_loss: 2.2761 - classification_loss: 0.7555
 670/1000 [===================>..........] - ETA: 2:29 - loss: 3.0270 - regression_loss: 2.2727 - classification_loss: 0.7543
 671/1000 [===================>..........] - ETA: 2:29 - loss: 3.0235 - regression_loss: 2.2693 - classification_loss: 0.7542
 672/1000 [===================>..........] - ETA: 2:28 - loss: 3.0229 - regression_loss: 2.2693 - classification_loss: 0.7536
 673/1000 [===================>..........] - ETA: 2:28 - loss: 3.0237 - regression_loss: 2.2696 - classification_loss: 0.7541
 674/1000 [===================>..........] - ETA: 2:27 - loss: 3.0228 - regression_loss: 2.2692 - classification_loss: 0.7536
 675/1000 [===================>..........] - ETA: 2:27 - loss: 3.0226 - regression_loss: 2.2694 - classification_loss: 0.7531
 676/1000 [===================>..........] - ETA: 2:26 - loss: 3.0215 - regression_loss: 2.2689 - classification_loss: 0.7526
 677/1000 [===================>..........] - ETA: 2:26 - loss: 3.0202 - regression_loss: 2.2680 - classification_loss: 0.7522
 678/1000 [===================>..........] - ETA: 2:25 - loss: 3.0213 - regression_loss: 2.2687 - classification_loss: 0.7526
 679/1000 [===================>..........] - ETA: 2:25 - loss: 3.0169 - regression_loss: 2.2654 - classification_loss: 0.7515
 680/1000 [===================>..........] - ETA: 2:24 - loss: 3.0124 - regression_loss: 2.2621 - classification_loss: 0.7504
 681/1000 [===================>..........] - ETA: 2:24 - loss: 3.0080 - regression_loss: 2.2588 - classification_loss: 0.7493
 682/1000 [===================>..........] - ETA: 2:24 - loss: 3.0036 - regression_loss: 2.2554 - classification_loss: 0.7482
 683/1000 [===================>..........] - ETA: 2:23 - loss: 3.0072 - regression_loss: 2.2580 - classification_loss: 0.7492
 684/1000 [===================>..........] - ETA: 2:23 - loss: 3.0090 - regression_loss: 2.2586 - classification_loss: 0.7503
 685/1000 [===================>..........] - ETA: 2:22 - loss: 3.0116 - regression_loss: 2.2603 - classification_loss: 0.7513
 686/1000 [===================>..........] - ETA: 2:22 - loss: 3.0111 - regression_loss: 2.2603 - classification_loss: 0.7508
 687/1000 [===================>..........] - ETA: 2:21 - loss: 3.0067 - regression_loss: 2.2571 - classification_loss: 0.7497
 688/1000 [===================>..........] - ETA: 2:21 - loss: 3.0074 - regression_loss: 2.2570 - classification_loss: 0.7504
 689/1000 [===================>..........] - ETA: 2:20 - loss: 3.0075 - regression_loss: 2.2576 - classification_loss: 0.7499
 690/1000 [===================>..........] - ETA: 2:20 - loss: 3.0085 - regression_loss: 2.2586 - classification_loss: 0.7499
 691/1000 [===================>..........] - ETA: 2:19 - loss: 3.0107 - regression_loss: 2.2611 - classification_loss: 0.7497
 692/1000 [===================>..........] - ETA: 2:19 - loss: 3.0092 - regression_loss: 2.2602 - classification_loss: 0.7490
 693/1000 [===================>..........] - ETA: 2:19 - loss: 3.0126 - regression_loss: 2.2629 - classification_loss: 0.7497
 694/1000 [===================>..........] - ETA: 2:18 - loss: 3.0136 - regression_loss: 2.2642 - classification_loss: 0.7494
 695/1000 [===================>..........] - ETA: 2:18 - loss: 3.0127 - regression_loss: 2.2638 - classification_loss: 0.7488
 696/1000 [===================>..........] - ETA: 2:17 - loss: 3.0113 - regression_loss: 2.2631 - classification_loss: 0.7482
 697/1000 [===================>..........] - ETA: 2:17 - loss: 3.0148 - regression_loss: 2.2654 - classification_loss: 0.7494
 698/1000 [===================>..........] - ETA: 2:16 - loss: 3.0105 - regression_loss: 2.2622 - classification_loss: 0.7483
 699/1000 [===================>..........] - ETA: 2:16 - loss: 3.0097 - regression_loss: 2.2612 - classification_loss: 0.7485
 700/1000 [====================>.........] - ETA: 2:15 - loss: 3.0118 - regression_loss: 2.2634 - classification_loss: 0.7484
 701/1000 [====================>.........] - ETA: 2:15 - loss: 3.0075 - regression_loss: 2.2601 - classification_loss: 0.7473
 702/1000 [====================>.........] - ETA: 2:14 - loss: 3.0094 - regression_loss: 2.2609 - classification_loss: 0.7485
 703/1000 [====================>.........] - ETA: 2:14 - loss: 3.0051 - regression_loss: 2.2576 - classification_loss: 0.7475
 704/1000 [====================>.........] - ETA: 2:14 - loss: 3.0008 - regression_loss: 2.2544 - classification_loss: 0.7464
 705/1000 [====================>.........] - ETA: 2:13 - loss: 3.0232 - regression_loss: 2.2512 - classification_loss: 0.7720
 706/1000 [====================>.........] - ETA: 2:13 - loss: 3.0253 - regression_loss: 2.2524 - classification_loss: 0.7729
 707/1000 [====================>.........] - ETA: 2:12 - loss: 3.0258 - regression_loss: 2.2533 - classification_loss: 0.7725
 708/1000 [====================>.........] - ETA: 2:12 - loss: 3.0215 - regression_loss: 2.2501 - classification_loss: 0.7714
 709/1000 [====================>.........] - ETA: 2:11 - loss: 3.0255 - regression_loss: 2.2522 - classification_loss: 0.7733
 710/1000 [====================>.........] - ETA: 2:11 - loss: 3.0251 - regression_loss: 2.2524 - classification_loss: 0.7728
 711/1000 [====================>.........] - ETA: 2:10 - loss: 3.0247 - regression_loss: 2.2523 - classification_loss: 0.7724
 712/1000 [====================>.........] - ETA: 2:10 - loss: 3.0251 - regression_loss: 2.2525 - classification_loss: 0.7726
 713/1000 [====================>.........] - ETA: 2:09 - loss: 3.0255 - regression_loss: 2.2530 - classification_loss: 0.7725
 714/1000 [====================>.........] - ETA: 2:09 - loss: 3.0212 - regression_loss: 2.2498 - classification_loss: 0.7714
 715/1000 [====================>.........] - ETA: 2:09 - loss: 3.0211 - regression_loss: 2.2500 - classification_loss: 0.7711
 716/1000 [====================>.........] - ETA: 2:08 - loss: 3.0203 - regression_loss: 2.2492 - classification_loss: 0.7710
 717/1000 [====================>.........] - ETA: 2:08 - loss: 3.0161 - regression_loss: 2.2461 - classification_loss: 0.7700
 718/1000 [====================>.........] - ETA: 2:07 - loss: 3.0120 - regression_loss: 2.2430 - classification_loss: 0.7690
 719/1000 [====================>.........] - ETA: 2:07 - loss: 3.0102 - regression_loss: 2.2419 - classification_loss: 0.7683
 720/1000 [====================>.........] - ETA: 2:06 - loss: 3.0108 - regression_loss: 2.2421 - classification_loss: 0.7687
 721/1000 [====================>.........] - ETA: 2:06 - loss: 3.0111 - regression_loss: 2.2427 - classification_loss: 0.7684
 722/1000 [====================>.........] - ETA: 2:05 - loss: 3.0153 - regression_loss: 2.2454 - classification_loss: 0.7699
 723/1000 [====================>.........] - ETA: 2:05 - loss: 3.0139 - regression_loss: 2.2446 - classification_loss: 0.7693
 724/1000 [====================>.........] - ETA: 2:05 - loss: 3.0152 - regression_loss: 2.2459 - classification_loss: 0.7693
 725/1000 [====================>.........] - ETA: 2:04 - loss: 3.0110 - regression_loss: 2.2428 - classification_loss: 0.7682
 726/1000 [====================>.........] - ETA: 2:04 - loss: 3.0138 - regression_loss: 2.2458 - classification_loss: 0.7680
 727/1000 [====================>.........] - ETA: 2:03 - loss: 3.0150 - regression_loss: 2.2466 - classification_loss: 0.7684
 728/1000 [====================>.........] - ETA: 2:03 - loss: 3.0175 - regression_loss: 2.2478 - classification_loss: 0.7698
 729/1000 [====================>.........] - ETA: 2:02 - loss: 3.0211 - regression_loss: 2.2498 - classification_loss: 0.7714
 730/1000 [====================>.........] - ETA: 2:02 - loss: 3.0226 - regression_loss: 2.2517 - classification_loss: 0.7709
 731/1000 [====================>.........] - ETA: 2:01 - loss: 3.0229 - regression_loss: 2.2522 - classification_loss: 0.7707
 732/1000 [====================>.........] - ETA: 2:01 - loss: 3.0228 - regression_loss: 2.2524 - classification_loss: 0.7703
 733/1000 [====================>.........] - ETA: 2:00 - loss: 3.0194 - regression_loss: 2.2494 - classification_loss: 0.7701
 734/1000 [=====================>........] - ETA: 2:00 - loss: 3.0189 - regression_loss: 2.2494 - classification_loss: 0.7696
 735/1000 [=====================>........] - ETA: 2:00 - loss: 3.0180 - regression_loss: 2.2487 - classification_loss: 0.7693
 736/1000 [=====================>........] - ETA: 1:59 - loss: 3.0172 - regression_loss: 2.2484 - classification_loss: 0.7688
 737/1000 [=====================>........] - ETA: 1:59 - loss: 3.0168 - regression_loss: 2.2487 - classification_loss: 0.7681
 738/1000 [=====================>........] - ETA: 1:58 - loss: 3.0168 - regression_loss: 2.2490 - classification_loss: 0.7678
 739/1000 [=====================>........] - ETA: 1:58 - loss: 3.0127 - regression_loss: 2.2459 - classification_loss: 0.7668
 740/1000 [=====================>........] - ETA: 1:57 - loss: 3.0136 - regression_loss: 2.2474 - classification_loss: 0.7662
 741/1000 [=====================>........] - ETA: 1:57 - loss: 3.0140 - regression_loss: 2.2481 - classification_loss: 0.7658
 742/1000 [=====================>........] - ETA: 1:56 - loss: 3.0100 - regression_loss: 2.2451 - classification_loss: 0.7648
 743/1000 [=====================>........] - ETA: 1:56 - loss: 3.0101 - regression_loss: 2.2457 - classification_loss: 0.7644
 744/1000 [=====================>........] - ETA: 1:55 - loss: 3.0093 - regression_loss: 2.2456 - classification_loss: 0.7637
 745/1000 [=====================>........] - ETA: 1:55 - loss: 3.0052 - regression_loss: 2.2426 - classification_loss: 0.7627
 746/1000 [=====================>........] - ETA: 1:55 - loss: 3.0012 - regression_loss: 2.2396 - classification_loss: 0.7617
 747/1000 [=====================>........] - ETA: 1:54 - loss: 3.0044 - regression_loss: 2.2416 - classification_loss: 0.7629
 748/1000 [=====================>........] - ETA: 1:54 - loss: 3.0027 - regression_loss: 2.2406 - classification_loss: 0.7621
 749/1000 [=====================>........] - ETA: 1:53 - loss: 3.0061 - regression_loss: 2.2430 - classification_loss: 0.7631
 750/1000 [=====================>........] - ETA: 1:53 - loss: 3.0063 - regression_loss: 2.2432 - classification_loss: 0.7631
 751/1000 [=====================>........] - ETA: 1:52 - loss: 3.0023 - regression_loss: 2.2403 - classification_loss: 0.7621
 752/1000 [=====================>........] - ETA: 1:52 - loss: 3.0014 - regression_loss: 2.2399 - classification_loss: 0.7615
 753/1000 [=====================>........] - ETA: 1:51 - loss: 3.0033 - regression_loss: 2.2405 - classification_loss: 0.7628
 754/1000 [=====================>........] - ETA: 1:51 - loss: 3.0027 - regression_loss: 2.2402 - classification_loss: 0.7625
 755/1000 [=====================>........] - ETA: 1:50 - loss: 3.0030 - regression_loss: 2.2408 - classification_loss: 0.7622
 756/1000 [=====================>........] - ETA: 1:50 - loss: 3.0030 - regression_loss: 2.2412 - classification_loss: 0.7618
 757/1000 [=====================>........] - ETA: 1:50 - loss: 3.0042 - regression_loss: 2.2425 - classification_loss: 0.7617
 758/1000 [=====================>........] - ETA: 1:49 - loss: 3.0003 - regression_loss: 2.2396 - classification_loss: 0.7607
 759/1000 [=====================>........] - ETA: 1:49 - loss: 2.9963 - regression_loss: 2.2366 - classification_loss: 0.7597
 760/1000 [=====================>........] - ETA: 1:48 - loss: 2.9983 - regression_loss: 2.2377 - classification_loss: 0.7606
 761/1000 [=====================>........] - ETA: 1:48 - loss: 3.0012 - regression_loss: 2.2394 - classification_loss: 0.7618
 762/1000 [=====================>........] - ETA: 1:47 - loss: 3.0023 - regression_loss: 2.2403 - classification_loss: 0.7620
 763/1000 [=====================>........] - ETA: 1:47 - loss: 3.0020 - regression_loss: 2.2404 - classification_loss: 0.7616
 764/1000 [=====================>........] - ETA: 1:46 - loss: 3.0013 - regression_loss: 2.2404 - classification_loss: 0.7609
 765/1000 [=====================>........] - ETA: 1:46 - loss: 3.0018 - regression_loss: 2.2410 - classification_loss: 0.7608
 766/1000 [=====================>........] - ETA: 1:45 - loss: 2.9979 - regression_loss: 2.2381 - classification_loss: 0.7598
 767/1000 [======================>.......] - ETA: 1:45 - loss: 3.0004 - regression_loss: 2.2402 - classification_loss: 0.7602
 768/1000 [======================>.......] - ETA: 1:45 - loss: 3.0004 - regression_loss: 2.2399 - classification_loss: 0.7604
 769/1000 [======================>.......] - ETA: 1:44 - loss: 3.0026 - regression_loss: 2.2410 - classification_loss: 0.7615
 770/1000 [======================>.......] - ETA: 1:44 - loss: 2.9987 - regression_loss: 2.2381 - classification_loss: 0.7605
 771/1000 [======================>.......] - ETA: 1:43 - loss: 3.0009 - regression_loss: 2.2399 - classification_loss: 0.7610
 772/1000 [======================>.......] - ETA: 1:43 - loss: 3.0011 - regression_loss: 2.2406 - classification_loss: 0.7605
 773/1000 [======================>.......] - ETA: 1:42 - loss: 2.9972 - regression_loss: 2.2377 - classification_loss: 0.7595
 774/1000 [======================>.......] - ETA: 1:42 - loss: 2.9975 - regression_loss: 2.2381 - classification_loss: 0.7594
 775/1000 [======================>.......] - ETA: 1:41 - loss: 3.0004 - regression_loss: 2.2399 - classification_loss: 0.7605
 776/1000 [======================>.......] - ETA: 1:41 - loss: 3.0037 - regression_loss: 2.2427 - classification_loss: 0.7610
 777/1000 [======================>.......] - ETA: 1:40 - loss: 3.0023 - regression_loss: 2.2419 - classification_loss: 0.7604
 778/1000 [======================>.......] - ETA: 1:40 - loss: 2.9985 - regression_loss: 2.2390 - classification_loss: 0.7595
 779/1000 [======================>.......] - ETA: 1:40 - loss: 2.9946 - regression_loss: 2.2361 - classification_loss: 0.7585
 780/1000 [======================>.......] - ETA: 1:39 - loss: 2.9908 - regression_loss: 2.2333 - classification_loss: 0.7575
 781/1000 [======================>.......] - ETA: 1:39 - loss: 2.9900 - regression_loss: 2.2329 - classification_loss: 0.7571
 782/1000 [======================>.......] - ETA: 1:38 - loss: 2.9931 - regression_loss: 2.2348 - classification_loss: 0.7583
 783/1000 [======================>.......] - ETA: 1:38 - loss: 2.9923 - regression_loss: 2.2343 - classification_loss: 0.7579
 784/1000 [======================>.......] - ETA: 1:37 - loss: 2.9927 - regression_loss: 2.2351 - classification_loss: 0.7575
 785/1000 [======================>.......] - ETA: 1:37 - loss: 2.9961 - regression_loss: 2.2365 - classification_loss: 0.7596
 786/1000 [======================>.......] - ETA: 1:36 - loss: 2.9980 - regression_loss: 2.2384 - classification_loss: 0.7595
 787/1000 [======================>.......] - ETA: 1:36 - loss: 2.9967 - regression_loss: 2.2377 - classification_loss: 0.7589
 788/1000 [======================>.......] - ETA: 1:36 - loss: 3.0004 - regression_loss: 2.2401 - classification_loss: 0.7603
 789/1000 [======================>.......] - ETA: 1:35 - loss: 2.9966 - regression_loss: 2.2373 - classification_loss: 0.7593
 790/1000 [======================>.......] - ETA: 1:35 - loss: 2.9990 - regression_loss: 2.2388 - classification_loss: 0.7603
 791/1000 [======================>.......] - ETA: 1:34 - loss: 3.0013 - regression_loss: 2.2399 - classification_loss: 0.7614
 792/1000 [======================>.......] - ETA: 1:34 - loss: 2.9975 - regression_loss: 2.2371 - classification_loss: 0.7605
 793/1000 [======================>.......] - ETA: 1:33 - loss: 2.9986 - regression_loss: 2.2380 - classification_loss: 0.7606
 794/1000 [======================>.......] - ETA: 1:33 - loss: 2.9994 - regression_loss: 2.2388 - classification_loss: 0.7606
 795/1000 [======================>.......] - ETA: 1:32 - loss: 3.0030 - regression_loss: 2.2404 - classification_loss: 0.7626
 796/1000 [======================>.......] - ETA: 1:32 - loss: 3.0047 - regression_loss: 2.2419 - classification_loss: 0.7628
 797/1000 [======================>.......] - ETA: 1:31 - loss: 3.0043 - regression_loss: 2.2422 - classification_loss: 0.7621
 798/1000 [======================>.......] - ETA: 1:31 - loss: 3.0056 - regression_loss: 2.2426 - classification_loss: 0.7630
 799/1000 [======================>.......] - ETA: 1:31 - loss: 3.0019 - regression_loss: 2.2398 - classification_loss: 0.7620
 800/1000 [=======================>......] - ETA: 1:30 - loss: 2.9981 - regression_loss: 2.2370 - classification_loss: 0.7611
 801/1000 [=======================>......] - ETA: 1:30 - loss: 2.9944 - regression_loss: 2.2342 - classification_loss: 0.7601
 802/1000 [=======================>......] - ETA: 1:29 - loss: 2.9969 - regression_loss: 2.2355 - classification_loss: 0.7614
 803/1000 [=======================>......] - ETA: 1:29 - loss: 2.9986 - regression_loss: 2.2369 - classification_loss: 0.7617
 804/1000 [=======================>......] - ETA: 1:28 - loss: 3.0039 - regression_loss: 2.2409 - classification_loss: 0.7630
 805/1000 [=======================>......] - ETA: 1:28 - loss: 3.0077 - regression_loss: 2.2437 - classification_loss: 0.7639
 806/1000 [=======================>......] - ETA: 1:27 - loss: 3.0082 - regression_loss: 2.2445 - classification_loss: 0.7636
 807/1000 [=======================>......] - ETA: 1:27 - loss: 3.0073 - regression_loss: 2.2417 - classification_loss: 0.7656
 808/1000 [=======================>......] - ETA: 1:26 - loss: 3.0073 - regression_loss: 2.2423 - classification_loss: 0.7650
 809/1000 [=======================>......] - ETA: 1:26 - loss: 3.0082 - regression_loss: 2.2418 - classification_loss: 0.7664
 810/1000 [=======================>......] - ETA: 1:26 - loss: 3.0108 - regression_loss: 2.2433 - classification_loss: 0.7674
 811/1000 [=======================>......] - ETA: 1:25 - loss: 3.0094 - regression_loss: 2.2425 - classification_loss: 0.7668
 812/1000 [=======================>......] - ETA: 1:25 - loss: 3.0098 - regression_loss: 2.2434 - classification_loss: 0.7663
 813/1000 [=======================>......] - ETA: 1:24 - loss: 3.0093 - regression_loss: 2.2433 - classification_loss: 0.7660
 814/1000 [=======================>......] - ETA: 1:24 - loss: 3.0056 - regression_loss: 2.2405 - classification_loss: 0.7651
 815/1000 [=======================>......] - ETA: 1:23 - loss: 3.0083 - regression_loss: 2.2430 - classification_loss: 0.7653
 816/1000 [=======================>......] - ETA: 1:23 - loss: 3.0047 - regression_loss: 2.2403 - classification_loss: 0.7644
 817/1000 [=======================>......] - ETA: 1:22 - loss: 3.0055 - regression_loss: 2.2406 - classification_loss: 0.7648
 818/1000 [=======================>......] - ETA: 1:22 - loss: 3.0078 - regression_loss: 2.2420 - classification_loss: 0.7658
 819/1000 [=======================>......] - ETA: 1:21 - loss: 3.0086 - regression_loss: 2.2431 - classification_loss: 0.7656
 820/1000 [=======================>......] - ETA: 1:21 - loss: 3.0099 - regression_loss: 2.2443 - classification_loss: 0.7656
 821/1000 [=======================>......] - ETA: 1:21 - loss: 3.0107 - regression_loss: 2.2445 - classification_loss: 0.7662
 822/1000 [=======================>......] - ETA: 1:20 - loss: 3.0130 - regression_loss: 2.2458 - classification_loss: 0.7672
 823/1000 [=======================>......] - ETA: 1:20 - loss: 3.0129 - regression_loss: 2.2454 - classification_loss: 0.7675
 824/1000 [=======================>......] - ETA: 1:19 - loss: 3.0140 - regression_loss: 2.2465 - classification_loss: 0.7675
 825/1000 [=======================>......] - ETA: 1:19 - loss: 3.0135 - regression_loss: 2.2464 - classification_loss: 0.7671
 826/1000 [=======================>......] - ETA: 1:18 - loss: 3.0159 - regression_loss: 2.2486 - classification_loss: 0.7673
 827/1000 [=======================>......] - ETA: 1:18 - loss: 3.0123 - regression_loss: 2.2459 - classification_loss: 0.7664
 828/1000 [=======================>......] - ETA: 1:17 - loss: 3.0138 - regression_loss: 2.2477 - classification_loss: 0.7661
 829/1000 [=======================>......] - ETA: 1:17 - loss: 3.0134 - regression_loss: 2.2473 - classification_loss: 0.7661
 830/1000 [=======================>......] - ETA: 1:16 - loss: 3.0153 - regression_loss: 2.2494 - classification_loss: 0.7659
 831/1000 [=======================>......] - ETA: 1:16 - loss: 3.0117 - regression_loss: 2.2467 - classification_loss: 0.7650
 832/1000 [=======================>......] - ETA: 1:16 - loss: 3.0130 - regression_loss: 2.2475 - classification_loss: 0.7655
 833/1000 [=======================>......] - ETA: 1:15 - loss: 3.0149 - regression_loss: 2.2495 - classification_loss: 0.7654
 834/1000 [========================>.....] - ETA: 1:15 - loss: 3.0121 - regression_loss: 2.2468 - classification_loss: 0.7653
 835/1000 [========================>.....] - ETA: 1:14 - loss: 3.0085 - regression_loss: 2.2441 - classification_loss: 0.7644
 836/1000 [========================>.....] - ETA: 1:14 - loss: 3.0049 - regression_loss: 2.2414 - classification_loss: 0.7635
 837/1000 [========================>.....] - ETA: 1:13 - loss: 3.0065 - regression_loss: 2.2433 - classification_loss: 0.7632
 838/1000 [========================>.....] - ETA: 1:13 - loss: 3.0071 - regression_loss: 2.2436 - classification_loss: 0.7635
 839/1000 [========================>.....] - ETA: 1:12 - loss: 3.0035 - regression_loss: 2.2410 - classification_loss: 0.7626
 840/1000 [========================>.....] - ETA: 1:12 - loss: 3.0039 - regression_loss: 2.2414 - classification_loss: 0.7625
 841/1000 [========================>.....] - ETA: 1:12 - loss: 3.0049 - regression_loss: 2.2411 - classification_loss: 0.7638
 842/1000 [========================>.....] - ETA: 1:11 - loss: 3.0072 - regression_loss: 2.2424 - classification_loss: 0.7648
 843/1000 [========================>.....] - ETA: 1:11 - loss: 3.0076 - regression_loss: 2.2433 - classification_loss: 0.7644
 844/1000 [========================>.....] - ETA: 1:10 - loss: 3.0085 - regression_loss: 2.2444 - classification_loss: 0.7641
 845/1000 [========================>.....] - ETA: 1:10 - loss: 3.0077 - regression_loss: 2.2442 - classification_loss: 0.7635
 846/1000 [========================>.....] - ETA: 1:09 - loss: 3.0083 - regression_loss: 2.2449 - classification_loss: 0.7634
 847/1000 [========================>.....] - ETA: 1:09 - loss: 3.0090 - regression_loss: 2.2457 - classification_loss: 0.7633
 848/1000 [========================>.....] - ETA: 1:08 - loss: 3.0105 - regression_loss: 2.2462 - classification_loss: 0.7643
 849/1000 [========================>.....] - ETA: 1:08 - loss: 3.0105 - regression_loss: 2.2464 - classification_loss: 0.7641
 850/1000 [========================>.....] - ETA: 1:07 - loss: 3.0070 - regression_loss: 2.2438 - classification_loss: 0.7632
 851/1000 [========================>.....] - ETA: 1:07 - loss: 3.0094 - regression_loss: 2.2452 - classification_loss: 0.7642
 852/1000 [========================>.....] - ETA: 1:07 - loss: 3.0140 - regression_loss: 2.2488 - classification_loss: 0.7652
 853/1000 [========================>.....] - ETA: 1:06 - loss: 3.0168 - regression_loss: 2.2515 - classification_loss: 0.7652
 854/1000 [========================>.....] - ETA: 1:06 - loss: 3.0172 - regression_loss: 2.2518 - classification_loss: 0.7654
 855/1000 [========================>.....] - ETA: 1:05 - loss: 3.0183 - regression_loss: 2.2525 - classification_loss: 0.7658
 856/1000 [========================>.....] - ETA: 1:05 - loss: 3.0196 - regression_loss: 2.2537 - classification_loss: 0.7658
 857/1000 [========================>.....] - ETA: 1:04 - loss: 3.0192 - regression_loss: 2.2538 - classification_loss: 0.7654
 858/1000 [========================>.....] - ETA: 1:04 - loss: 3.0216 - regression_loss: 2.2552 - classification_loss: 0.7665
 859/1000 [========================>.....] - ETA: 1:03 - loss: 3.0241 - regression_loss: 2.2565 - classification_loss: 0.7676
 860/1000 [========================>.....] - ETA: 1:03 - loss: 3.0258 - regression_loss: 2.2571 - classification_loss: 0.7687
 861/1000 [========================>.....] - ETA: 1:02 - loss: 3.0258 - regression_loss: 2.2576 - classification_loss: 0.7683
 862/1000 [========================>.....] - ETA: 1:02 - loss: 3.0226 - regression_loss: 2.2549 - classification_loss: 0.7677
 863/1000 [========================>.....] - ETA: 1:02 - loss: 3.0235 - regression_loss: 2.2559 - classification_loss: 0.7676
 864/1000 [========================>.....] - ETA: 1:01 - loss: 3.0240 - regression_loss: 2.2558 - classification_loss: 0.7682
 865/1000 [========================>.....] - ETA: 1:01 - loss: 3.0261 - regression_loss: 2.2570 - classification_loss: 0.7691
 866/1000 [========================>.....] - ETA: 1:00 - loss: 3.0285 - regression_loss: 2.2591 - classification_loss: 0.7694
 867/1000 [=========================>....] - ETA: 1:00 - loss: 3.0284 - regression_loss: 2.2594 - classification_loss: 0.7690
 868/1000 [=========================>....] - ETA: 59s - loss: 3.0294 - regression_loss: 2.2596 - classification_loss: 0.7699 
 869/1000 [=========================>....] - ETA: 59s - loss: 3.0260 - regression_loss: 2.2570 - classification_loss: 0.7690
 870/1000 [=========================>....] - ETA: 58s - loss: 3.0225 - regression_loss: 2.2544 - classification_loss: 0.7681
 871/1000 [=========================>....] - ETA: 58s - loss: 3.0216 - regression_loss: 2.2538 - classification_loss: 0.7678
 872/1000 [=========================>....] - ETA: 57s - loss: 3.0239 - regression_loss: 2.2558 - classification_loss: 0.7681
 873/1000 [=========================>....] - ETA: 57s - loss: 3.0205 - regression_loss: 2.2532 - classification_loss: 0.7673
 874/1000 [=========================>....] - ETA: 57s - loss: 3.0170 - regression_loss: 2.2506 - classification_loss: 0.7664
 875/1000 [=========================>....] - ETA: 56s - loss: 3.0175 - regression_loss: 2.2516 - classification_loss: 0.7659
 876/1000 [=========================>....] - ETA: 56s - loss: 3.0169 - regression_loss: 2.2514 - classification_loss: 0.7654
 877/1000 [=========================>....] - ETA: 55s - loss: 3.0153 - regression_loss: 2.2489 - classification_loss: 0.7664
 878/1000 [=========================>....] - ETA: 55s - loss: 3.0167 - regression_loss: 2.2494 - classification_loss: 0.7673
 879/1000 [=========================>....] - ETA: 54s - loss: 3.0165 - regression_loss: 2.2497 - classification_loss: 0.7667
 880/1000 [=========================>....] - ETA: 54s - loss: 3.0170 - regression_loss: 2.2504 - classification_loss: 0.7667
 881/1000 [=========================>....] - ETA: 53s - loss: 3.0178 - regression_loss: 2.2505 - classification_loss: 0.7674
 882/1000 [=========================>....] - ETA: 53s - loss: 3.0175 - regression_loss: 2.2507 - classification_loss: 0.7668
 883/1000 [=========================>....] - ETA: 52s - loss: 3.0141 - regression_loss: 2.2482 - classification_loss: 0.7659
 884/1000 [=========================>....] - ETA: 52s - loss: 3.0107 - regression_loss: 2.2456 - classification_loss: 0.7650
 885/1000 [=========================>....] - ETA: 52s - loss: 3.0073 - regression_loss: 2.2431 - classification_loss: 0.7642
 886/1000 [=========================>....] - ETA: 51s - loss: 3.0075 - regression_loss: 2.2433 - classification_loss: 0.7641
 887/1000 [=========================>....] - ETA: 51s - loss: 3.0041 - regression_loss: 2.2408 - classification_loss: 0.7633
 888/1000 [=========================>....] - ETA: 50s - loss: 3.0071 - regression_loss: 2.2428 - classification_loss: 0.7643
 889/1000 [=========================>....] - ETA: 50s - loss: 3.0078 - regression_loss: 2.2431 - classification_loss: 0.7648
 890/1000 [=========================>....] - ETA: 49s - loss: 3.0069 - regression_loss: 2.2426 - classification_loss: 0.7643
 891/1000 [=========================>....] - ETA: 49s - loss: 3.0074 - regression_loss: 2.2434 - classification_loss: 0.7641
 892/1000 [=========================>....] - ETA: 48s - loss: 3.0089 - regression_loss: 2.2439 - classification_loss: 0.7650
 893/1000 [=========================>....] - ETA: 48s - loss: 3.0098 - regression_loss: 2.2438 - classification_loss: 0.7659
 894/1000 [=========================>....] - ETA: 47s - loss: 3.0131 - regression_loss: 2.2451 - classification_loss: 0.7680
 895/1000 [=========================>....] - ETA: 47s - loss: 3.0132 - regression_loss: 2.2456 - classification_loss: 0.7676
 896/1000 [=========================>....] - ETA: 47s - loss: 3.0098 - regression_loss: 2.2431 - classification_loss: 0.7668
 897/1000 [=========================>....] - ETA: 46s - loss: 3.0065 - regression_loss: 2.2406 - classification_loss: 0.7659
 898/1000 [=========================>....] - ETA: 46s - loss: 3.0070 - regression_loss: 2.2398 - classification_loss: 0.7673
 899/1000 [=========================>....] - ETA: 45s - loss: 3.0075 - regression_loss: 2.2405 - classification_loss: 0.7670
 900/1000 [==========================>...] - ETA: 45s - loss: 3.0112 - regression_loss: 2.2421 - classification_loss: 0.7690
 901/1000 [==========================>...] - ETA: 44s - loss: 3.0122 - regression_loss: 2.2422 - classification_loss: 0.7699
 902/1000 [==========================>...] - ETA: 44s - loss: 3.0138 - regression_loss: 2.2435 - classification_loss: 0.7703
 903/1000 [==========================>...] - ETA: 43s - loss: 3.0147 - regression_loss: 2.2443 - classification_loss: 0.7704
 904/1000 [==========================>...] - ETA: 43s - loss: 3.0150 - regression_loss: 2.2444 - classification_loss: 0.7706
 905/1000 [==========================>...] - ETA: 43s - loss: 3.0167 - regression_loss: 2.2452 - classification_loss: 0.7715
 906/1000 [==========================>...] - ETA: 42s - loss: 3.0176 - regression_loss: 2.2461 - classification_loss: 0.7715
 907/1000 [==========================>...] - ETA: 42s - loss: 3.0143 - regression_loss: 2.2436 - classification_loss: 0.7707
 908/1000 [==========================>...] - ETA: 41s - loss: 3.0110 - regression_loss: 2.2412 - classification_loss: 0.7698
 909/1000 [==========================>...] - ETA: 41s - loss: 3.0145 - regression_loss: 2.2434 - classification_loss: 0.7711
 910/1000 [==========================>...] - ETA: 40s - loss: 3.0163 - regression_loss: 2.2450 - classification_loss: 0.7713
 911/1000 [==========================>...] - ETA: 40s - loss: 3.0159 - regression_loss: 2.2449 - classification_loss: 0.7709
 912/1000 [==========================>...] - ETA: 39s - loss: 3.0166 - regression_loss: 2.2443 - classification_loss: 0.7722
 913/1000 [==========================>...] - ETA: 39s - loss: 3.0165 - regression_loss: 2.2447 - classification_loss: 0.7717
 914/1000 [==========================>...] - ETA: 38s - loss: 3.0172 - regression_loss: 2.2451 - classification_loss: 0.7721
 915/1000 [==========================>...] - ETA: 38s - loss: 3.0165 - regression_loss: 2.2451 - classification_loss: 0.7715
 916/1000 [==========================>...] - ETA: 38s - loss: 3.0159 - regression_loss: 2.2445 - classification_loss: 0.7714
 917/1000 [==========================>...] - ETA: 37s - loss: 3.0175 - regression_loss: 2.2464 - classification_loss: 0.7711
 918/1000 [==========================>...] - ETA: 37s - loss: 3.0142 - regression_loss: 2.2440 - classification_loss: 0.7703
 919/1000 [==========================>...] - ETA: 36s - loss: 3.0146 - regression_loss: 2.2434 - classification_loss: 0.7712
 920/1000 [==========================>...] - ETA: 36s - loss: 3.0159 - regression_loss: 2.2442 - classification_loss: 0.7718
 921/1000 [==========================>...] - ETA: 35s - loss: 3.0157 - regression_loss: 2.2438 - classification_loss: 0.7719
 922/1000 [==========================>...] - ETA: 35s - loss: 3.0125 - regression_loss: 2.2414 - classification_loss: 0.7711
 923/1000 [==========================>...] - ETA: 34s - loss: 3.0092 - regression_loss: 2.2389 - classification_loss: 0.7703
 924/1000 [==========================>...] - ETA: 34s - loss: 3.0090 - regression_loss: 2.2392 - classification_loss: 0.7698
 925/1000 [==========================>...] - ETA: 33s - loss: 3.0101 - regression_loss: 2.2400 - classification_loss: 0.7702
 926/1000 [==========================>...] - ETA: 33s - loss: 3.0098 - regression_loss: 2.2401 - classification_loss: 0.7696
 927/1000 [==========================>...] - ETA: 33s - loss: 3.0114 - regression_loss: 2.2416 - classification_loss: 0.7698
 928/1000 [==========================>...] - ETA: 32s - loss: 3.0137 - regression_loss: 2.2432 - classification_loss: 0.7705
 929/1000 [==========================>...] - ETA: 32s - loss: 3.0162 - regression_loss: 2.2446 - classification_loss: 0.7715
 930/1000 [==========================>...] - ETA: 31s - loss: 3.0131 - regression_loss: 2.2422 - classification_loss: 0.7709
 931/1000 [==========================>...] - ETA: 31s - loss: 3.0124 - regression_loss: 2.2418 - classification_loss: 0.7705
 932/1000 [==========================>...] - ETA: 30s - loss: 3.0091 - regression_loss: 2.2394 - classification_loss: 0.7697
 933/1000 [==========================>...] - ETA: 30s - loss: 3.0088 - regression_loss: 2.2393 - classification_loss: 0.7695
 934/1000 [===========================>..] - ETA: 29s - loss: 3.0056 - regression_loss: 2.2369 - classification_loss: 0.7687
 935/1000 [===========================>..] - ETA: 29s - loss: 3.0030 - regression_loss: 2.2345 - classification_loss: 0.7685
 936/1000 [===========================>..] - ETA: 28s - loss: 3.0056 - regression_loss: 2.2369 - classification_loss: 0.7687
 937/1000 [===========================>..] - ETA: 28s - loss: 3.0024 - regression_loss: 2.2345 - classification_loss: 0.7679
 938/1000 [===========================>..] - ETA: 28s - loss: 3.0017 - regression_loss: 2.2341 - classification_loss: 0.7677
 939/1000 [===========================>..] - ETA: 27s - loss: 3.0024 - regression_loss: 2.2339 - classification_loss: 0.7685
 940/1000 [===========================>..] - ETA: 27s - loss: 2.9992 - regression_loss: 2.2315 - classification_loss: 0.7677
 941/1000 [===========================>..] - ETA: 26s - loss: 2.9960 - regression_loss: 2.2292 - classification_loss: 0.7669
 942/1000 [===========================>..] - ETA: 26s - loss: 2.9958 - regression_loss: 2.2291 - classification_loss: 0.7667
 943/1000 [===========================>..] - ETA: 25s - loss: 2.9992 - regression_loss: 2.2316 - classification_loss: 0.7676
 944/1000 [===========================>..] - ETA: 25s - loss: 3.0020 - regression_loss: 2.2333 - classification_loss: 0.7687
 945/1000 [===========================>..] - ETA: 24s - loss: 3.0021 - regression_loss: 2.2334 - classification_loss: 0.7687
 946/1000 [===========================>..] - ETA: 24s - loss: 2.9989 - regression_loss: 2.2310 - classification_loss: 0.7679
 947/1000 [===========================>..] - ETA: 23s - loss: 3.0016 - regression_loss: 2.2329 - classification_loss: 0.7687
 948/1000 [===========================>..] - ETA: 23s - loss: 2.9985 - regression_loss: 2.2306 - classification_loss: 0.7679
 949/1000 [===========================>..] - ETA: 23s - loss: 2.9984 - regression_loss: 2.2305 - classification_loss: 0.7679
 950/1000 [===========================>..] - ETA: 22s - loss: 2.9996 - regression_loss: 2.2316 - classification_loss: 0.7679
 951/1000 [===========================>..] - ETA: 22s - loss: 2.9964 - regression_loss: 2.2293 - classification_loss: 0.7671
 952/1000 [===========================>..] - ETA: 21s - loss: 2.9992 - regression_loss: 2.2309 - classification_loss: 0.7684
 953/1000 [===========================>..] - ETA: 21s - loss: 2.9986 - regression_loss: 2.2307 - classification_loss: 0.7679
 954/1000 [===========================>..] - ETA: 20s - loss: 2.9991 - regression_loss: 2.2312 - classification_loss: 0.7679
 955/1000 [===========================>..] - ETA: 20s - loss: 2.9992 - regression_loss: 2.2318 - classification_loss: 0.7674
 956/1000 [===========================>..] - ETA: 19s - loss: 2.9999 - regression_loss: 2.2328 - classification_loss: 0.7671
 957/1000 [===========================>..] - ETA: 19s - loss: 3.0013 - regression_loss: 2.2343 - classification_loss: 0.7670
 958/1000 [===========================>..] - ETA: 19s - loss: 3.0042 - regression_loss: 2.2363 - classification_loss: 0.7679
 959/1000 [===========================>..] - ETA: 18s - loss: 3.0070 - regression_loss: 2.2380 - classification_loss: 0.7690
 960/1000 [===========================>..] - ETA: 18s - loss: 3.0039 - regression_loss: 2.2357 - classification_loss: 0.7682
 961/1000 [===========================>..] - ETA: 17s - loss: 3.0007 - regression_loss: 2.2334 - classification_loss: 0.7674
 962/1000 [===========================>..] - ETA: 17s - loss: 3.0013 - regression_loss: 2.2334 - classification_loss: 0.7679
 963/1000 [===========================>..] - ETA: 16s - loss: 3.0017 - regression_loss: 2.2338 - classification_loss: 0.7679
 964/1000 [===========================>..] - ETA: 16s - loss: 3.0029 - regression_loss: 2.2350 - classification_loss: 0.7679
 965/1000 [===========================>..] - ETA: 15s - loss: 3.0034 - regression_loss: 2.2357 - classification_loss: 0.7678
 966/1000 [===========================>..] - ETA: 15s - loss: 3.0056 - regression_loss: 2.2377 - classification_loss: 0.7679
 967/1000 [============================>.] - ETA: 14s - loss: 3.0047 - regression_loss: 2.2371 - classification_loss: 0.7676
 968/1000 [============================>.] - ETA: 14s - loss: 3.0053 - regression_loss: 2.2370 - classification_loss: 0.7683
 969/1000 [============================>.] - ETA: 14s - loss: 3.0061 - regression_loss: 2.2380 - classification_loss: 0.7682
 970/1000 [============================>.] - ETA: 13s - loss: 3.0074 - regression_loss: 2.2387 - classification_loss: 0.7686
 971/1000 [============================>.] - ETA: 13s - loss: 3.0079 - regression_loss: 2.2394 - classification_loss: 0.7685
 972/1000 [============================>.] - ETA: 12s - loss: 3.0099 - regression_loss: 2.2405 - classification_loss: 0.7693
 973/1000 [============================>.] - ETA: 12s - loss: 3.0107 - regression_loss: 2.2411 - classification_loss: 0.7697
 974/1000 [============================>.] - ETA: 11s - loss: 3.0081 - regression_loss: 2.2388 - classification_loss: 0.7694
 975/1000 [============================>.] - ETA: 11s - loss: 3.0072 - regression_loss: 2.2384 - classification_loss: 0.7688
 976/1000 [============================>.] - ETA: 10s - loss: 3.0074 - regression_loss: 2.2382 - classification_loss: 0.7692
 977/1000 [============================>.] - ETA: 10s - loss: 3.0070 - regression_loss: 2.2379 - classification_loss: 0.7691
 978/1000 [============================>.] - ETA: 9s - loss: 3.0084 - regression_loss: 2.2390 - classification_loss: 0.7694 
 979/1000 [============================>.] - ETA: 9s - loss: 3.0087 - regression_loss: 2.2393 - classification_loss: 0.7694
 980/1000 [============================>.] - ETA: 9s - loss: 3.0087 - regression_loss: 2.2398 - classification_loss: 0.7690
 981/1000 [============================>.] - ETA: 8s - loss: 3.0104 - regression_loss: 2.2410 - classification_loss: 0.7694
 982/1000 [============================>.] - ETA: 8s - loss: 3.0108 - regression_loss: 2.2412 - classification_loss: 0.7696
 983/1000 [============================>.] - ETA: 7s - loss: 3.0117 - regression_loss: 2.2423 - classification_loss: 0.7694
 984/1000 [============================>.] - ETA: 7s - loss: 3.0120 - regression_loss: 2.2429 - classification_loss: 0.7690
 985/1000 [============================>.] - ETA: 6s - loss: 3.0133 - regression_loss: 2.2442 - classification_loss: 0.7691
 986/1000 [============================>.] - ETA: 6s - loss: 3.0127 - regression_loss: 2.2439 - classification_loss: 0.7688
 987/1000 [============================>.] - ETA: 5s - loss: 3.0132 - regression_loss: 2.2447 - classification_loss: 0.7685
 988/1000 [============================>.] - ETA: 5s - loss: 3.0123 - regression_loss: 2.2443 - classification_loss: 0.7680
 989/1000 [============================>.] - ETA: 4s - loss: 3.0119 - regression_loss: 2.2443 - classification_loss: 0.7676
 990/1000 [============================>.] - ETA: 4s - loss: 3.0121 - regression_loss: 2.2450 - classification_loss: 0.7671
 991/1000 [============================>.] - ETA: 4s - loss: 3.0117 - regression_loss: 2.2444 - classification_loss: 0.7673
 992/1000 [============================>.] - ETA: 3s - loss: 3.0123 - regression_loss: 2.2455 - classification_loss: 0.7668
 993/1000 [============================>.] - ETA: 3s - loss: 3.0125 - regression_loss: 2.2462 - classification_loss: 0.7664
 994/1000 [============================>.] - ETA: 2s - loss: 3.0129 - regression_loss: 2.2465 - classification_loss: 0.7664
 995/1000 [============================>.] - ETA: 2s - loss: 3.0133 - regression_loss: 2.2465 - classification_loss: 0.7668
 996/1000 [============================>.] - ETA: 1s - loss: 3.0134 - regression_loss: 2.2468 - classification_loss: 0.7666
 997/1000 [============================>.] - ETA: 1s - loss: 3.0108 - regression_loss: 2.2446 - classification_loss: 0.7662
 998/1000 [============================>.] - ETA: 0s - loss: 3.0078 - regression_loss: 2.2423 - classification_loss: 0.7654
 999/1000 [============================>.] - ETA: 0s - loss: 3.0086 - regression_loss: 2.2428 - classification_loss: 0.7658
1000/1000 [==============================] - 453s 453ms/step - loss: 3.0089 - regression_loss: 2.2433 - classification_loss: 0.7656

Epoch 00012: saving model to ./snapshots/resnet50_csv_12.h5
0/2001/2002/2003/2004/2005/2006/2007/2008/2009/20010/20011/20012/20013/20014/20015/20016/20017/20018/20019/20020/20021/20022/20023/20024/20025/20026/20027/20028/20029/20030/20031/20032/20033/20034/20035/20036/20037/20038/20039/20040/20041/20042/20043/20044/20045/20046/20047/20048/20049/20050/20051/20052/20053/20054/20055/20056/20057/20058/20059/20060/20061/20062/20063/20064/20065/20066/20067/20068/20069/20070/20071/20072/20073/20074/20075/20076/20077/20078/20079/20080/20081/20082/20083/20084/20085/20086/20087/20088/20089/20090/20091/20092/20093/20094/20095/20096/20097/20098/20099/200100/200101/200102/200103/200104/200105/200106/200107/200108/200109/200110/200111/200112/200113/200114/200115/200116/200117/200118/200119/200120/200121/200122/200123/200124/200125/200126/200127/200128/200129/200130/200131/200132/200133/200134/200135/200136/200137/200138/200139/200140/200141/200142/200143/200144/200145/200146/200147/200148/200149/200150/200151/200152/200153/200154/200155/200156/200157/200158/200159/200160/200161/200162/200163/200164/200165/200166/200167/200168/200169/200170/200171/200172/200173/200174/200175/200176/200177/200178/200179/200180/200181/200182/200183/200184/200185/200186/200187/200188/200189/200190/200191/200192/200193/200194/200195/200196/200197/200198/200199/2000/2001/2002/2003/2004/2005/2006/2007/2008/2009/20010/20011/20012/20013/20014/20015/20016/20017/20018/20019/20020/20021/20022/20023/20024/20025/20026/20027/20028/20029/20030/20031/20032/20033/20034/20035/20036/20037/20038/20039/20040/20041/20042/20043/20044/20045/20046/20047/20048/20049/20050/20051/20052/20053/20054/20055/20056/20057/20058/20059/20060/20061/20062/20063/20064/20065/20066/20067/20068/20069/20070/20071/20072/20073/20074/20075/20076/20077/20078/20079/20080/20081/20082/20083/20084/20085/20086/20087/20088/20089/20090/20091/20092/20093/20094/20095/20096/20097/20098/20099/200100/200101/200102/200103/200104/200105/200106/200107/200108/200109/200110/200111/200112/200113/200114/200115/200116/200117/200118/200119/200120/200121/200122/200123/200124/200125/200126/200127/200128/200129/200130/200131/200132/200133/200134/200135/200136/200137/200138/200139/200140/200141/200142/200143/200144/200145/200146/200147/200148/200149/200150/200151/200152/200153/200154/200155/200156/200157/200158/200159/200160/200161/200162/200163/200164/200165/200166/200167/200168/200169/200170/200171/200172/200173/200174/200175/200176/200177/200178/200179/200180/200181/200182/200183/200184/200185/200186/200187/200188/200189/200190/200191/200192/200193/200194/200195/200196/200197/200198/200199/200

M 0.1019
N 0.0158
mAP: 0.0589
Epoch 13/30

   1/1000 [..............................] - ETA: 7:24 - loss: 3.1530 - regression_loss: 2.3910 - classification_loss: 0.7620
   2/1000 [..............................] - ETA: 7:27 - loss: 3.1160 - regression_loss: 2.5320 - classification_loss: 0.5840
   3/1000 [..............................] - ETA: 7:26 - loss: 3.4170 - regression_loss: 2.7459 - classification_loss: 0.6711
   4/1000 [..............................] - ETA: 7:28 - loss: 3.3501 - regression_loss: 2.7516 - classification_loss: 0.5985
   5/1000 [..............................] - ETA: 7:28 - loss: 2.6844 - regression_loss: 2.2013 - classification_loss: 0.4831
   6/1000 [..............................] - ETA: 7:28 - loss: 2.6132 - regression_loss: 2.1666 - classification_loss: 0.4466
   7/1000 [..............................] - ETA: 7:28 - loss: 2.8587 - regression_loss: 2.3823 - classification_loss: 0.4764
   8/1000 [..............................] - ETA: 7:28 - loss: 2.7604 - regression_loss: 2.2920 - classification_loss: 0.4684
   9/1000 [..............................] - ETA: 7:27 - loss: 3.0044 - regression_loss: 2.4697 - classification_loss: 0.5347
  10/1000 [..............................] - ETA: 7:27 - loss: 2.9600 - regression_loss: 2.4324 - classification_loss: 0.5275
  11/1000 [..............................] - ETA: 7:25 - loss: 2.9735 - regression_loss: 2.4351 - classification_loss: 0.5384
  12/1000 [..............................] - ETA: 7:25 - loss: 3.0481 - regression_loss: 2.4673 - classification_loss: 0.5807
  13/1000 [..............................] - ETA: 7:24 - loss: 2.8137 - regression_loss: 2.2775 - classification_loss: 0.5361
  14/1000 [..............................] - ETA: 7:24 - loss: 2.8105 - regression_loss: 2.2466 - classification_loss: 0.5639
  15/1000 [..............................] - ETA: 7:23 - loss: 2.8444 - regression_loss: 2.2767 - classification_loss: 0.5677
  16/1000 [..............................] - ETA: 7:23 - loss: 2.8699 - regression_loss: 2.3061 - classification_loss: 0.5639
  17/1000 [..............................] - ETA: 7:22 - loss: 2.9096 - regression_loss: 2.3254 - classification_loss: 0.5842
  18/1000 [..............................] - ETA: 7:22 - loss: 2.7480 - regression_loss: 2.1962 - classification_loss: 0.5517
  19/1000 [..............................] - ETA: 7:21 - loss: 2.7250 - regression_loss: 2.1864 - classification_loss: 0.5386
  20/1000 [..............................] - ETA: 7:21 - loss: 2.8009 - regression_loss: 2.2494 - classification_loss: 0.5515
  21/1000 [..............................] - ETA: 7:20 - loss: 2.8363 - regression_loss: 2.2844 - classification_loss: 0.5519
  22/1000 [..............................] - ETA: 7:20 - loss: 2.7075 - regression_loss: 2.1805 - classification_loss: 0.5270
  23/1000 [..............................] - ETA: 7:20 - loss: 2.7744 - regression_loss: 2.2406 - classification_loss: 0.5338
  24/1000 [..............................] - ETA: 7:20 - loss: 2.6591 - regression_loss: 2.1472 - classification_loss: 0.5119
  25/1000 [..............................] - ETA: 7:19 - loss: 2.5529 - regression_loss: 2.0613 - classification_loss: 0.4915
  26/1000 [..............................] - ETA: 7:19 - loss: 2.4549 - regression_loss: 1.9821 - classification_loss: 0.4728
  27/1000 [..............................] - ETA: 7:19 - loss: 2.4557 - regression_loss: 1.9713 - classification_loss: 0.4843
  28/1000 [..............................] - ETA: 7:18 - loss: 2.4979 - regression_loss: 2.0059 - classification_loss: 0.4920
  29/1000 [..............................] - ETA: 7:17 - loss: 2.5263 - regression_loss: 1.9939 - classification_loss: 0.5324
  30/1000 [..............................] - ETA: 7:17 - loss: 2.5502 - regression_loss: 2.0154 - classification_loss: 0.5349
  31/1000 [..............................] - ETA: 7:17 - loss: 2.6262 - regression_loss: 2.0651 - classification_loss: 0.5610
  32/1000 [..............................] - ETA: 7:16 - loss: 2.6562 - regression_loss: 2.1013 - classification_loss: 0.5549
  33/1000 [..............................] - ETA: 7:16 - loss: 2.6731 - regression_loss: 2.1122 - classification_loss: 0.5609
  34/1000 [>.............................] - ETA: 7:15 - loss: 2.6724 - regression_loss: 2.1109 - classification_loss: 0.5616
  35/1000 [>.............................] - ETA: 7:14 - loss: 2.6874 - regression_loss: 2.1290 - classification_loss: 0.5585
  36/1000 [>.............................] - ETA: 7:14 - loss: 2.8402 - regression_loss: 2.2370 - classification_loss: 0.6033
  37/1000 [>.............................] - ETA: 7:13 - loss: 2.9073 - regression_loss: 2.2981 - classification_loss: 0.6092
  38/1000 [>.............................] - ETA: 7:13 - loss: 2.8308 - regression_loss: 2.2376 - classification_loss: 0.5932
  39/1000 [>.............................] - ETA: 7:12 - loss: 2.8168 - regression_loss: 2.2304 - classification_loss: 0.5864
  40/1000 [>.............................] - ETA: 7:12 - loss: 2.7464 - regression_loss: 2.1747 - classification_loss: 0.5717
  41/1000 [>.............................] - ETA: 7:12 - loss: 2.7778 - regression_loss: 2.2046 - classification_loss: 0.5731
  42/1000 [>.............................] - ETA: 7:11 - loss: 2.7802 - regression_loss: 2.2113 - classification_loss: 0.5688
  43/1000 [>.............................] - ETA: 7:11 - loss: 2.8199 - regression_loss: 2.2318 - classification_loss: 0.5881
  44/1000 [>.............................] - ETA: 7:10 - loss: 2.7558 - regression_loss: 2.1811 - classification_loss: 0.5747
  45/1000 [>.............................] - ETA: 7:10 - loss: 2.6946 - regression_loss: 2.1327 - classification_loss: 0.5619
  46/1000 [>.............................] - ETA: 7:10 - loss: 2.7624 - regression_loss: 2.1876 - classification_loss: 0.5748
  47/1000 [>.............................] - ETA: 7:09 - loss: 2.7814 - regression_loss: 2.2102 - classification_loss: 0.5712
  48/1000 [>.............................] - ETA: 7:09 - loss: 2.7235 - regression_loss: 2.1642 - classification_loss: 0.5593
  49/1000 [>.............................] - ETA: 7:09 - loss: 2.6679 - regression_loss: 2.1200 - classification_loss: 0.5479
  50/1000 [>.............................] - ETA: 7:08 - loss: 2.6163 - regression_loss: 2.0776 - classification_loss: 0.5387
  51/1000 [>.............................] - ETA: 7:08 - loss: 2.5650 - regression_loss: 2.0368 - classification_loss: 0.5282
  52/1000 [>.............................] - ETA: 7:07 - loss: 2.5626 - regression_loss: 2.0391 - classification_loss: 0.5235
  53/1000 [>.............................] - ETA: 7:07 - loss: 2.5762 - regression_loss: 2.0475 - classification_loss: 0.5288
  54/1000 [>.............................] - ETA: 7:07 - loss: 2.6272 - regression_loss: 2.0743 - classification_loss: 0.5529
  55/1000 [>.............................] - ETA: 7:06 - loss: 2.6414 - regression_loss: 2.0365 - classification_loss: 0.6049
  56/1000 [>.............................] - ETA: 7:06 - loss: 2.6549 - regression_loss: 2.0517 - classification_loss: 0.6032
  57/1000 [>.............................] - ETA: 7:06 - loss: 2.6083 - regression_loss: 2.0157 - classification_loss: 0.5926
  58/1000 [>.............................] - ETA: 7:05 - loss: 2.6193 - regression_loss: 2.0277 - classification_loss: 0.5916
  59/1000 [>.............................] - ETA: 7:05 - loss: 2.6416 - regression_loss: 2.0493 - classification_loss: 0.5922
  60/1000 [>.............................] - ETA: 7:04 - loss: 2.6575 - regression_loss: 2.0589 - classification_loss: 0.5986
  61/1000 [>.............................] - ETA: 7:04 - loss: 2.6497 - regression_loss: 2.0545 - classification_loss: 0.5952
  62/1000 [>.............................] - ETA: 7:04 - loss: 2.6887 - regression_loss: 2.0825 - classification_loss: 0.6062
  63/1000 [>.............................] - ETA: 7:03 - loss: 2.7051 - regression_loss: 2.0974 - classification_loss: 0.6078
  64/1000 [>.............................] - ETA: 7:03 - loss: 2.7281 - regression_loss: 2.0998 - classification_loss: 0.6283
  65/1000 [>.............................] - ETA: 7:03 - loss: 2.7271 - regression_loss: 2.1020 - classification_loss: 0.6250
  66/1000 [>.............................] - ETA: 7:02 - loss: 2.6858 - regression_loss: 2.0702 - classification_loss: 0.6156
  67/1000 [=>............................] - ETA: 7:02 - loss: 2.6728 - regression_loss: 2.0626 - classification_loss: 0.6102
  68/1000 [=>............................] - ETA: 7:01 - loss: 2.6335 - regression_loss: 2.0323 - classification_loss: 0.6013
  69/1000 [=>............................] - ETA: 7:01 - loss: 2.6633 - regression_loss: 2.0460 - classification_loss: 0.6173
  70/1000 [=>............................] - ETA: 7:00 - loss: 2.6840 - regression_loss: 2.0643 - classification_loss: 0.6197
  71/1000 [=>............................] - ETA: 7:00 - loss: 2.6462 - regression_loss: 2.0352 - classification_loss: 0.6110
  72/1000 [=>............................] - ETA: 7:00 - loss: 2.6742 - regression_loss: 2.0588 - classification_loss: 0.6154
  73/1000 [=>............................] - ETA: 6:59 - loss: 2.6688 - regression_loss: 2.0586 - classification_loss: 0.6102
  74/1000 [=>............................] - ETA: 6:59 - loss: 2.6328 - regression_loss: 2.0308 - classification_loss: 0.6019
  75/1000 [=>............................] - ETA: 6:58 - loss: 2.5976 - regression_loss: 2.0037 - classification_loss: 0.5939
  76/1000 [=>............................] - ETA: 6:58 - loss: 2.6166 - regression_loss: 2.0082 - classification_loss: 0.6084
  77/1000 [=>............................] - ETA: 6:57 - loss: 2.5827 - regression_loss: 1.9821 - classification_loss: 0.6005
  78/1000 [=>............................] - ETA: 6:57 - loss: 2.6069 - regression_loss: 1.9968 - classification_loss: 0.6101
  79/1000 [=>............................] - ETA: 6:56 - loss: 2.6130 - regression_loss: 2.0037 - classification_loss: 0.6093
  80/1000 [=>............................] - ETA: 6:56 - loss: 2.6224 - regression_loss: 1.9984 - classification_loss: 0.6240
  81/1000 [=>............................] - ETA: 6:55 - loss: 2.5900 - regression_loss: 1.9737 - classification_loss: 0.6163
  82/1000 [=>............................] - ETA: 6:55 - loss: 2.5820 - regression_loss: 1.9670 - classification_loss: 0.6151
  83/1000 [=>............................] - ETA: 6:55 - loss: 2.5988 - regression_loss: 1.9651 - classification_loss: 0.6337
  84/1000 [=>............................] - ETA: 6:54 - loss: 2.6305 - regression_loss: 1.9790 - classification_loss: 0.6515
  85/1000 [=>............................] - ETA: 6:54 - loss: 2.6552 - regression_loss: 1.9896 - classification_loss: 0.6657
  86/1000 [=>............................] - ETA: 6:53 - loss: 2.6617 - regression_loss: 1.9996 - classification_loss: 0.6621
  87/1000 [=>............................] - ETA: 6:53 - loss: 2.6312 - regression_loss: 1.9766 - classification_loss: 0.6545
  88/1000 [=>............................] - ETA: 6:52 - loss: 2.6426 - regression_loss: 1.9854 - classification_loss: 0.6572
  89/1000 [=>............................] - ETA: 6:52 - loss: 2.6484 - regression_loss: 1.9846 - classification_loss: 0.6638
  90/1000 [=>............................] - ETA: 6:51 - loss: 2.6190 - regression_loss: 1.9625 - classification_loss: 0.6564
  91/1000 [=>............................] - ETA: 6:51 - loss: 2.6271 - regression_loss: 1.9677 - classification_loss: 0.6595
  92/1000 [=>............................] - ETA: 6:51 - loss: 2.6528 - regression_loss: 1.9835 - classification_loss: 0.6692
  93/1000 [=>............................] - ETA: 6:50 - loss: 2.6505 - regression_loss: 1.9818 - classification_loss: 0.6687
  94/1000 [=>............................] - ETA: 6:50 - loss: 2.6747 - regression_loss: 1.9919 - classification_loss: 0.6827
  95/1000 [=>............................] - ETA: 6:49 - loss: 2.6711 - regression_loss: 1.9916 - classification_loss: 0.6795
  96/1000 [=>............................] - ETA: 6:49 - loss: 2.6818 - regression_loss: 1.9947 - classification_loss: 0.6870
  97/1000 [=>............................] - ETA: 6:48 - loss: 2.6918 - regression_loss: 2.0060 - classification_loss: 0.6857
  98/1000 [=>............................] - ETA: 6:48 - loss: 2.7075 - regression_loss: 2.0119 - classification_loss: 0.6956
  99/1000 [=>............................] - ETA: 6:48 - loss: 2.7164 - regression_loss: 2.0123 - classification_loss: 0.7041
 100/1000 [==>...........................] - ETA: 6:47 - loss: 2.6892 - regression_loss: 1.9921 - classification_loss: 0.6970
 101/1000 [==>...........................] - ETA: 6:47 - loss: 2.6863 - regression_loss: 1.9929 - classification_loss: 0.6934
 102/1000 [==>...........................] - ETA: 6:46 - loss: 2.6953 - regression_loss: 2.0002 - classification_loss: 0.6951
 103/1000 [==>...........................] - ETA: 6:46 - loss: 2.6692 - regression_loss: 1.9808 - classification_loss: 0.6884
 104/1000 [==>...........................] - ETA: 6:45 - loss: 2.6435 - regression_loss: 1.9617 - classification_loss: 0.6818
 105/1000 [==>...........................] - ETA: 6:45 - loss: 2.6523 - regression_loss: 1.9718 - classification_loss: 0.6806
 106/1000 [==>...........................] - ETA: 6:45 - loss: 2.6285 - regression_loss: 1.9532 - classification_loss: 0.6754
 107/1000 [==>...........................] - ETA: 6:44 - loss: 2.6545 - regression_loss: 1.9680 - classification_loss: 0.6865
 108/1000 [==>...........................] - ETA: 6:44 - loss: 2.6810 - regression_loss: 1.9925 - classification_loss: 0.6885
 109/1000 [==>...........................] - ETA: 6:43 - loss: 2.6567 - regression_loss: 1.9742 - classification_loss: 0.6825
 110/1000 [==>...........................] - ETA: 6:43 - loss: 2.6755 - regression_loss: 1.9947 - classification_loss: 0.6808
 111/1000 [==>...........................] - ETA: 6:42 - loss: 2.6796 - regression_loss: 1.9943 - classification_loss: 0.6853
 112/1000 [==>...........................] - ETA: 6:42 - loss: 2.6785 - regression_loss: 1.9941 - classification_loss: 0.6844
 113/1000 [==>...........................] - ETA: 6:41 - loss: 2.6888 - regression_loss: 2.0060 - classification_loss: 0.6828
 114/1000 [==>...........................] - ETA: 6:41 - loss: 2.6652 - regression_loss: 1.9884 - classification_loss: 0.6769
 115/1000 [==>...........................] - ETA: 6:40 - loss: 2.6667 - regression_loss: 1.9911 - classification_loss: 0.6756
 116/1000 [==>...........................] - ETA: 6:40 - loss: 2.6862 - regression_loss: 2.0007 - classification_loss: 0.6855
 117/1000 [==>...........................] - ETA: 6:40 - loss: 2.6633 - regression_loss: 1.9836 - classification_loss: 0.6798
 118/1000 [==>...........................] - ETA: 6:39 - loss: 2.6751 - regression_loss: 1.9967 - classification_loss: 0.6783
 119/1000 [==>...........................] - ETA: 6:39 - loss: 2.6904 - regression_loss: 2.0037 - classification_loss: 0.6866
 120/1000 [==>...........................] - ETA: 6:38 - loss: 2.7050 - regression_loss: 2.0207 - classification_loss: 0.6843
 121/1000 [==>...........................] - ETA: 6:38 - loss: 2.7103 - regression_loss: 2.0235 - classification_loss: 0.6868
 122/1000 [==>...........................] - ETA: 6:37 - loss: 2.7086 - regression_loss: 2.0240 - classification_loss: 0.6846
 123/1000 [==>...........................] - ETA: 6:37 - loss: 2.7286 - regression_loss: 2.0357 - classification_loss: 0.6928
 124/1000 [==>...........................] - ETA: 6:36 - loss: 2.7066 - regression_loss: 2.0193 - classification_loss: 0.6873
 125/1000 [==>...........................] - ETA: 6:36 - loss: 2.6851 - regression_loss: 2.0031 - classification_loss: 0.6819
 126/1000 [==>...........................] - ETA: 6:36 - loss: 2.6638 - regression_loss: 1.9872 - classification_loss: 0.6765
 127/1000 [==>...........................] - ETA: 6:35 - loss: 2.6577 - regression_loss: 1.9845 - classification_loss: 0.6732
 128/1000 [==>...........................] - ETA: 6:35 - loss: 2.6718 - regression_loss: 1.9969 - classification_loss: 0.6749
 129/1000 [==>...........................] - ETA: 6:34 - loss: 2.6770 - regression_loss: 2.0041 - classification_loss: 0.6729
 130/1000 [==>...........................] - ETA: 6:34 - loss: 2.6878 - regression_loss: 2.0056 - classification_loss: 0.6821
 131/1000 [==>...........................] - ETA: 6:33 - loss: 2.6673 - regression_loss: 1.9903 - classification_loss: 0.6769
 132/1000 [==>...........................] - ETA: 6:33 - loss: 2.6686 - regression_loss: 1.9929 - classification_loss: 0.6756
 133/1000 [==>...........................] - ETA: 6:32 - loss: 2.6797 - regression_loss: 1.9965 - classification_loss: 0.6832
 134/1000 [===>..........................] - ETA: 6:32 - loss: 2.6861 - regression_loss: 2.0010 - classification_loss: 0.6851
 135/1000 [===>..........................] - ETA: 6:31 - loss: 2.6985 - regression_loss: 2.0048 - classification_loss: 0.6938
 136/1000 [===>..........................] - ETA: 6:31 - loss: 2.7177 - regression_loss: 2.0158 - classification_loss: 0.7019
 137/1000 [===>..........................] - ETA: 6:31 - loss: 2.6979 - regression_loss: 2.0011 - classification_loss: 0.6967
 138/1000 [===>..........................] - ETA: 6:30 - loss: 2.7103 - regression_loss: 2.0085 - classification_loss: 0.7018
 139/1000 [===>..........................] - ETA: 6:30 - loss: 2.7110 - regression_loss: 2.0093 - classification_loss: 0.7017
 140/1000 [===>..........................] - ETA: 6:29 - loss: 2.6917 - regression_loss: 1.9950 - classification_loss: 0.6967
 141/1000 [===>..........................] - ETA: 6:29 - loss: 2.6956 - regression_loss: 1.9934 - classification_loss: 0.7022
 142/1000 [===>..........................] - ETA: 6:28 - loss: 2.6944 - regression_loss: 1.9933 - classification_loss: 0.7012
 143/1000 [===>..........................] - ETA: 6:28 - loss: 2.6756 - regression_loss: 1.9793 - classification_loss: 0.6963
 144/1000 [===>..........................] - ETA: 6:27 - loss: 2.6868 - regression_loss: 1.9863 - classification_loss: 0.7006
 145/1000 [===>..........................] - ETA: 6:27 - loss: 2.6844 - regression_loss: 1.9847 - classification_loss: 0.6997
 146/1000 [===>..........................] - ETA: 6:27 - loss: 2.6922 - regression_loss: 1.9932 - classification_loss: 0.6990
 147/1000 [===>..........................] - ETA: 6:26 - loss: 2.7002 - regression_loss: 2.0010 - classification_loss: 0.6991
 148/1000 [===>..........................] - ETA: 6:26 - loss: 2.6999 - regression_loss: 1.9998 - classification_loss: 0.7001
 149/1000 [===>..........................] - ETA: 6:25 - loss: 2.7016 - regression_loss: 2.0022 - classification_loss: 0.6994
 150/1000 [===>..........................] - ETA: 6:25 - loss: 2.7100 - regression_loss: 2.0096 - classification_loss: 0.7003
 151/1000 [===>..........................] - ETA: 6:24 - loss: 2.7116 - regression_loss: 2.0098 - classification_loss: 0.7018
 152/1000 [===>..........................] - ETA: 6:24 - loss: 2.7179 - regression_loss: 2.0126 - classification_loss: 0.7053
 153/1000 [===>..........................] - ETA: 6:24 - loss: 2.7104 - regression_loss: 2.0082 - classification_loss: 0.7022
 154/1000 [===>..........................] - ETA: 6:23 - loss: 2.7171 - regression_loss: 2.0158 - classification_loss: 0.7013
 155/1000 [===>..........................] - ETA: 6:23 - loss: 2.7186 - regression_loss: 2.0188 - classification_loss: 0.6998
 156/1000 [===>..........................] - ETA: 6:22 - loss: 2.7431 - regression_loss: 2.0352 - classification_loss: 0.7080
 157/1000 [===>..........................] - ETA: 6:22 - loss: 2.7454 - regression_loss: 2.0374 - classification_loss: 0.7080
 158/1000 [===>..........................] - ETA: 6:21 - loss: 2.7280 - regression_loss: 2.0245 - classification_loss: 0.7035
 159/1000 [===>..........................] - ETA: 6:21 - loss: 2.7339 - regression_loss: 2.0318 - classification_loss: 0.7022
 160/1000 [===>..........................] - ETA: 6:20 - loss: 2.7512 - regression_loss: 2.0441 - classification_loss: 0.7071
 161/1000 [===>..........................] - ETA: 6:20 - loss: 2.7579 - regression_loss: 2.0519 - classification_loss: 0.7060
 162/1000 [===>..........................] - ETA: 6:19 - loss: 2.7656 - regression_loss: 2.0577 - classification_loss: 0.7079
 163/1000 [===>..........................] - ETA: 6:19 - loss: 2.7486 - regression_loss: 2.0451 - classification_loss: 0.7035
 164/1000 [===>..........................] - ETA: 6:18 - loss: 2.7497 - regression_loss: 2.0461 - classification_loss: 0.7036
 165/1000 [===>..........................] - ETA: 6:18 - loss: 2.7631 - regression_loss: 2.0555 - classification_loss: 0.7075
 166/1000 [===>..........................] - ETA: 6:18 - loss: 2.7701 - regression_loss: 2.0651 - classification_loss: 0.7051
 167/1000 [====>.........................] - ETA: 6:17 - loss: 2.7701 - regression_loss: 2.0660 - classification_loss: 0.7041
 168/1000 [====>.........................] - ETA: 6:17 - loss: 2.7689 - regression_loss: 2.0674 - classification_loss: 0.7015
 169/1000 [====>.........................] - ETA: 6:16 - loss: 2.7773 - regression_loss: 2.0765 - classification_loss: 0.7008
 170/1000 [====>.........................] - ETA: 6:16 - loss: 2.7776 - regression_loss: 2.0780 - classification_loss: 0.6996
 171/1000 [====>.........................] - ETA: 6:15 - loss: 2.7810 - regression_loss: 2.0821 - classification_loss: 0.6989
 172/1000 [====>.........................] - ETA: 6:15 - loss: 2.7849 - regression_loss: 2.0872 - classification_loss: 0.6977
 173/1000 [====>.........................] - ETA: 6:15 - loss: 2.7921 - regression_loss: 2.0902 - classification_loss: 0.7019
 174/1000 [====>.........................] - ETA: 6:14 - loss: 2.7929 - regression_loss: 2.0913 - classification_loss: 0.7016
 175/1000 [====>.........................] - ETA: 6:14 - loss: 2.7923 - regression_loss: 2.0913 - classification_loss: 0.7009
 176/1000 [====>.........................] - ETA: 6:13 - loss: 2.8059 - regression_loss: 2.1005 - classification_loss: 0.7054
 177/1000 [====>.........................] - ETA: 6:13 - loss: 2.7902 - regression_loss: 2.0886 - classification_loss: 0.7015
 178/1000 [====>.........................] - ETA: 6:12 - loss: 2.7745 - regression_loss: 2.0769 - classification_loss: 0.6976
 179/1000 [====>.........................] - ETA: 6:12 - loss: 2.7777 - regression_loss: 2.0805 - classification_loss: 0.6971
 180/1000 [====>.........................] - ETA: 6:11 - loss: 2.7840 - regression_loss: 2.0863 - classification_loss: 0.6977
 181/1000 [====>.........................] - ETA: 6:11 - loss: 2.7853 - regression_loss: 2.0892 - classification_loss: 0.6961
 182/1000 [====>.........................] - ETA: 6:10 - loss: 2.7863 - regression_loss: 2.0906 - classification_loss: 0.6956
 183/1000 [====>.........................] - ETA: 6:10 - loss: 2.7901 - regression_loss: 2.0941 - classification_loss: 0.6960
 184/1000 [====>.........................] - ETA: 6:09 - loss: 2.7984 - regression_loss: 2.0994 - classification_loss: 0.6990
 185/1000 [====>.........................] - ETA: 6:09 - loss: 2.7901 - regression_loss: 2.0880 - classification_loss: 0.7021
 186/1000 [====>.........................] - ETA: 6:09 - loss: 2.7947 - regression_loss: 2.0918 - classification_loss: 0.7030
 187/1000 [====>.........................] - ETA: 6:08 - loss: 2.7942 - regression_loss: 2.0918 - classification_loss: 0.7024
 188/1000 [====>.........................] - ETA: 6:08 - loss: 2.7997 - regression_loss: 2.0982 - classification_loss: 0.7016
 189/1000 [====>.........................] - ETA: 6:07 - loss: 2.8006 - regression_loss: 2.0998 - classification_loss: 0.7008
 190/1000 [====>.........................] - ETA: 6:07 - loss: 2.8175 - regression_loss: 2.1147 - classification_loss: 0.7028
 191/1000 [====>.........................] - ETA: 6:06 - loss: 2.8188 - regression_loss: 2.1157 - classification_loss: 0.7031
 192/1000 [====>.........................] - ETA: 6:06 - loss: 2.8180 - regression_loss: 2.1151 - classification_loss: 0.7030
 193/1000 [====>.........................] - ETA: 6:05 - loss: 2.8187 - regression_loss: 2.1162 - classification_loss: 0.7025
 194/1000 [====>.........................] - ETA: 6:05 - loss: 2.8152 - regression_loss: 2.1144 - classification_loss: 0.7009
 195/1000 [====>.........................] - ETA: 6:04 - loss: 2.8218 - regression_loss: 2.1212 - classification_loss: 0.7005
 196/1000 [====>.........................] - ETA: 6:04 - loss: 2.8248 - regression_loss: 2.1247 - classification_loss: 0.7001
 197/1000 [====>.........................] - ETA: 6:03 - loss: 2.8313 - regression_loss: 2.1313 - classification_loss: 0.7001
 198/1000 [====>.........................] - ETA: 6:03 - loss: 2.8201 - regression_loss: 2.1205 - classification_loss: 0.6996
 199/1000 [====>.........................] - ETA: 6:02 - loss: 2.8212 - regression_loss: 2.1227 - classification_loss: 0.6986
 200/1000 [=====>........................] - ETA: 6:02 - loss: 2.8212 - regression_loss: 2.1238 - classification_loss: 0.6974
 201/1000 [=====>........................] - ETA: 6:02 - loss: 2.8205 - regression_loss: 2.1238 - classification_loss: 0.6967
 202/1000 [=====>........................] - ETA: 6:01 - loss: 2.8199 - regression_loss: 2.1255 - classification_loss: 0.6944
 203/1000 [=====>........................] - ETA: 6:01 - loss: 2.8213 - regression_loss: 2.1277 - classification_loss: 0.6936
 204/1000 [=====>........................] - ETA: 6:00 - loss: 2.8287 - regression_loss: 2.1308 - classification_loss: 0.6979
 205/1000 [=====>........................] - ETA: 6:00 - loss: 2.8369 - regression_loss: 2.1377 - classification_loss: 0.6991
 206/1000 [=====>........................] - ETA: 5:59 - loss: 2.8371 - regression_loss: 2.1391 - classification_loss: 0.6981
 207/1000 [=====>........................] - ETA: 5:59 - loss: 2.8239 - regression_loss: 2.1287 - classification_loss: 0.6951
 208/1000 [=====>........................] - ETA: 5:58 - loss: 2.8290 - regression_loss: 2.1321 - classification_loss: 0.6969
 209/1000 [=====>........................] - ETA: 5:58 - loss: 2.8155 - regression_loss: 2.1219 - classification_loss: 0.6936
 210/1000 [=====>........................] - ETA: 5:57 - loss: 2.8226 - regression_loss: 2.1291 - classification_loss: 0.6935
 211/1000 [=====>........................] - ETA: 5:57 - loss: 2.8240 - regression_loss: 2.1312 - classification_loss: 0.6928
 212/1000 [=====>........................] - ETA: 5:56 - loss: 2.8299 - regression_loss: 2.1334 - classification_loss: 0.6964
 213/1000 [=====>........................] - ETA: 5:56 - loss: 2.8293 - regression_loss: 2.1324 - classification_loss: 0.6969
 214/1000 [=====>........................] - ETA: 5:55 - loss: 2.8222 - regression_loss: 2.1276 - classification_loss: 0.6945
 215/1000 [=====>........................] - ETA: 5:55 - loss: 2.8403 - regression_loss: 2.1397 - classification_loss: 0.7005
 216/1000 [=====>........................] - ETA: 5:55 - loss: 2.8452 - regression_loss: 2.1458 - classification_loss: 0.6994
 217/1000 [=====>........................] - ETA: 5:54 - loss: 2.8481 - regression_loss: 2.1492 - classification_loss: 0.6989
 218/1000 [=====>........................] - ETA: 5:54 - loss: 2.8516 - regression_loss: 2.1545 - classification_loss: 0.6971
 219/1000 [=====>........................] - ETA: 5:53 - loss: 2.8573 - regression_loss: 2.1608 - classification_loss: 0.6965
 220/1000 [=====>........................] - ETA: 5:53 - loss: 2.8657 - regression_loss: 2.1682 - classification_loss: 0.6974
 221/1000 [=====>........................] - ETA: 5:52 - loss: 2.8643 - regression_loss: 2.1678 - classification_loss: 0.6965
 222/1000 [=====>........................] - ETA: 5:52 - loss: 2.8662 - regression_loss: 2.1686 - classification_loss: 0.6976
 223/1000 [=====>........................] - ETA: 5:51 - loss: 2.8714 - regression_loss: 2.1719 - classification_loss: 0.6995
 224/1000 [=====>........................] - ETA: 5:51 - loss: 2.8672 - regression_loss: 2.1698 - classification_loss: 0.6974
 225/1000 [=====>........................] - ETA: 5:51 - loss: 2.8712 - regression_loss: 2.1732 - classification_loss: 0.6980
 226/1000 [=====>........................] - ETA: 5:50 - loss: 2.8631 - regression_loss: 2.1636 - classification_loss: 0.6996
 227/1000 [=====>........................] - ETA: 5:50 - loss: 2.8661 - regression_loss: 2.1677 - classification_loss: 0.6984
 228/1000 [=====>........................] - ETA: 5:49 - loss: 2.8537 - regression_loss: 2.1582 - classification_loss: 0.6955
 229/1000 [=====>........................] - ETA: 5:49 - loss: 2.8533 - regression_loss: 2.1590 - classification_loss: 0.6943
 230/1000 [=====>........................] - ETA: 5:48 - loss: 2.8543 - regression_loss: 2.1598 - classification_loss: 0.6945
 231/1000 [=====>........................] - ETA: 5:48 - loss: 2.8522 - regression_loss: 2.1586 - classification_loss: 0.6936
 232/1000 [=====>........................] - ETA: 5:47 - loss: 2.8633 - regression_loss: 2.1688 - classification_loss: 0.6945
 233/1000 [=====>........................] - ETA: 5:47 - loss: 2.8517 - regression_loss: 2.1595 - classification_loss: 0.6921
 234/1000 [======>.......................] - ETA: 5:47 - loss: 2.8506 - regression_loss: 2.1602 - classification_loss: 0.6904
 235/1000 [======>.......................] - ETA: 5:46 - loss: 2.8659 - regression_loss: 2.1711 - classification_loss: 0.6947
 236/1000 [======>.......................] - ETA: 5:46 - loss: 2.8538 - regression_loss: 2.1619 - classification_loss: 0.6918
 237/1000 [======>.......................] - ETA: 5:45 - loss: 2.8612 - regression_loss: 2.1694 - classification_loss: 0.6918
 238/1000 [======>.......................] - ETA: 5:45 - loss: 2.8636 - regression_loss: 2.1725 - classification_loss: 0.6911
 239/1000 [======>.......................] - ETA: 5:44 - loss: 2.8518 - regression_loss: 2.1634 - classification_loss: 0.6884
 240/1000 [======>.......................] - ETA: 5:44 - loss: 2.8401 - regression_loss: 2.1544 - classification_loss: 0.6857
 241/1000 [======>.......................] - ETA: 5:43 - loss: 2.8470 - regression_loss: 2.1602 - classification_loss: 0.6868
 242/1000 [======>.......................] - ETA: 5:43 - loss: 2.8352 - regression_loss: 2.1513 - classification_loss: 0.6839
 243/1000 [======>.......................] - ETA: 5:43 - loss: 2.8341 - regression_loss: 2.1509 - classification_loss: 0.6832
 244/1000 [======>.......................] - ETA: 5:42 - loss: 2.8401 - regression_loss: 2.1565 - classification_loss: 0.6836
 245/1000 [======>.......................] - ETA: 5:42 - loss: 2.8380 - regression_loss: 2.1553 - classification_loss: 0.6826
 246/1000 [======>.......................] - ETA: 5:41 - loss: 2.8327 - regression_loss: 2.1520 - classification_loss: 0.6807
 247/1000 [======>.......................] - ETA: 5:41 - loss: 2.8366 - regression_loss: 2.1528 - classification_loss: 0.6837
 248/1000 [======>.......................] - ETA: 5:40 - loss: 2.8376 - regression_loss: 2.1519 - classification_loss: 0.6857
 249/1000 [======>.......................] - ETA: 5:40 - loss: 2.8262 - regression_loss: 2.1432 - classification_loss: 0.6829
 250/1000 [======>.......................] - ETA: 5:39 - loss: 2.8238 - regression_loss: 2.1421 - classification_loss: 0.6817
 251/1000 [======>.......................] - ETA: 5:39 - loss: 2.8243 - regression_loss: 2.1432 - classification_loss: 0.6811
 252/1000 [======>.......................] - ETA: 5:38 - loss: 2.8263 - regression_loss: 2.1451 - classification_loss: 0.6813
 253/1000 [======>.......................] - ETA: 5:38 - loss: 2.8310 - regression_loss: 2.1497 - classification_loss: 0.6814
 254/1000 [======>.......................] - ETA: 5:37 - loss: 2.8377 - regression_loss: 2.1559 - classification_loss: 0.6818
 255/1000 [======>.......................] - ETA: 5:37 - loss: 2.8266 - regression_loss: 2.1475 - classification_loss: 0.6791
 256/1000 [======>.......................] - ETA: 5:37 - loss: 2.8286 - regression_loss: 2.1501 - classification_loss: 0.6785
 257/1000 [======>.......................] - ETA: 5:36 - loss: 2.8294 - regression_loss: 2.1505 - classification_loss: 0.6788
 258/1000 [======>.......................] - ETA: 5:36 - loss: 2.8297 - regression_loss: 2.1517 - classification_loss: 0.6781
 259/1000 [======>.......................] - ETA: 5:35 - loss: 2.8400 - regression_loss: 2.1597 - classification_loss: 0.6803
 260/1000 [======>.......................] - ETA: 5:35 - loss: 2.8291 - regression_loss: 2.1514 - classification_loss: 0.6776
 261/1000 [======>.......................] - ETA: 5:34 - loss: 2.8182 - regression_loss: 2.1432 - classification_loss: 0.6751
 262/1000 [======>.......................] - ETA: 5:34 - loss: 2.8205 - regression_loss: 2.1458 - classification_loss: 0.6748
 263/1000 [======>.......................] - ETA: 5:33 - loss: 2.8214 - regression_loss: 2.1475 - classification_loss: 0.6739
 264/1000 [======>.......................] - ETA: 5:33 - loss: 2.8230 - regression_loss: 2.1487 - classification_loss: 0.6743
 265/1000 [======>.......................] - ETA: 5:32 - loss: 2.8123 - regression_loss: 2.1406 - classification_loss: 0.6718
 266/1000 [======>.......................] - ETA: 5:32 - loss: 2.8077 - regression_loss: 2.1376 - classification_loss: 0.6702
 267/1000 [=======>......................] - ETA: 5:32 - loss: 2.7988 - regression_loss: 2.1296 - classification_loss: 0.6693
 268/1000 [=======>......................] - ETA: 5:31 - loss: 2.8009 - regression_loss: 2.1326 - classification_loss: 0.6683
 269/1000 [=======>......................] - ETA: 5:31 - loss: 2.8100 - regression_loss: 2.1406 - classification_loss: 0.6694
 270/1000 [=======>......................] - ETA: 5:30 - loss: 2.8185 - regression_loss: 2.1446 - classification_loss: 0.6740
 271/1000 [=======>......................] - ETA: 5:30 - loss: 2.8159 - regression_loss: 2.1433 - classification_loss: 0.6726
 272/1000 [=======>......................] - ETA: 5:29 - loss: 2.8147 - regression_loss: 2.1431 - classification_loss: 0.6716
 273/1000 [=======>......................] - ETA: 5:29 - loss: 2.8201 - regression_loss: 2.1481 - classification_loss: 0.6720
 274/1000 [=======>......................] - ETA: 5:28 - loss: 2.8274 - regression_loss: 2.1516 - classification_loss: 0.6758
 275/1000 [=======>......................] - ETA: 5:28 - loss: 2.8171 - regression_loss: 2.1438 - classification_loss: 0.6733
 276/1000 [=======>......................] - ETA: 5:27 - loss: 2.8226 - regression_loss: 2.1475 - classification_loss: 0.6751
 277/1000 [=======>......................] - ETA: 5:27 - loss: 2.8124 - regression_loss: 2.1397 - classification_loss: 0.6727
 278/1000 [=======>......................] - ETA: 5:27 - loss: 2.8199 - regression_loss: 2.1447 - classification_loss: 0.6752
 279/1000 [=======>......................] - ETA: 5:26 - loss: 2.8267 - regression_loss: 2.1487 - classification_loss: 0.6780
 280/1000 [=======>......................] - ETA: 5:26 - loss: 2.8335 - regression_loss: 2.1520 - classification_loss: 0.6815
 281/1000 [=======>......................] - ETA: 5:25 - loss: 2.8367 - regression_loss: 2.1562 - classification_loss: 0.6805
 282/1000 [=======>......................] - ETA: 5:25 - loss: 2.8386 - regression_loss: 2.1567 - classification_loss: 0.6820
 283/1000 [=======>......................] - ETA: 5:24 - loss: 2.8286 - regression_loss: 2.1491 - classification_loss: 0.6795
 284/1000 [=======>......................] - ETA: 5:24 - loss: 2.8344 - regression_loss: 2.1527 - classification_loss: 0.6817
 285/1000 [=======>......................] - ETA: 5:23 - loss: 2.8405 - regression_loss: 2.1575 - classification_loss: 0.6830
 286/1000 [=======>......................] - ETA: 5:23 - loss: 2.8445 - regression_loss: 2.1607 - classification_loss: 0.6838
 287/1000 [=======>......................] - ETA: 5:22 - loss: 2.8447 - regression_loss: 2.1617 - classification_loss: 0.6830
 288/1000 [=======>......................] - ETA: 5:22 - loss: 2.8491 - regression_loss: 2.1656 - classification_loss: 0.6835
 289/1000 [=======>......................] - ETA: 5:22 - loss: 2.8526 - regression_loss: 2.1688 - classification_loss: 0.6838
 290/1000 [=======>......................] - ETA: 5:21 - loss: 2.8427 - regression_loss: 2.1613 - classification_loss: 0.6815
 291/1000 [=======>......................] - ETA: 5:21 - loss: 2.8475 - regression_loss: 2.1667 - classification_loss: 0.6808
 292/1000 [=======>......................] - ETA: 5:20 - loss: 2.8378 - regression_loss: 2.1593 - classification_loss: 0.6785
 293/1000 [=======>......................] - ETA: 5:20 - loss: 2.8448 - regression_loss: 2.1634 - classification_loss: 0.6814
 294/1000 [=======>......................] - ETA: 5:19 - loss: 2.8477 - regression_loss: 2.1653 - classification_loss: 0.6823
 295/1000 [=======>......................] - ETA: 5:19 - loss: 2.8380 - regression_loss: 2.1580 - classification_loss: 0.6800
 296/1000 [=======>......................] - ETA: 5:18 - loss: 2.8421 - regression_loss: 2.1611 - classification_loss: 0.6810
 297/1000 [=======>......................] - ETA: 5:18 - loss: 2.8421 - regression_loss: 2.1618 - classification_loss: 0.6804
 298/1000 [=======>......................] - ETA: 5:18 - loss: 2.8445 - regression_loss: 2.1639 - classification_loss: 0.6807
 299/1000 [=======>......................] - ETA: 5:17 - loss: 2.8479 - regression_loss: 2.1663 - classification_loss: 0.6816
 300/1000 [========>.....................] - ETA: 5:17 - loss: 2.8496 - regression_loss: 2.1673 - classification_loss: 0.6823
 301/1000 [========>.....................] - ETA: 5:16 - loss: 2.8498 - regression_loss: 2.1684 - classification_loss: 0.6814
 302/1000 [========>.....................] - ETA: 5:16 - loss: 2.8546 - regression_loss: 2.1703 - classification_loss: 0.6843
 303/1000 [========>.....................] - ETA: 5:15 - loss: 2.8583 - regression_loss: 2.1731 - classification_loss: 0.6853
 304/1000 [========>.....................] - ETA: 5:15 - loss: 2.8573 - regression_loss: 2.1733 - classification_loss: 0.6839
 305/1000 [========>.....................] - ETA: 5:14 - loss: 2.8566 - regression_loss: 2.1739 - classification_loss: 0.6826
 306/1000 [========>.....................] - ETA: 5:14 - loss: 2.8472 - regression_loss: 2.1668 - classification_loss: 0.6804
 307/1000 [========>.....................] - ETA: 5:13 - loss: 2.8519 - regression_loss: 2.1703 - classification_loss: 0.6816
 308/1000 [========>.....................] - ETA: 5:13 - loss: 2.8546 - regression_loss: 2.1718 - classification_loss: 0.6827
 309/1000 [========>.....................] - ETA: 5:13 - loss: 2.8532 - regression_loss: 2.1715 - classification_loss: 0.6817
 310/1000 [========>.....................] - ETA: 5:12 - loss: 2.8440 - regression_loss: 2.1645 - classification_loss: 0.6795
 311/1000 [========>.....................] - ETA: 5:12 - loss: 2.8349 - regression_loss: 2.1575 - classification_loss: 0.6774
 312/1000 [========>.....................] - ETA: 5:11 - loss: 2.8359 - regression_loss: 2.1586 - classification_loss: 0.6773
 313/1000 [========>.....................] - ETA: 5:11 - loss: 2.8269 - regression_loss: 2.1517 - classification_loss: 0.6752
 314/1000 [========>.....................] - ETA: 5:10 - loss: 2.8296 - regression_loss: 2.1543 - classification_loss: 0.6753
 315/1000 [========>.....................] - ETA: 5:10 - loss: 2.8338 - regression_loss: 2.1583 - classification_loss: 0.6755
 316/1000 [========>.....................] - ETA: 5:09 - loss: 2.8362 - regression_loss: 2.1606 - classification_loss: 0.6757
 317/1000 [========>.....................] - ETA: 5:09 - loss: 2.8439 - regression_loss: 2.1655 - classification_loss: 0.6785
 318/1000 [========>.....................] - ETA: 5:08 - loss: 2.8350 - regression_loss: 2.1587 - classification_loss: 0.6763
 319/1000 [========>.....................] - ETA: 5:08 - loss: 2.8356 - regression_loss: 2.1597 - classification_loss: 0.6759
 320/1000 [========>.....................] - ETA: 5:08 - loss: 2.8392 - regression_loss: 2.1625 - classification_loss: 0.6767
 321/1000 [========>.....................] - ETA: 5:07 - loss: 2.8462 - regression_loss: 2.1654 - classification_loss: 0.6807
 322/1000 [========>.....................] - ETA: 5:07 - loss: 2.8475 - regression_loss: 2.1652 - classification_loss: 0.6822
 323/1000 [========>.....................] - ETA: 5:06 - loss: 2.8521 - regression_loss: 2.1686 - classification_loss: 0.6836
 324/1000 [========>.....................] - ETA: 5:06 - loss: 2.8433 - regression_loss: 2.1619 - classification_loss: 0.6814
 325/1000 [========>.....................] - ETA: 5:05 - loss: 2.8436 - regression_loss: 2.1608 - classification_loss: 0.6829
 326/1000 [========>.....................] - ETA: 5:05 - loss: 2.8425 - regression_loss: 2.1603 - classification_loss: 0.6822
 327/1000 [========>.....................] - ETA: 5:04 - loss: 2.8338 - regression_loss: 2.1537 - classification_loss: 0.6802
 328/1000 [========>.....................] - ETA: 5:04 - loss: 2.8395 - regression_loss: 2.1566 - classification_loss: 0.6828
 329/1000 [========>.....................] - ETA: 5:04 - loss: 2.8434 - regression_loss: 2.1611 - classification_loss: 0.6823
 330/1000 [========>.....................] - ETA: 5:03 - loss: 2.8406 - regression_loss: 2.1587 - classification_loss: 0.6818
 331/1000 [========>.....................] - ETA: 5:03 - loss: 2.8434 - regression_loss: 2.1620 - classification_loss: 0.6815
 332/1000 [========>.....................] - ETA: 5:02 - loss: 2.8457 - regression_loss: 2.1652 - classification_loss: 0.6806
 333/1000 [========>.....................] - ETA: 5:02 - loss: 2.8446 - regression_loss: 2.1642 - classification_loss: 0.6805
 334/1000 [=========>....................] - ETA: 5:01 - loss: 2.8455 - regression_loss: 2.1663 - classification_loss: 0.6792
 335/1000 [=========>....................] - ETA: 5:01 - loss: 2.8452 - regression_loss: 2.1669 - classification_loss: 0.6783
 336/1000 [=========>....................] - ETA: 5:00 - loss: 2.8436 - regression_loss: 2.1663 - classification_loss: 0.6773
 337/1000 [=========>....................] - ETA: 5:00 - loss: 2.8496 - regression_loss: 2.1715 - classification_loss: 0.6781
 338/1000 [=========>....................] - ETA: 4:59 - loss: 2.8507 - regression_loss: 2.1728 - classification_loss: 0.6779
 339/1000 [=========>....................] - ETA: 4:59 - loss: 2.8530 - regression_loss: 2.1727 - classification_loss: 0.6803
 340/1000 [=========>....................] - ETA: 4:58 - loss: 2.8564 - regression_loss: 2.1755 - classification_loss: 0.6809
 341/1000 [=========>....................] - ETA: 4:58 - loss: 2.8615 - regression_loss: 2.1800 - classification_loss: 0.6814
 342/1000 [=========>....................] - ETA: 4:57 - loss: 2.8640 - regression_loss: 2.1835 - classification_loss: 0.6806
 343/1000 [=========>....................] - ETA: 4:57 - loss: 2.8636 - regression_loss: 2.1843 - classification_loss: 0.6793
 344/1000 [=========>....................] - ETA: 4:57 - loss: 2.8675 - regression_loss: 2.1888 - classification_loss: 0.6787
 345/1000 [=========>....................] - ETA: 4:56 - loss: 2.8699 - regression_loss: 2.1910 - classification_loss: 0.6788
 346/1000 [=========>....................] - ETA: 4:56 - loss: 2.8619 - regression_loss: 2.1847 - classification_loss: 0.6772
 347/1000 [=========>....................] - ETA: 4:55 - loss: 2.8614 - regression_loss: 2.1846 - classification_loss: 0.6767
 348/1000 [=========>....................] - ETA: 4:55 - loss: 2.8617 - regression_loss: 2.1857 - classification_loss: 0.6760
 349/1000 [=========>....................] - ETA: 4:54 - loss: 2.8538 - regression_loss: 2.1794 - classification_loss: 0.6743
 350/1000 [=========>....................] - ETA: 4:54 - loss: 2.8534 - regression_loss: 2.1800 - classification_loss: 0.6734
 351/1000 [=========>....................] - ETA: 4:53 - loss: 2.8576 - regression_loss: 2.1836 - classification_loss: 0.6739
 352/1000 [=========>....................] - ETA: 4:53 - loss: 2.8565 - regression_loss: 2.1830 - classification_loss: 0.6735
 353/1000 [=========>....................] - ETA: 4:53 - loss: 2.8555 - regression_loss: 2.1828 - classification_loss: 0.6727
 354/1000 [=========>....................] - ETA: 4:52 - loss: 2.8586 - regression_loss: 2.1853 - classification_loss: 0.6733
 355/1000 [=========>....................] - ETA: 4:52 - loss: 2.8591 - regression_loss: 2.1859 - classification_loss: 0.6732
 356/1000 [=========>....................] - ETA: 4:51 - loss: 2.8588 - regression_loss: 2.1863 - classification_loss: 0.6725
 357/1000 [=========>....................] - ETA: 4:51 - loss: 2.8602 - regression_loss: 2.1874 - classification_loss: 0.6729
 358/1000 [=========>....................] - ETA: 4:50 - loss: 2.8610 - regression_loss: 2.1880 - classification_loss: 0.6731
 359/1000 [=========>....................] - ETA: 4:50 - loss: 2.8531 - regression_loss: 2.1819 - classification_loss: 0.6712
 360/1000 [=========>....................] - ETA: 4:49 - loss: 2.8452 - regression_loss: 2.1758 - classification_loss: 0.6694
 361/1000 [=========>....................] - ETA: 4:49 - loss: 2.8501 - regression_loss: 2.1793 - classification_loss: 0.6707
 362/1000 [=========>....................] - ETA: 4:49 - loss: 2.8532 - regression_loss: 2.1818 - classification_loss: 0.6714
 363/1000 [=========>....................] - ETA: 4:48 - loss: 2.8585 - regression_loss: 2.1854 - classification_loss: 0.6731
 364/1000 [=========>....................] - ETA: 4:48 - loss: 2.8594 - regression_loss: 2.1855 - classification_loss: 0.6738
 365/1000 [=========>....................] - ETA: 4:47 - loss: 2.8648 - regression_loss: 2.1899 - classification_loss: 0.6749
 366/1000 [=========>....................] - ETA: 4:47 - loss: 2.8688 - regression_loss: 2.1924 - classification_loss: 0.6764
 367/1000 [==========>...................] - ETA: 4:46 - loss: 2.8715 - regression_loss: 2.1957 - classification_loss: 0.6757
 368/1000 [==========>...................] - ETA: 4:46 - loss: 2.8727 - regression_loss: 2.1964 - classification_loss: 0.6763
 369/1000 [==========>...................] - ETA: 4:45 - loss: 2.8657 - regression_loss: 2.1905 - classification_loss: 0.6752
 370/1000 [==========>...................] - ETA: 4:45 - loss: 2.8688 - regression_loss: 2.1928 - classification_loss: 0.6759
 371/1000 [==========>...................] - ETA: 4:44 - loss: 2.8709 - regression_loss: 2.1947 - classification_loss: 0.6762
 372/1000 [==========>...................] - ETA: 4:44 - loss: 2.8718 - regression_loss: 2.1960 - classification_loss: 0.6757
 373/1000 [==========>...................] - ETA: 4:44 - loss: 2.8641 - regression_loss: 2.1902 - classification_loss: 0.6739
 374/1000 [==========>...................] - ETA: 4:43 - loss: 2.8686 - regression_loss: 2.1939 - classification_loss: 0.6747
 375/1000 [==========>...................] - ETA: 4:43 - loss: 2.8723 - regression_loss: 2.1953 - classification_loss: 0.6770
 376/1000 [==========>...................] - ETA: 4:42 - loss: 2.8776 - regression_loss: 2.1976 - classification_loss: 0.6800
 377/1000 [==========>...................] - ETA: 4:42 - loss: 2.8868 - regression_loss: 2.2039 - classification_loss: 0.6829
 378/1000 [==========>...................] - ETA: 4:41 - loss: 2.8874 - regression_loss: 2.2055 - classification_loss: 0.6819
 379/1000 [==========>...................] - ETA: 4:41 - loss: 2.8959 - regression_loss: 2.2110 - classification_loss: 0.6849
 380/1000 [==========>...................] - ETA: 4:40 - loss: 2.8974 - regression_loss: 2.2126 - classification_loss: 0.6849
 381/1000 [==========>...................] - ETA: 4:40 - loss: 2.8983 - regression_loss: 2.2125 - classification_loss: 0.6858
 382/1000 [==========>...................] - ETA: 4:39 - loss: 2.8963 - regression_loss: 2.2115 - classification_loss: 0.6848
 383/1000 [==========>...................] - ETA: 4:39 - loss: 2.8938 - regression_loss: 2.2100 - classification_loss: 0.6838
 384/1000 [==========>...................] - ETA: 4:39 - loss: 2.8980 - regression_loss: 2.2138 - classification_loss: 0.6842
 385/1000 [==========>...................] - ETA: 4:38 - loss: 2.8905 - regression_loss: 2.2081 - classification_loss: 0.6824
 386/1000 [==========>...................] - ETA: 4:38 - loss: 2.8925 - regression_loss: 2.2097 - classification_loss: 0.6829
 387/1000 [==========>...................] - ETA: 4:37 - loss: 2.8942 - regression_loss: 2.2109 - classification_loss: 0.6833
 388/1000 [==========>...................] - ETA: 4:37 - loss: 2.8934 - regression_loss: 2.2106 - classification_loss: 0.6827
 389/1000 [==========>...................] - ETA: 4:36 - loss: 2.8859 - regression_loss: 2.2049 - classification_loss: 0.6810
 390/1000 [==========>...................] - ETA: 4:36 - loss: 2.8785 - regression_loss: 2.1993 - classification_loss: 0.6793
 391/1000 [==========>...................] - ETA: 4:35 - loss: 2.8780 - regression_loss: 2.1984 - classification_loss: 0.6796
 392/1000 [==========>...................] - ETA: 4:35 - loss: 2.8763 - regression_loss: 2.1973 - classification_loss: 0.6790
 393/1000 [==========>...................] - ETA: 4:34 - loss: 2.8789 - regression_loss: 2.2004 - classification_loss: 0.6785
 394/1000 [==========>...................] - ETA: 4:34 - loss: 2.8825 - regression_loss: 2.2017 - classification_loss: 0.6808
 395/1000 [==========>...................] - ETA: 4:34 - loss: 2.8759 - regression_loss: 2.1961 - classification_loss: 0.6798
 396/1000 [==========>...................] - ETA: 4:33 - loss: 2.8780 - regression_loss: 2.1988 - classification_loss: 0.6792
 397/1000 [==========>...................] - ETA: 4:33 - loss: 2.8809 - regression_loss: 2.2017 - classification_loss: 0.6791
 398/1000 [==========>...................] - ETA: 4:32 - loss: 2.8831 - regression_loss: 2.2034 - classification_loss: 0.6798
 399/1000 [==========>...................] - ETA: 4:32 - loss: 2.8863 - regression_loss: 2.2056 - classification_loss: 0.6807
 400/1000 [===========>..................] - ETA: 4:31 - loss: 2.8877 - regression_loss: 2.2060 - classification_loss: 0.6817
 401/1000 [===========>..................] - ETA: 4:31 - loss: 2.8915 - regression_loss: 2.2085 - classification_loss: 0.6830
 402/1000 [===========>..................] - ETA: 4:30 - loss: 2.8900 - regression_loss: 2.2076 - classification_loss: 0.6825
 403/1000 [===========>..................] - ETA: 4:30 - loss: 2.8936 - regression_loss: 2.2110 - classification_loss: 0.6826
 404/1000 [===========>..................] - ETA: 4:29 - loss: 2.8944 - regression_loss: 2.2112 - classification_loss: 0.6832
 405/1000 [===========>..................] - ETA: 4:29 - loss: 2.8996 - regression_loss: 2.2157 - classification_loss: 0.6839
 406/1000 [===========>..................] - ETA: 4:29 - loss: 2.9007 - regression_loss: 2.2173 - classification_loss: 0.6834
 407/1000 [===========>..................] - ETA: 4:28 - loss: 2.9008 - regression_loss: 2.2180 - classification_loss: 0.6827
 408/1000 [===========>..................] - ETA: 4:28 - loss: 2.9008 - regression_loss: 2.2187 - classification_loss: 0.6820
 409/1000 [===========>..................] - ETA: 4:27 - loss: 2.9046 - regression_loss: 2.2220 - classification_loss: 0.6826
 410/1000 [===========>..................] - ETA: 4:27 - loss: 2.9030 - regression_loss: 2.2212 - classification_loss: 0.6818
 411/1000 [===========>..................] - ETA: 4:26 - loss: 2.8997 - regression_loss: 2.2187 - classification_loss: 0.6810
 412/1000 [===========>..................] - ETA: 4:26 - loss: 2.8926 - regression_loss: 2.2133 - classification_loss: 0.6793
 413/1000 [===========>..................] - ETA: 4:25 - loss: 2.8856 - regression_loss: 2.2079 - classification_loss: 0.6777
 414/1000 [===========>..................] - ETA: 4:25 - loss: 2.8787 - regression_loss: 2.2026 - classification_loss: 0.6761
 415/1000 [===========>..................] - ETA: 4:25 - loss: 2.8795 - regression_loss: 2.2030 - classification_loss: 0.6765
 416/1000 [===========>..................] - ETA: 4:24 - loss: 2.8828 - regression_loss: 2.2051 - classification_loss: 0.6777
 417/1000 [===========>..................] - ETA: 4:24 - loss: 2.8760 - regression_loss: 2.1998 - classification_loss: 0.6762
 418/1000 [===========>..................] - ETA: 4:23 - loss: 2.8786 - regression_loss: 2.2007 - classification_loss: 0.6779
 419/1000 [===========>..................] - ETA: 4:23 - loss: 2.8804 - regression_loss: 2.2023 - classification_loss: 0.6781
 420/1000 [===========>..................] - ETA: 4:22 - loss: 2.8854 - regression_loss: 2.2053 - classification_loss: 0.6801
 421/1000 [===========>..................] - ETA: 4:22 - loss: 2.8862 - regression_loss: 2.2057 - classification_loss: 0.6805
 422/1000 [===========>..................] - ETA: 4:21 - loss: 2.8794 - regression_loss: 2.2005 - classification_loss: 0.6789
 423/1000 [===========>..................] - ETA: 4:21 - loss: 2.8812 - regression_loss: 2.2019 - classification_loss: 0.6794
 424/1000 [===========>..................] - ETA: 4:20 - loss: 2.8865 - regression_loss: 2.2067 - classification_loss: 0.6797
 425/1000 [===========>..................] - ETA: 4:20 - loss: 2.8864 - regression_loss: 2.2075 - classification_loss: 0.6790
 426/1000 [===========>..................] - ETA: 4:20 - loss: 2.8851 - regression_loss: 2.2068 - classification_loss: 0.6783
 427/1000 [===========>..................] - ETA: 4:19 - loss: 2.8867 - regression_loss: 2.2086 - classification_loss: 0.6781
 428/1000 [===========>..................] - ETA: 4:19 - loss: 2.8801 - regression_loss: 2.2035 - classification_loss: 0.6766
 429/1000 [===========>..................] - ETA: 4:18 - loss: 2.8833 - regression_loss: 2.2069 - classification_loss: 0.6764
 430/1000 [===========>..................] - ETA: 4:18 - loss: 2.8835 - regression_loss: 2.2077 - classification_loss: 0.6758
 431/1000 [===========>..................] - ETA: 4:17 - loss: 2.8860 - regression_loss: 2.2105 - classification_loss: 0.6754
 432/1000 [===========>..................] - ETA: 4:17 - loss: 2.8878 - regression_loss: 2.2126 - classification_loss: 0.6752
 433/1000 [===========>..................] - ETA: 4:16 - loss: 2.8813 - regression_loss: 2.2075 - classification_loss: 0.6738
 434/1000 [============>.................] - ETA: 4:16 - loss: 2.8847 - regression_loss: 2.2104 - classification_loss: 0.6742
 435/1000 [============>.................] - ETA: 4:15 - loss: 2.8780 - regression_loss: 2.2053 - classification_loss: 0.6727
 436/1000 [============>.................] - ETA: 4:15 - loss: 2.8799 - regression_loss: 2.2075 - classification_loss: 0.6724
 437/1000 [============>.................] - ETA: 4:15 - loss: 2.8803 - regression_loss: 2.2076 - classification_loss: 0.6728
 438/1000 [============>.................] - ETA: 4:14 - loss: 2.8776 - regression_loss: 2.2058 - classification_loss: 0.6718
 439/1000 [============>.................] - ETA: 4:14 - loss: 2.8710 - regression_loss: 2.2008 - classification_loss: 0.6703
 440/1000 [============>.................] - ETA: 4:13 - loss: 2.8697 - regression_loss: 2.2003 - classification_loss: 0.6694
 441/1000 [============>.................] - ETA: 4:13 - loss: 2.8632 - regression_loss: 2.1953 - classification_loss: 0.6679
 442/1000 [============>.................] - ETA: 4:12 - loss: 2.8647 - regression_loss: 2.1975 - classification_loss: 0.6671
 443/1000 [============>.................] - ETA: 4:12 - loss: 2.8644 - regression_loss: 2.1978 - classification_loss: 0.6666
 444/1000 [============>.................] - ETA: 4:11 - loss: 2.8718 - regression_loss: 2.2038 - classification_loss: 0.6680
 445/1000 [============>.................] - ETA: 4:11 - loss: 2.8739 - regression_loss: 2.2061 - classification_loss: 0.6678
 446/1000 [============>.................] - ETA: 4:11 - loss: 2.8764 - regression_loss: 2.2073 - classification_loss: 0.6691
 447/1000 [============>.................] - ETA: 4:10 - loss: 2.8699 - regression_loss: 2.2023 - classification_loss: 0.6676
 448/1000 [============>.................] - ETA: 4:10 - loss: 2.8751 - regression_loss: 2.2052 - classification_loss: 0.6699
 449/1000 [============>.................] - ETA: 4:09 - loss: 2.8798 - regression_loss: 2.2092 - classification_loss: 0.6707
 450/1000 [============>.................] - ETA: 4:09 - loss: 2.8798 - regression_loss: 2.2083 - classification_loss: 0.6715
 451/1000 [============>.................] - ETA: 4:08 - loss: 2.8826 - regression_loss: 2.2087 - classification_loss: 0.6739
 452/1000 [============>.................] - ETA: 4:08 - loss: 2.8823 - regression_loss: 2.2090 - classification_loss: 0.6734
 453/1000 [============>.................] - ETA: 4:07 - loss: 2.8808 - regression_loss: 2.2078 - classification_loss: 0.6730
 454/1000 [============>.................] - ETA: 4:07 - loss: 2.8795 - regression_loss: 2.2071 - classification_loss: 0.6724
 455/1000 [============>.................] - ETA: 4:06 - loss: 2.8816 - regression_loss: 2.2076 - classification_loss: 0.6740
 456/1000 [============>.................] - ETA: 4:06 - loss: 2.8841 - regression_loss: 2.2104 - classification_loss: 0.6737
 457/1000 [============>.................] - ETA: 4:06 - loss: 2.8778 - regression_loss: 2.2056 - classification_loss: 0.6722
 458/1000 [============>.................] - ETA: 4:05 - loss: 2.8828 - regression_loss: 2.2074 - classification_loss: 0.6754
 459/1000 [============>.................] - ETA: 4:05 - loss: 2.8821 - regression_loss: 2.2077 - classification_loss: 0.6745
 460/1000 [============>.................] - ETA: 4:04 - loss: 2.8840 - regression_loss: 2.2093 - classification_loss: 0.6747
 461/1000 [============>.................] - ETA: 4:04 - loss: 2.8778 - regression_loss: 2.2046 - classification_loss: 0.6732
 462/1000 [============>.................] - ETA: 4:03 - loss: 2.8802 - regression_loss: 2.2046 - classification_loss: 0.6756
 463/1000 [============>.................] - ETA: 4:03 - loss: 2.8824 - regression_loss: 2.2073 - classification_loss: 0.6751
 464/1000 [============>.................] - ETA: 4:02 - loss: 2.8852 - regression_loss: 2.2089 - classification_loss: 0.6763
 465/1000 [============>.................] - ETA: 4:02 - loss: 2.8890 - regression_loss: 2.2108 - classification_loss: 0.6783
 466/1000 [============>.................] - ETA: 4:01 - loss: 2.8868 - regression_loss: 2.2094 - classification_loss: 0.6774
 467/1000 [=============>................] - ETA: 4:01 - loss: 2.8879 - regression_loss: 2.2106 - classification_loss: 0.6773
 468/1000 [=============>................] - ETA: 4:01 - loss: 2.8898 - regression_loss: 2.2126 - classification_loss: 0.6772
 469/1000 [=============>................] - ETA: 4:00 - loss: 2.8898 - regression_loss: 2.2128 - classification_loss: 0.6770
 470/1000 [=============>................] - ETA: 4:00 - loss: 2.8903 - regression_loss: 2.2129 - classification_loss: 0.6774
 471/1000 [=============>................] - ETA: 3:59 - loss: 2.8843 - regression_loss: 2.2082 - classification_loss: 0.6761
 472/1000 [=============>................] - ETA: 3:59 - loss: 2.8839 - regression_loss: 2.2082 - classification_loss: 0.6757
 473/1000 [=============>................] - ETA: 3:58 - loss: 2.8779 - regression_loss: 2.2036 - classification_loss: 0.6743
 474/1000 [=============>................] - ETA: 3:58 - loss: 2.8769 - regression_loss: 2.2030 - classification_loss: 0.6739
 475/1000 [=============>................] - ETA: 3:57 - loss: 2.8780 - regression_loss: 2.2038 - classification_loss: 0.6742
 476/1000 [=============>................] - ETA: 3:57 - loss: 2.8790 - regression_loss: 2.2044 - classification_loss: 0.6746
 477/1000 [=============>................] - ETA: 3:56 - loss: 2.8783 - regression_loss: 2.2041 - classification_loss: 0.6741
 478/1000 [=============>................] - ETA: 3:56 - loss: 2.8800 - regression_loss: 2.2060 - classification_loss: 0.6740
 479/1000 [=============>................] - ETA: 3:56 - loss: 2.8834 - regression_loss: 2.2090 - classification_loss: 0.6744
 480/1000 [=============>................] - ETA: 3:55 - loss: 2.8854 - regression_loss: 2.2110 - classification_loss: 0.6744
 481/1000 [=============>................] - ETA: 3:55 - loss: 2.8841 - regression_loss: 2.2102 - classification_loss: 0.6738
 482/1000 [=============>................] - ETA: 3:54 - loss: 2.8781 - regression_loss: 2.2057 - classification_loss: 0.6724
 483/1000 [=============>................] - ETA: 3:54 - loss: 2.8789 - regression_loss: 2.2048 - classification_loss: 0.6741
 484/1000 [=============>................] - ETA: 3:53 - loss: 2.8730 - regression_loss: 2.2003 - classification_loss: 0.6727
 485/1000 [=============>................] - ETA: 3:53 - loss: 2.8671 - regression_loss: 2.1957 - classification_loss: 0.6713
 486/1000 [=============>................] - ETA: 3:52 - loss: 2.8705 - regression_loss: 2.1983 - classification_loss: 0.6721
 487/1000 [=============>................] - ETA: 3:52 - loss: 2.8726 - regression_loss: 2.2008 - classification_loss: 0.6718
 488/1000 [=============>................] - ETA: 3:51 - loss: 2.8714 - regression_loss: 2.1996 - classification_loss: 0.6718
 489/1000 [=============>................] - ETA: 3:51 - loss: 2.8711 - regression_loss: 2.1984 - classification_loss: 0.6726
 490/1000 [=============>................] - ETA: 3:51 - loss: 2.8652 - regression_loss: 2.1940 - classification_loss: 0.6713
 491/1000 [=============>................] - ETA: 3:50 - loss: 2.8596 - regression_loss: 2.1895 - classification_loss: 0.6701
 492/1000 [=============>................] - ETA: 3:50 - loss: 2.8537 - regression_loss: 2.1850 - classification_loss: 0.6687
 493/1000 [=============>................] - ETA: 3:49 - loss: 2.8480 - regression_loss: 2.1806 - classification_loss: 0.6674
 494/1000 [=============>................] - ETA: 3:49 - loss: 2.8499 - regression_loss: 2.1813 - classification_loss: 0.6686
 495/1000 [=============>................] - ETA: 3:48 - loss: 2.8446 - regression_loss: 2.1769 - classification_loss: 0.6677
 496/1000 [=============>................] - ETA: 3:48 - loss: 2.8388 - regression_loss: 2.1725 - classification_loss: 0.6663
 497/1000 [=============>................] - ETA: 3:47 - loss: 2.8423 - regression_loss: 2.1744 - classification_loss: 0.6679
 498/1000 [=============>................] - ETA: 3:47 - loss: 2.8429 - regression_loss: 2.1749 - classification_loss: 0.6680
 499/1000 [=============>................] - ETA: 3:46 - loss: 2.8449 - regression_loss: 2.1773 - classification_loss: 0.6675
 500/1000 [==============>...............] - ETA: 3:46 - loss: 2.8392 - regression_loss: 2.1730 - classification_loss: 0.6663
 501/1000 [==============>...............] - ETA: 3:45 - loss: 2.8392 - regression_loss: 2.1731 - classification_loss: 0.6662
 502/1000 [==============>...............] - ETA: 3:45 - loss: 2.8414 - regression_loss: 2.1754 - classification_loss: 0.6660
 503/1000 [==============>...............] - ETA: 3:45 - loss: 2.8357 - regression_loss: 2.1711 - classification_loss: 0.6647
 504/1000 [==============>...............] - ETA: 3:44 - loss: 2.8380 - regression_loss: 2.1723 - classification_loss: 0.6656
 505/1000 [==============>...............] - ETA: 3:44 - loss: 2.8368 - regression_loss: 2.1714 - classification_loss: 0.6654
 506/1000 [==============>...............] - ETA: 3:43 - loss: 2.8357 - regression_loss: 2.1707 - classification_loss: 0.6649
 507/1000 [==============>...............] - ETA: 3:43 - loss: 2.8301 - regression_loss: 2.1665 - classification_loss: 0.6636
 508/1000 [==============>...............] - ETA: 3:42 - loss: 2.8322 - regression_loss: 2.1668 - classification_loss: 0.6653
 509/1000 [==============>...............] - ETA: 3:42 - loss: 2.8332 - regression_loss: 2.1683 - classification_loss: 0.6650
 510/1000 [==============>...............] - ETA: 3:41 - loss: 2.8335 - regression_loss: 2.1682 - classification_loss: 0.6654
 511/1000 [==============>...............] - ETA: 3:41 - loss: 2.8280 - regression_loss: 2.1639 - classification_loss: 0.6640
 512/1000 [==============>...............] - ETA: 3:41 - loss: 2.8283 - regression_loss: 2.1642 - classification_loss: 0.6641
 513/1000 [==============>...............] - ETA: 3:40 - loss: 2.8228 - regression_loss: 2.1600 - classification_loss: 0.6628
 514/1000 [==============>...............] - ETA: 3:40 - loss: 2.8175 - regression_loss: 2.1558 - classification_loss: 0.6617
 515/1000 [==============>...............] - ETA: 3:39 - loss: 2.8224 - regression_loss: 2.1585 - classification_loss: 0.6639
 516/1000 [==============>...............] - ETA: 3:39 - loss: 2.8264 - regression_loss: 2.1605 - classification_loss: 0.6659
 517/1000 [==============>...............] - ETA: 3:38 - loss: 2.8324 - regression_loss: 2.1641 - classification_loss: 0.6683
 518/1000 [==============>...............] - ETA: 3:38 - loss: 2.8376 - regression_loss: 2.1651 - classification_loss: 0.6726
 519/1000 [==============>...............] - ETA: 3:37 - loss: 2.8322 - regression_loss: 2.1609 - classification_loss: 0.6713
 520/1000 [==============>...............] - ETA: 3:37 - loss: 2.8354 - regression_loss: 2.1627 - classification_loss: 0.6727
 521/1000 [==============>...............] - ETA: 3:36 - loss: 2.8353 - regression_loss: 2.1628 - classification_loss: 0.6725
 522/1000 [==============>...............] - ETA: 3:36 - loss: 2.8351 - regression_loss: 2.1626 - classification_loss: 0.6726
 523/1000 [==============>...............] - ETA: 3:36 - loss: 2.8382 - regression_loss: 2.1640 - classification_loss: 0.6742
 524/1000 [==============>...............] - ETA: 3:35 - loss: 2.8329 - regression_loss: 2.1599 - classification_loss: 0.6731
 525/1000 [==============>...............] - ETA: 3:35 - loss: 2.8381 - regression_loss: 2.1621 - classification_loss: 0.6760
 526/1000 [==============>...............] - ETA: 3:34 - loss: 2.8414 - regression_loss: 2.1637 - classification_loss: 0.6777
 527/1000 [==============>...............] - ETA: 3:34 - loss: 2.8421 - regression_loss: 2.1635 - classification_loss: 0.6786
 528/1000 [==============>...............] - ETA: 3:33 - loss: 2.8376 - regression_loss: 2.1594 - classification_loss: 0.6781
 529/1000 [==============>...............] - ETA: 3:33 - loss: 2.8388 - regression_loss: 2.1599 - classification_loss: 0.6789
 530/1000 [==============>...............] - ETA: 3:32 - loss: 2.8335 - regression_loss: 2.1558 - classification_loss: 0.6777
 531/1000 [==============>...............] - ETA: 3:32 - loss: 2.8353 - regression_loss: 2.1573 - classification_loss: 0.6780
 532/1000 [==============>...............] - ETA: 3:31 - loss: 2.8360 - regression_loss: 2.1578 - classification_loss: 0.6782
 533/1000 [==============>...............] - ETA: 3:31 - loss: 2.8307 - regression_loss: 2.1537 - classification_loss: 0.6770
 534/1000 [===============>..............] - ETA: 3:31 - loss: 2.8308 - regression_loss: 2.1538 - classification_loss: 0.6770
 535/1000 [===============>..............] - ETA: 3:30 - loss: 2.8326 - regression_loss: 2.1552 - classification_loss: 0.6774
 536/1000 [===============>..............] - ETA: 3:30 - loss: 2.8350 - regression_loss: 2.1578 - classification_loss: 0.6772
 537/1000 [===============>..............] - ETA: 3:29 - loss: 2.8339 - regression_loss: 2.1565 - classification_loss: 0.6774
 538/1000 [===============>..............] - ETA: 3:29 - loss: 2.8335 - regression_loss: 2.1566 - classification_loss: 0.6768
 539/1000 [===============>..............] - ETA: 3:28 - loss: 2.8377 - regression_loss: 2.1604 - classification_loss: 0.6773
 540/1000 [===============>..............] - ETA: 3:28 - loss: 2.8389 - regression_loss: 2.1602 - classification_loss: 0.6787
 541/1000 [===============>..............] - ETA: 3:27 - loss: 2.8400 - regression_loss: 2.1613 - classification_loss: 0.6787
 542/1000 [===============>..............] - ETA: 3:27 - loss: 2.8347 - regression_loss: 2.1573 - classification_loss: 0.6774
 543/1000 [===============>..............] - ETA: 3:27 - loss: 2.8365 - regression_loss: 2.1583 - classification_loss: 0.6782
 544/1000 [===============>..............] - ETA: 3:26 - loss: 2.8313 - regression_loss: 2.1543 - classification_loss: 0.6770
 545/1000 [===============>..............] - ETA: 3:26 - loss: 2.8346 - regression_loss: 2.1572 - classification_loss: 0.6774
 546/1000 [===============>..............] - ETA: 3:25 - loss: 2.8355 - regression_loss: 2.1568 - classification_loss: 0.6786
 547/1000 [===============>..............] - ETA: 3:25 - loss: 2.8303 - regression_loss: 2.1529 - classification_loss: 0.6774
 548/1000 [===============>..............] - ETA: 3:24 - loss: 2.8288 - regression_loss: 2.1523 - classification_loss: 0.6765
 549/1000 [===============>..............] - ETA: 3:24 - loss: 2.8237 - regression_loss: 2.1484 - classification_loss: 0.6753
 550/1000 [===============>..............] - ETA: 3:23 - loss: 2.8260 - regression_loss: 2.1504 - classification_loss: 0.6756
 551/1000 [===============>..............] - ETA: 3:23 - loss: 2.8272 - regression_loss: 2.1509 - classification_loss: 0.6763
 552/1000 [===============>..............] - ETA: 3:22 - loss: 2.8221 - regression_loss: 2.1470 - classification_loss: 0.6751
 553/1000 [===============>..............] - ETA: 3:22 - loss: 2.8170 - regression_loss: 2.1431 - classification_loss: 0.6739
 554/1000 [===============>..............] - ETA: 3:22 - loss: 2.8167 - regression_loss: 2.1434 - classification_loss: 0.6732
 555/1000 [===============>..............] - ETA: 3:21 - loss: 2.8196 - regression_loss: 2.1451 - classification_loss: 0.6745
 556/1000 [===============>..............] - ETA: 3:21 - loss: 2.8214 - regression_loss: 2.1465 - classification_loss: 0.6749
 557/1000 [===============>..............] - ETA: 3:20 - loss: 2.8164 - regression_loss: 2.1426 - classification_loss: 0.6738
 558/1000 [===============>..............] - ETA: 3:20 - loss: 2.8114 - regression_loss: 2.1388 - classification_loss: 0.6726
 559/1000 [===============>..............] - ETA: 3:19 - loss: 2.8138 - regression_loss: 2.1410 - classification_loss: 0.6728
 560/1000 [===============>..............] - ETA: 3:19 - loss: 2.8157 - regression_loss: 2.1427 - classification_loss: 0.6730
 561/1000 [===============>..............] - ETA: 3:18 - loss: 2.8151 - regression_loss: 2.1427 - classification_loss: 0.6724
 562/1000 [===============>..............] - ETA: 3:18 - loss: 2.8150 - regression_loss: 2.1425 - classification_loss: 0.6726
 563/1000 [===============>..............] - ETA: 3:17 - loss: 2.8165 - regression_loss: 2.1441 - classification_loss: 0.6724
 564/1000 [===============>..............] - ETA: 3:17 - loss: 2.8115 - regression_loss: 2.1403 - classification_loss: 0.6713
 565/1000 [===============>..............] - ETA: 3:17 - loss: 2.8160 - regression_loss: 2.1428 - classification_loss: 0.6732
 566/1000 [===============>..............] - ETA: 3:16 - loss: 2.8184 - regression_loss: 2.1440 - classification_loss: 0.6744
 567/1000 [================>.............] - ETA: 3:16 - loss: 2.8152 - regression_loss: 2.1403 - classification_loss: 0.6750
 568/1000 [================>.............] - ETA: 3:15 - loss: 2.8180 - regression_loss: 2.1426 - classification_loss: 0.6754
 569/1000 [================>.............] - ETA: 3:15 - loss: 2.8134 - regression_loss: 2.1389 - classification_loss: 0.6745
 570/1000 [================>.............] - ETA: 3:14 - loss: 2.8156 - regression_loss: 2.1393 - classification_loss: 0.6764
 571/1000 [================>.............] - ETA: 3:14 - loss: 2.8175 - regression_loss: 2.1393 - classification_loss: 0.6782
 572/1000 [================>.............] - ETA: 3:13 - loss: 2.8176 - regression_loss: 2.1397 - classification_loss: 0.6779
 573/1000 [================>.............] - ETA: 3:13 - loss: 2.8221 - regression_loss: 2.1422 - classification_loss: 0.6799
 574/1000 [================>.............] - ETA: 3:12 - loss: 2.8172 - regression_loss: 2.1385 - classification_loss: 0.6787
 575/1000 [================>.............] - ETA: 3:12 - loss: 2.8176 - regression_loss: 2.1394 - classification_loss: 0.6782
 576/1000 [================>.............] - ETA: 3:12 - loss: 2.8218 - regression_loss: 2.1414 - classification_loss: 0.6803
 577/1000 [================>.............] - ETA: 3:11 - loss: 2.8226 - regression_loss: 2.1417 - classification_loss: 0.6809
 578/1000 [================>.............] - ETA: 3:11 - loss: 2.8198 - regression_loss: 2.1380 - classification_loss: 0.6818
 579/1000 [================>.............] - ETA: 3:10 - loss: 2.8200 - regression_loss: 2.1380 - classification_loss: 0.6820
 580/1000 [================>.............] - ETA: 3:10 - loss: 2.8152 - regression_loss: 2.1343 - classification_loss: 0.6809
 581/1000 [================>.............] - ETA: 3:09 - loss: 2.8140 - regression_loss: 2.1339 - classification_loss: 0.6801
 582/1000 [================>.............] - ETA: 3:09 - loss: 2.8146 - regression_loss: 2.1337 - classification_loss: 0.6809
 583/1000 [================>.............] - ETA: 3:08 - loss: 2.8166 - regression_loss: 2.1357 - classification_loss: 0.6809
 584/1000 [================>.............] - ETA: 3:08 - loss: 2.8165 - regression_loss: 2.1355 - classification_loss: 0.6810
 585/1000 [================>.............] - ETA: 3:07 - loss: 2.8152 - regression_loss: 2.1347 - classification_loss: 0.6804
 586/1000 [================>.............] - ETA: 3:07 - loss: 2.8201 - regression_loss: 2.1380 - classification_loss: 0.6822
 587/1000 [================>.............] - ETA: 3:07 - loss: 2.8196 - regression_loss: 2.1380 - classification_loss: 0.6816
 588/1000 [================>.............] - ETA: 3:06 - loss: 2.8198 - regression_loss: 2.1384 - classification_loss: 0.6814
 589/1000 [================>.............] - ETA: 3:06 - loss: 2.8222 - regression_loss: 2.1401 - classification_loss: 0.6821
 590/1000 [================>.............] - ETA: 3:05 - loss: 2.8235 - regression_loss: 2.1420 - classification_loss: 0.6816
 591/1000 [================>.............] - ETA: 3:05 - loss: 2.8239 - regression_loss: 2.1428 - classification_loss: 0.6811
 592/1000 [================>.............] - ETA: 3:04 - loss: 2.8255 - regression_loss: 2.1443 - classification_loss: 0.6812
 593/1000 [================>.............] - ETA: 3:04 - loss: 2.8208 - regression_loss: 2.1407 - classification_loss: 0.6800
 594/1000 [================>.............] - ETA: 3:03 - loss: 2.8249 - regression_loss: 2.1442 - classification_loss: 0.6807
 595/1000 [================>.............] - ETA: 3:03 - loss: 2.8273 - regression_loss: 2.1465 - classification_loss: 0.6808
 596/1000 [================>.............] - ETA: 3:02 - loss: 2.8283 - regression_loss: 2.1475 - classification_loss: 0.6807
 597/1000 [================>.............] - ETA: 3:02 - loss: 2.8275 - regression_loss: 2.1471 - classification_loss: 0.6804
 598/1000 [================>.............] - ETA: 3:02 - loss: 2.8276 - regression_loss: 2.1473 - classification_loss: 0.6804
 599/1000 [================>.............] - ETA: 3:01 - loss: 2.8298 - regression_loss: 2.1488 - classification_loss: 0.6809
 600/1000 [=================>............] - ETA: 3:01 - loss: 2.8299 - regression_loss: 2.1490 - classification_loss: 0.6809
 601/1000 [=================>............] - ETA: 3:00 - loss: 2.8304 - regression_loss: 2.1501 - classification_loss: 0.6803
 602/1000 [=================>............] - ETA: 3:00 - loss: 2.8338 - regression_loss: 2.1529 - classification_loss: 0.6809
 603/1000 [=================>............] - ETA: 2:59 - loss: 2.8361 - regression_loss: 2.1553 - classification_loss: 0.6808
 604/1000 [=================>............] - ETA: 2:59 - loss: 2.8414 - regression_loss: 2.1606 - classification_loss: 0.6808
 605/1000 [=================>............] - ETA: 2:58 - loss: 2.8407 - regression_loss: 2.1604 - classification_loss: 0.6803
 606/1000 [=================>............] - ETA: 2:58 - loss: 2.8395 - regression_loss: 2.1599 - classification_loss: 0.6796
 607/1000 [=================>............] - ETA: 2:57 - loss: 2.8416 - regression_loss: 2.1617 - classification_loss: 0.6799
 608/1000 [=================>............] - ETA: 2:57 - loss: 2.8448 - regression_loss: 2.1634 - classification_loss: 0.6814
 609/1000 [=================>............] - ETA: 2:57 - loss: 2.8401 - regression_loss: 2.1598 - classification_loss: 0.6803
 610/1000 [=================>............] - ETA: 2:56 - loss: 2.8399 - regression_loss: 2.1598 - classification_loss: 0.6801
 611/1000 [=================>............] - ETA: 2:56 - loss: 2.8412 - regression_loss: 2.1609 - classification_loss: 0.6803
 612/1000 [=================>............] - ETA: 2:55 - loss: 2.8366 - regression_loss: 2.1574 - classification_loss: 0.6792
 613/1000 [=================>............] - ETA: 2:55 - loss: 2.8398 - regression_loss: 2.1600 - classification_loss: 0.6797
 614/1000 [=================>............] - ETA: 2:54 - loss: 2.8409 - regression_loss: 2.1606 - classification_loss: 0.6804
 615/1000 [=================>............] - ETA: 2:54 - loss: 2.8445 - regression_loss: 2.1627 - classification_loss: 0.6818
 616/1000 [=================>............] - ETA: 2:53 - loss: 2.8399 - regression_loss: 2.1591 - classification_loss: 0.6807
 617/1000 [=================>............] - ETA: 2:53 - loss: 2.8449 - regression_loss: 2.1630 - classification_loss: 0.6819
 618/1000 [=================>............] - ETA: 2:52 - loss: 2.8426 - regression_loss: 2.1615 - classification_loss: 0.6811
 619/1000 [=================>............] - ETA: 2:52 - loss: 2.8420 - regression_loss: 2.1614 - classification_loss: 0.6806
 620/1000 [=================>............] - ETA: 2:52 - loss: 2.8374 - regression_loss: 2.1579 - classification_loss: 0.6795
 621/1000 [=================>............] - ETA: 2:51 - loss: 2.8330 - regression_loss: 2.1544 - classification_loss: 0.6785
 622/1000 [=================>............] - ETA: 2:51 - loss: 2.8333 - regression_loss: 2.1546 - classification_loss: 0.6787
 623/1000 [=================>............] - ETA: 2:50 - loss: 2.8365 - regression_loss: 2.1575 - classification_loss: 0.6790
 624/1000 [=================>............] - ETA: 2:50 - loss: 2.8367 - regression_loss: 2.1573 - classification_loss: 0.6794
 625/1000 [=================>............] - ETA: 2:49 - loss: 2.8364 - regression_loss: 2.1572 - classification_loss: 0.6792
 626/1000 [=================>............] - ETA: 2:49 - loss: 2.8319 - regression_loss: 2.1538 - classification_loss: 0.6781
 627/1000 [=================>............] - ETA: 2:48 - loss: 2.8344 - regression_loss: 2.1559 - classification_loss: 0.6785
 628/1000 [=================>............] - ETA: 2:48 - loss: 2.8344 - regression_loss: 2.1564 - classification_loss: 0.6780
 629/1000 [=================>............] - ETA: 2:48 - loss: 2.8327 - regression_loss: 2.1556 - classification_loss: 0.6772
 630/1000 [=================>............] - ETA: 2:47 - loss: 2.8358 - regression_loss: 2.1579 - classification_loss: 0.6779
 631/1000 [=================>............] - ETA: 2:47 - loss: 2.8378 - regression_loss: 2.1601 - classification_loss: 0.6777
 632/1000 [=================>............] - ETA: 2:46 - loss: 2.8359 - regression_loss: 2.1587 - classification_loss: 0.6772
 633/1000 [=================>............] - ETA: 2:46 - loss: 2.8373 - regression_loss: 2.1601 - classification_loss: 0.6772
 634/1000 [==================>...........] - ETA: 2:45 - loss: 2.8377 - regression_loss: 2.1597 - classification_loss: 0.6780
 635/1000 [==================>...........] - ETA: 2:45 - loss: 2.8397 - regression_loss: 2.1597 - classification_loss: 0.6800
 636/1000 [==================>...........] - ETA: 2:44 - loss: 2.8424 - regression_loss: 2.1617 - classification_loss: 0.6806
 637/1000 [==================>...........] - ETA: 2:44 - loss: 2.8379 - regression_loss: 2.1584 - classification_loss: 0.6796
 638/1000 [==================>...........] - ETA: 2:43 - loss: 2.8392 - regression_loss: 2.1587 - classification_loss: 0.6806
 639/1000 [==================>...........] - ETA: 2:43 - loss: 2.8412 - regression_loss: 2.1605 - classification_loss: 0.6807
 640/1000 [==================>...........] - ETA: 2:43 - loss: 2.8463 - regression_loss: 2.1630 - classification_loss: 0.6832
 641/1000 [==================>...........] - ETA: 2:42 - loss: 2.8461 - regression_loss: 2.1633 - classification_loss: 0.6828
 642/1000 [==================>...........] - ETA: 2:42 - loss: 2.8488 - regression_loss: 2.1657 - classification_loss: 0.6831
 643/1000 [==================>...........] - ETA: 2:41 - loss: 2.8482 - regression_loss: 2.1657 - classification_loss: 0.6825
 644/1000 [==================>...........] - ETA: 2:41 - loss: 2.8520 - regression_loss: 2.1690 - classification_loss: 0.6830
 645/1000 [==================>...........] - ETA: 2:40 - loss: 2.8501 - regression_loss: 2.1677 - classification_loss: 0.6824
 646/1000 [==================>...........] - ETA: 2:40 - loss: 2.8457 - regression_loss: 2.1644 - classification_loss: 0.6813
 647/1000 [==================>...........] - ETA: 2:39 - loss: 2.8487 - regression_loss: 2.1671 - classification_loss: 0.6816
 648/1000 [==================>...........] - ETA: 2:39 - loss: 2.8507 - regression_loss: 2.1694 - classification_loss: 0.6813
 649/1000 [==================>...........] - ETA: 2:38 - loss: 2.8463 - regression_loss: 2.1661 - classification_loss: 0.6803
 650/1000 [==================>...........] - ETA: 2:38 - loss: 2.8480 - regression_loss: 2.1672 - classification_loss: 0.6808
 651/1000 [==================>...........] - ETA: 2:38 - loss: 2.8483 - regression_loss: 2.1678 - classification_loss: 0.6805
 652/1000 [==================>...........] - ETA: 2:37 - loss: 2.8524 - regression_loss: 2.1702 - classification_loss: 0.6822
 653/1000 [==================>...........] - ETA: 2:37 - loss: 2.8543 - regression_loss: 2.1711 - classification_loss: 0.6832
 654/1000 [==================>...........] - ETA: 2:36 - loss: 2.8589 - regression_loss: 2.1752 - classification_loss: 0.6837
 655/1000 [==================>...........] - ETA: 2:36 - loss: 2.8598 - regression_loss: 2.1761 - classification_loss: 0.6837
 656/1000 [==================>...........] - ETA: 2:35 - loss: 2.8567 - regression_loss: 2.1728 - classification_loss: 0.6839
 657/1000 [==================>...........] - ETA: 2:35 - loss: 2.8573 - regression_loss: 2.1735 - classification_loss: 0.6839
 658/1000 [==================>...........] - ETA: 2:34 - loss: 2.8583 - regression_loss: 2.1745 - classification_loss: 0.6838
 659/1000 [==================>...........] - ETA: 2:34 - loss: 2.8593 - regression_loss: 2.1756 - classification_loss: 0.6837
 660/1000 [==================>...........] - ETA: 2:33 - loss: 2.8624 - regression_loss: 2.1779 - classification_loss: 0.6845
 661/1000 [==================>...........] - ETA: 2:33 - loss: 2.8664 - regression_loss: 2.1812 - classification_loss: 0.6852
 662/1000 [==================>...........] - ETA: 2:33 - loss: 2.8692 - regression_loss: 2.1828 - classification_loss: 0.6864
 663/1000 [==================>...........] - ETA: 2:32 - loss: 2.8709 - regression_loss: 2.1845 - classification_loss: 0.6864
 664/1000 [==================>...........] - ETA: 2:32 - loss: 2.8715 - regression_loss: 2.1849 - classification_loss: 0.6866
 665/1000 [==================>...........] - ETA: 2:31 - loss: 2.8715 - regression_loss: 2.1847 - classification_loss: 0.6868
 666/1000 [==================>...........] - ETA: 2:31 - loss: 2.8723 - regression_loss: 2.1848 - classification_loss: 0.6875
 667/1000 [===================>..........] - ETA: 2:30 - loss: 2.8719 - regression_loss: 2.1848 - classification_loss: 0.6871
 668/1000 [===================>..........] - ETA: 2:30 - loss: 2.8758 - regression_loss: 2.1876 - classification_loss: 0.6882
 669/1000 [===================>..........] - ETA: 2:29 - loss: 2.8715 - regression_loss: 2.1843 - classification_loss: 0.6872
 670/1000 [===================>..........] - ETA: 2:29 - loss: 2.8729 - regression_loss: 2.1859 - classification_loss: 0.6870
 671/1000 [===================>..........] - ETA: 2:28 - loss: 2.8749 - regression_loss: 2.1881 - classification_loss: 0.6868
 672/1000 [===================>..........] - ETA: 2:28 - loss: 2.8760 - regression_loss: 2.1893 - classification_loss: 0.6866
 673/1000 [===================>..........] - ETA: 2:28 - loss: 2.8772 - regression_loss: 2.1904 - classification_loss: 0.6868
 674/1000 [===================>..........] - ETA: 2:27 - loss: 2.8785 - regression_loss: 2.1913 - classification_loss: 0.6872
 675/1000 [===================>..........] - ETA: 2:27 - loss: 2.8801 - regression_loss: 2.1929 - classification_loss: 0.6872
 676/1000 [===================>..........] - ETA: 2:26 - loss: 2.8828 - regression_loss: 2.1954 - classification_loss: 0.6874
 677/1000 [===================>..........] - ETA: 2:26 - loss: 2.8821 - regression_loss: 2.1950 - classification_loss: 0.6871
 678/1000 [===================>..........] - ETA: 2:25 - loss: 2.8836 - regression_loss: 2.1962 - classification_loss: 0.6873
 679/1000 [===================>..........] - ETA: 2:25 - loss: 2.8835 - regression_loss: 2.1962 - classification_loss: 0.6873
 680/1000 [===================>..........] - ETA: 2:24 - loss: 2.8836 - regression_loss: 2.1961 - classification_loss: 0.6875
 681/1000 [===================>..........] - ETA: 2:24 - loss: 2.8844 - regression_loss: 2.1971 - classification_loss: 0.6873
 682/1000 [===================>..........] - ETA: 2:24 - loss: 2.8842 - regression_loss: 2.1972 - classification_loss: 0.6871
 683/1000 [===================>..........] - ETA: 2:23 - loss: 2.8862 - regression_loss: 2.1994 - classification_loss: 0.6869
 684/1000 [===================>..........] - ETA: 2:23 - loss: 2.8858 - regression_loss: 2.1997 - classification_loss: 0.6862
 685/1000 [===================>..........] - ETA: 2:22 - loss: 2.8879 - regression_loss: 2.2009 - classification_loss: 0.6870
 686/1000 [===================>..........] - ETA: 2:22 - loss: 2.8865 - regression_loss: 2.1999 - classification_loss: 0.6866
 687/1000 [===================>..........] - ETA: 2:21 - loss: 2.8823 - regression_loss: 2.1967 - classification_loss: 0.6856
 688/1000 [===================>..........] - ETA: 2:21 - loss: 2.8829 - regression_loss: 2.1976 - classification_loss: 0.6852
 689/1000 [===================>..........] - ETA: 2:20 - loss: 2.8789 - regression_loss: 2.1945 - classification_loss: 0.6844
 690/1000 [===================>..........] - ETA: 2:20 - loss: 2.8795 - regression_loss: 2.1951 - classification_loss: 0.6843
 691/1000 [===================>..........] - ETA: 2:19 - loss: 2.8796 - regression_loss: 2.1955 - classification_loss: 0.6841
 692/1000 [===================>..........] - ETA: 2:19 - loss: 2.8801 - regression_loss: 2.1963 - classification_loss: 0.6838
 693/1000 [===================>..........] - ETA: 2:19 - loss: 2.8802 - regression_loss: 2.1968 - classification_loss: 0.6834
 694/1000 [===================>..........] - ETA: 2:18 - loss: 2.8816 - regression_loss: 2.1979 - classification_loss: 0.6837
 695/1000 [===================>..........] - ETA: 2:18 - loss: 2.8825 - regression_loss: 2.1990 - classification_loss: 0.6835
 696/1000 [===================>..........] - ETA: 2:17 - loss: 2.8849 - regression_loss: 2.2009 - classification_loss: 0.6840
 697/1000 [===================>..........] - ETA: 2:17 - loss: 2.8857 - regression_loss: 2.2006 - classification_loss: 0.6851
 698/1000 [===================>..........] - ETA: 2:16 - loss: 2.8825 - regression_loss: 2.1974 - classification_loss: 0.6851
 699/1000 [===================>..........] - ETA: 2:16 - loss: 2.8833 - regression_loss: 2.1981 - classification_loss: 0.6852
 700/1000 [====================>.........] - ETA: 2:15 - loss: 2.8831 - regression_loss: 2.1983 - classification_loss: 0.6849
 701/1000 [====================>.........] - ETA: 2:15 - loss: 2.8820 - regression_loss: 2.1976 - classification_loss: 0.6843
 702/1000 [====================>.........] - ETA: 2:14 - loss: 2.8848 - regression_loss: 2.1998 - classification_loss: 0.6851
 703/1000 [====================>.........] - ETA: 2:14 - loss: 2.8859 - regression_loss: 2.2011 - classification_loss: 0.6847
 704/1000 [====================>.........] - ETA: 2:13 - loss: 2.8818 - regression_loss: 2.1980 - classification_loss: 0.6838
 705/1000 [====================>.........] - ETA: 2:13 - loss: 2.8820 - regression_loss: 2.1983 - classification_loss: 0.6837
 706/1000 [====================>.........] - ETA: 2:13 - loss: 2.8833 - regression_loss: 2.2000 - classification_loss: 0.6833
 707/1000 [====================>.........] - ETA: 2:12 - loss: 2.8839 - regression_loss: 2.2006 - classification_loss: 0.6833
 708/1000 [====================>.........] - ETA: 2:12 - loss: 2.8836 - regression_loss: 2.2004 - classification_loss: 0.6832
 709/1000 [====================>.........] - ETA: 2:11 - loss: 2.8852 - regression_loss: 2.2024 - classification_loss: 0.6828
 710/1000 [====================>.........] - ETA: 2:11 - loss: 2.8813 - regression_loss: 2.1993 - classification_loss: 0.6819
 711/1000 [====================>.........] - ETA: 2:10 - loss: 2.8830 - regression_loss: 2.2009 - classification_loss: 0.6821
 712/1000 [====================>.........] - ETA: 2:10 - loss: 2.8824 - regression_loss: 2.2006 - classification_loss: 0.6818
 713/1000 [====================>.........] - ETA: 2:09 - loss: 2.8812 - regression_loss: 2.1998 - classification_loss: 0.6814
 714/1000 [====================>.........] - ETA: 2:09 - loss: 2.8772 - regression_loss: 2.1967 - classification_loss: 0.6805
 715/1000 [====================>.........] - ETA: 2:09 - loss: 2.8731 - regression_loss: 2.1936 - classification_loss: 0.6795
 716/1000 [====================>.........] - ETA: 2:08 - loss: 2.8742 - regression_loss: 2.1946 - classification_loss: 0.6796
 717/1000 [====================>.........] - ETA: 2:08 - loss: 2.8735 - regression_loss: 2.1943 - classification_loss: 0.6793
 718/1000 [====================>.........] - ETA: 2:07 - loss: 2.8770 - regression_loss: 2.1965 - classification_loss: 0.6805
 719/1000 [====================>.........] - ETA: 2:07 - loss: 2.8780 - regression_loss: 2.1967 - classification_loss: 0.6812
 720/1000 [====================>.........] - ETA: 2:06 - loss: 2.8784 - regression_loss: 2.1973 - classification_loss: 0.6810
 721/1000 [====================>.........] - ETA: 2:06 - loss: 2.8791 - regression_loss: 2.1977 - classification_loss: 0.6814
 722/1000 [====================>.........] - ETA: 2:05 - loss: 2.8817 - regression_loss: 2.1999 - classification_loss: 0.6818
 723/1000 [====================>.........] - ETA: 2:05 - loss: 2.8777 - regression_loss: 2.1968 - classification_loss: 0.6809
 724/1000 [====================>.........] - ETA: 2:04 - loss: 2.8738 - regression_loss: 2.1938 - classification_loss: 0.6800
 725/1000 [====================>.........] - ETA: 2:04 - loss: 2.8745 - regression_loss: 2.1943 - classification_loss: 0.6803
 726/1000 [====================>.........] - ETA: 2:04 - loss: 2.8752 - regression_loss: 2.1951 - classification_loss: 0.6800
 727/1000 [====================>.........] - ETA: 2:03 - loss: 2.8712 - regression_loss: 2.1921 - classification_loss: 0.6791
 728/1000 [====================>.........] - ETA: 2:03 - loss: 2.8732 - regression_loss: 2.1938 - classification_loss: 0.6794
 729/1000 [====================>.........] - ETA: 2:02 - loss: 2.8757 - regression_loss: 2.1956 - classification_loss: 0.6801
 730/1000 [====================>.........] - ETA: 2:02 - loss: 2.8717 - regression_loss: 2.1926 - classification_loss: 0.6792
 731/1000 [====================>.........] - ETA: 2:01 - loss: 2.8734 - regression_loss: 2.1943 - classification_loss: 0.6791
 732/1000 [====================>.........] - ETA: 2:01 - loss: 2.8694 - regression_loss: 2.1913 - classification_loss: 0.6781
 733/1000 [====================>.........] - ETA: 2:00 - loss: 2.8686 - regression_loss: 2.1909 - classification_loss: 0.6777
 734/1000 [=====================>........] - ETA: 2:00 - loss: 2.8653 - regression_loss: 2.1879 - classification_loss: 0.6774
 735/1000 [=====================>........] - ETA: 1:59 - loss: 2.8669 - regression_loss: 2.1883 - classification_loss: 0.6786
 736/1000 [=====================>........] - ETA: 1:59 - loss: 2.8688 - regression_loss: 2.1895 - classification_loss: 0.6794
 737/1000 [=====================>........] - ETA: 1:59 - loss: 2.8649 - regression_loss: 2.1865 - classification_loss: 0.6784
 738/1000 [=====================>........] - ETA: 1:58 - loss: 2.8663 - regression_loss: 2.1870 - classification_loss: 0.6794
 739/1000 [=====================>........] - ETA: 1:58 - loss: 2.8665 - regression_loss: 2.1873 - classification_loss: 0.6792
 740/1000 [=====================>........] - ETA: 1:57 - loss: 2.8690 - regression_loss: 2.1888 - classification_loss: 0.6802
 741/1000 [=====================>........] - ETA: 1:57 - loss: 2.8652 - regression_loss: 2.1859 - classification_loss: 0.6793
 742/1000 [=====================>........] - ETA: 1:56 - loss: 2.8617 - regression_loss: 2.1829 - classification_loss: 0.6788
 743/1000 [=====================>........] - ETA: 1:56 - loss: 2.8609 - regression_loss: 2.1823 - classification_loss: 0.6786
 744/1000 [=====================>........] - ETA: 1:55 - loss: 2.8643 - regression_loss: 2.1842 - classification_loss: 0.6801
 745/1000 [=====================>........] - ETA: 1:55 - loss: 2.8605 - regression_loss: 2.1812 - classification_loss: 0.6792
 746/1000 [=====================>........] - ETA: 1:54 - loss: 2.8599 - regression_loss: 2.1809 - classification_loss: 0.6791
 747/1000 [=====================>........] - ETA: 1:54 - loss: 2.8592 - regression_loss: 2.1804 - classification_loss: 0.6789
 748/1000 [=====================>........] - ETA: 1:54 - loss: 2.8610 - regression_loss: 2.1812 - classification_loss: 0.6798
 749/1000 [=====================>........] - ETA: 1:53 - loss: 2.8613 - regression_loss: 2.1814 - classification_loss: 0.6799
 750/1000 [=====================>........] - ETA: 1:53 - loss: 2.8622 - regression_loss: 2.1820 - classification_loss: 0.6802
 751/1000 [=====================>........] - ETA: 1:52 - loss: 2.8626 - regression_loss: 2.1822 - classification_loss: 0.6803
 752/1000 [=====================>........] - ETA: 1:52 - loss: 2.8588 - regression_loss: 2.1793 - classification_loss: 0.6794
 753/1000 [=====================>........] - ETA: 1:51 - loss: 2.8595 - regression_loss: 2.1801 - classification_loss: 0.6794
 754/1000 [=====================>........] - ETA: 1:51 - loss: 2.8617 - regression_loss: 2.1825 - classification_loss: 0.6792
 755/1000 [=====================>........] - ETA: 1:50 - loss: 2.8627 - regression_loss: 2.1835 - classification_loss: 0.6792
 756/1000 [=====================>........] - ETA: 1:50 - loss: 2.8630 - regression_loss: 2.1841 - classification_loss: 0.6790
 757/1000 [=====================>........] - ETA: 1:49 - loss: 2.8653 - regression_loss: 2.1855 - classification_loss: 0.6798
 758/1000 [=====================>........] - ETA: 1:49 - loss: 2.8616 - regression_loss: 2.1826 - classification_loss: 0.6790
 759/1000 [=====================>........] - ETA: 1:49 - loss: 2.8579 - regression_loss: 2.1797 - classification_loss: 0.6782
 760/1000 [=====================>........] - ETA: 1:48 - loss: 2.8542 - regression_loss: 2.1769 - classification_loss: 0.6773
 761/1000 [=====================>........] - ETA: 1:48 - loss: 2.8562 - regression_loss: 2.1783 - classification_loss: 0.6779
 762/1000 [=====================>........] - ETA: 1:47 - loss: 2.8574 - regression_loss: 2.1794 - classification_loss: 0.6780
 763/1000 [=====================>........] - ETA: 1:47 - loss: 2.8537 - regression_loss: 2.1766 - classification_loss: 0.6771
 764/1000 [=====================>........] - ETA: 1:46 - loss: 2.8526 - regression_loss: 2.1751 - classification_loss: 0.6775
 765/1000 [=====================>........] - ETA: 1:46 - loss: 2.8522 - regression_loss: 2.1745 - classification_loss: 0.6777
 766/1000 [=====================>........] - ETA: 1:45 - loss: 2.8525 - regression_loss: 2.1752 - classification_loss: 0.6772
 767/1000 [======================>.......] - ETA: 1:45 - loss: 2.8546 - regression_loss: 2.1759 - classification_loss: 0.6786
 768/1000 [======================>.......] - ETA: 1:45 - loss: 2.8551 - regression_loss: 2.1761 - classification_loss: 0.6790
 769/1000 [======================>.......] - ETA: 1:44 - loss: 2.8586 - regression_loss: 2.1797 - classification_loss: 0.6788
 770/1000 [======================>.......] - ETA: 1:44 - loss: 2.8616 - regression_loss: 2.1827 - classification_loss: 0.6789
 771/1000 [======================>.......] - ETA: 1:43 - loss: 2.8650 - regression_loss: 2.1849 - classification_loss: 0.6801
 772/1000 [======================>.......] - ETA: 1:43 - loss: 2.8613 - regression_loss: 2.1820 - classification_loss: 0.6793
 773/1000 [======================>.......] - ETA: 1:42 - loss: 2.8636 - regression_loss: 2.1831 - classification_loss: 0.6805
 774/1000 [======================>.......] - ETA: 1:42 - loss: 2.8642 - regression_loss: 2.1837 - classification_loss: 0.6804
 775/1000 [======================>.......] - ETA: 1:41 - loss: 2.8644 - regression_loss: 2.1846 - classification_loss: 0.6799
 776/1000 [======================>.......] - ETA: 1:41 - loss: 2.8650 - regression_loss: 2.1852 - classification_loss: 0.6798
 777/1000 [======================>.......] - ETA: 1:40 - loss: 2.8613 - regression_loss: 2.1824 - classification_loss: 0.6789
 778/1000 [======================>.......] - ETA: 1:40 - loss: 2.8644 - regression_loss: 2.1843 - classification_loss: 0.6801
 779/1000 [======================>.......] - ETA: 1:40 - loss: 2.8646 - regression_loss: 2.1837 - classification_loss: 0.6809
 780/1000 [======================>.......] - ETA: 1:39 - loss: 2.8685 - regression_loss: 2.1859 - classification_loss: 0.6825
 781/1000 [======================>.......] - ETA: 1:39 - loss: 2.8651 - regression_loss: 2.1831 - classification_loss: 0.6819
 782/1000 [======================>.......] - ETA: 1:38 - loss: 2.8645 - regression_loss: 2.1830 - classification_loss: 0.6815
 783/1000 [======================>.......] - ETA: 1:38 - loss: 2.8642 - regression_loss: 2.1827 - classification_loss: 0.6815
 784/1000 [======================>.......] - ETA: 1:37 - loss: 2.8674 - regression_loss: 2.1844 - classification_loss: 0.6830
 785/1000 [======================>.......] - ETA: 1:37 - loss: 2.8677 - regression_loss: 2.1848 - classification_loss: 0.6829
 786/1000 [======================>.......] - ETA: 1:36 - loss: 2.8704 - regression_loss: 2.1870 - classification_loss: 0.6834
 787/1000 [======================>.......] - ETA: 1:36 - loss: 2.8702 - regression_loss: 2.1871 - classification_loss: 0.6830
 788/1000 [======================>.......] - ETA: 1:35 - loss: 2.8665 - regression_loss: 2.1844 - classification_loss: 0.6822
 789/1000 [======================>.......] - ETA: 1:35 - loss: 2.8629 - regression_loss: 2.1816 - classification_loss: 0.6813
 790/1000 [======================>.......] - ETA: 1:35 - loss: 2.8643 - regression_loss: 2.1827 - classification_loss: 0.6816
 791/1000 [======================>.......] - ETA: 1:34 - loss: 2.8607 - regression_loss: 2.1799 - classification_loss: 0.6808
 792/1000 [======================>.......] - ETA: 1:34 - loss: 2.8628 - regression_loss: 2.1814 - classification_loss: 0.6814
 793/1000 [======================>.......] - ETA: 1:33 - loss: 2.8592 - regression_loss: 2.1786 - classification_loss: 0.6806
 794/1000 [======================>.......] - ETA: 1:33 - loss: 2.8556 - regression_loss: 2.1759 - classification_loss: 0.6797
 795/1000 [======================>.......] - ETA: 1:32 - loss: 2.8566 - regression_loss: 2.1769 - classification_loss: 0.6798
 796/1000 [======================>.......] - ETA: 1:32 - loss: 2.8572 - regression_loss: 2.1774 - classification_loss: 0.6798
 797/1000 [======================>.......] - ETA: 1:31 - loss: 2.8536 - regression_loss: 2.1747 - classification_loss: 0.6789
 798/1000 [======================>.......] - ETA: 1:31 - loss: 2.8541 - regression_loss: 2.1748 - classification_loss: 0.6792
 799/1000 [======================>.......] - ETA: 1:30 - loss: 2.8547 - regression_loss: 2.1750 - classification_loss: 0.6797
 800/1000 [=======================>......] - ETA: 1:30 - loss: 2.8511 - regression_loss: 2.1722 - classification_loss: 0.6789
 801/1000 [=======================>......] - ETA: 1:30 - loss: 2.8545 - regression_loss: 2.1755 - classification_loss: 0.6790
 802/1000 [=======================>......] - ETA: 1:29 - loss: 2.8581 - regression_loss: 2.1786 - classification_loss: 0.6795
 803/1000 [=======================>......] - ETA: 1:29 - loss: 2.8612 - regression_loss: 2.1803 - classification_loss: 0.6809
 804/1000 [=======================>......] - ETA: 1:28 - loss: 2.8608 - regression_loss: 2.1800 - classification_loss: 0.6808
 805/1000 [=======================>......] - ETA: 1:28 - loss: 2.8573 - regression_loss: 2.1773 - classification_loss: 0.6800
 806/1000 [=======================>......] - ETA: 1:27 - loss: 2.8565 - regression_loss: 2.1771 - classification_loss: 0.6794
 807/1000 [=======================>......] - ETA: 1:27 - loss: 2.8595 - regression_loss: 2.1785 - classification_loss: 0.6810
 808/1000 [=======================>......] - ETA: 1:26 - loss: 2.8559 - regression_loss: 2.1758 - classification_loss: 0.6802
 809/1000 [=======================>......] - ETA: 1:26 - loss: 2.8567 - regression_loss: 2.1765 - classification_loss: 0.6802
 810/1000 [=======================>......] - ETA: 1:25 - loss: 2.8532 - regression_loss: 2.1738 - classification_loss: 0.6794
 811/1000 [=======================>......] - ETA: 1:25 - loss: 2.8564 - regression_loss: 2.1753 - classification_loss: 0.6811
 812/1000 [=======================>......] - ETA: 1:25 - loss: 2.8588 - regression_loss: 2.1762 - classification_loss: 0.6825
 813/1000 [=======================>......] - ETA: 1:24 - loss: 2.8608 - regression_loss: 2.1785 - classification_loss: 0.6823
 814/1000 [=======================>......] - ETA: 1:24 - loss: 2.8618 - regression_loss: 2.1793 - classification_loss: 0.6825
 815/1000 [=======================>......] - ETA: 1:23 - loss: 2.8616 - regression_loss: 2.1793 - classification_loss: 0.6823
 816/1000 [=======================>......] - ETA: 1:23 - loss: 2.8606 - regression_loss: 2.1787 - classification_loss: 0.6818
 817/1000 [=======================>......] - ETA: 1:22 - loss: 2.8571 - regression_loss: 2.1761 - classification_loss: 0.6810
 818/1000 [=======================>......] - ETA: 1:22 - loss: 2.8578 - regression_loss: 2.1756 - classification_loss: 0.6822
 819/1000 [=======================>......] - ETA: 1:21 - loss: 2.8543 - regression_loss: 2.1729 - classification_loss: 0.6814
 820/1000 [=======================>......] - ETA: 1:21 - loss: 2.8580 - regression_loss: 2.1747 - classification_loss: 0.6832
 821/1000 [=======================>......] - ETA: 1:20 - loss: 2.8614 - regression_loss: 2.1769 - classification_loss: 0.6845
 822/1000 [=======================>......] - ETA: 1:20 - loss: 2.8639 - regression_loss: 2.1775 - classification_loss: 0.6864
 823/1000 [=======================>......] - ETA: 1:20 - loss: 2.8635 - regression_loss: 2.1771 - classification_loss: 0.6864
 824/1000 [=======================>......] - ETA: 1:19 - loss: 2.8635 - regression_loss: 2.1773 - classification_loss: 0.6862
 825/1000 [=======================>......] - ETA: 1:19 - loss: 2.8628 - regression_loss: 2.1768 - classification_loss: 0.6860
 826/1000 [=======================>......] - ETA: 1:18 - loss: 2.8593 - regression_loss: 2.1742 - classification_loss: 0.6852
 827/1000 [=======================>......] - ETA: 1:18 - loss: 2.8612 - regression_loss: 2.1750 - classification_loss: 0.6862
 828/1000 [=======================>......] - ETA: 1:17 - loss: 2.8617 - regression_loss: 2.1756 - classification_loss: 0.6862
 829/1000 [=======================>......] - ETA: 1:17 - loss: 2.8625 - regression_loss: 2.1760 - classification_loss: 0.6865
 830/1000 [=======================>......] - ETA: 1:16 - loss: 2.8619 - regression_loss: 2.1759 - classification_loss: 0.6860
 831/1000 [=======================>......] - ETA: 1:16 - loss: 2.8643 - regression_loss: 2.1777 - classification_loss: 0.6866
 832/1000 [=======================>......] - ETA: 1:16 - loss: 2.8608 - regression_loss: 2.1751 - classification_loss: 0.6858
 833/1000 [=======================>......] - ETA: 1:15 - loss: 2.8633 - regression_loss: 2.1770 - classification_loss: 0.6863
 834/1000 [========================>.....] - ETA: 1:15 - loss: 2.8600 - regression_loss: 2.1744 - classification_loss: 0.6856
 835/1000 [========================>.....] - ETA: 1:14 - loss: 2.8605 - regression_loss: 2.1751 - classification_loss: 0.6854
 836/1000 [========================>.....] - ETA: 1:14 - loss: 2.8571 - regression_loss: 2.1725 - classification_loss: 0.6846
 837/1000 [========================>.....] - ETA: 1:13 - loss: 2.8568 - regression_loss: 2.1720 - classification_loss: 0.6848
 838/1000 [========================>.....] - ETA: 1:13 - loss: 2.8566 - regression_loss: 2.1720 - classification_loss: 0.6846
 839/1000 [========================>.....] - ETA: 1:12 - loss: 2.8580 - regression_loss: 2.1731 - classification_loss: 0.6849
 840/1000 [========================>.....] - ETA: 1:12 - loss: 2.8607 - regression_loss: 2.1758 - classification_loss: 0.6849
 841/1000 [========================>.....] - ETA: 1:11 - loss: 2.8573 - regression_loss: 2.1732 - classification_loss: 0.6841
 842/1000 [========================>.....] - ETA: 1:11 - loss: 2.8575 - regression_loss: 2.1737 - classification_loss: 0.6837
 843/1000 [========================>.....] - ETA: 1:11 - loss: 2.8585 - regression_loss: 2.1751 - classification_loss: 0.6835
 844/1000 [========================>.....] - ETA: 1:10 - loss: 2.8551 - regression_loss: 2.1725 - classification_loss: 0.6827
 845/1000 [========================>.....] - ETA: 1:10 - loss: 2.8566 - regression_loss: 2.1728 - classification_loss: 0.6838
 846/1000 [========================>.....] - ETA: 1:09 - loss: 2.8532 - regression_loss: 2.1703 - classification_loss: 0.6830
 847/1000 [========================>.....] - ETA: 1:09 - loss: 2.8529 - regression_loss: 2.1705 - classification_loss: 0.6825
 848/1000 [========================>.....] - ETA: 1:08 - loss: 2.8533 - regression_loss: 2.1703 - classification_loss: 0.6830
 849/1000 [========================>.....] - ETA: 1:08 - loss: 2.8546 - regression_loss: 2.1714 - classification_loss: 0.6832
 850/1000 [========================>.....] - ETA: 1:07 - loss: 2.8552 - regression_loss: 2.1720 - classification_loss: 0.6832
 851/1000 [========================>.....] - ETA: 1:07 - loss: 2.8583 - regression_loss: 2.1751 - classification_loss: 0.6832
 852/1000 [========================>.....] - ETA: 1:06 - loss: 2.8597 - regression_loss: 2.1758 - classification_loss: 0.6839
 853/1000 [========================>.....] - ETA: 1:06 - loss: 2.8611 - regression_loss: 2.1768 - classification_loss: 0.6843
 854/1000 [========================>.....] - ETA: 1:06 - loss: 2.8633 - regression_loss: 2.1784 - classification_loss: 0.6849
 855/1000 [========================>.....] - ETA: 1:05 - loss: 2.8638 - regression_loss: 2.1793 - classification_loss: 0.6845
 856/1000 [========================>.....] - ETA: 1:05 - loss: 2.8629 - regression_loss: 2.1783 - classification_loss: 0.6845
 857/1000 [========================>.....] - ETA: 1:04 - loss: 2.8630 - regression_loss: 2.1788 - classification_loss: 0.6842
 858/1000 [========================>.....] - ETA: 1:04 - loss: 2.8646 - regression_loss: 2.1802 - classification_loss: 0.6844
 859/1000 [========================>.....] - ETA: 1:03 - loss: 2.8666 - regression_loss: 2.1817 - classification_loss: 0.6849
 860/1000 [========================>.....] - ETA: 1:03 - loss: 2.8685 - regression_loss: 2.1837 - classification_loss: 0.6848
 861/1000 [========================>.....] - ETA: 1:02 - loss: 2.8652 - regression_loss: 2.1812 - classification_loss: 0.6840
 862/1000 [========================>.....] - ETA: 1:02 - loss: 2.8673 - regression_loss: 2.1829 - classification_loss: 0.6844
 863/1000 [========================>.....] - ETA: 1:01 - loss: 2.8680 - regression_loss: 2.1838 - classification_loss: 0.6842
 864/1000 [========================>.....] - ETA: 1:01 - loss: 2.8677 - regression_loss: 2.1834 - classification_loss: 0.6843
 865/1000 [========================>.....] - ETA: 1:01 - loss: 2.8673 - regression_loss: 2.1833 - classification_loss: 0.6840
 866/1000 [========================>.....] - ETA: 1:00 - loss: 2.8671 - regression_loss: 2.1836 - classification_loss: 0.6836
 867/1000 [=========================>....] - ETA: 1:00 - loss: 2.8671 - regression_loss: 2.1839 - classification_loss: 0.6832
 868/1000 [=========================>....] - ETA: 59s - loss: 2.8676 - regression_loss: 2.1842 - classification_loss: 0.6834 
 869/1000 [=========================>....] - ETA: 59s - loss: 2.8643 - regression_loss: 2.1817 - classification_loss: 0.6826
 870/1000 [=========================>....] - ETA: 58s - loss: 2.8611 - regression_loss: 2.1792 - classification_loss: 0.6819
 871/1000 [=========================>....] - ETA: 58s - loss: 2.8605 - regression_loss: 2.1791 - classification_loss: 0.6814
 872/1000 [=========================>....] - ETA: 57s - loss: 2.8626 - regression_loss: 2.1806 - classification_loss: 0.6820
 873/1000 [=========================>....] - ETA: 57s - loss: 2.8648 - regression_loss: 2.1825 - classification_loss: 0.6822
 874/1000 [=========================>....] - ETA: 57s - loss: 2.8647 - regression_loss: 2.1828 - classification_loss: 0.6819
 875/1000 [=========================>....] - ETA: 56s - loss: 2.8614 - regression_loss: 2.1803 - classification_loss: 0.6811
 876/1000 [=========================>....] - ETA: 56s - loss: 2.8582 - regression_loss: 2.1778 - classification_loss: 0.6804
 877/1000 [=========================>....] - ETA: 55s - loss: 2.8606 - regression_loss: 2.1792 - classification_loss: 0.6814
 878/1000 [=========================>....] - ETA: 55s - loss: 2.8631 - regression_loss: 2.1808 - classification_loss: 0.6823
 879/1000 [=========================>....] - ETA: 54s - loss: 2.8640 - regression_loss: 2.1810 - classification_loss: 0.6830
 880/1000 [=========================>....] - ETA: 54s - loss: 2.8608 - regression_loss: 2.1785 - classification_loss: 0.6822
 881/1000 [=========================>....] - ETA: 53s - loss: 2.8623 - regression_loss: 2.1792 - classification_loss: 0.6832
 882/1000 [=========================>....] - ETA: 53s - loss: 2.8619 - regression_loss: 2.1789 - classification_loss: 0.6830
 883/1000 [=========================>....] - ETA: 52s - loss: 2.8637 - regression_loss: 2.1798 - classification_loss: 0.6839
 884/1000 [=========================>....] - ETA: 52s - loss: 2.8651 - regression_loss: 2.1805 - classification_loss: 0.6846
 885/1000 [=========================>....] - ETA: 52s - loss: 2.8618 - regression_loss: 2.1780 - classification_loss: 0.6838
 886/1000 [=========================>....] - ETA: 51s - loss: 2.8586 - regression_loss: 2.1755 - classification_loss: 0.6831
 887/1000 [=========================>....] - ETA: 51s - loss: 2.8599 - regression_loss: 2.1769 - classification_loss: 0.6830
 888/1000 [=========================>....] - ETA: 50s - loss: 2.8602 - regression_loss: 2.1769 - classification_loss: 0.6832
 889/1000 [=========================>....] - ETA: 50s - loss: 2.8607 - regression_loss: 2.1772 - classification_loss: 0.6835
 890/1000 [=========================>....] - ETA: 49s - loss: 2.8625 - regression_loss: 2.1779 - classification_loss: 0.6846
 891/1000 [=========================>....] - ETA: 49s - loss: 2.8592 - regression_loss: 2.1754 - classification_loss: 0.6838
 892/1000 [=========================>....] - ETA: 48s - loss: 2.8609 - regression_loss: 2.1772 - classification_loss: 0.6837
 893/1000 [=========================>....] - ETA: 48s - loss: 2.8608 - regression_loss: 2.1772 - classification_loss: 0.6836
 894/1000 [=========================>....] - ETA: 47s - loss: 2.8599 - regression_loss: 2.1768 - classification_loss: 0.6830
 895/1000 [=========================>....] - ETA: 47s - loss: 2.8620 - regression_loss: 2.1775 - classification_loss: 0.6844
 896/1000 [=========================>....] - ETA: 47s - loss: 2.8625 - regression_loss: 2.1782 - classification_loss: 0.6843
 897/1000 [=========================>....] - ETA: 46s - loss: 2.8593 - regression_loss: 2.1758 - classification_loss: 0.6836
 898/1000 [=========================>....] - ETA: 46s - loss: 2.8589 - regression_loss: 2.1758 - classification_loss: 0.6831
 899/1000 [=========================>....] - ETA: 45s - loss: 2.8586 - regression_loss: 2.1756 - classification_loss: 0.6829
 900/1000 [==========================>...] - ETA: 45s - loss: 2.8595 - regression_loss: 2.1760 - classification_loss: 0.6835
 901/1000 [==========================>...] - ETA: 44s - loss: 2.8608 - regression_loss: 2.1762 - classification_loss: 0.6846
 902/1000 [==========================>...] - ETA: 44s - loss: 2.8632 - regression_loss: 2.1770 - classification_loss: 0.6862
 903/1000 [==========================>...] - ETA: 43s - loss: 2.8624 - regression_loss: 2.1766 - classification_loss: 0.6857
 904/1000 [==========================>...] - ETA: 43s - loss: 2.8651 - regression_loss: 2.1781 - classification_loss: 0.6870
 905/1000 [==========================>...] - ETA: 42s - loss: 2.8658 - regression_loss: 2.1777 - classification_loss: 0.6881
 906/1000 [==========================>...] - ETA: 42s - loss: 2.8627 - regression_loss: 2.1753 - classification_loss: 0.6873
 907/1000 [==========================>...] - ETA: 42s - loss: 2.8595 - regression_loss: 2.1729 - classification_loss: 0.6866
 908/1000 [==========================>...] - ETA: 41s - loss: 2.8594 - regression_loss: 2.1731 - classification_loss: 0.6863
 909/1000 [==========================>...] - ETA: 41s - loss: 2.8562 - regression_loss: 2.1707 - classification_loss: 0.6855
 910/1000 [==========================>...] - ETA: 40s - loss: 2.8534 - regression_loss: 2.1684 - classification_loss: 0.6850
 911/1000 [==========================>...] - ETA: 40s - loss: 2.8548 - regression_loss: 2.1698 - classification_loss: 0.6849
 912/1000 [==========================>...] - ETA: 39s - loss: 2.8566 - regression_loss: 2.1708 - classification_loss: 0.6858
 913/1000 [==========================>...] - ETA: 39s - loss: 2.8588 - regression_loss: 2.1720 - classification_loss: 0.6869
 914/1000 [==========================>...] - ETA: 38s - loss: 2.8557 - regression_loss: 2.1696 - classification_loss: 0.6861
 915/1000 [==========================>...] - ETA: 38s - loss: 2.8567 - regression_loss: 2.1705 - classification_loss: 0.6861
 916/1000 [==========================>...] - ETA: 37s - loss: 2.8537 - regression_loss: 2.1682 - classification_loss: 0.6855
 917/1000 [==========================>...] - ETA: 37s - loss: 2.8559 - regression_loss: 2.1696 - classification_loss: 0.6863
 918/1000 [==========================>...] - ETA: 37s - loss: 2.8565 - regression_loss: 2.1703 - classification_loss: 0.6861
 919/1000 [==========================>...] - ETA: 36s - loss: 2.8585 - regression_loss: 2.1717 - classification_loss: 0.6868
 920/1000 [==========================>...] - ETA: 36s - loss: 2.8577 - regression_loss: 2.1713 - classification_loss: 0.6864
 921/1000 [==========================>...] - ETA: 35s - loss: 2.8586 - regression_loss: 2.1721 - classification_loss: 0.6865
 922/1000 [==========================>...] - ETA: 35s - loss: 2.8606 - regression_loss: 2.1728 - classification_loss: 0.6878
 923/1000 [==========================>...] - ETA: 34s - loss: 2.8628 - regression_loss: 2.1741 - classification_loss: 0.6887
 924/1000 [==========================>...] - ETA: 34s - loss: 2.8634 - regression_loss: 2.1745 - classification_loss: 0.6888
 925/1000 [==========================>...] - ETA: 33s - loss: 2.8648 - regression_loss: 2.1754 - classification_loss: 0.6894
 926/1000 [==========================>...] - ETA: 33s - loss: 2.8680 - regression_loss: 2.1775 - classification_loss: 0.6905
 927/1000 [==========================>...] - ETA: 33s - loss: 2.8699 - regression_loss: 2.1794 - classification_loss: 0.6904
 928/1000 [==========================>...] - ETA: 32s - loss: 2.8728 - regression_loss: 2.1813 - classification_loss: 0.6914
 929/1000 [==========================>...] - ETA: 32s - loss: 2.8729 - regression_loss: 2.1815 - classification_loss: 0.6914
 930/1000 [==========================>...] - ETA: 31s - loss: 2.8738 - regression_loss: 2.1822 - classification_loss: 0.6915
 931/1000 [==========================>...] - ETA: 31s - loss: 2.8769 - regression_loss: 2.1848 - classification_loss: 0.6921
 932/1000 [==========================>...] - ETA: 30s - loss: 2.8777 - regression_loss: 2.1849 - classification_loss: 0.6928
 933/1000 [==========================>...] - ETA: 30s - loss: 2.8782 - regression_loss: 2.1849 - classification_loss: 0.6933
 934/1000 [===========================>..] - ETA: 29s - loss: 2.8811 - regression_loss: 2.1877 - classification_loss: 0.6934
 935/1000 [===========================>..] - ETA: 29s - loss: 2.8826 - regression_loss: 2.1891 - classification_loss: 0.6935
 936/1000 [===========================>..] - ETA: 28s - loss: 2.8874 - regression_loss: 2.1936 - classification_loss: 0.6938
 937/1000 [===========================>..] - ETA: 28s - loss: 2.8843 - regression_loss: 2.1913 - classification_loss: 0.6931
 938/1000 [===========================>..] - ETA: 28s - loss: 2.8847 - regression_loss: 2.1920 - classification_loss: 0.6927
 939/1000 [===========================>..] - ETA: 27s - loss: 2.8876 - regression_loss: 2.1938 - classification_loss: 0.6938
 940/1000 [===========================>..] - ETA: 27s - loss: 2.8869 - regression_loss: 2.1936 - classification_loss: 0.6933
 941/1000 [===========================>..] - ETA: 26s - loss: 2.8839 - regression_loss: 2.1912 - classification_loss: 0.6927
 942/1000 [===========================>..] - ETA: 26s - loss: 2.8844 - regression_loss: 2.1915 - classification_loss: 0.6929
 943/1000 [===========================>..] - ETA: 25s - loss: 2.8838 - regression_loss: 2.1913 - classification_loss: 0.6925
 944/1000 [===========================>..] - ETA: 25s - loss: 2.8838 - regression_loss: 2.1916 - classification_loss: 0.6922
 945/1000 [===========================>..] - ETA: 24s - loss: 2.8867 - regression_loss: 2.1930 - classification_loss: 0.6937
 946/1000 [===========================>..] - ETA: 24s - loss: 2.8837 - regression_loss: 2.1907 - classification_loss: 0.6930
 947/1000 [===========================>..] - ETA: 23s - loss: 2.8861 - regression_loss: 2.1928 - classification_loss: 0.6933
 948/1000 [===========================>..] - ETA: 23s - loss: 2.8831 - regression_loss: 2.1905 - classification_loss: 0.6926
 949/1000 [===========================>..] - ETA: 23s - loss: 2.8849 - regression_loss: 2.1920 - classification_loss: 0.6929
 950/1000 [===========================>..] - ETA: 22s - loss: 2.8859 - regression_loss: 2.1930 - classification_loss: 0.6930
 951/1000 [===========================>..] - ETA: 22s - loss: 2.8829 - regression_loss: 2.1907 - classification_loss: 0.6922
 952/1000 [===========================>..] - ETA: 21s - loss: 2.8799 - regression_loss: 2.1884 - classification_loss: 0.6915
 953/1000 [===========================>..] - ETA: 21s - loss: 2.8837 - regression_loss: 2.1907 - classification_loss: 0.6930
 954/1000 [===========================>..] - ETA: 20s - loss: 2.8843 - regression_loss: 2.1905 - classification_loss: 0.6938
 955/1000 [===========================>..] - ETA: 20s - loss: 2.8838 - regression_loss: 2.1904 - classification_loss: 0.6934
 956/1000 [===========================>..] - ETA: 19s - loss: 2.8812 - regression_loss: 2.1881 - classification_loss: 0.6931
 957/1000 [===========================>..] - ETA: 19s - loss: 2.8782 - regression_loss: 2.1858 - classification_loss: 0.6924
 958/1000 [===========================>..] - ETA: 18s - loss: 2.8809 - regression_loss: 2.1874 - classification_loss: 0.6935
 959/1000 [===========================>..] - ETA: 18s - loss: 2.8779 - regression_loss: 2.1851 - classification_loss: 0.6927
 960/1000 [===========================>..] - ETA: 18s - loss: 2.8778 - regression_loss: 2.1854 - classification_loss: 0.6923
 961/1000 [===========================>..] - ETA: 17s - loss: 2.8748 - regression_loss: 2.1832 - classification_loss: 0.6916
 962/1000 [===========================>..] - ETA: 17s - loss: 2.8752 - regression_loss: 2.1832 - classification_loss: 0.6920
 963/1000 [===========================>..] - ETA: 16s - loss: 2.8722 - regression_loss: 2.1809 - classification_loss: 0.6913
 964/1000 [===========================>..] - ETA: 16s - loss: 2.8692 - regression_loss: 2.1786 - classification_loss: 0.6906
 965/1000 [===========================>..] - ETA: 15s - loss: 2.8711 - regression_loss: 2.1798 - classification_loss: 0.6913
 966/1000 [===========================>..] - ETA: 15s - loss: 2.8682 - regression_loss: 2.1776 - classification_loss: 0.6906
 967/1000 [============================>.] - ETA: 14s - loss: 2.8707 - regression_loss: 2.1792 - classification_loss: 0.6914
 968/1000 [============================>.] - ETA: 14s - loss: 2.8714 - regression_loss: 2.1801 - classification_loss: 0.6913
 969/1000 [============================>.] - ETA: 14s - loss: 2.8739 - regression_loss: 2.1813 - classification_loss: 0.6926
 970/1000 [============================>.] - ETA: 13s - loss: 2.8766 - regression_loss: 2.1826 - classification_loss: 0.6941
 971/1000 [============================>.] - ETA: 13s - loss: 2.8776 - regression_loss: 2.1836 - classification_loss: 0.6940
 972/1000 [============================>.] - ETA: 12s - loss: 2.8791 - regression_loss: 2.1841 - classification_loss: 0.6950
 973/1000 [============================>.] - ETA: 12s - loss: 2.8806 - regression_loss: 2.1853 - classification_loss: 0.6954
 974/1000 [============================>.] - ETA: 11s - loss: 2.8819 - regression_loss: 2.1858 - classification_loss: 0.6961
 975/1000 [============================>.] - ETA: 11s - loss: 2.8789 - regression_loss: 2.1835 - classification_loss: 0.6954
 976/1000 [============================>.] - ETA: 10s - loss: 2.8776 - regression_loss: 2.1826 - classification_loss: 0.6950
 977/1000 [============================>.] - ETA: 10s - loss: 2.8791 - regression_loss: 2.1836 - classification_loss: 0.6955
 978/1000 [============================>.] - ETA: 9s - loss: 2.8813 - regression_loss: 2.1844 - classification_loss: 0.6969 
 979/1000 [============================>.] - ETA: 9s - loss: 2.8825 - regression_loss: 2.1851 - classification_loss: 0.6974
 980/1000 [============================>.] - ETA: 9s - loss: 2.8796 - regression_loss: 2.1829 - classification_loss: 0.6967
 981/1000 [============================>.] - ETA: 8s - loss: 2.8799 - regression_loss: 2.1830 - classification_loss: 0.6969
 982/1000 [============================>.] - ETA: 8s - loss: 2.8807 - regression_loss: 2.1831 - classification_loss: 0.6976
 983/1000 [============================>.] - ETA: 7s - loss: 2.8823 - regression_loss: 2.1843 - classification_loss: 0.6980
 984/1000 [============================>.] - ETA: 7s - loss: 2.8826 - regression_loss: 2.1846 - classification_loss: 0.6980
 985/1000 [============================>.] - ETA: 6s - loss: 2.8824 - regression_loss: 2.1844 - classification_loss: 0.6979
 986/1000 [============================>.] - ETA: 6s - loss: 2.8794 - regression_loss: 2.1822 - classification_loss: 0.6972
 987/1000 [============================>.] - ETA: 5s - loss: 2.8794 - regression_loss: 2.1823 - classification_loss: 0.6971
 988/1000 [============================>.] - ETA: 5s - loss: 2.8806 - regression_loss: 2.1833 - classification_loss: 0.6973
 989/1000 [============================>.] - ETA: 4s - loss: 2.8815 - regression_loss: 2.1844 - classification_loss: 0.6972
 990/1000 [============================>.] - ETA: 4s - loss: 2.8837 - regression_loss: 2.1863 - classification_loss: 0.6974
 991/1000 [============================>.] - ETA: 4s - loss: 2.8843 - regression_loss: 2.1870 - classification_loss: 0.6973
 992/1000 [============================>.] - ETA: 3s - loss: 2.8867 - regression_loss: 2.1889 - classification_loss: 0.6978
 993/1000 [============================>.] - ETA: 3s - loss: 2.8867 - regression_loss: 2.1892 - classification_loss: 0.6975
 994/1000 [============================>.] - ETA: 2s - loss: 2.8881 - regression_loss: 2.1903 - classification_loss: 0.6978
 995/1000 [============================>.] - ETA: 2s - loss: 2.8852 - regression_loss: 2.1881 - classification_loss: 0.6971
 996/1000 [============================>.] - ETA: 1s - loss: 2.8866 - regression_loss: 2.1894 - classification_loss: 0.6972
 997/1000 [============================>.] - ETA: 1s - loss: 2.8964 - regression_loss: 2.1872 - classification_loss: 0.7092
 998/1000 [============================>.] - ETA: 0s - loss: 2.8958 - regression_loss: 2.1870 - classification_loss: 0.7088
 999/1000 [============================>.] - ETA: 0s - loss: 2.8929 - regression_loss: 2.1848 - classification_loss: 0.7081
1000/1000 [==============================] - 452s 452ms/step - loss: 2.8900 - regression_loss: 2.1826 - classification_loss: 0.7074

Epoch 00013: saving model to ./snapshots/resnet50_csv_13.h5
0/2001/2002/2003/2004/2005/2006/2007/2008/2009/20010/20011/20012/20013/20014/20015/20016/20017/20018/20019/20020/20021/20022/20023/20024/20025/20026/20027/20028/20029/20030/20031/20032/20033/20034/20035/20036/20037/20038/20039/20040/20041/20042/20043/20044/20045/20046/20047/20048/20049/20050/20051/20052/20053/20054/20055/20056/20057/20058/20059/20060/20061/20062/20063/20064/20065/20066/20067/20068/20069/20070/20071/20072/20073/20074/20075/20076/20077/20078/20079/20080/20081/20082/20083/20084/20085/20086/20087/20088/20089/20090/20091/20092/20093/20094/20095/20096/20097/20098/20099/200100/200101/200102/200103/200104/200105/200106/200107/200108/200109/200110/200111/200112/200113/200114/200115/200116/200117/200118/200119/200120/200121/200122/200123/200124/200125/200126/200127/200128/200129/200130/200131/200132/200133/200134/200135/200136/200137/200138/200139/200140/200141/200142/200143/200144/200145/200146/200147/200148/200149/200150/200151/200152/200153/200154/200155/200156/200157/200158/200159/200160/200161/200162/200163/200164/200165/200166/200167/200168/200169/200170/200171/200172/200173/200174/200175/200176/200177/200178/200179/200180/200181/200182/200183/200184/200185/200186/200187/200188/200189/200190/200191/200192/200193/200194/200195/200196/200197/200198/200199/2000/2001/2002/2003/2004/2005/2006/2007/2008/2009/20010/20011/20012/20013/20014/20015/20016/20017/20018/20019/20020/20021/20022/20023/20024/20025/20026/20027/20028/20029/20030/20031/20032/20033/20034/20035/20036/20037/20038/20039/20040/20041/20042/20043/20044/20045/20046/20047/20048/20049/20050/20051/20052/20053/20054/20055/20056/20057/20058/20059/20060/20061/20062/20063/20064/20065/20066/20067/20068/20069/20070/20071/20072/20073/20074/20075/20076/20077/20078/20079/20080/20081/20082/20083/20084/20085/20086/20087/20088/20089/20090/20091/20092/20093/20094/20095/20096/20097/20098/20099/200100/200101/200102/200103/200104/200105/200106/200107/200108/200109/200110/200111/200112/200113/200114/200115/200116/200117/200118/200119/200120/200121/200122/200123/200124/200125/200126/200127/200128/200129/200130/200131/200132/200133/200134/200135/200136/200137/200138/200139/200140/200141/200142/200143/200144/200145/200146/200147/200148/200149/200150/200151/200152/200153/200154/200155/200156/200157/200158/200159/200160/200161/200162/200163/200164/200165/200166/200167/200168/200169/200170/200171/200172/200173/200174/200175/200176/200177/200178/200179/200180/200181/200182/200183/200184/200185/200186/200187/200188/200189/200190/200191/200192/200193/200194/200195/200196/200197/200198/200199/200

M 0.1449
N 0.0068
mAP: 0.0758
Epoch 14/30

   1/1000 [..............................] - ETA: 7:23 - loss: 0.6340 - regression_loss: 0.0000e+00 - classification_loss: 0.6340
   2/1000 [..............................] - ETA: 7:26 - loss: 1.7309 - regression_loss: 1.1028 - classification_loss: 0.6281    
   3/1000 [..............................] - ETA: 7:27 - loss: 1.9489 - regression_loss: 1.3956 - classification_loss: 0.5533
   4/1000 [..............................] - ETA: 7:27 - loss: 1.4617 - regression_loss: 1.0467 - classification_loss: 0.4150
   5/1000 [..............................] - ETA: 7:27 - loss: 2.0906 - regression_loss: 1.5397 - classification_loss: 0.5509
   6/1000 [..............................] - ETA: 7:26 - loss: 2.0696 - regression_loss: 1.5721 - classification_loss: 0.4975
   7/1000 [..............................] - ETA: 7:26 - loss: 2.2643 - regression_loss: 1.6576 - classification_loss: 0.6066
   8/1000 [..............................] - ETA: 7:26 - loss: 2.4346 - regression_loss: 1.8443 - classification_loss: 0.5903
   9/1000 [..............................] - ETA: 7:26 - loss: 2.5432 - regression_loss: 1.8869 - classification_loss: 0.6563
  10/1000 [..............................] - ETA: 7:26 - loss: 2.8678 - regression_loss: 2.2206 - classification_loss: 0.6473
  11/1000 [..............................] - ETA: 7:25 - loss: 3.0176 - regression_loss: 2.3675 - classification_loss: 0.6502
  12/1000 [..............................] - ETA: 7:24 - loss: 2.8137 - regression_loss: 2.1702 - classification_loss: 0.6435
  13/1000 [..............................] - ETA: 7:25 - loss: 2.8536 - regression_loss: 2.1646 - classification_loss: 0.6890
  14/1000 [..............................] - ETA: 7:24 - loss: 2.7929 - regression_loss: 2.1236 - classification_loss: 0.6694
  15/1000 [..............................] - ETA: 7:24 - loss: 2.8435 - regression_loss: 2.1681 - classification_loss: 0.6754
  16/1000 [..............................] - ETA: 7:22 - loss: 3.0066 - regression_loss: 2.2728 - classification_loss: 0.7338
  17/1000 [..............................] - ETA: 7:21 - loss: 2.8301 - regression_loss: 2.1391 - classification_loss: 0.6910
  18/1000 [..............................] - ETA: 7:21 - loss: 2.9331 - regression_loss: 2.2232 - classification_loss: 0.7099
  19/1000 [..............................] - ETA: 7:20 - loss: 3.0681 - regression_loss: 2.3416 - classification_loss: 0.7265
  20/1000 [..............................] - ETA: 7:20 - loss: 3.0411 - regression_loss: 2.3280 - classification_loss: 0.7131
  21/1000 [..............................] - ETA: 7:20 - loss: 3.0345 - regression_loss: 2.3314 - classification_loss: 0.7031
  22/1000 [..............................] - ETA: 7:19 - loss: 3.1463 - regression_loss: 2.4132 - classification_loss: 0.7331
  23/1000 [..............................] - ETA: 7:18 - loss: 3.1508 - regression_loss: 2.4132 - classification_loss: 0.7376
  24/1000 [..............................] - ETA: 7:18 - loss: 3.3378 - regression_loss: 2.6044 - classification_loss: 0.7335
  25/1000 [..............................] - ETA: 7:17 - loss: 3.3782 - regression_loss: 2.6503 - classification_loss: 0.7279
  26/1000 [..............................] - ETA: 7:17 - loss: 3.4005 - regression_loss: 2.6691 - classification_loss: 0.7315
  27/1000 [..............................] - ETA: 7:17 - loss: 3.3944 - regression_loss: 2.6729 - classification_loss: 0.7215
  28/1000 [..............................] - ETA: 7:16 - loss: 3.3846 - regression_loss: 2.6752 - classification_loss: 0.7094
  29/1000 [..............................] - ETA: 7:16 - loss: 3.3924 - regression_loss: 2.6833 - classification_loss: 0.7091
  30/1000 [..............................] - ETA: 7:16 - loss: 3.3979 - regression_loss: 2.6818 - classification_loss: 0.7161
  31/1000 [..............................] - ETA: 7:15 - loss: 3.2885 - regression_loss: 2.5953 - classification_loss: 0.6932
  32/1000 [..............................] - ETA: 7:15 - loss: 3.3045 - regression_loss: 2.5987 - classification_loss: 0.7058
  33/1000 [..............................] - ETA: 7:15 - loss: 3.3341 - regression_loss: 2.6157 - classification_loss: 0.7185
  34/1000 [>.............................] - ETA: 7:14 - loss: 3.3053 - regression_loss: 2.5966 - classification_loss: 0.7087
  35/1000 [>.............................] - ETA: 7:13 - loss: 3.2746 - regression_loss: 2.5753 - classification_loss: 0.6994
  36/1000 [>.............................] - ETA: 7:13 - loss: 3.1887 - regression_loss: 2.5038 - classification_loss: 0.6850
  37/1000 [>.............................] - ETA: 7:13 - loss: 3.1778 - regression_loss: 2.4962 - classification_loss: 0.6816
  38/1000 [>.............................] - ETA: 7:12 - loss: 3.1636 - regression_loss: 2.4854 - classification_loss: 0.6782
  39/1000 [>.............................] - ETA: 7:12 - loss: 3.1933 - regression_loss: 2.5057 - classification_loss: 0.6877
  40/1000 [>.............................] - ETA: 7:12 - loss: 3.2304 - regression_loss: 2.5183 - classification_loss: 0.7121
  41/1000 [>.............................] - ETA: 7:11 - loss: 3.2177 - regression_loss: 2.5143 - classification_loss: 0.7035
  42/1000 [>.............................] - ETA: 7:11 - loss: 3.1413 - regression_loss: 2.4544 - classification_loss: 0.6869
  43/1000 [>.............................] - ETA: 7:10 - loss: 3.1369 - regression_loss: 2.4605 - classification_loss: 0.6763
  44/1000 [>.............................] - ETA: 7:09 - loss: 3.0656 - regression_loss: 2.4046 - classification_loss: 0.6610
  45/1000 [>.............................] - ETA: 7:09 - loss: 2.9975 - regression_loss: 2.3512 - classification_loss: 0.6463
  46/1000 [>.............................] - ETA: 7:09 - loss: 2.9629 - regression_loss: 2.3210 - classification_loss: 0.6419
  47/1000 [>.............................] - ETA: 7:08 - loss: 2.8998 - regression_loss: 2.2716 - classification_loss: 0.6282
  48/1000 [>.............................] - ETA: 7:08 - loss: 2.9376 - regression_loss: 2.2879 - classification_loss: 0.6496
  49/1000 [>.............................] - ETA: 7:08 - loss: 2.8776 - regression_loss: 2.2412 - classification_loss: 0.6364
  50/1000 [>.............................] - ETA: 7:07 - loss: 2.9127 - regression_loss: 2.2737 - classification_loss: 0.6390
  51/1000 [>.............................] - ETA: 7:07 - loss: 2.9124 - regression_loss: 2.2788 - classification_loss: 0.6336
  52/1000 [>.............................] - ETA: 7:06 - loss: 2.9229 - regression_loss: 2.2916 - classification_loss: 0.6313
  53/1000 [>.............................] - ETA: 7:06 - loss: 2.9627 - regression_loss: 2.3294 - classification_loss: 0.6333
  54/1000 [>.............................] - ETA: 7:06 - loss: 3.1333 - regression_loss: 2.4732 - classification_loss: 0.6601
  55/1000 [>.............................] - ETA: 7:05 - loss: 3.1796 - regression_loss: 2.5094 - classification_loss: 0.6702
  56/1000 [>.............................] - ETA: 7:05 - loss: 3.1228 - regression_loss: 2.4646 - classification_loss: 0.6582
  57/1000 [>.............................] - ETA: 7:04 - loss: 3.0682 - regression_loss: 2.4213 - classification_loss: 0.6469
  58/1000 [>.............................] - ETA: 7:04 - loss: 3.0850 - regression_loss: 2.4236 - classification_loss: 0.6615
  59/1000 [>.............................] - ETA: 7:03 - loss: 3.1183 - regression_loss: 2.4401 - classification_loss: 0.6781
  60/1000 [>.............................] - ETA: 7:03 - loss: 3.1139 - regression_loss: 2.4364 - classification_loss: 0.6775
  61/1000 [>.............................] - ETA: 7:02 - loss: 3.1320 - regression_loss: 2.4463 - classification_loss: 0.6858
  62/1000 [>.............................] - ETA: 7:02 - loss: 3.1146 - regression_loss: 2.4335 - classification_loss: 0.6811
  63/1000 [>.............................] - ETA: 7:01 - loss: 3.1473 - regression_loss: 2.4520 - classification_loss: 0.6952
  64/1000 [>.............................] - ETA: 7:01 - loss: 3.1588 - regression_loss: 2.4556 - classification_loss: 0.7032
  65/1000 [>.............................] - ETA: 7:01 - loss: 3.1102 - regression_loss: 2.4178 - classification_loss: 0.6924
  66/1000 [>.............................] - ETA: 7:00 - loss: 3.1156 - regression_loss: 2.4208 - classification_loss: 0.6948
  67/1000 [=>............................] - ETA: 7:00 - loss: 3.1334 - regression_loss: 2.4353 - classification_loss: 0.6980
  68/1000 [=>............................] - ETA: 7:00 - loss: 3.1716 - regression_loss: 2.4523 - classification_loss: 0.7193
  69/1000 [=>............................] - ETA: 6:59 - loss: 3.2144 - regression_loss: 2.4861 - classification_loss: 0.7283
  70/1000 [=>............................] - ETA: 6:59 - loss: 3.1685 - regression_loss: 2.4506 - classification_loss: 0.7179
  71/1000 [=>............................] - ETA: 6:58 - loss: 3.1872 - regression_loss: 2.4560 - classification_loss: 0.7312
  72/1000 [=>............................] - ETA: 6:57 - loss: 3.1429 - regression_loss: 2.4219 - classification_loss: 0.7211
  73/1000 [=>............................] - ETA: 6:57 - loss: 3.1400 - regression_loss: 2.4236 - classification_loss: 0.7165
  74/1000 [=>............................] - ETA: 6:57 - loss: 3.1471 - regression_loss: 2.4167 - classification_loss: 0.7304
  75/1000 [=>............................] - ETA: 6:56 - loss: 3.1614 - regression_loss: 2.4265 - classification_loss: 0.7349
  76/1000 [=>............................] - ETA: 6:56 - loss: 3.1769 - regression_loss: 2.4389 - classification_loss: 0.7379
  77/1000 [=>............................] - ETA: 6:55 - loss: 3.1886 - regression_loss: 2.4516 - classification_loss: 0.7370
  78/1000 [=>............................] - ETA: 6:55 - loss: 3.2152 - regression_loss: 2.4605 - classification_loss: 0.7547
  79/1000 [=>............................] - ETA: 6:54 - loss: 3.1745 - regression_loss: 2.4293 - classification_loss: 0.7452
  80/1000 [=>............................] - ETA: 6:54 - loss: 3.1706 - regression_loss: 2.4306 - classification_loss: 0.7399
  81/1000 [=>............................] - ETA: 6:54 - loss: 3.1314 - regression_loss: 2.4006 - classification_loss: 0.7308
  82/1000 [=>............................] - ETA: 6:53 - loss: 3.1559 - regression_loss: 2.4266 - classification_loss: 0.7293
  83/1000 [=>............................] - ETA: 6:53 - loss: 3.1563 - regression_loss: 2.4298 - classification_loss: 0.7265
  84/1000 [=>............................] - ETA: 6:52 - loss: 3.1769 - regression_loss: 2.4477 - classification_loss: 0.7291
  85/1000 [=>............................] - ETA: 6:52 - loss: 3.1929 - regression_loss: 2.4622 - classification_loss: 0.7307
  86/1000 [=>............................] - ETA: 6:52 - loss: 3.1923 - regression_loss: 2.4615 - classification_loss: 0.7308
  87/1000 [=>............................] - ETA: 6:51 - loss: 3.1837 - regression_loss: 2.4594 - classification_loss: 0.7243
  88/1000 [=>............................] - ETA: 6:51 - loss: 3.1656 - regression_loss: 2.4466 - classification_loss: 0.7190
  89/1000 [=>............................] - ETA: 6:50 - loss: 3.1301 - regression_loss: 2.4191 - classification_loss: 0.7109
  90/1000 [=>............................] - ETA: 6:50 - loss: 3.1241 - regression_loss: 2.4130 - classification_loss: 0.7111
  91/1000 [=>............................] - ETA: 6:50 - loss: 3.1211 - regression_loss: 2.4076 - classification_loss: 0.7135
  92/1000 [=>............................] - ETA: 6:49 - loss: 3.0908 - regression_loss: 2.3814 - classification_loss: 0.7094
  93/1000 [=>............................] - ETA: 6:49 - loss: 3.1070 - regression_loss: 2.3948 - classification_loss: 0.7122
  94/1000 [=>............................] - ETA: 6:48 - loss: 3.0740 - regression_loss: 2.3693 - classification_loss: 0.7047
  95/1000 [=>............................] - ETA: 6:48 - loss: 3.0984 - regression_loss: 2.3890 - classification_loss: 0.7094
  96/1000 [=>............................] - ETA: 6:47 - loss: 3.0661 - regression_loss: 2.3641 - classification_loss: 0.7020
  97/1000 [=>............................] - ETA: 6:47 - loss: 3.0765 - regression_loss: 2.3736 - classification_loss: 0.7029
  98/1000 [=>............................] - ETA: 6:46 - loss: 3.0588 - regression_loss: 2.3608 - classification_loss: 0.6980
  99/1000 [=>............................] - ETA: 6:46 - loss: 3.0613 - regression_loss: 2.3602 - classification_loss: 0.7011
 100/1000 [==>...........................] - ETA: 6:46 - loss: 3.0645 - regression_loss: 2.3662 - classification_loss: 0.6983
 101/1000 [==>...........................] - ETA: 6:45 - loss: 3.0342 - regression_loss: 2.3428 - classification_loss: 0.6914
 102/1000 [==>...........................] - ETA: 6:45 - loss: 3.0262 - regression_loss: 2.3382 - classification_loss: 0.6880
 103/1000 [==>...........................] - ETA: 6:44 - loss: 2.9969 - regression_loss: 2.3155 - classification_loss: 0.6814
 104/1000 [==>...........................] - ETA: 6:44 - loss: 3.0178 - regression_loss: 2.3392 - classification_loss: 0.6786
 105/1000 [==>...........................] - ETA: 6:44 - loss: 3.0114 - regression_loss: 2.3353 - classification_loss: 0.6762
 106/1000 [==>...........................] - ETA: 6:43 - loss: 3.0126 - regression_loss: 2.3382 - classification_loss: 0.6744
 107/1000 [==>...........................] - ETA: 6:43 - loss: 2.9974 - regression_loss: 2.3278 - classification_loss: 0.6696
 108/1000 [==>...........................] - ETA: 6:42 - loss: 3.0055 - regression_loss: 2.3364 - classification_loss: 0.6690
 109/1000 [==>...........................] - ETA: 6:42 - loss: 3.0115 - regression_loss: 2.3349 - classification_loss: 0.6766
 110/1000 [==>...........................] - ETA: 6:41 - loss: 3.0315 - regression_loss: 2.3438 - classification_loss: 0.6877
 111/1000 [==>...........................] - ETA: 6:41 - loss: 3.0042 - regression_loss: 2.3227 - classification_loss: 0.6815
 112/1000 [==>...........................] - ETA: 6:40 - loss: 3.0050 - regression_loss: 2.3222 - classification_loss: 0.6828
 113/1000 [==>...........................] - ETA: 6:40 - loss: 2.9784 - regression_loss: 2.3017 - classification_loss: 0.6768
 114/1000 [==>...........................] - ETA: 6:40 - loss: 2.9894 - regression_loss: 2.3031 - classification_loss: 0.6863
 115/1000 [==>...........................] - ETA: 6:39 - loss: 2.9634 - regression_loss: 2.2831 - classification_loss: 0.6803
 116/1000 [==>...........................] - ETA: 6:39 - loss: 2.9378 - regression_loss: 2.2634 - classification_loss: 0.6744
 117/1000 [==>...........................] - ETA: 6:38 - loss: 2.9532 - regression_loss: 2.2786 - classification_loss: 0.6746
 118/1000 [==>...........................] - ETA: 6:38 - loss: 2.9561 - regression_loss: 2.2831 - classification_loss: 0.6730
 119/1000 [==>...........................] - ETA: 6:37 - loss: 2.9631 - regression_loss: 2.2833 - classification_loss: 0.6798
 120/1000 [==>...........................] - ETA: 6:37 - loss: 2.9670 - regression_loss: 2.2880 - classification_loss: 0.6790
 121/1000 [==>...........................] - ETA: 6:37 - loss: 2.9425 - regression_loss: 2.2691 - classification_loss: 0.6734
 122/1000 [==>...........................] - ETA: 6:36 - loss: 2.9399 - regression_loss: 2.2705 - classification_loss: 0.6694
 123/1000 [==>...........................] - ETA: 6:36 - loss: 2.9433 - regression_loss: 2.2738 - classification_loss: 0.6696
 124/1000 [==>...........................] - ETA: 6:35 - loss: 2.9388 - regression_loss: 2.2716 - classification_loss: 0.6671
 125/1000 [==>...........................] - ETA: 6:35 - loss: 2.9392 - regression_loss: 2.2729 - classification_loss: 0.6663
 126/1000 [==>...........................] - ETA: 6:34 - loss: 2.9401 - regression_loss: 2.2740 - classification_loss: 0.6661
 127/1000 [==>...........................] - ETA: 6:34 - loss: 2.9169 - regression_loss: 2.2561 - classification_loss: 0.6609
 128/1000 [==>...........................] - ETA: 6:33 - loss: 2.9305 - regression_loss: 2.2635 - classification_loss: 0.6670
 129/1000 [==>...........................] - ETA: 6:33 - loss: 2.9344 - regression_loss: 2.2686 - classification_loss: 0.6658
 130/1000 [==>...........................] - ETA: 6:33 - loss: 2.9135 - regression_loss: 2.2512 - classification_loss: 0.6623
 131/1000 [==>...........................] - ETA: 6:32 - loss: 2.9141 - regression_loss: 2.2545 - classification_loss: 0.6596
 132/1000 [==>...........................] - ETA: 6:32 - loss: 2.9158 - regression_loss: 2.2574 - classification_loss: 0.6584
 133/1000 [==>...........................] - ETA: 6:31 - loss: 2.9321 - regression_loss: 2.2660 - classification_loss: 0.6661
 134/1000 [===>..........................] - ETA: 6:31 - loss: 2.9399 - regression_loss: 2.2742 - classification_loss: 0.6657
 135/1000 [===>..........................] - ETA: 6:30 - loss: 2.9409 - regression_loss: 2.2767 - classification_loss: 0.6642
 136/1000 [===>..........................] - ETA: 6:30 - loss: 2.9377 - regression_loss: 2.2757 - classification_loss: 0.6621
 137/1000 [===>..........................] - ETA: 6:30 - loss: 2.9429 - regression_loss: 2.2796 - classification_loss: 0.6633
 138/1000 [===>..........................] - ETA: 6:29 - loss: 2.9216 - regression_loss: 2.2631 - classification_loss: 0.6585
 139/1000 [===>..........................] - ETA: 6:29 - loss: 2.9245 - regression_loss: 2.2639 - classification_loss: 0.6606
 140/1000 [===>..........................] - ETA: 6:28 - loss: 2.9506 - regression_loss: 2.2819 - classification_loss: 0.6686
 141/1000 [===>..........................] - ETA: 6:28 - loss: 2.9662 - regression_loss: 2.2888 - classification_loss: 0.6773
 142/1000 [===>..........................] - ETA: 6:27 - loss: 2.9630 - regression_loss: 2.2867 - classification_loss: 0.6763
 143/1000 [===>..........................] - ETA: 6:27 - loss: 2.9678 - regression_loss: 2.2930 - classification_loss: 0.6747
 144/1000 [===>..........................] - ETA: 6:26 - loss: 2.9738 - regression_loss: 2.2986 - classification_loss: 0.6752
 145/1000 [===>..........................] - ETA: 6:26 - loss: 2.9820 - regression_loss: 2.3063 - classification_loss: 0.6757
 146/1000 [===>..........................] - ETA: 6:26 - loss: 2.9816 - regression_loss: 2.3086 - classification_loss: 0.6729
 147/1000 [===>..........................] - ETA: 6:25 - loss: 2.9894 - regression_loss: 2.3124 - classification_loss: 0.6770
 148/1000 [===>..........................] - ETA: 6:24 - loss: 2.9897 - regression_loss: 2.3139 - classification_loss: 0.6759
 149/1000 [===>..........................] - ETA: 6:24 - loss: 2.9896 - regression_loss: 2.3165 - classification_loss: 0.6731
 150/1000 [===>..........................] - ETA: 6:24 - loss: 2.9855 - regression_loss: 2.3145 - classification_loss: 0.6710
 151/1000 [===>..........................] - ETA: 6:23 - loss: 2.9820 - regression_loss: 2.3116 - classification_loss: 0.6704
 152/1000 [===>..........................] - ETA: 6:23 - loss: 2.9946 - regression_loss: 2.3171 - classification_loss: 0.6775
 153/1000 [===>..........................] - ETA: 6:22 - loss: 3.0123 - regression_loss: 2.3305 - classification_loss: 0.6818
 154/1000 [===>..........................] - ETA: 6:22 - loss: 2.9928 - regression_loss: 2.3154 - classification_loss: 0.6774
 155/1000 [===>..........................] - ETA: 6:21 - loss: 2.9961 - regression_loss: 2.3181 - classification_loss: 0.6780
 156/1000 [===>..........................] - ETA: 6:21 - loss: 2.9779 - regression_loss: 2.3032 - classification_loss: 0.6746
 157/1000 [===>..........................] - ETA: 6:20 - loss: 2.9589 - regression_loss: 2.2886 - classification_loss: 0.6703
 158/1000 [===>..........................] - ETA: 6:20 - loss: 2.9601 - regression_loss: 2.2917 - classification_loss: 0.6684
 159/1000 [===>..........................] - ETA: 6:19 - loss: 2.9438 - regression_loss: 2.2773 - classification_loss: 0.6665
 160/1000 [===>..........................] - ETA: 6:19 - loss: 2.9514 - regression_loss: 2.2819 - classification_loss: 0.6695
 161/1000 [===>..........................] - ETA: 6:18 - loss: 2.9421 - regression_loss: 2.2756 - classification_loss: 0.6666
 162/1000 [===>..........................] - ETA: 6:18 - loss: 2.9565 - regression_loss: 2.2827 - classification_loss: 0.6738
 163/1000 [===>..........................] - ETA: 6:18 - loss: 2.9635 - regression_loss: 2.2886 - classification_loss: 0.6749
 164/1000 [===>..........................] - ETA: 6:17 - loss: 2.9682 - regression_loss: 2.2947 - classification_loss: 0.6736
 165/1000 [===>..........................] - ETA: 6:17 - loss: 2.9693 - regression_loss: 2.2977 - classification_loss: 0.6716
 166/1000 [===>..........................] - ETA: 6:16 - loss: 2.9756 - regression_loss: 2.3058 - classification_loss: 0.6698
 167/1000 [====>.........................] - ETA: 6:16 - loss: 2.9809 - regression_loss: 2.3074 - classification_loss: 0.6736
 168/1000 [====>.........................] - ETA: 6:15 - loss: 2.9632 - regression_loss: 2.2936 - classification_loss: 0.6696
 169/1000 [====>.........................] - ETA: 6:15 - loss: 2.9456 - regression_loss: 2.2801 - classification_loss: 0.6656
 170/1000 [====>.........................] - ETA: 6:14 - loss: 2.9538 - regression_loss: 2.2877 - classification_loss: 0.6661
 171/1000 [====>.........................] - ETA: 6:14 - loss: 2.9366 - regression_loss: 2.2743 - classification_loss: 0.6623
 172/1000 [====>.........................] - ETA: 6:13 - loss: 2.9363 - regression_loss: 2.2752 - classification_loss: 0.6611
 173/1000 [====>.........................] - ETA: 6:13 - loss: 2.9193 - regression_loss: 2.2620 - classification_loss: 0.6573
 174/1000 [====>.........................] - ETA: 6:13 - loss: 2.9205 - regression_loss: 2.2632 - classification_loss: 0.6573
 175/1000 [====>.........................] - ETA: 6:12 - loss: 2.9244 - regression_loss: 2.2676 - classification_loss: 0.6568
 176/1000 [====>.........................] - ETA: 6:12 - loss: 2.9334 - regression_loss: 2.2733 - classification_loss: 0.6601
 177/1000 [====>.........................] - ETA: 6:11 - loss: 2.9423 - regression_loss: 2.2753 - classification_loss: 0.6670
 178/1000 [====>.........................] - ETA: 6:11 - loss: 2.9507 - regression_loss: 2.2819 - classification_loss: 0.6688
 179/1000 [====>.........................] - ETA: 6:10 - loss: 2.9476 - regression_loss: 2.2776 - classification_loss: 0.6700
 180/1000 [====>.........................] - ETA: 6:10 - loss: 2.9312 - regression_loss: 2.2649 - classification_loss: 0.6662
 181/1000 [====>.........................] - ETA: 6:09 - loss: 2.9387 - regression_loss: 2.2721 - classification_loss: 0.6665
 182/1000 [====>.........................] - ETA: 6:09 - loss: 2.9225 - regression_loss: 2.2597 - classification_loss: 0.6629
 183/1000 [====>.........................] - ETA: 6:09 - loss: 2.9065 - regression_loss: 2.2473 - classification_loss: 0.6592
 184/1000 [====>.........................] - ETA: 6:08 - loss: 2.9124 - regression_loss: 2.2531 - classification_loss: 0.6593
 185/1000 [====>.........................] - ETA: 6:08 - loss: 2.8967 - regression_loss: 2.2409 - classification_loss: 0.6557
 186/1000 [====>.........................] - ETA: 6:07 - loss: 2.9116 - regression_loss: 2.2487 - classification_loss: 0.6629
 187/1000 [====>.........................] - ETA: 6:07 - loss: 2.9201 - regression_loss: 2.2501 - classification_loss: 0.6700
 188/1000 [====>.........................] - ETA: 6:06 - loss: 2.9199 - regression_loss: 2.2516 - classification_loss: 0.6683
 189/1000 [====>.........................] - ETA: 6:06 - loss: 2.9169 - regression_loss: 2.2497 - classification_loss: 0.6672
 190/1000 [====>.........................] - ETA: 6:05 - loss: 2.9279 - regression_loss: 2.2561 - classification_loss: 0.6719
 191/1000 [====>.........................] - ETA: 6:05 - loss: 2.9368 - regression_loss: 2.2641 - classification_loss: 0.6727
 192/1000 [====>.........................] - ETA: 6:05 - loss: 2.9431 - regression_loss: 2.2701 - classification_loss: 0.6730
 193/1000 [====>.........................] - ETA: 6:04 - loss: 2.9520 - regression_loss: 2.2738 - classification_loss: 0.6782
 194/1000 [====>.........................] - ETA: 6:04 - loss: 2.9540 - regression_loss: 2.2748 - classification_loss: 0.6792
 195/1000 [====>.........................] - ETA: 6:03 - loss: 2.9603 - regression_loss: 2.2780 - classification_loss: 0.6823
 196/1000 [====>.........................] - ETA: 6:03 - loss: 2.9711 - regression_loss: 2.2883 - classification_loss: 0.6828
 197/1000 [====>.........................] - ETA: 6:02 - loss: 2.9560 - regression_loss: 2.2767 - classification_loss: 0.6794
 198/1000 [====>.........................] - ETA: 6:02 - loss: 2.9518 - regression_loss: 2.2725 - classification_loss: 0.6793
 199/1000 [====>.........................] - ETA: 6:01 - loss: 2.9549 - regression_loss: 2.2611 - classification_loss: 0.6938
 200/1000 [=====>........................] - ETA: 6:01 - loss: 2.9607 - regression_loss: 2.2618 - classification_loss: 0.6989
 201/1000 [=====>........................] - ETA: 6:01 - loss: 2.9608 - regression_loss: 2.2637 - classification_loss: 0.6970
 202/1000 [=====>........................] - ETA: 6:00 - loss: 2.9461 - regression_loss: 2.2525 - classification_loss: 0.6936
 203/1000 [=====>........................] - ETA: 6:00 - loss: 2.9493 - regression_loss: 2.2564 - classification_loss: 0.6929
 204/1000 [=====>........................] - ETA: 5:59 - loss: 2.9349 - regression_loss: 2.2454 - classification_loss: 0.6895
 205/1000 [=====>........................] - ETA: 5:59 - loss: 2.9206 - regression_loss: 2.2344 - classification_loss: 0.6862
 206/1000 [=====>........................] - ETA: 5:58 - loss: 2.9216 - regression_loss: 2.2366 - classification_loss: 0.6850
 207/1000 [=====>........................] - ETA: 5:58 - loss: 2.9181 - regression_loss: 2.2351 - classification_loss: 0.6830
 208/1000 [=====>........................] - ETA: 5:57 - loss: 2.9199 - regression_loss: 2.2373 - classification_loss: 0.6826
 209/1000 [=====>........................] - ETA: 5:57 - loss: 2.9194 - regression_loss: 2.2359 - classification_loss: 0.6835
 210/1000 [=====>........................] - ETA: 5:57 - loss: 2.9228 - regression_loss: 2.2402 - classification_loss: 0.6826
 211/1000 [=====>........................] - ETA: 5:56 - loss: 2.9333 - regression_loss: 2.2452 - classification_loss: 0.6880
 212/1000 [=====>........................] - ETA: 5:56 - loss: 2.9461 - regression_loss: 2.2562 - classification_loss: 0.6899
 213/1000 [=====>........................] - ETA: 5:55 - loss: 2.9451 - regression_loss: 2.2562 - classification_loss: 0.6889
 214/1000 [=====>........................] - ETA: 5:55 - loss: 2.9472 - regression_loss: 2.2549 - classification_loss: 0.6923
 215/1000 [=====>........................] - ETA: 5:54 - loss: 2.9505 - regression_loss: 2.2576 - classification_loss: 0.6929
 216/1000 [=====>........................] - ETA: 5:54 - loss: 2.9368 - regression_loss: 2.2471 - classification_loss: 0.6897
 217/1000 [=====>........................] - ETA: 5:53 - loss: 2.9398 - regression_loss: 2.2494 - classification_loss: 0.6904
 218/1000 [=====>........................] - ETA: 5:53 - loss: 2.9412 - regression_loss: 2.2521 - classification_loss: 0.6891
 219/1000 [=====>........................] - ETA: 5:53 - loss: 2.9278 - regression_loss: 2.2419 - classification_loss: 0.6859
 220/1000 [=====>........................] - ETA: 5:52 - loss: 2.9354 - regression_loss: 2.2452 - classification_loss: 0.6902
 221/1000 [=====>........................] - ETA: 5:52 - loss: 2.9221 - regression_loss: 2.2350 - classification_loss: 0.6870
 222/1000 [=====>........................] - ETA: 5:51 - loss: 2.9089 - regression_loss: 2.2250 - classification_loss: 0.6839
 223/1000 [=====>........................] - ETA: 5:51 - loss: 2.9219 - regression_loss: 2.2337 - classification_loss: 0.6882
 224/1000 [=====>........................] - ETA: 5:50 - loss: 2.9210 - regression_loss: 2.2339 - classification_loss: 0.6872
 225/1000 [=====>........................] - ETA: 5:50 - loss: 2.9210 - regression_loss: 2.2357 - classification_loss: 0.6853
 226/1000 [=====>........................] - ETA: 5:50 - loss: 2.9285 - regression_loss: 2.2382 - classification_loss: 0.6904
 227/1000 [=====>........................] - ETA: 5:49 - loss: 2.9310 - regression_loss: 2.2386 - classification_loss: 0.6924
 228/1000 [=====>........................] - ETA: 5:49 - loss: 2.9340 - regression_loss: 2.2416 - classification_loss: 0.6925
 229/1000 [=====>........................] - ETA: 5:48 - loss: 2.9431 - regression_loss: 2.2469 - classification_loss: 0.6962
 230/1000 [=====>........................] - ETA: 5:48 - loss: 2.9515 - regression_loss: 2.2512 - classification_loss: 0.7003
 231/1000 [=====>........................] - ETA: 5:47 - loss: 2.9482 - regression_loss: 2.2493 - classification_loss: 0.6990
 232/1000 [=====>........................] - ETA: 5:47 - loss: 2.9518 - regression_loss: 2.2507 - classification_loss: 0.7011
 233/1000 [=====>........................] - ETA: 5:46 - loss: 2.9391 - regression_loss: 2.2410 - classification_loss: 0.6981
 234/1000 [======>.......................] - ETA: 5:46 - loss: 2.9433 - regression_loss: 2.2450 - classification_loss: 0.6983
 235/1000 [======>.......................] - ETA: 5:46 - loss: 2.9477 - regression_loss: 2.2503 - classification_loss: 0.6973
 236/1000 [======>.......................] - ETA: 5:45 - loss: 2.9353 - regression_loss: 2.2408 - classification_loss: 0.6945
 237/1000 [======>.......................] - ETA: 5:45 - loss: 2.9347 - regression_loss: 2.2412 - classification_loss: 0.6936
 238/1000 [======>.......................] - ETA: 5:44 - loss: 2.9417 - regression_loss: 2.2477 - classification_loss: 0.6940
 239/1000 [======>.......................] - ETA: 5:44 - loss: 2.9410 - regression_loss: 2.2474 - classification_loss: 0.6935
 240/1000 [======>.......................] - ETA: 5:43 - loss: 2.9408 - regression_loss: 2.2489 - classification_loss: 0.6918
 241/1000 [======>.......................] - ETA: 5:43 - loss: 2.9381 - regression_loss: 2.2467 - classification_loss: 0.6914
 242/1000 [======>.......................] - ETA: 5:42 - loss: 2.9379 - regression_loss: 2.2465 - classification_loss: 0.6913
 243/1000 [======>.......................] - ETA: 5:42 - loss: 2.9492 - regression_loss: 2.2528 - classification_loss: 0.6964
 244/1000 [======>.......................] - ETA: 5:41 - loss: 2.9493 - regression_loss: 2.2540 - classification_loss: 0.6952
 245/1000 [======>.......................] - ETA: 5:41 - loss: 2.9502 - regression_loss: 2.2564 - classification_loss: 0.6938
 246/1000 [======>.......................] - ETA: 5:41 - loss: 2.9476 - regression_loss: 2.2548 - classification_loss: 0.6928
 247/1000 [======>.......................] - ETA: 5:40 - loss: 2.9481 - regression_loss: 2.2457 - classification_loss: 0.7024
 248/1000 [======>.......................] - ETA: 5:40 - loss: 2.9362 - regression_loss: 2.2366 - classification_loss: 0.6996
 249/1000 [======>.......................] - ETA: 5:39 - loss: 2.9244 - regression_loss: 2.2276 - classification_loss: 0.6968
 250/1000 [======>.......................] - ETA: 5:39 - loss: 2.9186 - regression_loss: 2.2236 - classification_loss: 0.6951
 251/1000 [======>.......................] - ETA: 5:38 - loss: 2.9174 - regression_loss: 2.2234 - classification_loss: 0.6940
 252/1000 [======>.......................] - ETA: 5:38 - loss: 2.9058 - regression_loss: 2.2146 - classification_loss: 0.6912
 253/1000 [======>.......................] - ETA: 5:37 - loss: 2.9092 - regression_loss: 2.2166 - classification_loss: 0.6927
 254/1000 [======>.......................] - ETA: 5:37 - loss: 2.8978 - regression_loss: 2.2078 - classification_loss: 0.6899
 255/1000 [======>.......................] - ETA: 5:36 - loss: 2.8864 - regression_loss: 2.1992 - classification_loss: 0.6872
 256/1000 [======>.......................] - ETA: 5:36 - loss: 2.8806 - regression_loss: 2.1906 - classification_loss: 0.6900
 257/1000 [======>.......................] - ETA: 5:36 - loss: 2.8694 - regression_loss: 2.1821 - classification_loss: 0.6873
 258/1000 [======>.......................] - ETA: 5:35 - loss: 2.8584 - regression_loss: 2.1736 - classification_loss: 0.6848
 259/1000 [======>.......................] - ETA: 5:35 - loss: 2.8618 - regression_loss: 2.1764 - classification_loss: 0.6854
 260/1000 [======>.......................] - ETA: 5:34 - loss: 2.8625 - regression_loss: 2.1764 - classification_loss: 0.6861
 261/1000 [======>.......................] - ETA: 5:34 - loss: 2.8642 - regression_loss: 2.1765 - classification_loss: 0.6877
 262/1000 [======>.......................] - ETA: 5:33 - loss: 2.8651 - regression_loss: 2.1773 - classification_loss: 0.6878
 263/1000 [======>.......................] - ETA: 5:33 - loss: 2.8623 - regression_loss: 2.1751 - classification_loss: 0.6871
 264/1000 [======>.......................] - ETA: 5:32 - loss: 2.8656 - regression_loss: 2.1785 - classification_loss: 0.6871
 265/1000 [======>.......................] - ETA: 5:32 - loss: 2.8719 - regression_loss: 2.1815 - classification_loss: 0.6905
 266/1000 [======>.......................] - ETA: 5:32 - loss: 2.8710 - regression_loss: 2.1806 - classification_loss: 0.6903
 267/1000 [=======>......................] - ETA: 5:31 - loss: 2.8821 - regression_loss: 2.1881 - classification_loss: 0.6940
 268/1000 [=======>......................] - ETA: 5:31 - loss: 2.8793 - regression_loss: 2.1864 - classification_loss: 0.6928
 269/1000 [=======>......................] - ETA: 5:30 - loss: 2.8826 - regression_loss: 2.1887 - classification_loss: 0.6939
 270/1000 [=======>......................] - ETA: 5:30 - loss: 2.8854 - regression_loss: 2.1925 - classification_loss: 0.6928
 271/1000 [=======>......................] - ETA: 5:29 - loss: 2.8798 - regression_loss: 2.1844 - classification_loss: 0.6953
 272/1000 [=======>......................] - ETA: 5:29 - loss: 2.8692 - regression_loss: 2.1764 - classification_loss: 0.6928
 273/1000 [=======>......................] - ETA: 5:28 - loss: 2.8753 - regression_loss: 2.1792 - classification_loss: 0.6961
 274/1000 [=======>......................] - ETA: 5:28 - loss: 2.8835 - regression_loss: 2.1873 - classification_loss: 0.6961
 275/1000 [=======>......................] - ETA: 5:28 - loss: 2.8940 - regression_loss: 2.1946 - classification_loss: 0.6993
 276/1000 [=======>......................] - ETA: 5:27 - loss: 2.8837 - regression_loss: 2.1867 - classification_loss: 0.6970
 277/1000 [=======>......................] - ETA: 5:27 - loss: 2.8733 - regression_loss: 2.1788 - classification_loss: 0.6945
 278/1000 [=======>......................] - ETA: 5:26 - loss: 2.8629 - regression_loss: 2.1710 - classification_loss: 0.6920
 279/1000 [=======>......................] - ETA: 5:26 - loss: 2.8527 - regression_loss: 2.1632 - classification_loss: 0.6895
 280/1000 [=======>......................] - ETA: 5:25 - loss: 2.8591 - regression_loss: 2.1645 - classification_loss: 0.6946
 281/1000 [=======>......................] - ETA: 5:25 - loss: 2.8489 - regression_loss: 2.1568 - classification_loss: 0.6921
 282/1000 [=======>......................] - ETA: 5:24 - loss: 2.8388 - regression_loss: 2.1492 - classification_loss: 0.6897
 283/1000 [=======>......................] - ETA: 5:24 - loss: 2.8376 - regression_loss: 2.1478 - classification_loss: 0.6898
 284/1000 [=======>......................] - ETA: 5:23 - loss: 2.8383 - regression_loss: 2.1476 - classification_loss: 0.6907
 285/1000 [=======>......................] - ETA: 5:23 - loss: 2.8436 - regression_loss: 2.1501 - classification_loss: 0.6935
 286/1000 [=======>......................] - ETA: 5:22 - loss: 2.8458 - regression_loss: 2.1505 - classification_loss: 0.6952
 287/1000 [=======>......................] - ETA: 5:22 - loss: 2.8489 - regression_loss: 2.1522 - classification_loss: 0.6967
 288/1000 [=======>......................] - ETA: 5:22 - loss: 2.8469 - regression_loss: 2.1504 - classification_loss: 0.6965
 289/1000 [=======>......................] - ETA: 5:21 - loss: 2.8371 - regression_loss: 2.1430 - classification_loss: 0.6941
 290/1000 [=======>......................] - ETA: 5:21 - loss: 2.8412 - regression_loss: 2.1434 - classification_loss: 0.6978
 291/1000 [=======>......................] - ETA: 5:20 - loss: 2.8314 - regression_loss: 2.1361 - classification_loss: 0.6954
 292/1000 [=======>......................] - ETA: 5:20 - loss: 2.8218 - regression_loss: 2.1288 - classification_loss: 0.6930
 293/1000 [=======>......................] - ETA: 5:19 - loss: 2.8297 - regression_loss: 2.1333 - classification_loss: 0.6964
 294/1000 [=======>......................] - ETA: 5:19 - loss: 2.8201 - regression_loss: 2.1260 - classification_loss: 0.6941
 295/1000 [=======>......................] - ETA: 5:18 - loss: 2.8105 - regression_loss: 2.1188 - classification_loss: 0.6917
 296/1000 [=======>......................] - ETA: 5:18 - loss: 2.8128 - regression_loss: 2.1211 - classification_loss: 0.6918
 297/1000 [=======>......................] - ETA: 5:18 - loss: 2.8308 - regression_loss: 2.1323 - classification_loss: 0.6984
 298/1000 [=======>......................] - ETA: 5:17 - loss: 2.8370 - regression_loss: 2.1364 - classification_loss: 0.7006
 299/1000 [=======>......................] - ETA: 5:17 - loss: 2.8483 - regression_loss: 2.1443 - classification_loss: 0.7040
 300/1000 [========>.....................] - ETA: 5:16 - loss: 2.8555 - regression_loss: 2.1515 - classification_loss: 0.7040
 301/1000 [========>.....................] - ETA: 5:16 - loss: 2.8613 - regression_loss: 2.1540 - classification_loss: 0.7074
 302/1000 [========>.....................] - ETA: 5:15 - loss: 2.8667 - regression_loss: 2.1563 - classification_loss: 0.7104
 303/1000 [========>.....................] - ETA: 5:15 - loss: 2.8690 - regression_loss: 2.1561 - classification_loss: 0.7129
 304/1000 [========>.....................] - ETA: 5:14 - loss: 2.8769 - regression_loss: 2.1626 - classification_loss: 0.7143
 305/1000 [========>.....................] - ETA: 5:14 - loss: 2.8801 - regression_loss: 2.1629 - classification_loss: 0.7172
 306/1000 [========>.....................] - ETA: 5:13 - loss: 2.8795 - regression_loss: 2.1623 - classification_loss: 0.7172
 307/1000 [========>.....................] - ETA: 5:13 - loss: 2.8773 - regression_loss: 2.1611 - classification_loss: 0.7161
 308/1000 [========>.....................] - ETA: 5:13 - loss: 2.8731 - regression_loss: 2.1581 - classification_loss: 0.7151
 309/1000 [========>.....................] - ETA: 5:12 - loss: 2.8638 - regression_loss: 2.1511 - classification_loss: 0.7127
 310/1000 [========>.....................] - ETA: 5:12 - loss: 2.8546 - regression_loss: 2.1442 - classification_loss: 0.7104
 311/1000 [========>.....................] - ETA: 5:11 - loss: 2.8590 - regression_loss: 2.1478 - classification_loss: 0.7112
 312/1000 [========>.....................] - ETA: 5:11 - loss: 2.8651 - regression_loss: 2.1506 - classification_loss: 0.7145
 313/1000 [========>.....................] - ETA: 5:10 - loss: 2.8569 - regression_loss: 2.1437 - classification_loss: 0.7132
 314/1000 [========>.....................] - ETA: 5:10 - loss: 2.8617 - regression_loss: 2.1459 - classification_loss: 0.7158
 315/1000 [========>.....................] - ETA: 5:09 - loss: 2.8673 - regression_loss: 2.1490 - classification_loss: 0.7183
 316/1000 [========>.....................] - ETA: 5:09 - loss: 2.8665 - regression_loss: 2.1482 - classification_loss: 0.7183
 317/1000 [========>.....................] - ETA: 5:08 - loss: 2.8574 - regression_loss: 2.1414 - classification_loss: 0.7160
 318/1000 [========>.....................] - ETA: 5:08 - loss: 2.8487 - regression_loss: 2.1347 - classification_loss: 0.7140
 319/1000 [========>.....................] - ETA: 5:07 - loss: 2.8502 - regression_loss: 2.1348 - classification_loss: 0.7154
 320/1000 [========>.....................] - ETA: 5:07 - loss: 2.8503 - regression_loss: 2.1355 - classification_loss: 0.7148
 321/1000 [========>.....................] - ETA: 5:07 - loss: 2.8414 - regression_loss: 2.1289 - classification_loss: 0.7125
 322/1000 [========>.....................] - ETA: 5:06 - loss: 2.8412 - regression_loss: 2.1280 - classification_loss: 0.7132
 323/1000 [========>.....................] - ETA: 5:06 - loss: 2.8324 - regression_loss: 2.1214 - classification_loss: 0.7110
 324/1000 [========>.....................] - ETA: 5:05 - loss: 2.8376 - regression_loss: 2.1247 - classification_loss: 0.7129
 325/1000 [========>.....................] - ETA: 5:05 - loss: 2.8289 - regression_loss: 2.1181 - classification_loss: 0.7107
 326/1000 [========>.....................] - ETA: 5:04 - loss: 2.8202 - regression_loss: 2.1116 - classification_loss: 0.7086
 327/1000 [========>.....................] - ETA: 5:04 - loss: 2.8195 - regression_loss: 2.1104 - classification_loss: 0.7091
 328/1000 [========>.....................] - ETA: 5:03 - loss: 2.8215 - regression_loss: 2.1130 - classification_loss: 0.7084
 329/1000 [========>.....................] - ETA: 5:03 - loss: 2.8230 - regression_loss: 2.1148 - classification_loss: 0.7082
 330/1000 [========>.....................] - ETA: 5:03 - loss: 2.8144 - regression_loss: 2.1084 - classification_loss: 0.7060
 331/1000 [========>.....................] - ETA: 5:02 - loss: 2.8197 - regression_loss: 2.1124 - classification_loss: 0.7073
 332/1000 [========>.....................] - ETA: 5:02 - loss: 2.8226 - regression_loss: 2.1149 - classification_loss: 0.7077
 333/1000 [========>.....................] - ETA: 5:01 - loss: 2.8227 - regression_loss: 2.1150 - classification_loss: 0.7078
 334/1000 [=========>....................] - ETA: 5:01 - loss: 2.8219 - regression_loss: 2.1131 - classification_loss: 0.7087
 335/1000 [=========>....................] - ETA: 5:00 - loss: 2.8134 - regression_loss: 2.1068 - classification_loss: 0.7066
 336/1000 [=========>....................] - ETA: 5:00 - loss: 2.8232 - regression_loss: 2.1139 - classification_loss: 0.7093
 337/1000 [=========>....................] - ETA: 4:59 - loss: 2.8267 - regression_loss: 2.1166 - classification_loss: 0.7101
 338/1000 [=========>....................] - ETA: 4:59 - loss: 2.8184 - regression_loss: 2.1103 - classification_loss: 0.7081
 339/1000 [=========>....................] - ETA: 4:59 - loss: 2.8101 - regression_loss: 2.1041 - classification_loss: 0.7060
 340/1000 [=========>....................] - ETA: 4:58 - loss: 2.8171 - regression_loss: 2.1088 - classification_loss: 0.7084
 341/1000 [=========>....................] - ETA: 4:58 - loss: 2.8204 - regression_loss: 2.1100 - classification_loss: 0.7105
 342/1000 [=========>....................] - ETA: 4:57 - loss: 2.8205 - regression_loss: 2.1093 - classification_loss: 0.7112
 343/1000 [=========>....................] - ETA: 4:57 - loss: 2.8230 - regression_loss: 2.1099 - classification_loss: 0.7132
 344/1000 [=========>....................] - ETA: 4:56 - loss: 2.8279 - regression_loss: 2.1112 - classification_loss: 0.7167
 345/1000 [=========>....................] - ETA: 4:56 - loss: 2.8279 - regression_loss: 2.1109 - classification_loss: 0.7170
 346/1000 [=========>....................] - ETA: 4:55 - loss: 2.8197 - regression_loss: 2.1048 - classification_loss: 0.7149
 347/1000 [=========>....................] - ETA: 4:55 - loss: 2.8219 - regression_loss: 2.1062 - classification_loss: 0.7158
 348/1000 [=========>....................] - ETA: 4:54 - loss: 2.8239 - regression_loss: 2.1065 - classification_loss: 0.7175
 349/1000 [=========>....................] - ETA: 4:54 - loss: 2.8298 - regression_loss: 2.1121 - classification_loss: 0.7178
 350/1000 [=========>....................] - ETA: 4:54 - loss: 2.8288 - regression_loss: 2.1120 - classification_loss: 0.7167
 351/1000 [=========>....................] - ETA: 4:53 - loss: 2.8316 - regression_loss: 2.1150 - classification_loss: 0.7165
 352/1000 [=========>....................] - ETA: 4:53 - loss: 2.8368 - regression_loss: 2.1174 - classification_loss: 0.7193
 353/1000 [=========>....................] - ETA: 4:52 - loss: 2.8413 - regression_loss: 2.1210 - classification_loss: 0.7203
 354/1000 [=========>....................] - ETA: 4:52 - loss: 2.8455 - regression_loss: 2.1237 - classification_loss: 0.7218
 355/1000 [=========>....................] - ETA: 4:51 - loss: 2.8384 - regression_loss: 2.1177 - classification_loss: 0.7207
 356/1000 [=========>....................] - ETA: 4:51 - loss: 2.8421 - regression_loss: 2.1200 - classification_loss: 0.7221
 357/1000 [=========>....................] - ETA: 4:50 - loss: 2.8391 - regression_loss: 2.1184 - classification_loss: 0.7207
 358/1000 [=========>....................] - ETA: 4:50 - loss: 2.8430 - regression_loss: 2.1222 - classification_loss: 0.7207
 359/1000 [=========>....................] - ETA: 4:49 - loss: 2.8351 - regression_loss: 2.1163 - classification_loss: 0.7187
 360/1000 [=========>....................] - ETA: 4:49 - loss: 2.8272 - regression_loss: 2.1105 - classification_loss: 0.7167
 361/1000 [=========>....................] - ETA: 4:49 - loss: 2.8321 - regression_loss: 2.1149 - classification_loss: 0.7172
 362/1000 [=========>....................] - ETA: 4:48 - loss: 2.8394 - regression_loss: 2.1202 - classification_loss: 0.7192
 363/1000 [=========>....................] - ETA: 4:48 - loss: 2.8450 - regression_loss: 2.1238 - classification_loss: 0.7212
 364/1000 [=========>....................] - ETA: 4:47 - loss: 2.8495 - regression_loss: 2.1281 - classification_loss: 0.7213
 365/1000 [=========>....................] - ETA: 4:47 - loss: 2.8430 - regression_loss: 2.1223 - classification_loss: 0.7208
 366/1000 [=========>....................] - ETA: 4:46 - loss: 2.8439 - regression_loss: 2.1230 - classification_loss: 0.7209
 367/1000 [==========>...................] - ETA: 4:46 - loss: 2.8432 - regression_loss: 2.1229 - classification_loss: 0.7204
 368/1000 [==========>...................] - ETA: 4:45 - loss: 2.8437 - regression_loss: 2.1238 - classification_loss: 0.7200
 369/1000 [==========>...................] - ETA: 4:45 - loss: 2.8485 - regression_loss: 2.1286 - classification_loss: 0.7199
 370/1000 [==========>...................] - ETA: 4:44 - loss: 2.8481 - regression_loss: 2.1287 - classification_loss: 0.7195
 371/1000 [==========>...................] - ETA: 4:44 - loss: 2.8408 - regression_loss: 2.1229 - classification_loss: 0.7178
 372/1000 [==========>...................] - ETA: 4:44 - loss: 2.8449 - regression_loss: 2.1255 - classification_loss: 0.7193
 373/1000 [==========>...................] - ETA: 4:43 - loss: 2.8374 - regression_loss: 2.1198 - classification_loss: 0.7175
 374/1000 [==========>...................] - ETA: 4:43 - loss: 2.8395 - regression_loss: 2.1226 - classification_loss: 0.7170
 375/1000 [==========>...................] - ETA: 4:42 - loss: 2.8458 - regression_loss: 2.1269 - classification_loss: 0.7189
 376/1000 [==========>...................] - ETA: 4:42 - loss: 2.8494 - regression_loss: 2.1289 - classification_loss: 0.7205
 377/1000 [==========>...................] - ETA: 4:41 - loss: 2.8503 - regression_loss: 2.1297 - classification_loss: 0.7207
 378/1000 [==========>...................] - ETA: 4:41 - loss: 2.8428 - regression_loss: 2.1240 - classification_loss: 0.7188
 379/1000 [==========>...................] - ETA: 4:40 - loss: 2.8470 - regression_loss: 2.1263 - classification_loss: 0.7207
 380/1000 [==========>...................] - ETA: 4:40 - loss: 2.8464 - regression_loss: 2.1257 - classification_loss: 0.7207
 381/1000 [==========>...................] - ETA: 4:39 - loss: 2.8475 - regression_loss: 2.1268 - classification_loss: 0.7207
 382/1000 [==========>...................] - ETA: 4:39 - loss: 2.8459 - regression_loss: 2.1261 - classification_loss: 0.7198
 383/1000 [==========>...................] - ETA: 4:39 - loss: 2.8469 - regression_loss: 2.1276 - classification_loss: 0.7192
 384/1000 [==========>...................] - ETA: 4:38 - loss: 2.8484 - regression_loss: 2.1295 - classification_loss: 0.7190
 385/1000 [==========>...................] - ETA: 4:38 - loss: 2.8517 - regression_loss: 2.1325 - classification_loss: 0.7192
 386/1000 [==========>...................] - ETA: 4:37 - loss: 2.8491 - regression_loss: 2.1304 - classification_loss: 0.7187
 387/1000 [==========>...................] - ETA: 4:37 - loss: 2.8476 - regression_loss: 2.1298 - classification_loss: 0.7178
 388/1000 [==========>...................] - ETA: 4:36 - loss: 2.8538 - regression_loss: 2.1348 - classification_loss: 0.7190
 389/1000 [==========>...................] - ETA: 4:36 - loss: 2.8574 - regression_loss: 2.1386 - classification_loss: 0.7189
 390/1000 [==========>...................] - ETA: 4:35 - loss: 2.8598 - regression_loss: 2.1409 - classification_loss: 0.7189
 391/1000 [==========>...................] - ETA: 4:35 - loss: 2.8623 - regression_loss: 2.1428 - classification_loss: 0.7195
 392/1000 [==========>...................] - ETA: 4:35 - loss: 2.8638 - regression_loss: 2.1447 - classification_loss: 0.7191
 393/1000 [==========>...................] - ETA: 4:34 - loss: 2.8566 - regression_loss: 2.1393 - classification_loss: 0.7173
 394/1000 [==========>...................] - ETA: 4:34 - loss: 2.8577 - regression_loss: 2.1411 - classification_loss: 0.7166
 395/1000 [==========>...................] - ETA: 4:33 - loss: 2.8587 - regression_loss: 2.1420 - classification_loss: 0.7167
 396/1000 [==========>...................] - ETA: 4:33 - loss: 2.8605 - regression_loss: 2.1439 - classification_loss: 0.7166
 397/1000 [==========>...................] - ETA: 4:32 - loss: 2.8655 - regression_loss: 2.1488 - classification_loss: 0.7167
 398/1000 [==========>...................] - ETA: 4:32 - loss: 2.8583 - regression_loss: 2.1434 - classification_loss: 0.7149
 399/1000 [==========>...................] - ETA: 4:31 - loss: 2.8588 - regression_loss: 2.1449 - classification_loss: 0.7139
 400/1000 [===========>..................] - ETA: 4:31 - loss: 2.8517 - regression_loss: 2.1396 - classification_loss: 0.7122
 401/1000 [===========>..................] - ETA: 4:30 - loss: 2.8540 - regression_loss: 2.1422 - classification_loss: 0.7118
 402/1000 [===========>..................] - ETA: 4:30 - loss: 2.8469 - regression_loss: 2.1369 - classification_loss: 0.7100
 403/1000 [===========>..................] - ETA: 4:30 - loss: 2.8511 - regression_loss: 2.1407 - classification_loss: 0.7104
 404/1000 [===========>..................] - ETA: 4:29 - loss: 2.8510 - regression_loss: 2.1415 - classification_loss: 0.7095
 405/1000 [===========>..................] - ETA: 4:29 - loss: 2.8540 - regression_loss: 2.1428 - classification_loss: 0.7112
 406/1000 [===========>..................] - ETA: 4:28 - loss: 2.8541 - regression_loss: 2.1436 - classification_loss: 0.7105
 407/1000 [===========>..................] - ETA: 4:28 - loss: 2.8529 - regression_loss: 2.1432 - classification_loss: 0.7097
 408/1000 [===========>..................] - ETA: 4:27 - loss: 2.8508 - regression_loss: 2.1420 - classification_loss: 0.7087
 409/1000 [===========>..................] - ETA: 4:27 - loss: 2.8533 - regression_loss: 2.1447 - classification_loss: 0.7086
 410/1000 [===========>..................] - ETA: 4:26 - loss: 2.8524 - regression_loss: 2.1442 - classification_loss: 0.7083
 411/1000 [===========>..................] - ETA: 4:26 - loss: 2.8455 - regression_loss: 2.1390 - classification_loss: 0.7065
 412/1000 [===========>..................] - ETA: 4:26 - loss: 2.8441 - regression_loss: 2.1380 - classification_loss: 0.7061
 413/1000 [===========>..................] - ETA: 4:25 - loss: 2.8483 - regression_loss: 2.1412 - classification_loss: 0.7072
 414/1000 [===========>..................] - ETA: 4:25 - loss: 2.8512 - regression_loss: 2.1441 - classification_loss: 0.7071
 415/1000 [===========>..................] - ETA: 4:24 - loss: 2.8537 - regression_loss: 2.1470 - classification_loss: 0.7067
 416/1000 [===========>..................] - ETA: 4:24 - loss: 2.8525 - regression_loss: 2.1418 - classification_loss: 0.7107
 417/1000 [===========>..................] - ETA: 4:23 - loss: 2.8524 - regression_loss: 2.1418 - classification_loss: 0.7106
 418/1000 [===========>..................] - ETA: 4:23 - loss: 2.8500 - regression_loss: 2.1405 - classification_loss: 0.7095
 419/1000 [===========>..................] - ETA: 4:22 - loss: 2.8504 - regression_loss: 2.1413 - classification_loss: 0.7092
 420/1000 [===========>..................] - ETA: 4:22 - loss: 2.8518 - regression_loss: 2.1429 - classification_loss: 0.7089
 421/1000 [===========>..................] - ETA: 4:21 - loss: 2.8505 - regression_loss: 2.1424 - classification_loss: 0.7081
 422/1000 [===========>..................] - ETA: 4:21 - loss: 2.8494 - regression_loss: 2.1421 - classification_loss: 0.7073
 423/1000 [===========>..................] - ETA: 4:20 - loss: 2.8516 - regression_loss: 2.1442 - classification_loss: 0.7075
 424/1000 [===========>..................] - ETA: 4:20 - loss: 2.8449 - regression_loss: 2.1391 - classification_loss: 0.7058
 425/1000 [===========>..................] - ETA: 4:20 - loss: 2.8453 - regression_loss: 2.1392 - classification_loss: 0.7062
 426/1000 [===========>..................] - ETA: 4:19 - loss: 2.8432 - regression_loss: 2.1382 - classification_loss: 0.7050
 427/1000 [===========>..................] - ETA: 4:19 - loss: 2.8365 - regression_loss: 2.1332 - classification_loss: 0.7033
 428/1000 [===========>..................] - ETA: 4:18 - loss: 2.8349 - regression_loss: 2.1323 - classification_loss: 0.7026
 429/1000 [===========>..................] - ETA: 4:18 - loss: 2.8405 - regression_loss: 2.1354 - classification_loss: 0.7051
 430/1000 [===========>..................] - ETA: 4:17 - loss: 2.8403 - regression_loss: 2.1356 - classification_loss: 0.7046
 431/1000 [===========>..................] - ETA: 4:17 - loss: 2.8337 - regression_loss: 2.1307 - classification_loss: 0.7030
 432/1000 [===========>..................] - ETA: 4:16 - loss: 2.8337 - regression_loss: 2.1318 - classification_loss: 0.7020
 433/1000 [===========>..................] - ETA: 4:16 - loss: 2.8328 - regression_loss: 2.1307 - classification_loss: 0.7021
 434/1000 [============>.................] - ETA: 4:15 - loss: 2.8350 - regression_loss: 2.1332 - classification_loss: 0.7018
 435/1000 [============>.................] - ETA: 4:15 - loss: 2.8385 - regression_loss: 2.1347 - classification_loss: 0.7038
 436/1000 [============>.................] - ETA: 4:15 - loss: 2.8409 - regression_loss: 2.1369 - classification_loss: 0.7040
 437/1000 [============>.................] - ETA: 4:14 - loss: 2.8424 - regression_loss: 2.1375 - classification_loss: 0.7049
 438/1000 [============>.................] - ETA: 4:14 - loss: 2.8359 - regression_loss: 2.1326 - classification_loss: 0.7033
 439/1000 [============>.................] - ETA: 4:13 - loss: 2.8357 - regression_loss: 2.1332 - classification_loss: 0.7025
 440/1000 [============>.................] - ETA: 4:13 - loss: 2.8374 - regression_loss: 2.1350 - classification_loss: 0.7024
 441/1000 [============>.................] - ETA: 4:12 - loss: 2.8358 - regression_loss: 2.1344 - classification_loss: 0.7014
 442/1000 [============>.................] - ETA: 4:12 - loss: 2.8376 - regression_loss: 2.1365 - classification_loss: 0.7012
 443/1000 [============>.................] - ETA: 4:11 - loss: 2.8405 - regression_loss: 2.1396 - classification_loss: 0.7009
 444/1000 [============>.................] - ETA: 4:11 - loss: 2.8411 - regression_loss: 2.1404 - classification_loss: 0.7007
 445/1000 [============>.................] - ETA: 4:11 - loss: 2.8462 - regression_loss: 2.1442 - classification_loss: 0.7020
 446/1000 [============>.................] - ETA: 4:10 - loss: 2.8471 - regression_loss: 2.1445 - classification_loss: 0.7026
 447/1000 [============>.................] - ETA: 4:10 - loss: 2.8483 - regression_loss: 2.1459 - classification_loss: 0.7024
 448/1000 [============>.................] - ETA: 4:09 - loss: 2.8420 - regression_loss: 2.1411 - classification_loss: 0.7008
 449/1000 [============>.................] - ETA: 4:09 - loss: 2.8427 - regression_loss: 2.1413 - classification_loss: 0.7014
 450/1000 [============>.................] - ETA: 4:08 - loss: 2.8411 - regression_loss: 2.1403 - classification_loss: 0.7009
 451/1000 [============>.................] - ETA: 4:08 - loss: 2.8402 - regression_loss: 2.1401 - classification_loss: 0.7001
 452/1000 [============>.................] - ETA: 4:07 - loss: 2.8460 - regression_loss: 2.1441 - classification_loss: 0.7020
 453/1000 [============>.................] - ETA: 4:07 - loss: 2.8505 - regression_loss: 2.1479 - classification_loss: 0.7026
 454/1000 [============>.................] - ETA: 4:06 - loss: 2.8524 - regression_loss: 2.1495 - classification_loss: 0.7029
 455/1000 [============>.................] - ETA: 4:06 - loss: 2.8590 - regression_loss: 2.1537 - classification_loss: 0.7053
 456/1000 [============>.................] - ETA: 4:06 - loss: 2.8600 - regression_loss: 2.1552 - classification_loss: 0.7049
 457/1000 [============>.................] - ETA: 4:05 - loss: 2.8602 - regression_loss: 2.1551 - classification_loss: 0.7051
 458/1000 [============>.................] - ETA: 4:05 - loss: 2.8539 - regression_loss: 2.1504 - classification_loss: 0.7035
 459/1000 [============>.................] - ETA: 4:04 - loss: 2.8529 - regression_loss: 2.1496 - classification_loss: 0.7032
 460/1000 [============>.................] - ETA: 4:04 - loss: 2.8556 - regression_loss: 2.1524 - classification_loss: 0.7033
 461/1000 [============>.................] - ETA: 4:03 - loss: 2.8565 - regression_loss: 2.1538 - classification_loss: 0.7027
 462/1000 [============>.................] - ETA: 4:03 - loss: 2.8615 - regression_loss: 2.1576 - classification_loss: 0.7039
 463/1000 [============>.................] - ETA: 4:02 - loss: 2.8629 - regression_loss: 2.1592 - classification_loss: 0.7037
 464/1000 [============>.................] - ETA: 4:02 - loss: 2.8642 - regression_loss: 2.1608 - classification_loss: 0.7034
 465/1000 [============>.................] - ETA: 4:01 - loss: 2.8636 - regression_loss: 2.1611 - classification_loss: 0.7025
 466/1000 [============>.................] - ETA: 4:01 - loss: 2.8645 - regression_loss: 2.1615 - classification_loss: 0.7030
 467/1000 [=============>................] - ETA: 4:01 - loss: 2.8668 - regression_loss: 2.1638 - classification_loss: 0.7030
 468/1000 [=============>................] - ETA: 4:00 - loss: 2.8655 - regression_loss: 2.1591 - classification_loss: 0.7063
 469/1000 [=============>................] - ETA: 4:00 - loss: 2.8597 - regression_loss: 2.1545 - classification_loss: 0.7052
 470/1000 [=============>................] - ETA: 3:59 - loss: 2.8597 - regression_loss: 2.1549 - classification_loss: 0.7048
 471/1000 [=============>................] - ETA: 3:59 - loss: 2.8544 - regression_loss: 2.1503 - classification_loss: 0.7040
 472/1000 [=============>................] - ETA: 3:58 - loss: 2.8619 - regression_loss: 2.1558 - classification_loss: 0.7061
 473/1000 [=============>................] - ETA: 3:58 - loss: 2.8669 - regression_loss: 2.1593 - classification_loss: 0.7075
 474/1000 [=============>................] - ETA: 3:57 - loss: 2.8708 - regression_loss: 2.1631 - classification_loss: 0.7078
 475/1000 [=============>................] - ETA: 3:57 - loss: 2.8652 - regression_loss: 2.1585 - classification_loss: 0.7067
 476/1000 [=============>................] - ETA: 3:56 - loss: 2.8636 - regression_loss: 2.1577 - classification_loss: 0.7058
 477/1000 [=============>................] - ETA: 3:56 - loss: 2.8641 - regression_loss: 2.1579 - classification_loss: 0.7061
 478/1000 [=============>................] - ETA: 3:56 - loss: 2.8648 - regression_loss: 2.1585 - classification_loss: 0.7063
 479/1000 [=============>................] - ETA: 3:55 - loss: 2.8674 - regression_loss: 2.1607 - classification_loss: 0.7067
 480/1000 [=============>................] - ETA: 3:55 - loss: 2.8621 - regression_loss: 2.1562 - classification_loss: 0.7059
 481/1000 [=============>................] - ETA: 3:54 - loss: 2.8561 - regression_loss: 2.1517 - classification_loss: 0.7045
 482/1000 [=============>................] - ETA: 3:54 - loss: 2.8571 - regression_loss: 2.1530 - classification_loss: 0.7041
 483/1000 [=============>................] - ETA: 3:53 - loss: 2.8584 - regression_loss: 2.1544 - classification_loss: 0.7039
 484/1000 [=============>................] - ETA: 3:53 - loss: 2.8615 - regression_loss: 2.1570 - classification_loss: 0.7045
 485/1000 [=============>................] - ETA: 3:52 - loss: 2.8624 - regression_loss: 2.1576 - classification_loss: 0.7048
 486/1000 [=============>................] - ETA: 3:52 - loss: 2.8617 - regression_loss: 2.1569 - classification_loss: 0.7048
 487/1000 [=============>................] - ETA: 3:51 - loss: 2.8661 - regression_loss: 2.1608 - classification_loss: 0.7053
 488/1000 [=============>................] - ETA: 3:51 - loss: 2.8664 - regression_loss: 2.1608 - classification_loss: 0.7056
 489/1000 [=============>................] - ETA: 3:51 - loss: 2.8665 - regression_loss: 2.1614 - classification_loss: 0.7051
 490/1000 [=============>................] - ETA: 3:50 - loss: 2.8607 - regression_loss: 2.1569 - classification_loss: 0.7037
 491/1000 [=============>................] - ETA: 3:50 - loss: 2.8603 - regression_loss: 2.1569 - classification_loss: 0.7034
 492/1000 [=============>................] - ETA: 3:49 - loss: 2.8639 - regression_loss: 2.1598 - classification_loss: 0.7041
 493/1000 [=============>................] - ETA: 3:49 - loss: 2.8637 - regression_loss: 2.1601 - classification_loss: 0.7037
 494/1000 [=============>................] - ETA: 3:48 - loss: 2.8670 - regression_loss: 2.1626 - classification_loss: 0.7044
 495/1000 [=============>................] - ETA: 3:48 - loss: 2.8671 - regression_loss: 2.1633 - classification_loss: 0.7038
 496/1000 [=============>................] - ETA: 3:47 - loss: 2.8615 - regression_loss: 2.1589 - classification_loss: 0.7026
 497/1000 [=============>................] - ETA: 3:47 - loss: 2.8643 - regression_loss: 2.1616 - classification_loss: 0.7027
 498/1000 [=============>................] - ETA: 3:46 - loss: 2.8587 - regression_loss: 2.1573 - classification_loss: 0.7014
 499/1000 [=============>................] - ETA: 3:46 - loss: 2.8650 - regression_loss: 2.1614 - classification_loss: 0.7036
 500/1000 [==============>...............] - ETA: 3:46 - loss: 2.8656 - regression_loss: 2.1624 - classification_loss: 0.7032
 501/1000 [==============>...............] - ETA: 3:45 - loss: 2.8668 - regression_loss: 2.1633 - classification_loss: 0.7035
 502/1000 [==============>...............] - ETA: 3:45 - loss: 2.8704 - regression_loss: 2.1660 - classification_loss: 0.7044
 503/1000 [==============>...............] - ETA: 3:44 - loss: 2.8647 - regression_loss: 2.1617 - classification_loss: 0.7030
 504/1000 [==============>...............] - ETA: 3:44 - loss: 2.8651 - regression_loss: 2.1618 - classification_loss: 0.7033
 505/1000 [==============>...............] - ETA: 3:43 - loss: 2.8668 - regression_loss: 2.1626 - classification_loss: 0.7042
 506/1000 [==============>...............] - ETA: 3:43 - loss: 2.8665 - regression_loss: 2.1631 - classification_loss: 0.7034
 507/1000 [==============>...............] - ETA: 3:42 - loss: 2.8669 - regression_loss: 2.1641 - classification_loss: 0.7028
 508/1000 [==============>...............] - ETA: 3:42 - loss: 2.8667 - regression_loss: 2.1639 - classification_loss: 0.7028
 509/1000 [==============>...............] - ETA: 3:42 - loss: 2.8612 - regression_loss: 2.1597 - classification_loss: 0.7015
 510/1000 [==============>...............] - ETA: 3:41 - loss: 2.8637 - regression_loss: 2.1615 - classification_loss: 0.7023
 511/1000 [==============>...............] - ETA: 3:41 - loss: 2.8583 - regression_loss: 2.1573 - classification_loss: 0.7010
 512/1000 [==============>...............] - ETA: 3:40 - loss: 2.8612 - regression_loss: 2.1593 - classification_loss: 0.7019
 513/1000 [==============>...............] - ETA: 3:40 - loss: 2.8556 - regression_loss: 2.1551 - classification_loss: 0.7005
 514/1000 [==============>...............] - ETA: 3:39 - loss: 2.8555 - regression_loss: 2.1546 - classification_loss: 0.7009
 515/1000 [==============>...............] - ETA: 3:39 - loss: 2.8580 - regression_loss: 2.1569 - classification_loss: 0.7011
 516/1000 [==============>...............] - ETA: 3:38 - loss: 2.8525 - regression_loss: 2.1527 - classification_loss: 0.6997
 517/1000 [==============>...............] - ETA: 3:38 - loss: 2.8507 - regression_loss: 2.1517 - classification_loss: 0.6990
 518/1000 [==============>...............] - ETA: 3:37 - loss: 2.8537 - regression_loss: 2.1547 - classification_loss: 0.6990
 519/1000 [==============>...............] - ETA: 3:37 - loss: 2.8599 - regression_loss: 2.1590 - classification_loss: 0.7009
 520/1000 [==============>...............] - ETA: 3:37 - loss: 2.8544 - regression_loss: 2.1549 - classification_loss: 0.6995
 521/1000 [==============>...............] - ETA: 3:36 - loss: 2.8534 - regression_loss: 2.1542 - classification_loss: 0.6993
 522/1000 [==============>...............] - ETA: 3:36 - loss: 2.8576 - regression_loss: 2.1577 - classification_loss: 0.6999
 523/1000 [==============>...............] - ETA: 3:35 - loss: 2.8570 - regression_loss: 2.1568 - classification_loss: 0.7003
 524/1000 [==============>...............] - ETA: 3:35 - loss: 2.8596 - regression_loss: 2.1579 - classification_loss: 0.7017
 525/1000 [==============>...............] - ETA: 3:34 - loss: 2.8624 - regression_loss: 2.1597 - classification_loss: 0.7026
 526/1000 [==============>...............] - ETA: 3:34 - loss: 2.8569 - regression_loss: 2.1556 - classification_loss: 0.7013
 527/1000 [==============>...............] - ETA: 3:33 - loss: 2.8515 - regression_loss: 2.1515 - classification_loss: 0.7000
 528/1000 [==============>...............] - ETA: 3:33 - loss: 2.8461 - regression_loss: 2.1475 - classification_loss: 0.6986
 529/1000 [==============>...............] - ETA: 3:32 - loss: 2.8502 - regression_loss: 2.1502 - classification_loss: 0.7000
 530/1000 [==============>...............] - ETA: 3:32 - loss: 2.8513 - regression_loss: 2.1514 - classification_loss: 0.6999
 531/1000 [==============>...............] - ETA: 3:32 - loss: 2.8518 - regression_loss: 2.1525 - classification_loss: 0.6993
 532/1000 [==============>...............] - ETA: 3:31 - loss: 2.8527 - regression_loss: 2.1529 - classification_loss: 0.6998
 533/1000 [==============>...............] - ETA: 3:31 - loss: 2.8543 - regression_loss: 2.1545 - classification_loss: 0.6998
 534/1000 [===============>..............] - ETA: 3:30 - loss: 2.8538 - regression_loss: 2.1546 - classification_loss: 0.6991
 535/1000 [===============>..............] - ETA: 3:30 - loss: 2.8580 - regression_loss: 2.1572 - classification_loss: 0.7008
 536/1000 [===============>..............] - ETA: 3:29 - loss: 2.8579 - regression_loss: 2.1572 - classification_loss: 0.7008
 537/1000 [===============>..............] - ETA: 3:29 - loss: 2.8627 - regression_loss: 2.1608 - classification_loss: 0.7019
 538/1000 [===============>..............] - ETA: 3:28 - loss: 2.8639 - regression_loss: 2.1615 - classification_loss: 0.7023
 539/1000 [===============>..............] - ETA: 3:28 - loss: 2.8639 - regression_loss: 2.1618 - classification_loss: 0.7022
 540/1000 [===============>..............] - ETA: 3:27 - loss: 2.8675 - regression_loss: 2.1637 - classification_loss: 0.7039
 541/1000 [===============>..............] - ETA: 3:27 - loss: 2.8622 - regression_loss: 2.1597 - classification_loss: 0.7026
 542/1000 [===============>..............] - ETA: 3:27 - loss: 2.8639 - regression_loss: 2.1600 - classification_loss: 0.7039
 543/1000 [===============>..............] - ETA: 3:26 - loss: 2.8667 - regression_loss: 2.1609 - classification_loss: 0.7058
 544/1000 [===============>..............] - ETA: 3:26 - loss: 2.8667 - regression_loss: 2.1608 - classification_loss: 0.7058
 545/1000 [===============>..............] - ETA: 3:25 - loss: 2.8614 - regression_loss: 2.1568 - classification_loss: 0.7046
 546/1000 [===============>..............] - ETA: 3:25 - loss: 2.8610 - regression_loss: 2.1568 - classification_loss: 0.7042
 547/1000 [===============>..............] - ETA: 3:24 - loss: 2.8601 - regression_loss: 2.1566 - classification_loss: 0.7035
 548/1000 [===============>..............] - ETA: 3:24 - loss: 2.8591 - regression_loss: 2.1560 - classification_loss: 0.7031
 549/1000 [===============>..............] - ETA: 3:23 - loss: 2.8636 - regression_loss: 2.1591 - classification_loss: 0.7045
 550/1000 [===============>..............] - ETA: 3:23 - loss: 2.8677 - regression_loss: 2.1627 - classification_loss: 0.7049
 551/1000 [===============>..............] - ETA: 3:23 - loss: 2.8692 - regression_loss: 2.1645 - classification_loss: 0.7046
 552/1000 [===============>..............] - ETA: 3:22 - loss: 2.8724 - regression_loss: 2.1673 - classification_loss: 0.7051
 553/1000 [===============>..............] - ETA: 3:22 - loss: 2.8728 - regression_loss: 2.1674 - classification_loss: 0.7053
 554/1000 [===============>..............] - ETA: 3:21 - loss: 2.8715 - regression_loss: 2.1665 - classification_loss: 0.7050
 555/1000 [===============>..............] - ETA: 3:21 - loss: 2.8782 - regression_loss: 2.1718 - classification_loss: 0.7064
 556/1000 [===============>..............] - ETA: 3:20 - loss: 2.8812 - regression_loss: 2.1742 - classification_loss: 0.7070
 557/1000 [===============>..............] - ETA: 3:20 - loss: 2.8826 - regression_loss: 2.1753 - classification_loss: 0.7073
 558/1000 [===============>..............] - ETA: 3:19 - loss: 2.8853 - regression_loss: 2.1780 - classification_loss: 0.7073
 559/1000 [===============>..............] - ETA: 3:19 - loss: 2.8884 - regression_loss: 2.1798 - classification_loss: 0.7086
 560/1000 [===============>..............] - ETA: 3:18 - loss: 2.8834 - regression_loss: 2.1759 - classification_loss: 0.7075
 561/1000 [===============>..............] - ETA: 3:18 - loss: 2.8859 - regression_loss: 2.1781 - classification_loss: 0.7077
 562/1000 [===============>..............] - ETA: 3:18 - loss: 2.8808 - regression_loss: 2.1743 - classification_loss: 0.7065
 563/1000 [===============>..............] - ETA: 3:17 - loss: 2.8823 - regression_loss: 2.1760 - classification_loss: 0.7064
 564/1000 [===============>..............] - ETA: 3:17 - loss: 2.8850 - regression_loss: 2.1784 - classification_loss: 0.7066
 565/1000 [===============>..............] - ETA: 3:16 - loss: 2.8865 - regression_loss: 2.1798 - classification_loss: 0.7067
 566/1000 [===============>..............] - ETA: 3:16 - loss: 2.8814 - regression_loss: 2.1760 - classification_loss: 0.7055
 567/1000 [================>.............] - ETA: 3:15 - loss: 2.8832 - regression_loss: 2.1777 - classification_loss: 0.7055
 568/1000 [================>.............] - ETA: 3:15 - loss: 2.8840 - regression_loss: 2.1785 - classification_loss: 0.7055
 569/1000 [================>.............] - ETA: 3:14 - loss: 2.8791 - regression_loss: 2.1746 - classification_loss: 0.7045
 570/1000 [================>.............] - ETA: 3:14 - loss: 2.8792 - regression_loss: 2.1746 - classification_loss: 0.7046
 571/1000 [================>.............] - ETA: 3:13 - loss: 2.8802 - regression_loss: 2.1757 - classification_loss: 0.7046
 572/1000 [================>.............] - ETA: 3:13 - loss: 2.8842 - regression_loss: 2.1786 - classification_loss: 0.7056
 573/1000 [================>.............] - ETA: 3:13 - loss: 2.8845 - regression_loss: 2.1790 - classification_loss: 0.7055
 574/1000 [================>.............] - ETA: 3:12 - loss: 2.8831 - regression_loss: 2.1782 - classification_loss: 0.7049
 575/1000 [================>.............] - ETA: 3:12 - loss: 2.8835 - regression_loss: 2.1792 - classification_loss: 0.7043
 576/1000 [================>.............] - ETA: 3:11 - loss: 2.8841 - regression_loss: 2.1798 - classification_loss: 0.7043
 577/1000 [================>.............] - ETA: 3:11 - loss: 2.8872 - regression_loss: 2.1830 - classification_loss: 0.7043
 578/1000 [================>.............] - ETA: 3:10 - loss: 2.8823 - regression_loss: 2.1792 - classification_loss: 0.7031
 579/1000 [================>.............] - ETA: 3:10 - loss: 2.8836 - regression_loss: 2.1805 - classification_loss: 0.7032
 580/1000 [================>.............] - ETA: 3:09 - loss: 2.8858 - regression_loss: 2.1830 - classification_loss: 0.7028
 581/1000 [================>.............] - ETA: 3:09 - loss: 2.8809 - regression_loss: 2.1792 - classification_loss: 0.7016
 582/1000 [================>.............] - ETA: 3:09 - loss: 2.8759 - regression_loss: 2.1755 - classification_loss: 0.7004
 583/1000 [================>.............] - ETA: 3:08 - loss: 2.8710 - regression_loss: 2.1717 - classification_loss: 0.6993
 584/1000 [================>.............] - ETA: 3:08 - loss: 2.8707 - regression_loss: 2.1717 - classification_loss: 0.6990
 585/1000 [================>.............] - ETA: 3:07 - loss: 2.8742 - regression_loss: 2.1748 - classification_loss: 0.6993
 586/1000 [================>.............] - ETA: 3:07 - loss: 2.8693 - regression_loss: 2.1711 - classification_loss: 0.6982
 587/1000 [================>.............] - ETA: 3:06 - loss: 2.8728 - regression_loss: 2.1738 - classification_loss: 0.6989
 588/1000 [================>.............] - ETA: 3:06 - loss: 2.8679 - regression_loss: 2.1701 - classification_loss: 0.6977
 589/1000 [================>.............] - ETA: 3:05 - loss: 2.8697 - regression_loss: 2.1716 - classification_loss: 0.6981
 590/1000 [================>.............] - ETA: 3:05 - loss: 2.8648 - regression_loss: 2.1679 - classification_loss: 0.6969
 591/1000 [================>.............] - ETA: 3:04 - loss: 2.8600 - regression_loss: 2.1643 - classification_loss: 0.6957
 592/1000 [================>.............] - ETA: 3:04 - loss: 2.8592 - regression_loss: 2.1637 - classification_loss: 0.6955
 593/1000 [================>.............] - ETA: 3:04 - loss: 2.8628 - regression_loss: 2.1658 - classification_loss: 0.6970
 594/1000 [================>.............] - ETA: 3:03 - loss: 2.8654 - regression_loss: 2.1680 - classification_loss: 0.6974
 595/1000 [================>.............] - ETA: 3:03 - loss: 2.8697 - regression_loss: 2.1704 - classification_loss: 0.6993
 596/1000 [================>.............] - ETA: 3:02 - loss: 2.8649 - regression_loss: 2.1668 - classification_loss: 0.6981
 597/1000 [================>.............] - ETA: 3:02 - loss: 2.8678 - regression_loss: 2.1692 - classification_loss: 0.6986
 598/1000 [================>.............] - ETA: 3:01 - loss: 2.8690 - regression_loss: 2.1709 - classification_loss: 0.6981
 599/1000 [================>.............] - ETA: 3:01 - loss: 2.8655 - regression_loss: 2.1673 - classification_loss: 0.6982
 600/1000 [=================>............] - ETA: 3:00 - loss: 2.8607 - regression_loss: 2.1637 - classification_loss: 0.6970
 601/1000 [=================>............] - ETA: 3:00 - loss: 2.8648 - regression_loss: 2.1681 - classification_loss: 0.6967
 602/1000 [=================>............] - ETA: 3:00 - loss: 2.8600 - regression_loss: 2.1645 - classification_loss: 0.6955
 603/1000 [=================>............] - ETA: 2:59 - loss: 2.8553 - regression_loss: 2.1609 - classification_loss: 0.6944
 604/1000 [=================>............] - ETA: 2:59 - loss: 2.8560 - regression_loss: 2.1616 - classification_loss: 0.6944
 605/1000 [=================>............] - ETA: 2:58 - loss: 2.8617 - regression_loss: 2.1649 - classification_loss: 0.6968
 606/1000 [=================>............] - ETA: 2:58 - loss: 2.8612 - regression_loss: 2.1647 - classification_loss: 0.6965
 607/1000 [=================>............] - ETA: 2:57 - loss: 2.8673 - regression_loss: 2.1678 - classification_loss: 0.6995
 608/1000 [=================>............] - ETA: 2:57 - loss: 2.8667 - regression_loss: 2.1678 - classification_loss: 0.6990
 609/1000 [=================>............] - ETA: 2:56 - loss: 2.8671 - regression_loss: 2.1677 - classification_loss: 0.6995
 610/1000 [=================>............] - ETA: 2:56 - loss: 2.8681 - regression_loss: 2.1690 - classification_loss: 0.6991
 611/1000 [=================>............] - ETA: 2:55 - loss: 2.8634 - regression_loss: 2.1655 - classification_loss: 0.6980
 612/1000 [=================>............] - ETA: 2:55 - loss: 2.8671 - regression_loss: 2.1676 - classification_loss: 0.6995
 613/1000 [=================>............] - ETA: 2:55 - loss: 2.8667 - regression_loss: 2.1674 - classification_loss: 0.6993
 614/1000 [=================>............] - ETA: 2:54 - loss: 2.8681 - regression_loss: 2.1689 - classification_loss: 0.6992
 615/1000 [=================>............] - ETA: 2:54 - loss: 2.8674 - regression_loss: 2.1689 - classification_loss: 0.6985
 616/1000 [=================>............] - ETA: 2:53 - loss: 2.8688 - regression_loss: 2.1702 - classification_loss: 0.6985
 617/1000 [=================>............] - ETA: 2:53 - loss: 2.8748 - regression_loss: 2.1744 - classification_loss: 0.7004
 618/1000 [=================>............] - ETA: 2:52 - loss: 2.8764 - regression_loss: 2.1755 - classification_loss: 0.7009
 619/1000 [=================>............] - ETA: 2:52 - loss: 2.8788 - regression_loss: 2.1758 - classification_loss: 0.7030
 620/1000 [=================>............] - ETA: 2:51 - loss: 2.8834 - regression_loss: 2.1782 - classification_loss: 0.7052
 621/1000 [=================>............] - ETA: 2:51 - loss: 2.8788 - regression_loss: 2.1747 - classification_loss: 0.7041
 622/1000 [=================>............] - ETA: 2:50 - loss: 2.8803 - regression_loss: 2.1766 - classification_loss: 0.7036
 623/1000 [=================>............] - ETA: 2:50 - loss: 2.8785 - regression_loss: 2.1757 - classification_loss: 0.7028
 624/1000 [=================>............] - ETA: 2:50 - loss: 2.8815 - regression_loss: 2.1772 - classification_loss: 0.7042
 625/1000 [=================>............] - ETA: 2:49 - loss: 2.8838 - regression_loss: 2.1779 - classification_loss: 0.7059
 626/1000 [=================>............] - ETA: 2:49 - loss: 2.8844 - regression_loss: 2.1789 - classification_loss: 0.7056
 627/1000 [=================>............] - ETA: 2:48 - loss: 2.8847 - regression_loss: 2.1785 - classification_loss: 0.7062
 628/1000 [=================>............] - ETA: 2:48 - loss: 2.8850 - regression_loss: 2.1780 - classification_loss: 0.7070
 629/1000 [=================>............] - ETA: 2:47 - loss: 2.8884 - regression_loss: 2.1808 - classification_loss: 0.7076
 630/1000 [=================>............] - ETA: 2:47 - loss: 2.8838 - regression_loss: 2.1773 - classification_loss: 0.7065
 631/1000 [=================>............] - ETA: 2:46 - loss: 2.8833 - regression_loss: 2.1773 - classification_loss: 0.7060
 632/1000 [=================>............] - ETA: 2:46 - loss: 2.8788 - regression_loss: 2.1739 - classification_loss: 0.7049
 633/1000 [=================>............] - ETA: 2:46 - loss: 2.8742 - regression_loss: 2.1704 - classification_loss: 0.7038
 634/1000 [==================>...........] - ETA: 2:45 - loss: 2.8770 - regression_loss: 2.1726 - classification_loss: 0.7045
 635/1000 [==================>...........] - ETA: 2:45 - loss: 2.8782 - regression_loss: 2.1691 - classification_loss: 0.7091
 636/1000 [==================>...........] - ETA: 2:44 - loss: 2.8794 - regression_loss: 2.1703 - classification_loss: 0.7091
 637/1000 [==================>...........] - ETA: 2:44 - loss: 2.8784 - regression_loss: 2.1669 - classification_loss: 0.7115
 638/1000 [==================>...........] - ETA: 2:43 - loss: 2.8739 - regression_loss: 2.1635 - classification_loss: 0.7104
 639/1000 [==================>...........] - ETA: 2:43 - loss: 2.8721 - regression_loss: 2.1601 - classification_loss: 0.7120
 640/1000 [==================>...........] - ETA: 2:42 - loss: 2.8717 - regression_loss: 2.1602 - classification_loss: 0.7115
 641/1000 [==================>...........] - ETA: 2:42 - loss: 2.8672 - regression_loss: 2.1568 - classification_loss: 0.7104
 642/1000 [==================>...........] - ETA: 2:41 - loss: 2.8628 - regression_loss: 2.1534 - classification_loss: 0.7093
 643/1000 [==================>...........] - ETA: 2:41 - loss: 2.8644 - regression_loss: 2.1536 - classification_loss: 0.7108
 644/1000 [==================>...........] - ETA: 2:41 - loss: 2.8655 - regression_loss: 2.1548 - classification_loss: 0.7107
 645/1000 [==================>...........] - ETA: 2:40 - loss: 2.8677 - regression_loss: 2.1568 - classification_loss: 0.7108
 646/1000 [==================>...........] - ETA: 2:40 - loss: 2.8633 - regression_loss: 2.1535 - classification_loss: 0.7098
 647/1000 [==================>...........] - ETA: 2:39 - loss: 2.8631 - regression_loss: 2.1534 - classification_loss: 0.7097
 648/1000 [==================>...........] - ETA: 2:39 - loss: 2.8633 - regression_loss: 2.1527 - classification_loss: 0.7105
 649/1000 [==================>...........] - ETA: 2:38 - loss: 2.8668 - regression_loss: 2.1548 - classification_loss: 0.7120
 650/1000 [==================>...........] - ETA: 2:38 - loss: 2.8667 - regression_loss: 2.1549 - classification_loss: 0.7118
 651/1000 [==================>...........] - ETA: 2:37 - loss: 2.8623 - regression_loss: 2.1516 - classification_loss: 0.7108
 652/1000 [==================>...........] - ETA: 2:37 - loss: 2.8661 - regression_loss: 2.1543 - classification_loss: 0.7118
 653/1000 [==================>...........] - ETA: 2:36 - loss: 2.8680 - regression_loss: 2.1558 - classification_loss: 0.7121
 654/1000 [==================>...........] - ETA: 2:36 - loss: 2.8691 - regression_loss: 2.1571 - classification_loss: 0.7121
 655/1000 [==================>...........] - ETA: 2:36 - loss: 2.8727 - regression_loss: 2.1592 - classification_loss: 0.7135
 656/1000 [==================>...........] - ETA: 2:35 - loss: 2.8726 - regression_loss: 2.1597 - classification_loss: 0.7129
 657/1000 [==================>...........] - ETA: 2:35 - loss: 2.8722 - regression_loss: 2.1589 - classification_loss: 0.7133
 658/1000 [==================>...........] - ETA: 2:34 - loss: 2.8750 - regression_loss: 2.1612 - classification_loss: 0.7137
 659/1000 [==================>...........] - ETA: 2:34 - loss: 2.8770 - regression_loss: 2.1626 - classification_loss: 0.7144
 660/1000 [==================>...........] - ETA: 2:33 - loss: 2.8793 - regression_loss: 2.1639 - classification_loss: 0.7155
 661/1000 [==================>...........] - ETA: 2:33 - loss: 2.8806 - regression_loss: 2.1647 - classification_loss: 0.7158
 662/1000 [==================>...........] - ETA: 2:32 - loss: 2.8808 - regression_loss: 2.1646 - classification_loss: 0.7162
 663/1000 [==================>...........] - ETA: 2:32 - loss: 2.8822 - regression_loss: 2.1659 - classification_loss: 0.7163
 664/1000 [==================>...........] - ETA: 2:31 - loss: 2.8779 - regression_loss: 2.1627 - classification_loss: 0.7152
 665/1000 [==================>...........] - ETA: 2:31 - loss: 2.8789 - regression_loss: 2.1637 - classification_loss: 0.7152
 666/1000 [==================>...........] - ETA: 2:31 - loss: 2.8777 - regression_loss: 2.1628 - classification_loss: 0.7149
 667/1000 [===================>..........] - ETA: 2:30 - loss: 2.8819 - regression_loss: 2.1657 - classification_loss: 0.7161
 668/1000 [===================>..........] - ETA: 2:30 - loss: 2.8775 - regression_loss: 2.1625 - classification_loss: 0.7151
 669/1000 [===================>..........] - ETA: 2:29 - loss: 2.8777 - regression_loss: 2.1620 - classification_loss: 0.7157
 670/1000 [===================>..........] - ETA: 2:29 - loss: 2.8783 - regression_loss: 2.1621 - classification_loss: 0.7162
 671/1000 [===================>..........] - ETA: 2:28 - loss: 2.8792 - regression_loss: 2.1629 - classification_loss: 0.7163
 672/1000 [===================>..........] - ETA: 2:28 - loss: 2.8798 - regression_loss: 2.1640 - classification_loss: 0.7158
 673/1000 [===================>..........] - ETA: 2:27 - loss: 2.8803 - regression_loss: 2.1645 - classification_loss: 0.7158
 674/1000 [===================>..........] - ETA: 2:27 - loss: 2.8760 - regression_loss: 2.1613 - classification_loss: 0.7147
 675/1000 [===================>..........] - ETA: 2:26 - loss: 2.8772 - regression_loss: 2.1625 - classification_loss: 0.7147
 676/1000 [===================>..........] - ETA: 2:26 - loss: 2.8781 - regression_loss: 2.1633 - classification_loss: 0.7148
 677/1000 [===================>..........] - ETA: 2:26 - loss: 2.8803 - regression_loss: 2.1653 - classification_loss: 0.7150
 678/1000 [===================>..........] - ETA: 2:25 - loss: 2.8813 - regression_loss: 2.1665 - classification_loss: 0.7147
 679/1000 [===================>..........] - ETA: 2:25 - loss: 2.8770 - regression_loss: 2.1634 - classification_loss: 0.7137
 680/1000 [===================>..........] - ETA: 2:24 - loss: 2.8778 - regression_loss: 2.1645 - classification_loss: 0.7133
 681/1000 [===================>..........] - ETA: 2:24 - loss: 2.8757 - regression_loss: 2.1629 - classification_loss: 0.7128
 682/1000 [===================>..........] - ETA: 2:23 - loss: 2.8790 - regression_loss: 2.1646 - classification_loss: 0.7143
 683/1000 [===================>..........] - ETA: 2:23 - loss: 2.8779 - regression_loss: 2.1640 - classification_loss: 0.7138
 684/1000 [===================>..........] - ETA: 2:22 - loss: 2.8815 - regression_loss: 2.1659 - classification_loss: 0.7156
 685/1000 [===================>..........] - ETA: 2:22 - loss: 2.8834 - regression_loss: 2.1673 - classification_loss: 0.7161
 686/1000 [===================>..........] - ETA: 2:22 - loss: 2.8842 - regression_loss: 2.1684 - classification_loss: 0.7157
 687/1000 [===================>..........] - ETA: 2:21 - loss: 2.8864 - regression_loss: 2.1696 - classification_loss: 0.7168
 688/1000 [===================>..........] - ETA: 2:21 - loss: 2.8852 - regression_loss: 2.1689 - classification_loss: 0.7162
 689/1000 [===================>..........] - ETA: 2:20 - loss: 2.8848 - regression_loss: 2.1690 - classification_loss: 0.7158
 690/1000 [===================>..........] - ETA: 2:20 - loss: 2.8832 - regression_loss: 2.1679 - classification_loss: 0.7153
 691/1000 [===================>..........] - ETA: 2:19 - loss: 2.8850 - regression_loss: 2.1699 - classification_loss: 0.7151
 692/1000 [===================>..........] - ETA: 2:19 - loss: 2.8868 - regression_loss: 2.1709 - classification_loss: 0.7159
 693/1000 [===================>..........] - ETA: 2:18 - loss: 2.8879 - regression_loss: 2.1723 - classification_loss: 0.7156
 694/1000 [===================>..........] - ETA: 2:18 - loss: 2.8902 - regression_loss: 2.1746 - classification_loss: 0.7156
 695/1000 [===================>..........] - ETA: 2:17 - loss: 2.8863 - regression_loss: 2.1715 - classification_loss: 0.7148
 696/1000 [===================>..........] - ETA: 2:17 - loss: 2.8869 - regression_loss: 2.1726 - classification_loss: 0.7144
 697/1000 [===================>..........] - ETA: 2:17 - loss: 2.8878 - regression_loss: 2.1735 - classification_loss: 0.7142
 698/1000 [===================>..........] - ETA: 2:16 - loss: 2.8862 - regression_loss: 2.1727 - classification_loss: 0.7135
 699/1000 [===================>..........] - ETA: 2:16 - loss: 2.8855 - regression_loss: 2.1724 - classification_loss: 0.7132
 700/1000 [====================>.........] - ETA: 2:15 - loss: 2.8876 - regression_loss: 2.1737 - classification_loss: 0.7139
 701/1000 [====================>.........] - ETA: 2:15 - loss: 2.8886 - regression_loss: 2.1745 - classification_loss: 0.7141
 702/1000 [====================>.........] - ETA: 2:14 - loss: 2.8845 - regression_loss: 2.1714 - classification_loss: 0.7131
 703/1000 [====================>.........] - ETA: 2:14 - loss: 2.8850 - regression_loss: 2.1722 - classification_loss: 0.7128
 704/1000 [====================>.........] - ETA: 2:13 - loss: 2.8874 - regression_loss: 2.1741 - classification_loss: 0.7133
 705/1000 [====================>.........] - ETA: 2:13 - loss: 2.8900 - regression_loss: 2.1762 - classification_loss: 0.7138
 706/1000 [====================>.........] - ETA: 2:12 - loss: 2.8899 - regression_loss: 2.1763 - classification_loss: 0.7136
 707/1000 [====================>.........] - ETA: 2:12 - loss: 2.8892 - regression_loss: 2.1761 - classification_loss: 0.7131
 708/1000 [====================>.........] - ETA: 2:12 - loss: 2.8893 - regression_loss: 2.1763 - classification_loss: 0.7131
 709/1000 [====================>.........] - ETA: 2:11 - loss: 2.8853 - regression_loss: 2.1732 - classification_loss: 0.7120
 710/1000 [====================>.........] - ETA: 2:11 - loss: 2.8846 - regression_loss: 2.1731 - classification_loss: 0.7115
 711/1000 [====================>.........] - ETA: 2:10 - loss: 2.8869 - regression_loss: 2.1745 - classification_loss: 0.7124
 712/1000 [====================>.........] - ETA: 2:10 - loss: 2.8828 - regression_loss: 2.1715 - classification_loss: 0.7114
 713/1000 [====================>.........] - ETA: 2:09 - loss: 2.8821 - regression_loss: 2.1713 - classification_loss: 0.7108
 714/1000 [====================>.........] - ETA: 2:09 - loss: 2.8783 - regression_loss: 2.1683 - classification_loss: 0.7100
 715/1000 [====================>.........] - ETA: 2:08 - loss: 2.8743 - regression_loss: 2.1652 - classification_loss: 0.7091
 716/1000 [====================>.........] - ETA: 2:08 - loss: 2.8773 - regression_loss: 2.1669 - classification_loss: 0.7104
 717/1000 [====================>.........] - ETA: 2:08 - loss: 2.8773 - regression_loss: 2.1676 - classification_loss: 0.7097
 718/1000 [====================>.........] - ETA: 2:07 - loss: 2.8779 - regression_loss: 2.1687 - classification_loss: 0.7092
 719/1000 [====================>.........] - ETA: 2:07 - loss: 2.8772 - regression_loss: 2.1682 - classification_loss: 0.7090
 720/1000 [====================>.........] - ETA: 2:06 - loss: 2.8733 - regression_loss: 2.1651 - classification_loss: 0.7081
 721/1000 [====================>.........] - ETA: 2:06 - loss: 2.8714 - regression_loss: 2.1640 - classification_loss: 0.7074
 722/1000 [====================>.........] - ETA: 2:05 - loss: 2.8740 - regression_loss: 2.1653 - classification_loss: 0.7087
 723/1000 [====================>.........] - ETA: 2:05 - loss: 2.8743 - regression_loss: 2.1662 - classification_loss: 0.7081
 724/1000 [====================>.........] - ETA: 2:04 - loss: 2.8703 - regression_loss: 2.1632 - classification_loss: 0.7072
 725/1000 [====================>.........] - ETA: 2:04 - loss: 2.8714 - regression_loss: 2.1635 - classification_loss: 0.7079
 726/1000 [====================>.........] - ETA: 2:03 - loss: 2.8741 - regression_loss: 2.1662 - classification_loss: 0.7080
 727/1000 [====================>.........] - ETA: 2:03 - loss: 2.8757 - regression_loss: 2.1676 - classification_loss: 0.7081
 728/1000 [====================>.........] - ETA: 2:03 - loss: 2.8763 - regression_loss: 2.1683 - classification_loss: 0.7080
 729/1000 [====================>.........] - ETA: 2:02 - loss: 2.8760 - regression_loss: 2.1683 - classification_loss: 0.7076
 730/1000 [====================>.........] - ETA: 2:02 - loss: 2.8720 - regression_loss: 2.1654 - classification_loss: 0.7067
 731/1000 [====================>.........] - ETA: 2:01 - loss: 2.8720 - regression_loss: 2.1657 - classification_loss: 0.7063
 732/1000 [====================>.........] - ETA: 2:01 - loss: 2.8681 - regression_loss: 2.1628 - classification_loss: 0.7053
 733/1000 [====================>.........] - ETA: 2:00 - loss: 2.8687 - regression_loss: 2.1636 - classification_loss: 0.7051
 734/1000 [=====================>........] - ETA: 2:00 - loss: 2.8713 - regression_loss: 2.1643 - classification_loss: 0.7070
 735/1000 [=====================>........] - ETA: 1:59 - loss: 2.8750 - regression_loss: 2.1666 - classification_loss: 0.7084
 736/1000 [=====================>........] - ETA: 1:59 - loss: 2.8768 - regression_loss: 2.1678 - classification_loss: 0.7091
 737/1000 [=====================>........] - ETA: 1:58 - loss: 2.8773 - regression_loss: 2.1685 - classification_loss: 0.7087
 738/1000 [=====================>........] - ETA: 1:58 - loss: 2.8788 - regression_loss: 2.1699 - classification_loss: 0.7088
 739/1000 [=====================>........] - ETA: 1:58 - loss: 2.8833 - regression_loss: 2.1723 - classification_loss: 0.7110
 740/1000 [=====================>........] - ETA: 1:57 - loss: 2.8850 - regression_loss: 2.1733 - classification_loss: 0.7116
 741/1000 [=====================>........] - ETA: 1:57 - loss: 2.8856 - regression_loss: 2.1741 - classification_loss: 0.7115
 742/1000 [=====================>........] - ETA: 1:56 - loss: 2.8821 - regression_loss: 2.1712 - classification_loss: 0.7109
 743/1000 [=====================>........] - ETA: 1:56 - loss: 2.8828 - regression_loss: 2.1707 - classification_loss: 0.7122
 744/1000 [=====================>........] - ETA: 1:55 - loss: 2.8855 - regression_loss: 2.1724 - classification_loss: 0.7131
 745/1000 [=====================>........] - ETA: 1:55 - loss: 2.8855 - regression_loss: 2.1723 - classification_loss: 0.7132
 746/1000 [=====================>........] - ETA: 1:54 - loss: 2.8854 - regression_loss: 2.1725 - classification_loss: 0.7129
 747/1000 [=====================>........] - ETA: 1:54 - loss: 2.8869 - regression_loss: 2.1727 - classification_loss: 0.7142
 748/1000 [=====================>........] - ETA: 1:53 - loss: 2.8870 - regression_loss: 2.1724 - classification_loss: 0.7147
 749/1000 [=====================>........] - ETA: 1:53 - loss: 2.8882 - regression_loss: 2.1734 - classification_loss: 0.7149
 750/1000 [=====================>........] - ETA: 1:53 - loss: 2.8893 - regression_loss: 2.1745 - classification_loss: 0.7148
 751/1000 [=====================>........] - ETA: 1:52 - loss: 2.8855 - regression_loss: 2.1716 - classification_loss: 0.7139
 752/1000 [=====================>........] - ETA: 1:52 - loss: 2.8870 - regression_loss: 2.1732 - classification_loss: 0.7137
 753/1000 [=====================>........] - ETA: 1:51 - loss: 2.8890 - regression_loss: 2.1756 - classification_loss: 0.7134
 754/1000 [=====================>........] - ETA: 1:51 - loss: 2.8907 - regression_loss: 2.1771 - classification_loss: 0.7136
 755/1000 [=====================>........] - ETA: 1:50 - loss: 2.8869 - regression_loss: 2.1742 - classification_loss: 0.7126
 756/1000 [=====================>........] - ETA: 1:50 - loss: 2.8884 - regression_loss: 2.1758 - classification_loss: 0.7126
 757/1000 [=====================>........] - ETA: 1:49 - loss: 2.8845 - regression_loss: 2.1729 - classification_loss: 0.7116
 758/1000 [=====================>........] - ETA: 1:49 - loss: 2.8849 - regression_loss: 2.1730 - classification_loss: 0.7118
 759/1000 [=====================>........] - ETA: 1:49 - loss: 2.8869 - regression_loss: 2.1747 - classification_loss: 0.7122
 760/1000 [=====================>........] - ETA: 1:48 - loss: 2.8883 - regression_loss: 2.1760 - classification_loss: 0.7123
 761/1000 [=====================>........] - ETA: 1:48 - loss: 2.8845 - regression_loss: 2.1731 - classification_loss: 0.7113
 762/1000 [=====================>........] - ETA: 1:47 - loss: 2.8808 - regression_loss: 2.1703 - classification_loss: 0.7105
 763/1000 [=====================>........] - ETA: 1:47 - loss: 2.8809 - regression_loss: 2.1709 - classification_loss: 0.7100
 764/1000 [=====================>........] - ETA: 1:46 - loss: 2.8815 - regression_loss: 2.1714 - classification_loss: 0.7102
 765/1000 [=====================>........] - ETA: 1:46 - loss: 2.8827 - regression_loss: 2.1727 - classification_loss: 0.7100
 766/1000 [=====================>........] - ETA: 1:45 - loss: 2.8834 - regression_loss: 2.1731 - classification_loss: 0.7103
 767/1000 [======================>.......] - ETA: 1:45 - loss: 2.8832 - regression_loss: 2.1732 - classification_loss: 0.7100
 768/1000 [======================>.......] - ETA: 1:44 - loss: 2.8795 - regression_loss: 2.1704 - classification_loss: 0.7091
 769/1000 [======================>.......] - ETA: 1:44 - loss: 2.8757 - regression_loss: 2.1675 - classification_loss: 0.7082
 770/1000 [======================>.......] - ETA: 1:44 - loss: 2.8750 - regression_loss: 2.1672 - classification_loss: 0.7078
 771/1000 [======================>.......] - ETA: 1:43 - loss: 2.8765 - regression_loss: 2.1677 - classification_loss: 0.7088
 772/1000 [======================>.......] - ETA: 1:43 - loss: 2.8787 - regression_loss: 2.1695 - classification_loss: 0.7092
 773/1000 [======================>.......] - ETA: 1:42 - loss: 2.8750 - regression_loss: 2.1667 - classification_loss: 0.7083
 774/1000 [======================>.......] - ETA: 1:42 - loss: 2.8778 - regression_loss: 2.1686 - classification_loss: 0.7093
 775/1000 [======================>.......] - ETA: 1:41 - loss: 2.8790 - regression_loss: 2.1692 - classification_loss: 0.7098
 776/1000 [======================>.......] - ETA: 1:41 - loss: 2.8796 - regression_loss: 2.1682 - classification_loss: 0.7114
 777/1000 [======================>.......] - ETA: 1:40 - loss: 2.8810 - regression_loss: 2.1697 - classification_loss: 0.7113
 778/1000 [======================>.......] - ETA: 1:40 - loss: 2.8835 - regression_loss: 2.1714 - classification_loss: 0.7121
 779/1000 [======================>.......] - ETA: 1:39 - loss: 2.8846 - regression_loss: 2.1716 - classification_loss: 0.7130
 780/1000 [======================>.......] - ETA: 1:39 - loss: 2.8846 - regression_loss: 2.1718 - classification_loss: 0.7129
 781/1000 [======================>.......] - ETA: 1:39 - loss: 2.8809 - regression_loss: 2.1690 - classification_loss: 0.7119
 782/1000 [======================>.......] - ETA: 1:38 - loss: 2.8772 - regression_loss: 2.1662 - classification_loss: 0.7110
 783/1000 [======================>.......] - ETA: 1:38 - loss: 2.8768 - regression_loss: 2.1664 - classification_loss: 0.7104
 784/1000 [======================>.......] - ETA: 1:37 - loss: 2.8786 - regression_loss: 2.1675 - classification_loss: 0.7111
 785/1000 [======================>.......] - ETA: 1:37 - loss: 2.8773 - regression_loss: 2.1669 - classification_loss: 0.7105
 786/1000 [======================>.......] - ETA: 1:36 - loss: 2.8790 - regression_loss: 2.1685 - classification_loss: 0.7105
 787/1000 [======================>.......] - ETA: 1:36 - loss: 2.8824 - regression_loss: 2.1705 - classification_loss: 0.7119
 788/1000 [======================>.......] - ETA: 1:35 - loss: 2.8862 - regression_loss: 2.1727 - classification_loss: 0.7135
 789/1000 [======================>.......] - ETA: 1:35 - loss: 2.8869 - regression_loss: 2.1737 - classification_loss: 0.7132
 790/1000 [======================>.......] - ETA: 1:34 - loss: 2.8891 - regression_loss: 2.1746 - classification_loss: 0.7145
 791/1000 [======================>.......] - ETA: 1:34 - loss: 2.8885 - regression_loss: 2.1743 - classification_loss: 0.7143
 792/1000 [======================>.......] - ETA: 1:34 - loss: 2.8891 - regression_loss: 2.1749 - classification_loss: 0.7141
 793/1000 [======================>.......] - ETA: 1:33 - loss: 2.8884 - regression_loss: 2.1745 - classification_loss: 0.7139
 794/1000 [======================>.......] - ETA: 1:33 - loss: 2.8848 - regression_loss: 2.1718 - classification_loss: 0.7130
 795/1000 [======================>.......] - ETA: 1:32 - loss: 2.8812 - regression_loss: 2.1691 - classification_loss: 0.7121
 796/1000 [======================>.......] - ETA: 1:32 - loss: 2.8778 - regression_loss: 2.1663 - classification_loss: 0.7115
 797/1000 [======================>.......] - ETA: 1:31 - loss: 2.8742 - regression_loss: 2.1636 - classification_loss: 0.7106
 798/1000 [======================>.......] - ETA: 1:31 - loss: 2.8707 - regression_loss: 2.1609 - classification_loss: 0.7098
 799/1000 [======================>.......] - ETA: 1:30 - loss: 2.8699 - regression_loss: 2.1608 - classification_loss: 0.7091
 800/1000 [=======================>......] - ETA: 1:30 - loss: 2.8713 - regression_loss: 2.1614 - classification_loss: 0.7098
 801/1000 [=======================>......] - ETA: 1:30 - loss: 2.8713 - regression_loss: 2.1618 - classification_loss: 0.7095
 802/1000 [=======================>......] - ETA: 1:29 - loss: 2.8724 - regression_loss: 2.1628 - classification_loss: 0.7096
 803/1000 [=======================>......] - ETA: 1:29 - loss: 2.8743 - regression_loss: 2.1648 - classification_loss: 0.7095
 804/1000 [=======================>......] - ETA: 1:28 - loss: 2.8725 - regression_loss: 2.1636 - classification_loss: 0.7089
 805/1000 [=======================>......] - ETA: 1:28 - loss: 2.8753 - regression_loss: 2.1653 - classification_loss: 0.7099
 806/1000 [=======================>......] - ETA: 1:27 - loss: 2.8717 - regression_loss: 2.1627 - classification_loss: 0.7091
 807/1000 [=======================>......] - ETA: 1:27 - loss: 2.8710 - regression_loss: 2.1624 - classification_loss: 0.7086
 808/1000 [=======================>......] - ETA: 1:26 - loss: 2.8687 - regression_loss: 2.1598 - classification_loss: 0.7090
 809/1000 [=======================>......] - ETA: 1:26 - loss: 2.8717 - regression_loss: 2.1612 - classification_loss: 0.7105
 810/1000 [=======================>......] - ETA: 1:25 - loss: 2.8723 - regression_loss: 2.1617 - classification_loss: 0.7106
 811/1000 [=======================>......] - ETA: 1:25 - loss: 2.8716 - regression_loss: 2.1615 - classification_loss: 0.7102
 812/1000 [=======================>......] - ETA: 1:25 - loss: 2.8681 - regression_loss: 2.1588 - classification_loss: 0.7093
 813/1000 [=======================>......] - ETA: 1:24 - loss: 2.8712 - regression_loss: 2.1608 - classification_loss: 0.7104
 814/1000 [=======================>......] - ETA: 1:24 - loss: 2.8677 - regression_loss: 2.1581 - classification_loss: 0.7096
 815/1000 [=======================>......] - ETA: 1:23 - loss: 2.8721 - regression_loss: 2.1617 - classification_loss: 0.7105
 816/1000 [=======================>......] - ETA: 1:23 - loss: 2.8721 - regression_loss: 2.1618 - classification_loss: 0.7103
 817/1000 [=======================>......] - ETA: 1:22 - loss: 2.8731 - regression_loss: 2.1629 - classification_loss: 0.7102
 818/1000 [=======================>......] - ETA: 1:22 - loss: 2.8756 - regression_loss: 2.1647 - classification_loss: 0.7108
 819/1000 [=======================>......] - ETA: 1:21 - loss: 2.8794 - regression_loss: 2.1672 - classification_loss: 0.7121
 820/1000 [=======================>......] - ETA: 1:21 - loss: 2.8759 - regression_loss: 2.1646 - classification_loss: 0.7113
 821/1000 [=======================>......] - ETA: 1:20 - loss: 2.8771 - regression_loss: 2.1659 - classification_loss: 0.7113
 822/1000 [=======================>......] - ETA: 1:20 - loss: 2.8776 - regression_loss: 2.1664 - classification_loss: 0.7112
 823/1000 [=======================>......] - ETA: 1:20 - loss: 2.8741 - regression_loss: 2.1638 - classification_loss: 0.7103
 824/1000 [=======================>......] - ETA: 1:19 - loss: 2.8706 - regression_loss: 2.1611 - classification_loss: 0.7095
 825/1000 [=======================>......] - ETA: 1:19 - loss: 2.8676 - regression_loss: 2.1585 - classification_loss: 0.7091
 826/1000 [=======================>......] - ETA: 1:18 - loss: 2.8687 - regression_loss: 2.1596 - classification_loss: 0.7091
 827/1000 [=======================>......] - ETA: 1:18 - loss: 2.8708 - regression_loss: 2.1606 - classification_loss: 0.7102
 828/1000 [=======================>......] - ETA: 1:17 - loss: 2.8715 - regression_loss: 2.1616 - classification_loss: 0.7099
 829/1000 [=======================>......] - ETA: 1:17 - loss: 2.8739 - regression_loss: 2.1636 - classification_loss: 0.7104
 830/1000 [=======================>......] - ETA: 1:16 - loss: 2.8771 - regression_loss: 2.1650 - classification_loss: 0.7121
 831/1000 [=======================>......] - ETA: 1:16 - loss: 2.8737 - regression_loss: 2.1624 - classification_loss: 0.7112
 832/1000 [=======================>......] - ETA: 1:15 - loss: 2.8754 - regression_loss: 2.1638 - classification_loss: 0.7116
 833/1000 [=======================>......] - ETA: 1:15 - loss: 2.8775 - regression_loss: 2.1647 - classification_loss: 0.7128
 834/1000 [========================>.....] - ETA: 1:15 - loss: 2.8784 - regression_loss: 2.1654 - classification_loss: 0.7130
 835/1000 [========================>.....] - ETA: 1:14 - loss: 2.8749 - regression_loss: 2.1628 - classification_loss: 0.7122
 836/1000 [========================>.....] - ETA: 1:14 - loss: 2.8759 - regression_loss: 2.1641 - classification_loss: 0.7118
 837/1000 [========================>.....] - ETA: 1:13 - loss: 2.8774 - regression_loss: 2.1645 - classification_loss: 0.7128
 838/1000 [========================>.....] - ETA: 1:13 - loss: 2.8774 - regression_loss: 2.1647 - classification_loss: 0.7127
 839/1000 [========================>.....] - ETA: 1:12 - loss: 2.8798 - regression_loss: 2.1661 - classification_loss: 0.7138
 840/1000 [========================>.....] - ETA: 1:12 - loss: 2.8804 - regression_loss: 2.1666 - classification_loss: 0.7137
 841/1000 [========================>.....] - ETA: 1:11 - loss: 2.8807 - regression_loss: 2.1669 - classification_loss: 0.7139
 842/1000 [========================>.....] - ETA: 1:11 - loss: 2.8799 - regression_loss: 2.1662 - classification_loss: 0.7137
 843/1000 [========================>.....] - ETA: 1:11 - loss: 2.8812 - regression_loss: 2.1670 - classification_loss: 0.7142
 844/1000 [========================>.....] - ETA: 1:10 - loss: 2.8778 - regression_loss: 2.1645 - classification_loss: 0.7133
 845/1000 [========================>.....] - ETA: 1:10 - loss: 2.8744 - regression_loss: 2.1619 - classification_loss: 0.7125
 846/1000 [========================>.....] - ETA: 1:09 - loss: 2.8735 - regression_loss: 2.1612 - classification_loss: 0.7123
 847/1000 [========================>.....] - ETA: 1:09 - loss: 2.8738 - regression_loss: 2.1616 - classification_loss: 0.7123
 848/1000 [========================>.....] - ETA: 1:08 - loss: 2.8704 - regression_loss: 2.1590 - classification_loss: 0.7114
 849/1000 [========================>.....] - ETA: 1:08 - loss: 2.8671 - regression_loss: 2.1565 - classification_loss: 0.7106
 850/1000 [========================>.....] - ETA: 1:07 - loss: 2.8675 - regression_loss: 2.1572 - classification_loss: 0.7104
 851/1000 [========================>.....] - ETA: 1:07 - loss: 2.8678 - regression_loss: 2.1574 - classification_loss: 0.7104
 852/1000 [========================>.....] - ETA: 1:06 - loss: 2.8695 - regression_loss: 2.1587 - classification_loss: 0.7109
 853/1000 [========================>.....] - ETA: 1:06 - loss: 2.8707 - regression_loss: 2.1602 - classification_loss: 0.7105
 854/1000 [========================>.....] - ETA: 1:06 - loss: 2.8704 - regression_loss: 2.1603 - classification_loss: 0.7102
 855/1000 [========================>.....] - ETA: 1:05 - loss: 2.8671 - regression_loss: 2.1577 - classification_loss: 0.7094
 856/1000 [========================>.....] - ETA: 1:05 - loss: 2.8810 - regression_loss: 2.1598 - classification_loss: 0.7212
 857/1000 [========================>.....] - ETA: 1:04 - loss: 2.8819 - regression_loss: 2.1606 - classification_loss: 0.7212
 858/1000 [========================>.....] - ETA: 1:04 - loss: 2.8785 - regression_loss: 2.1581 - classification_loss: 0.7204
 859/1000 [========================>.....] - ETA: 1:03 - loss: 2.8817 - regression_loss: 2.1602 - classification_loss: 0.7215
 860/1000 [========================>.....] - ETA: 1:03 - loss: 2.8868 - regression_loss: 2.1634 - classification_loss: 0.7234
 861/1000 [========================>.....] - ETA: 1:02 - loss: 2.8878 - regression_loss: 2.1646 - classification_loss: 0.7232
 862/1000 [========================>.....] - ETA: 1:02 - loss: 2.8877 - regression_loss: 2.1644 - classification_loss: 0.7232
 863/1000 [========================>.....] - ETA: 1:01 - loss: 2.8844 - regression_loss: 2.1619 - classification_loss: 0.7225
 864/1000 [========================>.....] - ETA: 1:01 - loss: 2.8811 - regression_loss: 2.1594 - classification_loss: 0.7217
 865/1000 [========================>.....] - ETA: 1:01 - loss: 2.8813 - regression_loss: 2.1599 - classification_loss: 0.7214
 866/1000 [========================>.....] - ETA: 1:00 - loss: 2.8835 - regression_loss: 2.1617 - classification_loss: 0.7218
 867/1000 [=========================>....] - ETA: 1:00 - loss: 2.8838 - regression_loss: 2.1617 - classification_loss: 0.7221
 868/1000 [=========================>....] - ETA: 59s - loss: 2.8864 - regression_loss: 2.1629 - classification_loss: 0.7235 
 869/1000 [=========================>....] - ETA: 59s - loss: 2.8831 - regression_loss: 2.1604 - classification_loss: 0.7227
 870/1000 [=========================>....] - ETA: 58s - loss: 2.8828 - regression_loss: 2.1605 - classification_loss: 0.7223
 871/1000 [=========================>....] - ETA: 58s - loss: 2.8847 - regression_loss: 2.1623 - classification_loss: 0.7223
 872/1000 [=========================>....] - ETA: 57s - loss: 2.8848 - regression_loss: 2.1627 - classification_loss: 0.7221
 873/1000 [=========================>....] - ETA: 57s - loss: 2.8855 - regression_loss: 2.1635 - classification_loss: 0.7220
 874/1000 [=========================>....] - ETA: 56s - loss: 2.8893 - regression_loss: 2.1660 - classification_loss: 0.7233
 875/1000 [=========================>....] - ETA: 56s - loss: 2.8890 - regression_loss: 2.1659 - classification_loss: 0.7231
 876/1000 [=========================>....] - ETA: 56s - loss: 2.8857 - regression_loss: 2.1635 - classification_loss: 0.7222
 877/1000 [=========================>....] - ETA: 55s - loss: 2.8866 - regression_loss: 2.1645 - classification_loss: 0.7221
 878/1000 [=========================>....] - ETA: 55s - loss: 2.8833 - regression_loss: 2.1620 - classification_loss: 0.7213
 879/1000 [=========================>....] - ETA: 54s - loss: 2.8860 - regression_loss: 2.1644 - classification_loss: 0.7216
 880/1000 [=========================>....] - ETA: 54s - loss: 2.8878 - regression_loss: 2.1659 - classification_loss: 0.7219
 881/1000 [=========================>....] - ETA: 53s - loss: 2.8897 - regression_loss: 2.1676 - classification_loss: 0.7221
 882/1000 [=========================>....] - ETA: 53s - loss: 2.8902 - regression_loss: 2.1684 - classification_loss: 0.7218
 883/1000 [=========================>....] - ETA: 52s - loss: 2.8920 - regression_loss: 2.1698 - classification_loss: 0.7222
 884/1000 [=========================>....] - ETA: 52s - loss: 2.8951 - regression_loss: 2.1725 - classification_loss: 0.7225
 885/1000 [=========================>....] - ETA: 52s - loss: 2.8946 - regression_loss: 2.1725 - classification_loss: 0.7221
 886/1000 [=========================>....] - ETA: 51s - loss: 2.8949 - regression_loss: 2.1730 - classification_loss: 0.7219
 887/1000 [=========================>....] - ETA: 51s - loss: 2.8917 - regression_loss: 2.1705 - classification_loss: 0.7212
 888/1000 [=========================>....] - ETA: 50s - loss: 2.8884 - regression_loss: 2.1681 - classification_loss: 0.7203
 889/1000 [=========================>....] - ETA: 50s - loss: 2.8880 - regression_loss: 2.1680 - classification_loss: 0.7200
 890/1000 [=========================>....] - ETA: 49s - loss: 2.8897 - regression_loss: 2.1698 - classification_loss: 0.7199
 891/1000 [=========================>....] - ETA: 49s - loss: 2.8916 - regression_loss: 2.1718 - classification_loss: 0.7198
 892/1000 [=========================>....] - ETA: 48s - loss: 2.8884 - regression_loss: 2.1693 - classification_loss: 0.7190
 893/1000 [=========================>....] - ETA: 48s - loss: 2.8894 - regression_loss: 2.1705 - classification_loss: 0.7189
 894/1000 [=========================>....] - ETA: 47s - loss: 2.8910 - regression_loss: 2.1721 - classification_loss: 0.7190
 895/1000 [=========================>....] - ETA: 47s - loss: 2.8905 - regression_loss: 2.1719 - classification_loss: 0.7185
 896/1000 [=========================>....] - ETA: 47s - loss: 2.8906 - regression_loss: 2.1721 - classification_loss: 0.7185
 897/1000 [=========================>....] - ETA: 46s - loss: 2.8931 - regression_loss: 2.1745 - classification_loss: 0.7186
 898/1000 [=========================>....] - ETA: 46s - loss: 2.8899 - regression_loss: 2.1721 - classification_loss: 0.7178
 899/1000 [=========================>....] - ETA: 45s - loss: 2.8867 - regression_loss: 2.1697 - classification_loss: 0.7170
 900/1000 [==========================>...] - ETA: 45s - loss: 2.8835 - regression_loss: 2.1673 - classification_loss: 0.7162
 901/1000 [==========================>...] - ETA: 44s - loss: 2.8803 - regression_loss: 2.1649 - classification_loss: 0.7155
 902/1000 [==========================>...] - ETA: 44s - loss: 2.8823 - regression_loss: 2.1663 - classification_loss: 0.7160
 903/1000 [==========================>...] - ETA: 43s - loss: 2.8818 - regression_loss: 2.1662 - classification_loss: 0.7156
 904/1000 [==========================>...] - ETA: 43s - loss: 2.8814 - regression_loss: 2.1662 - classification_loss: 0.7152
 905/1000 [==========================>...] - ETA: 42s - loss: 2.8843 - regression_loss: 2.1676 - classification_loss: 0.7167
 906/1000 [==========================>...] - ETA: 42s - loss: 2.8847 - regression_loss: 2.1677 - classification_loss: 0.7170
 907/1000 [==========================>...] - ETA: 42s - loss: 2.8852 - regression_loss: 2.1682 - classification_loss: 0.7170
 908/1000 [==========================>...] - ETA: 41s - loss: 2.8854 - regression_loss: 2.1686 - classification_loss: 0.7168
 909/1000 [==========================>...] - ETA: 41s - loss: 2.8872 - regression_loss: 2.1689 - classification_loss: 0.7183
 910/1000 [==========================>...] - ETA: 40s - loss: 2.8898 - regression_loss: 2.1704 - classification_loss: 0.7194
 911/1000 [==========================>...] - ETA: 40s - loss: 2.8900 - regression_loss: 2.1704 - classification_loss: 0.7196
 912/1000 [==========================>...] - ETA: 39s - loss: 2.8906 - regression_loss: 2.1712 - classification_loss: 0.7194
 913/1000 [==========================>...] - ETA: 39s - loss: 2.8894 - regression_loss: 2.1706 - classification_loss: 0.7188
 914/1000 [==========================>...] - ETA: 38s - loss: 2.8893 - regression_loss: 2.1705 - classification_loss: 0.7188
 915/1000 [==========================>...] - ETA: 38s - loss: 2.8909 - regression_loss: 2.1710 - classification_loss: 0.7199
 916/1000 [==========================>...] - ETA: 37s - loss: 2.8925 - regression_loss: 2.1725 - classification_loss: 0.7201
 917/1000 [==========================>...] - ETA: 37s - loss: 2.8937 - regression_loss: 2.1735 - classification_loss: 0.7201
 918/1000 [==========================>...] - ETA: 37s - loss: 2.8953 - regression_loss: 2.1749 - classification_loss: 0.7203
 919/1000 [==========================>...] - ETA: 36s - loss: 2.8946 - regression_loss: 2.1747 - classification_loss: 0.7199
 920/1000 [==========================>...] - ETA: 36s - loss: 2.8915 - regression_loss: 2.1723 - classification_loss: 0.7192
 921/1000 [==========================>...] - ETA: 35s - loss: 2.8936 - regression_loss: 2.1741 - classification_loss: 0.7195
 922/1000 [==========================>...] - ETA: 35s - loss: 2.8947 - regression_loss: 2.1752 - classification_loss: 0.7194
 923/1000 [==========================>...] - ETA: 34s - loss: 2.8955 - regression_loss: 2.1761 - classification_loss: 0.7194
 924/1000 [==========================>...] - ETA: 34s - loss: 2.8964 - regression_loss: 2.1769 - classification_loss: 0.7195
 925/1000 [==========================>...] - ETA: 33s - loss: 2.8980 - regression_loss: 2.1783 - classification_loss: 0.7197
 926/1000 [==========================>...] - ETA: 33s - loss: 2.8970 - regression_loss: 2.1778 - classification_loss: 0.7192
 927/1000 [==========================>...] - ETA: 33s - loss: 2.8972 - regression_loss: 2.1783 - classification_loss: 0.7189
 928/1000 [==========================>...] - ETA: 32s - loss: 2.8981 - regression_loss: 2.1793 - classification_loss: 0.7188
 929/1000 [==========================>...] - ETA: 32s - loss: 2.8996 - regression_loss: 2.1806 - classification_loss: 0.7190
 930/1000 [==========================>...] - ETA: 31s - loss: 2.8964 - regression_loss: 2.1782 - classification_loss: 0.7182
 931/1000 [==========================>...] - ETA: 31s - loss: 2.8974 - regression_loss: 2.1786 - classification_loss: 0.7189
 932/1000 [==========================>...] - ETA: 30s - loss: 2.8970 - regression_loss: 2.1787 - classification_loss: 0.7183
 933/1000 [==========================>...] - ETA: 30s - loss: 2.8972 - regression_loss: 2.1791 - classification_loss: 0.7181
 934/1000 [===========================>..] - ETA: 29s - loss: 2.8987 - regression_loss: 2.1800 - classification_loss: 0.7187
 935/1000 [===========================>..] - ETA: 29s - loss: 2.8990 - regression_loss: 2.1806 - classification_loss: 0.7184
 936/1000 [===========================>..] - ETA: 28s - loss: 2.8992 - regression_loss: 2.1810 - classification_loss: 0.7183
 937/1000 [===========================>..] - ETA: 28s - loss: 2.8962 - regression_loss: 2.1787 - classification_loss: 0.7175
 938/1000 [===========================>..] - ETA: 28s - loss: 2.8931 - regression_loss: 2.1763 - classification_loss: 0.7167
 939/1000 [===========================>..] - ETA: 27s - loss: 2.8926 - regression_loss: 2.1761 - classification_loss: 0.7165
 940/1000 [===========================>..] - ETA: 27s - loss: 2.8940 - regression_loss: 2.1771 - classification_loss: 0.7168
 941/1000 [===========================>..] - ETA: 26s - loss: 2.8942 - regression_loss: 2.1776 - classification_loss: 0.7165
 942/1000 [===========================>..] - ETA: 26s - loss: 2.8935 - regression_loss: 2.1773 - classification_loss: 0.7161
 943/1000 [===========================>..] - ETA: 25s - loss: 2.8904 - regression_loss: 2.1750 - classification_loss: 0.7154
 944/1000 [===========================>..] - ETA: 25s - loss: 2.8880 - regression_loss: 2.1727 - classification_loss: 0.7152
 945/1000 [===========================>..] - ETA: 24s - loss: 2.8849 - regression_loss: 2.1704 - classification_loss: 0.7145
 946/1000 [===========================>..] - ETA: 24s - loss: 2.8860 - regression_loss: 2.1715 - classification_loss: 0.7145
 947/1000 [===========================>..] - ETA: 23s - loss: 2.8890 - regression_loss: 2.1732 - classification_loss: 0.7159
 948/1000 [===========================>..] - ETA: 23s - loss: 2.8910 - regression_loss: 2.1737 - classification_loss: 0.7172
 949/1000 [===========================>..] - ETA: 23s - loss: 2.8917 - regression_loss: 2.1744 - classification_loss: 0.7172
 950/1000 [===========================>..] - ETA: 22s - loss: 2.8928 - regression_loss: 2.1746 - classification_loss: 0.7182
 951/1000 [===========================>..] - ETA: 22s - loss: 2.8957 - regression_loss: 2.1763 - classification_loss: 0.7194
 952/1000 [===========================>..] - ETA: 21s - loss: 2.8955 - regression_loss: 2.1761 - classification_loss: 0.7193
 953/1000 [===========================>..] - ETA: 21s - loss: 2.8964 - regression_loss: 2.1762 - classification_loss: 0.7202
 954/1000 [===========================>..] - ETA: 20s - loss: 2.8971 - regression_loss: 2.1767 - classification_loss: 0.7203
 955/1000 [===========================>..] - ETA: 20s - loss: 2.8975 - regression_loss: 2.1774 - classification_loss: 0.7202
 956/1000 [===========================>..] - ETA: 19s - loss: 2.8976 - regression_loss: 2.1776 - classification_loss: 0.7200
 957/1000 [===========================>..] - ETA: 19s - loss: 2.8946 - regression_loss: 2.1753 - classification_loss: 0.7192
 958/1000 [===========================>..] - ETA: 18s - loss: 2.8961 - regression_loss: 2.1766 - classification_loss: 0.7195
 959/1000 [===========================>..] - ETA: 18s - loss: 2.8977 - regression_loss: 2.1772 - classification_loss: 0.7205
 960/1000 [===========================>..] - ETA: 18s - loss: 2.8947 - regression_loss: 2.1749 - classification_loss: 0.7197
 961/1000 [===========================>..] - ETA: 17s - loss: 2.8937 - regression_loss: 2.1742 - classification_loss: 0.7195
 962/1000 [===========================>..] - ETA: 17s - loss: 2.8958 - regression_loss: 2.1753 - classification_loss: 0.7205
 963/1000 [===========================>..] - ETA: 16s - loss: 2.8966 - regression_loss: 2.1753 - classification_loss: 0.7212
 964/1000 [===========================>..] - ETA: 16s - loss: 2.8971 - regression_loss: 2.1760 - classification_loss: 0.7211
 965/1000 [===========================>..] - ETA: 15s - loss: 2.8941 - regression_loss: 2.1737 - classification_loss: 0.7203
 966/1000 [===========================>..] - ETA: 15s - loss: 2.8911 - regression_loss: 2.1715 - classification_loss: 0.7196
 967/1000 [============================>.] - ETA: 14s - loss: 2.8917 - regression_loss: 2.1712 - classification_loss: 0.7204
 968/1000 [============================>.] - ETA: 14s - loss: 2.8938 - regression_loss: 2.1732 - classification_loss: 0.7206
 969/1000 [============================>.] - ETA: 14s - loss: 2.8942 - regression_loss: 2.1738 - classification_loss: 0.7204
 970/1000 [============================>.] - ETA: 13s - loss: 2.8913 - regression_loss: 2.1716 - classification_loss: 0.7198
 971/1000 [============================>.] - ETA: 13s - loss: 2.8883 - regression_loss: 2.1693 - classification_loss: 0.7190
 972/1000 [============================>.] - ETA: 12s - loss: 2.8895 - regression_loss: 2.1694 - classification_loss: 0.7202
 973/1000 [============================>.] - ETA: 12s - loss: 2.8906 - regression_loss: 2.1696 - classification_loss: 0.7210
 974/1000 [============================>.] - ETA: 11s - loss: 2.8876 - regression_loss: 2.1674 - classification_loss: 0.7202
 975/1000 [============================>.] - ETA: 11s - loss: 2.8877 - regression_loss: 2.1676 - classification_loss: 0.7201
 976/1000 [============================>.] - ETA: 10s - loss: 2.8884 - regression_loss: 2.1686 - classification_loss: 0.7198
 977/1000 [============================>.] - ETA: 10s - loss: 2.8915 - regression_loss: 2.1708 - classification_loss: 0.7207
 978/1000 [============================>.] - ETA: 9s - loss: 2.8929 - regression_loss: 2.1721 - classification_loss: 0.7208 
 979/1000 [============================>.] - ETA: 9s - loss: 2.8899 - regression_loss: 2.1698 - classification_loss: 0.7201
 980/1000 [============================>.] - ETA: 9s - loss: 2.8910 - regression_loss: 2.1710 - classification_loss: 0.7200
 981/1000 [============================>.] - ETA: 8s - loss: 2.8942 - regression_loss: 2.1738 - classification_loss: 0.7204
 982/1000 [============================>.] - ETA: 8s - loss: 2.8937 - regression_loss: 2.1733 - classification_loss: 0.7204
 983/1000 [============================>.] - ETA: 7s - loss: 2.8908 - regression_loss: 2.1711 - classification_loss: 0.7197
 984/1000 [============================>.] - ETA: 7s - loss: 2.8920 - regression_loss: 2.1722 - classification_loss: 0.7198
 985/1000 [============================>.] - ETA: 6s - loss: 2.8920 - regression_loss: 2.1727 - classification_loss: 0.7193
 986/1000 [============================>.] - ETA: 6s - loss: 2.8924 - regression_loss: 2.1731 - classification_loss: 0.7192
 987/1000 [============================>.] - ETA: 5s - loss: 2.8936 - regression_loss: 2.1740 - classification_loss: 0.7196
 988/1000 [============================>.] - ETA: 5s - loss: 2.8940 - regression_loss: 2.1747 - classification_loss: 0.7194
 989/1000 [============================>.] - ETA: 4s - loss: 2.8939 - regression_loss: 2.1747 - classification_loss: 0.7192
 990/1000 [============================>.] - ETA: 4s - loss: 2.8944 - regression_loss: 2.1753 - classification_loss: 0.7191
 991/1000 [============================>.] - ETA: 4s - loss: 2.8954 - regression_loss: 2.1765 - classification_loss: 0.7189
 992/1000 [============================>.] - ETA: 3s - loss: 2.8960 - regression_loss: 2.1774 - classification_loss: 0.7186
 993/1000 [============================>.] - ETA: 3s - loss: 2.8974 - regression_loss: 2.1782 - classification_loss: 0.7193
 994/1000 [============================>.] - ETA: 2s - loss: 2.8988 - regression_loss: 2.1790 - classification_loss: 0.7199
 995/1000 [============================>.] - ETA: 2s - loss: 2.8991 - regression_loss: 2.1795 - classification_loss: 0.7196
 996/1000 [============================>.] - ETA: 1s - loss: 2.9002 - regression_loss: 2.1800 - classification_loss: 0.7202
 997/1000 [============================>.] - ETA: 1s - loss: 2.8995 - regression_loss: 2.1796 - classification_loss: 0.7200
 998/1000 [============================>.] - ETA: 0s - loss: 2.8966 - regression_loss: 2.1774 - classification_loss: 0.7193
 999/1000 [============================>.] - ETA: 0s - loss: 2.8958 - regression_loss: 2.1768 - classification_loss: 0.7190
1000/1000 [==============================] - 452s 452ms/step - loss: 2.8962 - regression_loss: 2.1776 - classification_loss: 0.7187

Epoch 00014: saving model to ./snapshots/resnet50_csv_14.h5
0/2001/2002/2003/2004/2005/2006/2007/2008/2009/20010/20011/20012/20013/20014/20015/20016/20017/20018/20019/20020/20021/20022/20023/20024/20025/20026/20027/20028/20029/20030/20031/20032/20033/20034/20035/20036/20037/20038/20039/20040/20041/20042/20043/20044/20045/20046/20047/20048/20049/20050/20051/20052/20053/20054/20055/20056/20057/20058/20059/20060/20061/20062/20063/20064/20065/20066/20067/20068/20069/20070/20071/20072/20073/20074/20075/20076/20077/20078/20079/20080/20081/20082/20083/20084/20085/20086/20087/20088/20089/20090/20091/20092/20093/20094/20095/20096/20097/20098/20099/200100/200101/200102/200103/200104/200105/200106/200107/200108/200109/200110/200111/200112/200113/200114/200115/200116/200117/200118/200119/200120/200121/200122/200123/200124/200125/200126/200127/200128/200129/200130/200131/200132/200133/200134/200135/200136/200137/200138/200139/200140/200141/200142/200143/200144/200145/200146/200147/200148/200149/200150/200151/200152/200153/200154/200155/200156/200157/200158/200159/200160/200161/200162/200163/200164/200165/200166/200167/200168/200169/200170/200171/200172/200173/200174/200175/200176/200177/200178/200179/200180/200181/200182/200183/200184/200185/200186/200187/200188/200189/200190/200191/200192/200193/200194/200195/200196/200197/200198/200199/2000/2001/2002/2003/2004/2005/2006/2007/2008/2009/20010/20011/20012/20013/20014/20015/20016/20017/20018/20019/20020/20021/20022/20023/20024/20025/20026/20027/20028/20029/20030/20031/20032/20033/20034/20035/20036/20037/20038/20039/20040/20041/20042/20043/20044/20045/20046/20047/20048/20049/20050/20051/20052/20053/20054/20055/20056/20057/20058/20059/20060/20061/20062/20063/20064/20065/20066/20067/20068/20069/20070/20071/20072/20073/20074/20075/20076/20077/20078/20079/20080/20081/20082/20083/20084/20085/20086/20087/20088/20089/20090/20091/20092/20093/20094/20095/20096/20097/20098/20099/200100/200101/200102/200103/200104/200105/200106/200107/200108/200109/200110/200111/200112/200113/200114/200115/200116/200117/200118/200119/200120/200121/200122/200123/200124/200125/200126/200127/200128/200129/200130/200131/200132/200133/200134/200135/200136/200137/200138/200139/200140/200141/200142/200143/200144/200145/200146/200147/200148/200149/200150/200151/200152/200153/200154/200155/200156/200157/200158/200159/200160/200161/200162/200163/200164/200165/200166/200167/200168/200169/200170/200171/200172/200173/200174/200175/200176/200177/200178/200179/200180/200181/200182/200183/200184/200185/200186/200187/200188/200189/200190/200191/200192/200193/200194/200195/200196/200197/200198/200199/200

M 0.1266
N 0.0092
mAP: 0.0679
Epoch 15/30

   1/1000 [..............................] - ETA: 7:22 - loss: 1.7337 - regression_loss: 1.3936 - classification_loss: 0.3402
   2/1000 [..............................] - ETA: 7:25 - loss: 2.1356 - regression_loss: 1.6172 - classification_loss: 0.5185
   3/1000 [..............................] - ETA: 7:25 - loss: 2.8917 - regression_loss: 2.3217 - classification_loss: 0.5700
   4/1000 [..............................] - ETA: 7:25 - loss: 2.1689 - regression_loss: 1.7413 - classification_loss: 0.4276
   5/1000 [..............................] - ETA: 7:28 - loss: 2.2257 - regression_loss: 1.7944 - classification_loss: 0.4313
   6/1000 [..............................] - ETA: 7:28 - loss: 1.8547 - regression_loss: 1.4953 - classification_loss: 0.3594
   7/1000 [..............................] - ETA: 7:28 - loss: 2.1093 - regression_loss: 1.7355 - classification_loss: 0.3737
   8/1000 [..............................] - ETA: 7:27 - loss: 2.2550 - regression_loss: 1.8135 - classification_loss: 0.4415
   9/1000 [..............................] - ETA: 7:27 - loss: 2.2786 - regression_loss: 1.8294 - classification_loss: 0.4493
  10/1000 [..............................] - ETA: 7:27 - loss: 2.4403 - regression_loss: 1.9514 - classification_loss: 0.4889
  11/1000 [..............................] - ETA: 7:27 - loss: 2.2185 - regression_loss: 1.7740 - classification_loss: 0.4445
  12/1000 [..............................] - ETA: 7:26 - loss: 2.2780 - regression_loss: 1.8331 - classification_loss: 0.4449
  13/1000 [..............................] - ETA: 7:25 - loss: 2.3512 - regression_loss: 1.8977 - classification_loss: 0.4535
  14/1000 [..............................] - ETA: 7:25 - loss: 2.3637 - regression_loss: 1.9171 - classification_loss: 0.4466
  15/1000 [..............................] - ETA: 7:24 - loss: 2.4196 - regression_loss: 1.9735 - classification_loss: 0.4461
  16/1000 [..............................] - ETA: 7:24 - loss: 2.2683 - regression_loss: 1.8501 - classification_loss: 0.4182
  17/1000 [..............................] - ETA: 7:24 - loss: 2.2477 - regression_loss: 1.8426 - classification_loss: 0.4051
  18/1000 [..............................] - ETA: 7:23 - loss: 2.3455 - regression_loss: 1.9283 - classification_loss: 0.4172
  19/1000 [..............................] - ETA: 7:23 - loss: 2.4485 - regression_loss: 2.0092 - classification_loss: 0.4394
  20/1000 [..............................] - ETA: 7:22 - loss: 2.4234 - regression_loss: 1.9969 - classification_loss: 0.4264
  21/1000 [..............................] - ETA: 7:22 - loss: 2.5032 - regression_loss: 2.0289 - classification_loss: 0.4743
  22/1000 [..............................] - ETA: 7:21 - loss: 2.6959 - regression_loss: 2.1461 - classification_loss: 0.5498
  23/1000 [..............................] - ETA: 7:21 - loss: 2.8249 - regression_loss: 2.2309 - classification_loss: 0.5940
  24/1000 [..............................] - ETA: 7:21 - loss: 2.7072 - regression_loss: 2.1379 - classification_loss: 0.5693
  25/1000 [..............................] - ETA: 7:20 - loss: 2.6975 - regression_loss: 2.1408 - classification_loss: 0.5566
  26/1000 [..............................] - ETA: 7:20 - loss: 2.6058 - regression_loss: 2.0585 - classification_loss: 0.5473
  27/1000 [..............................] - ETA: 7:19 - loss: 2.6409 - regression_loss: 2.0789 - classification_loss: 0.5620
  28/1000 [..............................] - ETA: 7:19 - loss: 2.5466 - regression_loss: 2.0047 - classification_loss: 0.5419
  29/1000 [..............................] - ETA: 7:19 - loss: 2.4588 - regression_loss: 1.9355 - classification_loss: 0.5233
  30/1000 [..............................] - ETA: 7:17 - loss: 2.4752 - regression_loss: 1.9587 - classification_loss: 0.5165
  31/1000 [..............................] - ETA: 7:16 - loss: 2.5564 - regression_loss: 2.0015 - classification_loss: 0.5550
  32/1000 [..............................] - ETA: 7:16 - loss: 2.4766 - regression_loss: 1.9389 - classification_loss: 0.5377
  33/1000 [..............................] - ETA: 7:15 - loss: 2.4831 - regression_loss: 1.9545 - classification_loss: 0.5286
  34/1000 [>.............................] - ETA: 7:15 - loss: 2.5231 - regression_loss: 1.9883 - classification_loss: 0.5349
  35/1000 [>.............................] - ETA: 7:15 - loss: 2.5854 - regression_loss: 2.0323 - classification_loss: 0.5531
  36/1000 [>.............................] - ETA: 7:14 - loss: 2.5136 - regression_loss: 1.9758 - classification_loss: 0.5378
  37/1000 [>.............................] - ETA: 7:14 - loss: 2.5931 - regression_loss: 2.0282 - classification_loss: 0.5649
  38/1000 [>.............................] - ETA: 7:13 - loss: 2.5988 - regression_loss: 2.0370 - classification_loss: 0.5618
  39/1000 [>.............................] - ETA: 7:12 - loss: 2.6410 - regression_loss: 2.0770 - classification_loss: 0.5640
  40/1000 [>.............................] - ETA: 7:12 - loss: 2.6547 - regression_loss: 2.0983 - classification_loss: 0.5563
  41/1000 [>.............................] - ETA: 7:11 - loss: 2.6454 - regression_loss: 2.0973 - classification_loss: 0.5481
  42/1000 [>.............................] - ETA: 7:11 - loss: 2.6775 - regression_loss: 2.1179 - classification_loss: 0.5597
  43/1000 [>.............................] - ETA: 7:10 - loss: 2.6914 - regression_loss: 2.1273 - classification_loss: 0.5642
  44/1000 [>.............................] - ETA: 7:10 - loss: 2.7259 - regression_loss: 2.1366 - classification_loss: 0.5894
  45/1000 [>.............................] - ETA: 7:10 - loss: 2.7820 - regression_loss: 2.1673 - classification_loss: 0.6148
  46/1000 [>.............................] - ETA: 7:09 - loss: 2.7361 - regression_loss: 2.1202 - classification_loss: 0.6160
  47/1000 [>.............................] - ETA: 7:09 - loss: 2.6823 - regression_loss: 2.0751 - classification_loss: 0.6073
  48/1000 [>.............................] - ETA: 7:09 - loss: 2.7101 - regression_loss: 2.1018 - classification_loss: 0.6082
  49/1000 [>.............................] - ETA: 7:08 - loss: 2.7614 - regression_loss: 2.1355 - classification_loss: 0.6259
  50/1000 [>.............................] - ETA: 7:08 - loss: 2.7062 - regression_loss: 2.0928 - classification_loss: 0.6134
  51/1000 [>.............................] - ETA: 7:08 - loss: 2.7155 - regression_loss: 2.0991 - classification_loss: 0.6164
  52/1000 [>.............................] - ETA: 7:07 - loss: 2.7221 - regression_loss: 2.1060 - classification_loss: 0.6161
  53/1000 [>.............................] - ETA: 7:07 - loss: 2.7134 - regression_loss: 2.0929 - classification_loss: 0.6205
  54/1000 [>.............................] - ETA: 7:06 - loss: 2.6631 - regression_loss: 2.0542 - classification_loss: 0.6090
  55/1000 [>.............................] - ETA: 7:06 - loss: 2.6624 - regression_loss: 2.0509 - classification_loss: 0.6115
  56/1000 [>.............................] - ETA: 7:06 - loss: 2.7028 - regression_loss: 2.0714 - classification_loss: 0.6315
  57/1000 [>.............................] - ETA: 7:05 - loss: 2.7037 - regression_loss: 2.0767 - classification_loss: 0.6270
  58/1000 [>.............................] - ETA: 7:05 - loss: 2.6572 - regression_loss: 2.0409 - classification_loss: 0.6163
  59/1000 [>.............................] - ETA: 7:04 - loss: 2.6628 - regression_loss: 2.0442 - classification_loss: 0.6187
  60/1000 [>.............................] - ETA: 7:04 - loss: 2.6880 - regression_loss: 2.0707 - classification_loss: 0.6173
  61/1000 [>.............................] - ETA: 7:04 - loss: 2.6979 - regression_loss: 2.0718 - classification_loss: 0.6260
  62/1000 [>.............................] - ETA: 7:03 - loss: 2.6954 - regression_loss: 2.0676 - classification_loss: 0.6278
  63/1000 [>.............................] - ETA: 7:03 - loss: 2.7665 - regression_loss: 2.1144 - classification_loss: 0.6522
  64/1000 [>.............................] - ETA: 7:02 - loss: 2.7591 - regression_loss: 2.1088 - classification_loss: 0.6503
  65/1000 [>.............................] - ETA: 7:02 - loss: 2.7659 - regression_loss: 2.1132 - classification_loss: 0.6527
  66/1000 [>.............................] - ETA: 7:01 - loss: 2.7239 - regression_loss: 2.0812 - classification_loss: 0.6428
  67/1000 [=>............................] - ETA: 7:01 - loss: 2.7335 - regression_loss: 2.0879 - classification_loss: 0.6457
  68/1000 [=>............................] - ETA: 7:01 - loss: 2.7266 - regression_loss: 2.0833 - classification_loss: 0.6433
  69/1000 [=>............................] - ETA: 7:00 - loss: 2.7358 - regression_loss: 2.0916 - classification_loss: 0.6442
  70/1000 [=>............................] - ETA: 7:00 - loss: 2.7295 - regression_loss: 2.0848 - classification_loss: 0.6447
  71/1000 [=>............................] - ETA: 6:59 - loss: 2.6911 - regression_loss: 2.0555 - classification_loss: 0.6357
  72/1000 [=>............................] - ETA: 6:59 - loss: 2.7029 - regression_loss: 2.0670 - classification_loss: 0.6359
  73/1000 [=>............................] - ETA: 6:59 - loss: 2.6659 - regression_loss: 2.0387 - classification_loss: 0.6272
  74/1000 [=>............................] - ETA: 6:58 - loss: 2.7008 - regression_loss: 2.0698 - classification_loss: 0.6311
  75/1000 [=>............................] - ETA: 6:58 - loss: 2.6648 - regression_loss: 2.0422 - classification_loss: 0.6227
  76/1000 [=>............................] - ETA: 6:57 - loss: 2.6568 - regression_loss: 2.0380 - classification_loss: 0.6188
  77/1000 [=>............................] - ETA: 6:57 - loss: 2.6688 - regression_loss: 2.0400 - classification_loss: 0.6289
  78/1000 [=>............................] - ETA: 6:57 - loss: 2.6346 - regression_loss: 2.0138 - classification_loss: 0.6208
  79/1000 [=>............................] - ETA: 6:56 - loss: 2.6013 - regression_loss: 1.9883 - classification_loss: 0.6130
  80/1000 [=>............................] - ETA: 6:55 - loss: 2.5688 - regression_loss: 1.9635 - classification_loss: 0.6053
  81/1000 [=>............................] - ETA: 6:55 - loss: 2.5872 - regression_loss: 1.9800 - classification_loss: 0.6071
  82/1000 [=>............................] - ETA: 6:54 - loss: 2.6295 - regression_loss: 2.0071 - classification_loss: 0.6223
  83/1000 [=>............................] - ETA: 6:54 - loss: 2.6190 - regression_loss: 1.9830 - classification_loss: 0.6360
  84/1000 [=>............................] - ETA: 6:53 - loss: 2.6350 - regression_loss: 1.9931 - classification_loss: 0.6419
  85/1000 [=>............................] - ETA: 6:53 - loss: 2.6492 - regression_loss: 2.0037 - classification_loss: 0.6455
  86/1000 [=>............................] - ETA: 6:53 - loss: 2.6732 - regression_loss: 2.0144 - classification_loss: 0.6588
  87/1000 [=>............................] - ETA: 6:52 - loss: 2.6805 - regression_loss: 2.0199 - classification_loss: 0.6606
  88/1000 [=>............................] - ETA: 6:52 - loss: 2.7061 - regression_loss: 2.0388 - classification_loss: 0.6673
  89/1000 [=>............................] - ETA: 6:51 - loss: 2.7204 - regression_loss: 2.0159 - classification_loss: 0.7045
  90/1000 [=>............................] - ETA: 6:51 - loss: 2.7489 - regression_loss: 2.0320 - classification_loss: 0.7170
  91/1000 [=>............................] - ETA: 6:50 - loss: 2.7187 - regression_loss: 2.0096 - classification_loss: 0.7091
  92/1000 [=>............................] - ETA: 6:50 - loss: 2.7212 - regression_loss: 2.0063 - classification_loss: 0.7148
  93/1000 [=>............................] - ETA: 6:50 - loss: 2.7241 - regression_loss: 2.0132 - classification_loss: 0.7110
  94/1000 [=>............................] - ETA: 6:49 - loss: 2.7427 - regression_loss: 2.0260 - classification_loss: 0.7167
  95/1000 [=>............................] - ETA: 6:49 - loss: 2.7391 - regression_loss: 2.0253 - classification_loss: 0.7138
  96/1000 [=>............................] - ETA: 6:48 - loss: 2.7106 - regression_loss: 2.0042 - classification_loss: 0.7064
  97/1000 [=>............................] - ETA: 6:48 - loss: 2.7101 - regression_loss: 2.0062 - classification_loss: 0.7039
  98/1000 [=>............................] - ETA: 6:47 - loss: 2.7223 - regression_loss: 2.0196 - classification_loss: 0.7026
  99/1000 [=>............................] - ETA: 6:47 - loss: 2.7124 - regression_loss: 2.0132 - classification_loss: 0.6992
 100/1000 [==>...........................] - ETA: 6:46 - loss: 2.7147 - regression_loss: 2.0188 - classification_loss: 0.6959
 101/1000 [==>...........................] - ETA: 6:46 - loss: 2.7500 - regression_loss: 2.0443 - classification_loss: 0.7058
 102/1000 [==>...........................] - ETA: 6:45 - loss: 2.7544 - regression_loss: 2.0490 - classification_loss: 0.7054
 103/1000 [==>...........................] - ETA: 6:45 - loss: 2.7805 - regression_loss: 2.0686 - classification_loss: 0.7119
 104/1000 [==>...........................] - ETA: 6:44 - loss: 2.7538 - regression_loss: 2.0487 - classification_loss: 0.7051
 105/1000 [==>...........................] - ETA: 6:44 - loss: 2.7598 - regression_loss: 2.0574 - classification_loss: 0.7024
 106/1000 [==>...........................] - ETA: 6:44 - loss: 2.7672 - regression_loss: 2.0607 - classification_loss: 0.7065
 107/1000 [==>...........................] - ETA: 6:43 - loss: 2.7660 - regression_loss: 2.0634 - classification_loss: 0.7026
 108/1000 [==>...........................] - ETA: 6:43 - loss: 2.7868 - regression_loss: 2.0803 - classification_loss: 0.7065
 109/1000 [==>...........................] - ETA: 6:42 - loss: 2.8187 - regression_loss: 2.1028 - classification_loss: 0.7158
 110/1000 [==>...........................] - ETA: 6:41 - loss: 2.8366 - regression_loss: 2.1176 - classification_loss: 0.7190
 111/1000 [==>...........................] - ETA: 6:41 - loss: 2.8311 - regression_loss: 2.1157 - classification_loss: 0.7154
 112/1000 [==>...........................] - ETA: 6:41 - loss: 2.8339 - regression_loss: 2.1200 - classification_loss: 0.7138
 113/1000 [==>...........................] - ETA: 6:40 - loss: 2.8088 - regression_loss: 2.1013 - classification_loss: 0.7075
 114/1000 [==>...........................] - ETA: 6:40 - loss: 2.8203 - regression_loss: 2.1132 - classification_loss: 0.7071
 115/1000 [==>...........................] - ETA: 6:39 - loss: 2.8252 - regression_loss: 2.1198 - classification_loss: 0.7054
 116/1000 [==>...........................] - ETA: 6:39 - loss: 2.8008 - regression_loss: 2.1015 - classification_loss: 0.6993
 117/1000 [==>...........................] - ETA: 6:38 - loss: 2.7769 - regression_loss: 2.0836 - classification_loss: 0.6933
 118/1000 [==>...........................] - ETA: 6:38 - loss: 2.7534 - regression_loss: 2.0659 - classification_loss: 0.6874
 119/1000 [==>...........................] - ETA: 6:37 - loss: 2.7546 - regression_loss: 2.0647 - classification_loss: 0.6899
 120/1000 [==>...........................] - ETA: 6:37 - loss: 2.7544 - regression_loss: 2.0644 - classification_loss: 0.6900
 121/1000 [==>...........................] - ETA: 6:37 - loss: 2.7524 - regression_loss: 2.0649 - classification_loss: 0.6875
 122/1000 [==>...........................] - ETA: 6:36 - loss: 2.7615 - regression_loss: 2.0734 - classification_loss: 0.6881
 123/1000 [==>...........................] - ETA: 6:36 - loss: 2.7390 - regression_loss: 2.0565 - classification_loss: 0.6825
 124/1000 [==>...........................] - ETA: 6:35 - loss: 2.7480 - regression_loss: 2.0620 - classification_loss: 0.6860
 125/1000 [==>...........................] - ETA: 6:35 - loss: 2.7573 - regression_loss: 2.0687 - classification_loss: 0.6886
 126/1000 [==>...........................] - ETA: 6:34 - loss: 2.7591 - regression_loss: 2.0667 - classification_loss: 0.6924
 127/1000 [==>...........................] - ETA: 6:34 - loss: 2.7649 - regression_loss: 2.0737 - classification_loss: 0.6912
 128/1000 [==>...........................] - ETA: 6:34 - loss: 2.7592 - regression_loss: 2.0705 - classification_loss: 0.6888
 129/1000 [==>...........................] - ETA: 6:33 - loss: 2.7770 - regression_loss: 2.0850 - classification_loss: 0.6920
 130/1000 [==>...........................] - ETA: 6:33 - loss: 2.7901 - regression_loss: 2.0905 - classification_loss: 0.6996
 131/1000 [==>...........................] - ETA: 6:32 - loss: 2.7903 - regression_loss: 2.0897 - classification_loss: 0.7006
 132/1000 [==>...........................] - ETA: 6:32 - loss: 2.7853 - regression_loss: 2.0867 - classification_loss: 0.6986
 133/1000 [==>...........................] - ETA: 6:31 - loss: 2.7889 - regression_loss: 2.0928 - classification_loss: 0.6962
 134/1000 [===>..........................] - ETA: 6:31 - loss: 2.7681 - regression_loss: 2.0772 - classification_loss: 0.6910
 135/1000 [===>..........................] - ETA: 6:31 - loss: 2.7674 - regression_loss: 2.0785 - classification_loss: 0.6889
 136/1000 [===>..........................] - ETA: 6:30 - loss: 2.7640 - regression_loss: 2.0747 - classification_loss: 0.6893
 137/1000 [===>..........................] - ETA: 6:29 - loss: 2.7438 - regression_loss: 2.0596 - classification_loss: 0.6843
 138/1000 [===>..........................] - ETA: 6:29 - loss: 2.7493 - regression_loss: 2.0650 - classification_loss: 0.6843
 139/1000 [===>..........................] - ETA: 6:28 - loss: 2.7621 - regression_loss: 2.0797 - classification_loss: 0.6823
 140/1000 [===>..........................] - ETA: 6:28 - loss: 2.7426 - regression_loss: 2.0649 - classification_loss: 0.6777
 141/1000 [===>..........................] - ETA: 6:28 - loss: 2.7295 - regression_loss: 2.0502 - classification_loss: 0.6793
 142/1000 [===>..........................] - ETA: 6:27 - loss: 2.7103 - regression_loss: 2.0358 - classification_loss: 0.6745
 143/1000 [===>..........................] - ETA: 6:27 - loss: 2.7141 - regression_loss: 2.0408 - classification_loss: 0.6732
 144/1000 [===>..........................] - ETA: 6:26 - loss: 2.7102 - regression_loss: 2.0389 - classification_loss: 0.6713
 145/1000 [===>..........................] - ETA: 6:26 - loss: 2.7134 - regression_loss: 2.0417 - classification_loss: 0.6717
 146/1000 [===>..........................] - ETA: 6:25 - loss: 2.7075 - regression_loss: 2.0373 - classification_loss: 0.6701
 147/1000 [===>..........................] - ETA: 6:25 - loss: 2.7168 - regression_loss: 2.0427 - classification_loss: 0.6740
 148/1000 [===>..........................] - ETA: 6:24 - loss: 2.6984 - regression_loss: 2.0289 - classification_loss: 0.6694
 149/1000 [===>..........................] - ETA: 6:24 - loss: 2.6803 - regression_loss: 2.0153 - classification_loss: 0.6650
 150/1000 [===>..........................] - ETA: 6:24 - loss: 2.6752 - regression_loss: 2.0123 - classification_loss: 0.6629
 151/1000 [===>..........................] - ETA: 6:23 - loss: 2.6822 - regression_loss: 2.0182 - classification_loss: 0.6640
 152/1000 [===>..........................] - ETA: 6:23 - loss: 2.6674 - regression_loss: 2.0050 - classification_loss: 0.6625
 153/1000 [===>..........................] - ETA: 6:22 - loss: 2.6642 - regression_loss: 2.0041 - classification_loss: 0.6601
 154/1000 [===>..........................] - ETA: 6:22 - loss: 2.6732 - regression_loss: 2.0111 - classification_loss: 0.6621
 155/1000 [===>..........................] - ETA: 6:21 - loss: 2.6787 - regression_loss: 2.0128 - classification_loss: 0.6658
 156/1000 [===>..........................] - ETA: 6:21 - loss: 2.6762 - regression_loss: 2.0115 - classification_loss: 0.6646
 157/1000 [===>..........................] - ETA: 6:20 - loss: 2.6591 - regression_loss: 1.9987 - classification_loss: 0.6604
 158/1000 [===>..........................] - ETA: 6:20 - loss: 2.6423 - regression_loss: 1.9861 - classification_loss: 0.6562
 159/1000 [===>..........................] - ETA: 6:20 - loss: 2.6257 - regression_loss: 1.9736 - classification_loss: 0.6521
 160/1000 [===>..........................] - ETA: 6:19 - loss: 2.6274 - regression_loss: 1.9762 - classification_loss: 0.6512
 161/1000 [===>..........................] - ETA: 6:19 - loss: 2.6379 - regression_loss: 1.9784 - classification_loss: 0.6595
 162/1000 [===>..........................] - ETA: 6:18 - loss: 2.6394 - regression_loss: 1.9777 - classification_loss: 0.6618
 163/1000 [===>..........................] - ETA: 6:18 - loss: 2.6642 - regression_loss: 2.0021 - classification_loss: 0.6621
 164/1000 [===>..........................] - ETA: 6:17 - loss: 2.6662 - regression_loss: 2.0047 - classification_loss: 0.6615
 165/1000 [===>..........................] - ETA: 6:17 - loss: 2.6687 - regression_loss: 2.0083 - classification_loss: 0.6603
 166/1000 [===>..........................] - ETA: 6:16 - loss: 2.6526 - regression_loss: 1.9962 - classification_loss: 0.6564
 167/1000 [====>.........................] - ETA: 6:16 - loss: 2.6584 - regression_loss: 2.0014 - classification_loss: 0.6569
 168/1000 [====>.........................] - ETA: 6:16 - loss: 2.6551 - regression_loss: 1.9986 - classification_loss: 0.6565
 169/1000 [====>.........................] - ETA: 6:15 - loss: 2.6650 - regression_loss: 2.0016 - classification_loss: 0.6634
 170/1000 [====>.........................] - ETA: 6:15 - loss: 2.6698 - regression_loss: 2.0034 - classification_loss: 0.6664
 171/1000 [====>.........................] - ETA: 6:14 - loss: 2.6682 - regression_loss: 2.0042 - classification_loss: 0.6640
 172/1000 [====>.........................] - ETA: 6:14 - loss: 2.6700 - regression_loss: 2.0057 - classification_loss: 0.6644
 173/1000 [====>.........................] - ETA: 6:13 - loss: 2.6737 - regression_loss: 2.0079 - classification_loss: 0.6658
 174/1000 [====>.........................] - ETA: 6:13 - loss: 2.6824 - regression_loss: 2.0162 - classification_loss: 0.6663
 175/1000 [====>.........................] - ETA: 6:12 - loss: 2.6671 - regression_loss: 2.0046 - classification_loss: 0.6625
 176/1000 [====>.........................] - ETA: 6:12 - loss: 2.6699 - regression_loss: 2.0051 - classification_loss: 0.6648
 177/1000 [====>.........................] - ETA: 6:11 - loss: 2.6686 - regression_loss: 2.0055 - classification_loss: 0.6631
 178/1000 [====>.........................] - ETA: 6:11 - loss: 2.6697 - regression_loss: 2.0068 - classification_loss: 0.6629
 179/1000 [====>.........................] - ETA: 6:10 - loss: 2.6674 - regression_loss: 2.0073 - classification_loss: 0.6602
 180/1000 [====>.........................] - ETA: 6:10 - loss: 2.6741 - regression_loss: 2.0126 - classification_loss: 0.6614
 181/1000 [====>.........................] - ETA: 6:10 - loss: 2.6593 - regression_loss: 2.0015 - classification_loss: 0.6578
 182/1000 [====>.........................] - ETA: 6:09 - loss: 2.6628 - regression_loss: 2.0015 - classification_loss: 0.6613
 183/1000 [====>.........................] - ETA: 6:09 - loss: 2.6720 - regression_loss: 2.0087 - classification_loss: 0.6632
 184/1000 [====>.........................] - ETA: 6:08 - loss: 2.6806 - regression_loss: 2.0176 - classification_loss: 0.6630
 185/1000 [====>.........................] - ETA: 6:08 - loss: 2.6934 - regression_loss: 2.0256 - classification_loss: 0.6679
 186/1000 [====>.........................] - ETA: 6:07 - loss: 2.6790 - regression_loss: 2.0147 - classification_loss: 0.6643
 187/1000 [====>.........................] - ETA: 6:07 - loss: 2.6777 - regression_loss: 2.0151 - classification_loss: 0.6626
 188/1000 [====>.........................] - ETA: 6:06 - loss: 2.6918 - regression_loss: 2.0246 - classification_loss: 0.6671
 189/1000 [====>.........................] - ETA: 6:06 - loss: 2.6775 - regression_loss: 2.0139 - classification_loss: 0.6636
 190/1000 [====>.........................] - ETA: 6:06 - loss: 2.6635 - regression_loss: 2.0033 - classification_loss: 0.6601
 191/1000 [====>.........................] - ETA: 6:05 - loss: 2.6657 - regression_loss: 2.0077 - classification_loss: 0.6580
 192/1000 [====>.........................] - ETA: 6:05 - loss: 2.6737 - regression_loss: 2.0161 - classification_loss: 0.6576
 193/1000 [====>.........................] - ETA: 6:04 - loss: 2.6738 - regression_loss: 2.0152 - classification_loss: 0.6586
 194/1000 [====>.........................] - ETA: 6:04 - loss: 2.6668 - regression_loss: 2.0102 - classification_loss: 0.6566
 195/1000 [====>.........................] - ETA: 6:03 - loss: 2.6715 - regression_loss: 2.0150 - classification_loss: 0.6564
 196/1000 [====>.........................] - ETA: 6:03 - loss: 2.6725 - regression_loss: 2.0164 - classification_loss: 0.6561
 197/1000 [====>.........................] - ETA: 6:03 - loss: 2.6683 - regression_loss: 2.0143 - classification_loss: 0.6540
 198/1000 [====>.........................] - ETA: 6:02 - loss: 2.6548 - regression_loss: 2.0041 - classification_loss: 0.6507
 199/1000 [====>.........................] - ETA: 6:02 - loss: 2.6549 - regression_loss: 2.0036 - classification_loss: 0.6513
 200/1000 [=====>........................] - ETA: 6:01 - loss: 2.6703 - regression_loss: 2.0108 - classification_loss: 0.6595
 201/1000 [=====>........................] - ETA: 6:01 - loss: 2.6715 - regression_loss: 2.0135 - classification_loss: 0.6580
 202/1000 [=====>........................] - ETA: 6:00 - loss: 2.6743 - regression_loss: 2.0120 - classification_loss: 0.6623
 203/1000 [=====>........................] - ETA: 6:00 - loss: 2.6738 - regression_loss: 2.0127 - classification_loss: 0.6611
 204/1000 [=====>........................] - ETA: 5:59 - loss: 2.6607 - regression_loss: 2.0028 - classification_loss: 0.6578
 205/1000 [=====>........................] - ETA: 5:59 - loss: 2.6487 - regression_loss: 1.9931 - classification_loss: 0.6556
 206/1000 [=====>........................] - ETA: 5:58 - loss: 2.6593 - regression_loss: 2.0003 - classification_loss: 0.6590
 207/1000 [=====>........................] - ETA: 5:58 - loss: 2.6671 - regression_loss: 2.0080 - classification_loss: 0.6591
 208/1000 [=====>........................] - ETA: 5:58 - loss: 2.6678 - regression_loss: 2.0088 - classification_loss: 0.6590
 209/1000 [=====>........................] - ETA: 5:57 - loss: 2.6753 - regression_loss: 2.0118 - classification_loss: 0.6635
 210/1000 [=====>........................] - ETA: 5:57 - loss: 2.6771 - regression_loss: 2.0145 - classification_loss: 0.6626
 211/1000 [=====>........................] - ETA: 5:56 - loss: 2.6644 - regression_loss: 2.0049 - classification_loss: 0.6595
 212/1000 [=====>........................] - ETA: 5:56 - loss: 2.6655 - regression_loss: 2.0071 - classification_loss: 0.6584
 213/1000 [=====>........................] - ETA: 5:55 - loss: 2.6729 - regression_loss: 2.0127 - classification_loss: 0.6602
 214/1000 [=====>........................] - ETA: 5:55 - loss: 2.6773 - regression_loss: 2.0188 - classification_loss: 0.6586
 215/1000 [=====>........................] - ETA: 5:54 - loss: 2.6848 - regression_loss: 2.0226 - classification_loss: 0.6622
 216/1000 [=====>........................] - ETA: 5:54 - loss: 2.6953 - regression_loss: 2.0293 - classification_loss: 0.6660
 217/1000 [=====>........................] - ETA: 5:54 - loss: 2.6980 - regression_loss: 2.0326 - classification_loss: 0.6654
 218/1000 [=====>........................] - ETA: 5:53 - loss: 2.6856 - regression_loss: 2.0232 - classification_loss: 0.6624
 219/1000 [=====>........................] - ETA: 5:53 - loss: 2.6758 - regression_loss: 2.0140 - classification_loss: 0.6618
 220/1000 [=====>........................] - ETA: 5:52 - loss: 2.6636 - regression_loss: 2.0048 - classification_loss: 0.6588
 221/1000 [=====>........................] - ETA: 5:52 - loss: 2.6715 - regression_loss: 2.0102 - classification_loss: 0.6613
 222/1000 [=====>........................] - ETA: 5:51 - loss: 2.6595 - regression_loss: 2.0012 - classification_loss: 0.6583
 223/1000 [=====>........................] - ETA: 5:51 - loss: 2.6578 - regression_loss: 2.0003 - classification_loss: 0.6574
 224/1000 [=====>........................] - ETA: 5:50 - loss: 2.6618 - regression_loss: 2.0047 - classification_loss: 0.6571
 225/1000 [=====>........................] - ETA: 5:50 - loss: 2.6678 - regression_loss: 2.0079 - classification_loss: 0.6598
 226/1000 [=====>........................] - ETA: 5:50 - loss: 2.6763 - regression_loss: 2.0112 - classification_loss: 0.6651
 227/1000 [=====>........................] - ETA: 5:49 - loss: 2.6645 - regression_loss: 2.0023 - classification_loss: 0.6622
 228/1000 [=====>........................] - ETA: 5:49 - loss: 2.6689 - regression_loss: 2.0072 - classification_loss: 0.6617
 229/1000 [=====>........................] - ETA: 5:48 - loss: 2.6675 - regression_loss: 2.0053 - classification_loss: 0.6621
 230/1000 [=====>........................] - ETA: 5:48 - loss: 2.6641 - regression_loss: 2.0030 - classification_loss: 0.6611
 231/1000 [=====>........................] - ETA: 5:47 - loss: 2.6707 - regression_loss: 2.0094 - classification_loss: 0.6613
 232/1000 [=====>........................] - ETA: 5:47 - loss: 2.6821 - regression_loss: 2.0166 - classification_loss: 0.6655
 233/1000 [=====>........................] - ETA: 5:46 - loss: 2.6839 - regression_loss: 2.0189 - classification_loss: 0.6649
 234/1000 [======>.......................] - ETA: 5:46 - loss: 2.6916 - regression_loss: 2.0256 - classification_loss: 0.6661
 235/1000 [======>.......................] - ETA: 5:46 - loss: 2.6802 - regression_loss: 2.0169 - classification_loss: 0.6632
 236/1000 [======>.......................] - ETA: 5:45 - loss: 2.6822 - regression_loss: 2.0197 - classification_loss: 0.6626
 237/1000 [======>.......................] - ETA: 5:45 - loss: 2.6709 - regression_loss: 2.0111 - classification_loss: 0.6598
 238/1000 [======>.......................] - ETA: 5:44 - loss: 2.6713 - regression_loss: 2.0121 - classification_loss: 0.6592
 239/1000 [======>.......................] - ETA: 5:44 - loss: 2.6788 - regression_loss: 2.0189 - classification_loss: 0.6599
 240/1000 [======>.......................] - ETA: 5:43 - loss: 2.6771 - regression_loss: 2.0183 - classification_loss: 0.6588
 241/1000 [======>.......................] - ETA: 5:43 - loss: 2.6863 - regression_loss: 2.0225 - classification_loss: 0.6638
 242/1000 [======>.......................] - ETA: 5:42 - loss: 2.6899 - regression_loss: 2.0266 - classification_loss: 0.6634
 243/1000 [======>.......................] - ETA: 5:42 - loss: 2.6931 - regression_loss: 2.0306 - classification_loss: 0.6624
 244/1000 [======>.......................] - ETA: 5:41 - loss: 2.6914 - regression_loss: 2.0305 - classification_loss: 0.6609
 245/1000 [======>.......................] - ETA: 5:41 - loss: 2.6906 - regression_loss: 2.0305 - classification_loss: 0.6601
 246/1000 [======>.......................] - ETA: 5:41 - loss: 2.6990 - regression_loss: 2.0355 - classification_loss: 0.6635
 247/1000 [======>.......................] - ETA: 5:40 - loss: 2.6999 - regression_loss: 2.0347 - classification_loss: 0.6653
 248/1000 [======>.......................] - ETA: 5:40 - loss: 2.7043 - regression_loss: 2.0348 - classification_loss: 0.6695
 249/1000 [======>.......................] - ETA: 5:39 - loss: 2.7064 - regression_loss: 2.0371 - classification_loss: 0.6693
 250/1000 [======>.......................] - ETA: 5:39 - loss: 2.7127 - regression_loss: 2.0429 - classification_loss: 0.6699
 251/1000 [======>.......................] - ETA: 5:38 - loss: 2.7135 - regression_loss: 2.0445 - classification_loss: 0.6690
 252/1000 [======>.......................] - ETA: 5:38 - loss: 2.7112 - regression_loss: 2.0424 - classification_loss: 0.6687
 253/1000 [======>.......................] - ETA: 5:37 - loss: 2.7069 - regression_loss: 2.0399 - classification_loss: 0.6670
 254/1000 [======>.......................] - ETA: 5:37 - loss: 2.7062 - regression_loss: 2.0393 - classification_loss: 0.6669
 255/1000 [======>.......................] - ETA: 5:36 - loss: 2.7084 - regression_loss: 2.0404 - classification_loss: 0.6680
 256/1000 [======>.......................] - ETA: 5:36 - loss: 2.7013 - regression_loss: 2.0324 - classification_loss: 0.6688
 257/1000 [======>.......................] - ETA: 5:36 - loss: 2.6921 - regression_loss: 2.0245 - classification_loss: 0.6676
 258/1000 [======>.......................] - ETA: 5:35 - loss: 2.6817 - regression_loss: 2.0167 - classification_loss: 0.6650
 259/1000 [======>.......................] - ETA: 5:35 - loss: 2.6714 - regression_loss: 2.0089 - classification_loss: 0.6625
 260/1000 [======>.......................] - ETA: 5:34 - loss: 2.6778 - regression_loss: 2.0159 - classification_loss: 0.6619
 261/1000 [======>.......................] - ETA: 5:34 - loss: 2.6785 - regression_loss: 2.0176 - classification_loss: 0.6609
 262/1000 [======>.......................] - ETA: 5:33 - loss: 2.6683 - regression_loss: 2.0099 - classification_loss: 0.6584
 263/1000 [======>.......................] - ETA: 5:33 - loss: 2.6726 - regression_loss: 2.0122 - classification_loss: 0.6605
 264/1000 [======>.......................] - ETA: 5:33 - loss: 2.6710 - regression_loss: 2.0112 - classification_loss: 0.6598
 265/1000 [======>.......................] - ETA: 5:32 - loss: 2.6745 - regression_loss: 2.0129 - classification_loss: 0.6617
 266/1000 [======>.......................] - ETA: 5:32 - loss: 2.6646 - regression_loss: 2.0053 - classification_loss: 0.6593
 267/1000 [=======>......................] - ETA: 5:31 - loss: 2.6632 - regression_loss: 2.0050 - classification_loss: 0.6581
 268/1000 [=======>......................] - ETA: 5:31 - loss: 2.6532 - regression_loss: 1.9976 - classification_loss: 0.6557
 269/1000 [=======>......................] - ETA: 5:30 - loss: 2.6546 - regression_loss: 1.9985 - classification_loss: 0.6561
 270/1000 [=======>......................] - ETA: 5:30 - loss: 2.6619 - regression_loss: 2.0043 - classification_loss: 0.6575
 271/1000 [=======>......................] - ETA: 5:29 - loss: 2.6666 - regression_loss: 2.0078 - classification_loss: 0.6587
 272/1000 [=======>......................] - ETA: 5:29 - loss: 2.6568 - regression_loss: 2.0004 - classification_loss: 0.6563
 273/1000 [=======>......................] - ETA: 5:29 - loss: 2.6659 - regression_loss: 2.0046 - classification_loss: 0.6613
 274/1000 [=======>......................] - ETA: 5:28 - loss: 2.6561 - regression_loss: 1.9973 - classification_loss: 0.6589
 275/1000 [=======>......................] - ETA: 5:28 - loss: 2.6558 - regression_loss: 1.9980 - classification_loss: 0.6578
 276/1000 [=======>......................] - ETA: 5:27 - loss: 2.6623 - regression_loss: 2.0028 - classification_loss: 0.6596
 277/1000 [=======>......................] - ETA: 5:27 - loss: 2.6647 - regression_loss: 2.0044 - classification_loss: 0.6603
 278/1000 [=======>......................] - ETA: 5:26 - loss: 2.6757 - regression_loss: 2.0105 - classification_loss: 0.6653
 279/1000 [=======>......................] - ETA: 5:26 - loss: 2.6770 - regression_loss: 2.0126 - classification_loss: 0.6644
 280/1000 [=======>......................] - ETA: 5:25 - loss: 2.6787 - regression_loss: 2.0138 - classification_loss: 0.6650
 281/1000 [=======>......................] - ETA: 5:25 - loss: 2.6873 - regression_loss: 2.0206 - classification_loss: 0.6667
 282/1000 [=======>......................] - ETA: 5:24 - loss: 2.6950 - regression_loss: 2.0231 - classification_loss: 0.6718
 283/1000 [=======>......................] - ETA: 5:24 - loss: 2.6962 - regression_loss: 2.0250 - classification_loss: 0.6712
 284/1000 [=======>......................] - ETA: 5:24 - loss: 2.6975 - regression_loss: 2.0267 - classification_loss: 0.6708
 285/1000 [=======>......................] - ETA: 5:23 - loss: 2.6967 - regression_loss: 2.0260 - classification_loss: 0.6708
 286/1000 [=======>......................] - ETA: 5:23 - loss: 2.6971 - regression_loss: 2.0260 - classification_loss: 0.6711
 287/1000 [=======>......................] - ETA: 5:22 - loss: 2.6999 - regression_loss: 2.0291 - classification_loss: 0.6708
 288/1000 [=======>......................] - ETA: 5:22 - loss: 2.7011 - regression_loss: 2.0313 - classification_loss: 0.6698
 289/1000 [=======>......................] - ETA: 5:21 - loss: 2.7007 - regression_loss: 2.0318 - classification_loss: 0.6689
 290/1000 [=======>......................] - ETA: 5:21 - loss: 2.6984 - regression_loss: 2.0299 - classification_loss: 0.6686
 291/1000 [=======>......................] - ETA: 5:20 - loss: 2.6984 - regression_loss: 2.0308 - classification_loss: 0.6676
 292/1000 [=======>......................] - ETA: 5:20 - loss: 2.6988 - regression_loss: 2.0313 - classification_loss: 0.6675
 293/1000 [=======>......................] - ETA: 5:19 - loss: 2.6971 - regression_loss: 2.0311 - classification_loss: 0.6660
 294/1000 [=======>......................] - ETA: 5:19 - loss: 2.7006 - regression_loss: 2.0346 - classification_loss: 0.6661
 295/1000 [=======>......................] - ETA: 5:19 - loss: 2.6917 - regression_loss: 2.0277 - classification_loss: 0.6641
 296/1000 [=======>......................] - ETA: 5:18 - loss: 2.6895 - regression_loss: 2.0268 - classification_loss: 0.6627
 297/1000 [=======>......................] - ETA: 5:18 - loss: 2.6828 - regression_loss: 2.0200 - classification_loss: 0.6629
 298/1000 [=======>......................] - ETA: 5:17 - loss: 2.6746 - regression_loss: 2.0132 - classification_loss: 0.6614
 299/1000 [=======>......................] - ETA: 5:17 - loss: 2.6765 - regression_loss: 2.0161 - classification_loss: 0.6604
 300/1000 [========>.....................] - ETA: 5:16 - loss: 2.6848 - regression_loss: 2.0229 - classification_loss: 0.6619
 301/1000 [========>.....................] - ETA: 5:16 - loss: 2.6839 - regression_loss: 2.0226 - classification_loss: 0.6613
 302/1000 [========>.....................] - ETA: 5:15 - loss: 2.6893 - regression_loss: 2.0265 - classification_loss: 0.6628
 303/1000 [========>.....................] - ETA: 5:15 - loss: 2.6909 - regression_loss: 2.0288 - classification_loss: 0.6621
 304/1000 [========>.....................] - ETA: 5:15 - loss: 2.6947 - regression_loss: 2.0332 - classification_loss: 0.6615
 305/1000 [========>.....................] - ETA: 5:14 - loss: 2.6975 - regression_loss: 2.0363 - classification_loss: 0.6613
 306/1000 [========>.....................] - ETA: 5:14 - loss: 2.7031 - regression_loss: 2.0415 - classification_loss: 0.6616
 307/1000 [========>.....................] - ETA: 5:13 - loss: 2.7057 - regression_loss: 2.0450 - classification_loss: 0.6607
 308/1000 [========>.....................] - ETA: 5:13 - loss: 2.7045 - regression_loss: 2.0438 - classification_loss: 0.6607
 309/1000 [========>.....................] - ETA: 5:12 - loss: 2.6958 - regression_loss: 2.0372 - classification_loss: 0.6586
 310/1000 [========>.....................] - ETA: 5:12 - loss: 2.6873 - regression_loss: 2.0306 - classification_loss: 0.6566
 311/1000 [========>.....................] - ETA: 5:11 - loss: 2.6786 - regression_loss: 2.0241 - classification_loss: 0.6545
 312/1000 [========>.....................] - ETA: 5:11 - loss: 2.6706 - regression_loss: 2.0176 - classification_loss: 0.6530
 313/1000 [========>.....................] - ETA: 5:10 - loss: 2.6626 - regression_loss: 2.0112 - classification_loss: 0.6514
 314/1000 [========>.....................] - ETA: 5:10 - loss: 2.6654 - regression_loss: 2.0137 - classification_loss: 0.6516
 315/1000 [========>.....................] - ETA: 5:10 - loss: 2.6576 - regression_loss: 2.0073 - classification_loss: 0.6503
 316/1000 [========>.....................] - ETA: 5:09 - loss: 2.6493 - regression_loss: 2.0010 - classification_loss: 0.6483
 317/1000 [========>.....................] - ETA: 5:09 - loss: 2.6584 - regression_loss: 2.0087 - classification_loss: 0.6497
 318/1000 [========>.....................] - ETA: 5:08 - loss: 2.6628 - regression_loss: 2.0120 - classification_loss: 0.6508
 319/1000 [========>.....................] - ETA: 5:08 - loss: 2.6630 - regression_loss: 2.0120 - classification_loss: 0.6511
 320/1000 [========>.....................] - ETA: 5:07 - loss: 2.6682 - regression_loss: 2.0164 - classification_loss: 0.6517
 321/1000 [========>.....................] - ETA: 5:07 - loss: 2.6683 - regression_loss: 2.0171 - classification_loss: 0.6513
 322/1000 [========>.....................] - ETA: 5:06 - loss: 2.6600 - regression_loss: 2.0108 - classification_loss: 0.6492
 323/1000 [========>.....................] - ETA: 5:06 - loss: 2.6612 - regression_loss: 2.0102 - classification_loss: 0.6509
 324/1000 [========>.....................] - ETA: 5:05 - loss: 2.6666 - regression_loss: 2.0144 - classification_loss: 0.6522
 325/1000 [========>.....................] - ETA: 5:05 - loss: 2.6584 - regression_loss: 2.0082 - classification_loss: 0.6502
 326/1000 [========>.....................] - ETA: 5:05 - loss: 2.6621 - regression_loss: 2.0104 - classification_loss: 0.6517
 327/1000 [========>.....................] - ETA: 5:04 - loss: 2.6540 - regression_loss: 2.0043 - classification_loss: 0.6497
 328/1000 [========>.....................] - ETA: 5:04 - loss: 2.6596 - regression_loss: 2.0082 - classification_loss: 0.6514
 329/1000 [========>.....................] - ETA: 5:03 - loss: 2.6515 - regression_loss: 2.0021 - classification_loss: 0.6494
 330/1000 [========>.....................] - ETA: 5:03 - loss: 2.6553 - regression_loss: 2.0059 - classification_loss: 0.6494
 331/1000 [========>.....................] - ETA: 5:02 - loss: 2.6473 - regression_loss: 1.9998 - classification_loss: 0.6474
 332/1000 [========>.....................] - ETA: 5:02 - loss: 2.6539 - regression_loss: 2.0041 - classification_loss: 0.6499
 333/1000 [========>.....................] - ETA: 5:01 - loss: 2.6557 - regression_loss: 2.0059 - classification_loss: 0.6497
 334/1000 [=========>....................] - ETA: 5:01 - loss: 2.6575 - regression_loss: 2.0067 - classification_loss: 0.6508
 335/1000 [=========>....................] - ETA: 5:00 - loss: 2.6629 - regression_loss: 2.0100 - classification_loss: 0.6529
 336/1000 [=========>....................] - ETA: 5:00 - loss: 2.6658 - regression_loss: 2.0124 - classification_loss: 0.6534
 337/1000 [=========>....................] - ETA: 5:00 - loss: 2.6654 - regression_loss: 2.0127 - classification_loss: 0.6527
 338/1000 [=========>....................] - ETA: 4:59 - loss: 2.6729 - regression_loss: 2.0171 - classification_loss: 0.6558
 339/1000 [=========>....................] - ETA: 4:59 - loss: 2.6812 - regression_loss: 2.0234 - classification_loss: 0.6577
 340/1000 [=========>....................] - ETA: 4:58 - loss: 2.6733 - regression_loss: 2.0175 - classification_loss: 0.6558
 341/1000 [=========>....................] - ETA: 4:58 - loss: 2.6825 - regression_loss: 2.0241 - classification_loss: 0.6585
 342/1000 [=========>....................] - ETA: 4:57 - loss: 2.6894 - regression_loss: 2.0284 - classification_loss: 0.6610
 343/1000 [=========>....................] - ETA: 4:57 - loss: 2.6882 - regression_loss: 2.0267 - classification_loss: 0.6615
 344/1000 [=========>....................] - ETA: 4:56 - loss: 2.6861 - regression_loss: 2.0256 - classification_loss: 0.6606
 345/1000 [=========>....................] - ETA: 4:56 - loss: 2.6913 - regression_loss: 2.0277 - classification_loss: 0.6636
 346/1000 [=========>....................] - ETA: 4:55 - loss: 2.6950 - regression_loss: 2.0301 - classification_loss: 0.6649
 347/1000 [=========>....................] - ETA: 4:55 - loss: 2.6990 - regression_loss: 2.0339 - classification_loss: 0.6650
 348/1000 [=========>....................] - ETA: 4:55 - loss: 2.7045 - regression_loss: 2.0390 - classification_loss: 0.6654
 349/1000 [=========>....................] - ETA: 4:54 - loss: 2.7077 - regression_loss: 2.0427 - classification_loss: 0.6650
 350/1000 [=========>....................] - ETA: 4:54 - loss: 2.7090 - regression_loss: 2.0442 - classification_loss: 0.6649
 351/1000 [=========>....................] - ETA: 4:53 - loss: 2.7084 - regression_loss: 2.0440 - classification_loss: 0.6644
 352/1000 [=========>....................] - ETA: 4:53 - loss: 2.7073 - regression_loss: 2.0441 - classification_loss: 0.6633
 353/1000 [=========>....................] - ETA: 4:52 - loss: 2.7046 - regression_loss: 2.0427 - classification_loss: 0.6620
 354/1000 [=========>....................] - ETA: 4:52 - loss: 2.7048 - regression_loss: 2.0434 - classification_loss: 0.6614
 355/1000 [=========>....................] - ETA: 4:51 - loss: 2.7034 - regression_loss: 2.0421 - classification_loss: 0.6612
 356/1000 [=========>....................] - ETA: 4:51 - loss: 2.7045 - regression_loss: 2.0437 - classification_loss: 0.6608
 357/1000 [=========>....................] - ETA: 4:50 - loss: 2.7082 - regression_loss: 2.0463 - classification_loss: 0.6619
 358/1000 [=========>....................] - ETA: 4:50 - loss: 2.7072 - regression_loss: 2.0467 - classification_loss: 0.6605
 359/1000 [=========>....................] - ETA: 4:50 - loss: 2.7091 - regression_loss: 2.0477 - classification_loss: 0.6615
 360/1000 [=========>....................] - ETA: 4:49 - loss: 2.7021 - regression_loss: 2.0420 - classification_loss: 0.6601
 361/1000 [=========>....................] - ETA: 4:49 - loss: 2.7018 - regression_loss: 2.0422 - classification_loss: 0.6596
 362/1000 [=========>....................] - ETA: 4:48 - loss: 2.6983 - regression_loss: 2.0365 - classification_loss: 0.6618
 363/1000 [=========>....................] - ETA: 4:48 - loss: 2.7000 - regression_loss: 2.0384 - classification_loss: 0.6617
 364/1000 [=========>....................] - ETA: 4:47 - loss: 2.7054 - regression_loss: 2.0433 - classification_loss: 0.6621
 365/1000 [=========>....................] - ETA: 4:47 - loss: 2.6984 - regression_loss: 2.0377 - classification_loss: 0.6607
 366/1000 [=========>....................] - ETA: 4:46 - loss: 2.7030 - regression_loss: 2.0403 - classification_loss: 0.6627
 367/1000 [==========>...................] - ETA: 4:46 - loss: 2.7025 - regression_loss: 2.0406 - classification_loss: 0.6618
 368/1000 [==========>...................] - ETA: 4:46 - loss: 2.7042 - regression_loss: 2.0418 - classification_loss: 0.6624
 369/1000 [==========>...................] - ETA: 4:45 - loss: 2.7052 - regression_loss: 2.0437 - classification_loss: 0.6615
 370/1000 [==========>...................] - ETA: 4:45 - loss: 2.6983 - regression_loss: 2.0381 - classification_loss: 0.6601
 371/1000 [==========>...................] - ETA: 4:44 - loss: 2.6910 - regression_loss: 2.0326 - classification_loss: 0.6584
 372/1000 [==========>...................] - ETA: 4:44 - loss: 2.6942 - regression_loss: 2.0343 - classification_loss: 0.6600
 373/1000 [==========>...................] - ETA: 4:43 - loss: 2.6870 - regression_loss: 2.0288 - classification_loss: 0.6582
 374/1000 [==========>...................] - ETA: 4:43 - loss: 2.6910 - regression_loss: 2.0327 - classification_loss: 0.6583
 375/1000 [==========>...................] - ETA: 4:42 - loss: 2.6959 - regression_loss: 2.0376 - classification_loss: 0.6583
 376/1000 [==========>...................] - ETA: 4:42 - loss: 2.6887 - regression_loss: 2.0322 - classification_loss: 0.6565
 377/1000 [==========>...................] - ETA: 4:41 - loss: 2.6904 - regression_loss: 2.0345 - classification_loss: 0.6559
 378/1000 [==========>...................] - ETA: 4:41 - loss: 2.6833 - regression_loss: 2.0292 - classification_loss: 0.6542
 379/1000 [==========>...................] - ETA: 4:41 - loss: 2.6857 - regression_loss: 2.0315 - classification_loss: 0.6542
 380/1000 [==========>...................] - ETA: 4:40 - loss: 2.6880 - regression_loss: 2.0323 - classification_loss: 0.6557
 381/1000 [==========>...................] - ETA: 4:40 - loss: 2.6909 - regression_loss: 2.0332 - classification_loss: 0.6577
 382/1000 [==========>...................] - ETA: 4:39 - loss: 2.6906 - regression_loss: 2.0333 - classification_loss: 0.6572
 383/1000 [==========>...................] - ETA: 4:39 - loss: 2.6942 - regression_loss: 2.0364 - classification_loss: 0.6578
 384/1000 [==========>...................] - ETA: 4:38 - loss: 2.6872 - regression_loss: 2.0311 - classification_loss: 0.6560
 385/1000 [==========>...................] - ETA: 4:38 - loss: 2.6913 - regression_loss: 2.0351 - classification_loss: 0.6562
 386/1000 [==========>...................] - ETA: 4:37 - loss: 2.6967 - regression_loss: 2.0394 - classification_loss: 0.6572
 387/1000 [==========>...................] - ETA: 4:37 - loss: 2.6974 - regression_loss: 2.0403 - classification_loss: 0.6572
 388/1000 [==========>...................] - ETA: 4:37 - loss: 2.7033 - regression_loss: 2.0437 - classification_loss: 0.6596
 389/1000 [==========>...................] - ETA: 4:36 - loss: 2.7046 - regression_loss: 2.0439 - classification_loss: 0.6607
 390/1000 [==========>...................] - ETA: 4:36 - loss: 2.7123 - regression_loss: 2.0492 - classification_loss: 0.6631
 391/1000 [==========>...................] - ETA: 4:35 - loss: 2.7173 - regression_loss: 2.0518 - classification_loss: 0.6655
 392/1000 [==========>...................] - ETA: 4:35 - loss: 2.7104 - regression_loss: 2.0466 - classification_loss: 0.6638
 393/1000 [==========>...................] - ETA: 4:34 - loss: 2.7143 - regression_loss: 2.0503 - classification_loss: 0.6640
 394/1000 [==========>...................] - ETA: 4:34 - loss: 2.7155 - regression_loss: 2.0523 - classification_loss: 0.6631
 395/1000 [==========>...................] - ETA: 4:33 - loss: 2.7135 - regression_loss: 2.0512 - classification_loss: 0.6623
 396/1000 [==========>...................] - ETA: 4:33 - loss: 2.7067 - regression_loss: 2.0460 - classification_loss: 0.6607
 397/1000 [==========>...................] - ETA: 4:32 - loss: 2.7120 - regression_loss: 2.0487 - classification_loss: 0.6633
 398/1000 [==========>...................] - ETA: 4:32 - loss: 2.7190 - regression_loss: 2.0549 - classification_loss: 0.6641
 399/1000 [==========>...................] - ETA: 4:32 - loss: 2.7213 - regression_loss: 2.0577 - classification_loss: 0.6636
 400/1000 [===========>..................] - ETA: 4:31 - loss: 2.7216 - regression_loss: 2.0590 - classification_loss: 0.6626
 401/1000 [===========>..................] - ETA: 4:31 - loss: 2.7150 - regression_loss: 2.0538 - classification_loss: 0.6612
 402/1000 [===========>..................] - ETA: 4:30 - loss: 2.7192 - regression_loss: 2.0576 - classification_loss: 0.6616
 403/1000 [===========>..................] - ETA: 4:30 - loss: 2.7289 - regression_loss: 2.0677 - classification_loss: 0.6613
 404/1000 [===========>..................] - ETA: 4:29 - loss: 2.7273 - regression_loss: 2.0666 - classification_loss: 0.6608
 405/1000 [===========>..................] - ETA: 4:29 - loss: 2.7318 - regression_loss: 2.0712 - classification_loss: 0.6606
 406/1000 [===========>..................] - ETA: 4:28 - loss: 2.7362 - regression_loss: 2.0747 - classification_loss: 0.6615
 407/1000 [===========>..................] - ETA: 4:28 - loss: 2.7362 - regression_loss: 2.0741 - classification_loss: 0.6621
 408/1000 [===========>..................] - ETA: 4:27 - loss: 2.7295 - regression_loss: 2.0691 - classification_loss: 0.6605
 409/1000 [===========>..................] - ETA: 4:27 - loss: 2.7324 - regression_loss: 2.0705 - classification_loss: 0.6619
 410/1000 [===========>..................] - ETA: 4:27 - loss: 2.7257 - regression_loss: 2.0654 - classification_loss: 0.6603
 411/1000 [===========>..................] - ETA: 4:26 - loss: 2.7272 - regression_loss: 2.0657 - classification_loss: 0.6616
 412/1000 [===========>..................] - ETA: 4:26 - loss: 2.7301 - regression_loss: 2.0679 - classification_loss: 0.6622
 413/1000 [===========>..................] - ETA: 4:25 - loss: 2.7235 - regression_loss: 2.0629 - classification_loss: 0.6606
 414/1000 [===========>..................] - ETA: 4:25 - loss: 2.7234 - regression_loss: 2.0635 - classification_loss: 0.6599
 415/1000 [===========>..................] - ETA: 4:24 - loss: 2.7225 - regression_loss: 2.0633 - classification_loss: 0.6592
 416/1000 [===========>..................] - ETA: 4:24 - loss: 2.7207 - regression_loss: 2.0624 - classification_loss: 0.6583
 417/1000 [===========>..................] - ETA: 4:23 - loss: 2.7142 - regression_loss: 2.0575 - classification_loss: 0.6567
 418/1000 [===========>..................] - ETA: 4:23 - loss: 2.7160 - regression_loss: 2.0600 - classification_loss: 0.6560
 419/1000 [===========>..................] - ETA: 4:23 - loss: 2.7161 - regression_loss: 2.0608 - classification_loss: 0.6553
 420/1000 [===========>..................] - ETA: 4:22 - loss: 2.7175 - regression_loss: 2.0632 - classification_loss: 0.6542
 421/1000 [===========>..................] - ETA: 4:22 - loss: 2.7209 - regression_loss: 2.0659 - classification_loss: 0.6549
 422/1000 [===========>..................] - ETA: 4:21 - loss: 2.7220 - regression_loss: 2.0675 - classification_loss: 0.6545
 423/1000 [===========>..................] - ETA: 4:21 - loss: 2.7212 - regression_loss: 2.0673 - classification_loss: 0.6539
 424/1000 [===========>..................] - ETA: 4:20 - loss: 2.7148 - regression_loss: 2.0625 - classification_loss: 0.6524
 425/1000 [===========>..................] - ETA: 4:20 - loss: 2.7166 - regression_loss: 2.0641 - classification_loss: 0.6526
 426/1000 [===========>..................] - ETA: 4:19 - loss: 2.7192 - regression_loss: 2.0658 - classification_loss: 0.6534
 427/1000 [===========>..................] - ETA: 4:19 - loss: 2.7220 - regression_loss: 2.0689 - classification_loss: 0.6532
 428/1000 [===========>..................] - ETA: 4:18 - loss: 2.7157 - regression_loss: 2.0640 - classification_loss: 0.6516
 429/1000 [===========>..................] - ETA: 4:18 - loss: 2.7093 - regression_loss: 2.0592 - classification_loss: 0.6501
 430/1000 [===========>..................] - ETA: 4:18 - loss: 2.7163 - regression_loss: 2.0628 - classification_loss: 0.6535
 431/1000 [===========>..................] - ETA: 4:17 - loss: 2.7136 - regression_loss: 2.0610 - classification_loss: 0.6526
 432/1000 [===========>..................] - ETA: 4:17 - loss: 2.7156 - regression_loss: 2.0628 - classification_loss: 0.6528
 433/1000 [===========>..................] - ETA: 4:16 - loss: 2.7184 - regression_loss: 2.0637 - classification_loss: 0.6547
 434/1000 [============>.................] - ETA: 4:16 - loss: 2.7121 - regression_loss: 2.0589 - classification_loss: 0.6532
 435/1000 [============>.................] - ETA: 4:15 - loss: 2.7149 - regression_loss: 2.0615 - classification_loss: 0.6534
 436/1000 [============>.................] - ETA: 4:15 - loss: 2.7153 - regression_loss: 2.0621 - classification_loss: 0.6532
 437/1000 [============>.................] - ETA: 4:14 - loss: 2.7153 - regression_loss: 2.0623 - classification_loss: 0.6530
 438/1000 [============>.................] - ETA: 4:14 - loss: 2.7234 - regression_loss: 2.0672 - classification_loss: 0.6563
 439/1000 [============>.................] - ETA: 4:13 - loss: 2.7263 - regression_loss: 2.0687 - classification_loss: 0.6575
 440/1000 [============>.................] - ETA: 4:13 - loss: 2.7265 - regression_loss: 2.0640 - classification_loss: 0.6625
 441/1000 [============>.................] - ETA: 4:13 - loss: 2.7249 - regression_loss: 2.0631 - classification_loss: 0.6618
 442/1000 [============>.................] - ETA: 4:12 - loss: 2.7253 - regression_loss: 2.0642 - classification_loss: 0.6611
 443/1000 [============>.................] - ETA: 4:12 - loss: 2.7276 - regression_loss: 2.0667 - classification_loss: 0.6609
 444/1000 [============>.................] - ETA: 4:11 - loss: 2.7272 - regression_loss: 2.0670 - classification_loss: 0.6601
 445/1000 [============>.................] - ETA: 4:11 - loss: 2.7260 - regression_loss: 2.0665 - classification_loss: 0.6594
 446/1000 [============>.................] - ETA: 4:10 - loss: 2.7285 - regression_loss: 2.0685 - classification_loss: 0.6599
 447/1000 [============>.................] - ETA: 4:10 - loss: 2.7285 - regression_loss: 2.0687 - classification_loss: 0.6598
 448/1000 [============>.................] - ETA: 4:09 - loss: 2.7225 - regression_loss: 2.0640 - classification_loss: 0.6584
 449/1000 [============>.................] - ETA: 4:09 - loss: 2.7248 - regression_loss: 2.0669 - classification_loss: 0.6579
 450/1000 [============>.................] - ETA: 4:08 - loss: 2.7266 - regression_loss: 2.0680 - classification_loss: 0.6586
 451/1000 [============>.................] - ETA: 4:08 - loss: 2.7314 - regression_loss: 2.0719 - classification_loss: 0.6595
 452/1000 [============>.................] - ETA: 4:08 - loss: 2.7254 - regression_loss: 2.0673 - classification_loss: 0.6581
 453/1000 [============>.................] - ETA: 4:07 - loss: 2.7278 - regression_loss: 2.0701 - classification_loss: 0.6577
 454/1000 [============>.................] - ETA: 4:07 - loss: 2.7218 - regression_loss: 2.0655 - classification_loss: 0.6563
 455/1000 [============>.................] - ETA: 4:06 - loss: 2.7158 - regression_loss: 2.0610 - classification_loss: 0.6548
 456/1000 [============>.................] - ETA: 4:06 - loss: 2.7194 - regression_loss: 2.0642 - classification_loss: 0.6552
 457/1000 [============>.................] - ETA: 4:05 - loss: 2.7134 - regression_loss: 2.0597 - classification_loss: 0.6538
 458/1000 [============>.................] - ETA: 4:05 - loss: 2.7137 - regression_loss: 2.0603 - classification_loss: 0.6533
 459/1000 [============>.................] - ETA: 4:04 - loss: 2.7144 - regression_loss: 2.0613 - classification_loss: 0.6531
 460/1000 [============>.................] - ETA: 4:04 - loss: 2.7177 - regression_loss: 2.0639 - classification_loss: 0.6538
 461/1000 [============>.................] - ETA: 4:03 - loss: 2.7202 - regression_loss: 2.0661 - classification_loss: 0.6541
 462/1000 [============>.................] - ETA: 4:03 - loss: 2.7143 - regression_loss: 2.0616 - classification_loss: 0.6527
 463/1000 [============>.................] - ETA: 4:03 - loss: 2.7183 - regression_loss: 2.0653 - classification_loss: 0.6530
 464/1000 [============>.................] - ETA: 4:02 - loss: 2.7251 - regression_loss: 2.0697 - classification_loss: 0.6554
 465/1000 [============>.................] - ETA: 4:02 - loss: 2.7260 - regression_loss: 2.0709 - classification_loss: 0.6551
 466/1000 [============>.................] - ETA: 4:01 - loss: 2.7205 - regression_loss: 2.0665 - classification_loss: 0.6540
 467/1000 [=============>................] - ETA: 4:01 - loss: 2.7146 - regression_loss: 2.0621 - classification_loss: 0.6526
 468/1000 [=============>................] - ETA: 4:00 - loss: 2.7088 - regression_loss: 2.0577 - classification_loss: 0.6512
 469/1000 [=============>................] - ETA: 4:00 - loss: 2.7122 - regression_loss: 2.0590 - classification_loss: 0.6532
 470/1000 [=============>................] - ETA: 3:59 - loss: 2.7064 - regression_loss: 2.0546 - classification_loss: 0.6518
 471/1000 [=============>................] - ETA: 3:59 - loss: 2.7087 - regression_loss: 2.0555 - classification_loss: 0.6532
 472/1000 [=============>................] - ETA: 3:58 - loss: 2.7088 - regression_loss: 2.0554 - classification_loss: 0.6534
 473/1000 [=============>................] - ETA: 3:58 - loss: 2.7148 - regression_loss: 2.0587 - classification_loss: 0.6561
 474/1000 [=============>................] - ETA: 3:58 - loss: 2.7145 - regression_loss: 2.0584 - classification_loss: 0.6560
 475/1000 [=============>................] - ETA: 3:57 - loss: 2.7088 - regression_loss: 2.0541 - classification_loss: 0.6547
 476/1000 [=============>................] - ETA: 3:57 - loss: 2.7101 - regression_loss: 2.0544 - classification_loss: 0.6556
 477/1000 [=============>................] - ETA: 3:56 - loss: 2.7104 - regression_loss: 2.0541 - classification_loss: 0.6563
 478/1000 [=============>................] - ETA: 3:56 - loss: 2.7165 - regression_loss: 2.0568 - classification_loss: 0.6597
 479/1000 [=============>................] - ETA: 3:55 - loss: 2.7187 - regression_loss: 2.0592 - classification_loss: 0.6595
 480/1000 [=============>................] - ETA: 3:55 - loss: 2.7131 - regression_loss: 2.0549 - classification_loss: 0.6581
 481/1000 [=============>................] - ETA: 3:54 - loss: 2.7147 - regression_loss: 2.0559 - classification_loss: 0.6588
 482/1000 [=============>................] - ETA: 3:54 - loss: 2.7184 - regression_loss: 2.0589 - classification_loss: 0.6595
 483/1000 [=============>................] - ETA: 3:53 - loss: 2.7229 - regression_loss: 2.0621 - classification_loss: 0.6608
 484/1000 [=============>................] - ETA: 3:53 - loss: 2.7296 - regression_loss: 2.0670 - classification_loss: 0.6626
 485/1000 [=============>................] - ETA: 3:53 - loss: 2.7298 - regression_loss: 2.0675 - classification_loss: 0.6623
 486/1000 [=============>................] - ETA: 3:52 - loss: 2.7352 - regression_loss: 2.0719 - classification_loss: 0.6634
 487/1000 [=============>................] - ETA: 3:52 - loss: 2.7401 - regression_loss: 2.0742 - classification_loss: 0.6659
 488/1000 [=============>................] - ETA: 3:51 - loss: 2.7345 - regression_loss: 2.0699 - classification_loss: 0.6646
 489/1000 [=============>................] - ETA: 3:51 - loss: 2.7355 - regression_loss: 2.0714 - classification_loss: 0.6641
 490/1000 [=============>................] - ETA: 3:50 - loss: 2.7306 - regression_loss: 2.0672 - classification_loss: 0.6635
 491/1000 [=============>................] - ETA: 3:50 - loss: 2.7321 - regression_loss: 2.0680 - classification_loss: 0.6641
 492/1000 [=============>................] - ETA: 3:49 - loss: 2.7323 - regression_loss: 2.0688 - classification_loss: 0.6635
 493/1000 [=============>................] - ETA: 3:49 - loss: 2.7267 - regression_loss: 2.0646 - classification_loss: 0.6621
 494/1000 [=============>................] - ETA: 3:49 - loss: 2.7273 - regression_loss: 2.0654 - classification_loss: 0.6618
 495/1000 [=============>................] - ETA: 3:48 - loss: 2.7303 - regression_loss: 2.0676 - classification_loss: 0.6628
 496/1000 [=============>................] - ETA: 3:48 - loss: 2.7327 - regression_loss: 2.0688 - classification_loss: 0.6639
 497/1000 [=============>................] - ETA: 3:47 - loss: 2.7287 - regression_loss: 2.0647 - classification_loss: 0.6641
 498/1000 [=============>................] - ETA: 3:47 - loss: 2.7233 - regression_loss: 2.0605 - classification_loss: 0.6627
 499/1000 [=============>................] - ETA: 3:46 - loss: 2.7221 - regression_loss: 2.0598 - classification_loss: 0.6624
 500/1000 [==============>...............] - ETA: 3:46 - loss: 2.7167 - regression_loss: 2.0556 - classification_loss: 0.6610
 501/1000 [==============>...............] - ETA: 3:45 - loss: 2.7191 - regression_loss: 2.0567 - classification_loss: 0.6625
 502/1000 [==============>...............] - ETA: 3:45 - loss: 2.7140 - regression_loss: 2.0526 - classification_loss: 0.6615
 503/1000 [==============>...............] - ETA: 3:44 - loss: 2.7151 - regression_loss: 2.0534 - classification_loss: 0.6617
 504/1000 [==============>...............] - ETA: 3:44 - loss: 2.7153 - regression_loss: 2.0538 - classification_loss: 0.6615
 505/1000 [==============>...............] - ETA: 3:44 - loss: 2.7100 - regression_loss: 2.0497 - classification_loss: 0.6602
 506/1000 [==============>...............] - ETA: 3:43 - loss: 2.7099 - regression_loss: 2.0497 - classification_loss: 0.6602
 507/1000 [==============>...............] - ETA: 3:43 - loss: 2.7120 - regression_loss: 2.0514 - classification_loss: 0.6606
 508/1000 [==============>...............] - ETA: 3:42 - loss: 2.7156 - regression_loss: 2.0529 - classification_loss: 0.6627
 509/1000 [==============>...............] - ETA: 3:42 - loss: 2.7103 - regression_loss: 2.0489 - classification_loss: 0.6614
 510/1000 [==============>...............] - ETA: 3:41 - loss: 2.7116 - regression_loss: 2.0499 - classification_loss: 0.6617
 511/1000 [==============>...............] - ETA: 3:41 - loss: 2.7114 - regression_loss: 2.0497 - classification_loss: 0.6617
 512/1000 [==============>...............] - ETA: 3:40 - loss: 2.7110 - regression_loss: 2.0492 - classification_loss: 0.6618
 513/1000 [==============>...............] - ETA: 3:40 - loss: 2.7124 - regression_loss: 2.0505 - classification_loss: 0.6619
 514/1000 [==============>...............] - ETA: 3:39 - loss: 2.7114 - regression_loss: 2.0498 - classification_loss: 0.6616
 515/1000 [==============>...............] - ETA: 3:39 - loss: 2.7128 - regression_loss: 2.0516 - classification_loss: 0.6612
 516/1000 [==============>...............] - ETA: 3:39 - loss: 2.7175 - regression_loss: 2.0558 - classification_loss: 0.6617
 517/1000 [==============>...............] - ETA: 3:38 - loss: 2.7123 - regression_loss: 2.0519 - classification_loss: 0.6604
 518/1000 [==============>...............] - ETA: 3:38 - loss: 2.7148 - regression_loss: 2.0538 - classification_loss: 0.6610
 519/1000 [==============>...............] - ETA: 3:37 - loss: 2.7095 - regression_loss: 2.0498 - classification_loss: 0.6597
 520/1000 [==============>...............] - ETA: 3:37 - loss: 2.7123 - regression_loss: 2.0511 - classification_loss: 0.6612
 521/1000 [==============>...............] - ETA: 3:36 - loss: 2.7137 - regression_loss: 2.0521 - classification_loss: 0.6615
 522/1000 [==============>...............] - ETA: 3:36 - loss: 2.7184 - regression_loss: 2.0547 - classification_loss: 0.6637
 523/1000 [==============>...............] - ETA: 3:35 - loss: 2.7228 - regression_loss: 2.0573 - classification_loss: 0.6655
 524/1000 [==============>...............] - ETA: 3:35 - loss: 2.7176 - regression_loss: 2.0534 - classification_loss: 0.6642
 525/1000 [==============>...............] - ETA: 3:34 - loss: 2.7125 - regression_loss: 2.0495 - classification_loss: 0.6630
 526/1000 [==============>...............] - ETA: 3:34 - loss: 2.7126 - regression_loss: 2.0501 - classification_loss: 0.6624
 527/1000 [==============>...............] - ETA: 3:34 - loss: 2.7146 - regression_loss: 2.0522 - classification_loss: 0.6623
 528/1000 [==============>...............] - ETA: 3:33 - loss: 2.7186 - regression_loss: 2.0556 - classification_loss: 0.6630
 529/1000 [==============>...............] - ETA: 3:33 - loss: 2.7201 - regression_loss: 2.0560 - classification_loss: 0.6640
 530/1000 [==============>...............] - ETA: 3:32 - loss: 2.7208 - regression_loss: 2.0557 - classification_loss: 0.6650
 531/1000 [==============>...............] - ETA: 3:32 - loss: 2.7229 - regression_loss: 2.0571 - classification_loss: 0.6658
 532/1000 [==============>...............] - ETA: 3:31 - loss: 2.7251 - regression_loss: 2.0581 - classification_loss: 0.6670
 533/1000 [==============>...............] - ETA: 3:31 - loss: 2.7270 - regression_loss: 2.0584 - classification_loss: 0.6686
 534/1000 [===============>..............] - ETA: 3:30 - loss: 2.7219 - regression_loss: 2.0546 - classification_loss: 0.6673
 535/1000 [===============>..............] - ETA: 3:30 - loss: 2.7168 - regression_loss: 2.0507 - classification_loss: 0.6661
 536/1000 [===============>..............] - ETA: 3:30 - loss: 2.7117 - regression_loss: 2.0469 - classification_loss: 0.6648
 537/1000 [===============>..............] - ETA: 3:29 - loss: 2.7145 - regression_loss: 2.0475 - classification_loss: 0.6670
 538/1000 [===============>..............] - ETA: 3:29 - loss: 2.7140 - regression_loss: 2.0474 - classification_loss: 0.6667
 539/1000 [===============>..............] - ETA: 3:28 - loss: 2.7132 - regression_loss: 2.0466 - classification_loss: 0.6666
 540/1000 [===============>..............] - ETA: 3:28 - loss: 2.7141 - regression_loss: 2.0463 - classification_loss: 0.6677
 541/1000 [===============>..............] - ETA: 3:27 - loss: 2.7091 - regression_loss: 2.0426 - classification_loss: 0.6665
 542/1000 [===============>..............] - ETA: 3:27 - loss: 2.7125 - regression_loss: 2.0442 - classification_loss: 0.6683
 543/1000 [===============>..............] - ETA: 3:26 - loss: 2.7127 - regression_loss: 2.0448 - classification_loss: 0.6679
 544/1000 [===============>..............] - ETA: 3:26 - loss: 2.7077 - regression_loss: 2.0411 - classification_loss: 0.6666
 545/1000 [===============>..............] - ETA: 3:25 - loss: 2.7027 - regression_loss: 2.0373 - classification_loss: 0.6654
 546/1000 [===============>..............] - ETA: 3:25 - loss: 2.7029 - regression_loss: 2.0378 - classification_loss: 0.6651
 547/1000 [===============>..............] - ETA: 3:25 - loss: 2.7051 - regression_loss: 2.0395 - classification_loss: 0.6656
 548/1000 [===============>..............] - ETA: 3:24 - loss: 2.7002 - regression_loss: 2.0358 - classification_loss: 0.6644
 549/1000 [===============>..............] - ETA: 3:24 - loss: 2.6954 - regression_loss: 2.0321 - classification_loss: 0.6634
 550/1000 [===============>..............] - ETA: 3:23 - loss: 2.6970 - regression_loss: 2.0342 - classification_loss: 0.6628
 551/1000 [===============>..............] - ETA: 3:23 - loss: 2.6957 - regression_loss: 2.0334 - classification_loss: 0.6622
 552/1000 [===============>..............] - ETA: 3:22 - loss: 2.6930 - regression_loss: 2.0298 - classification_loss: 0.6632
 553/1000 [===============>..............] - ETA: 3:22 - loss: 2.6941 - regression_loss: 2.0308 - classification_loss: 0.6633
 554/1000 [===============>..............] - ETA: 3:21 - loss: 2.6937 - regression_loss: 2.0307 - classification_loss: 0.6631
 555/1000 [===============>..............] - ETA: 3:21 - loss: 2.6966 - regression_loss: 2.0315 - classification_loss: 0.6651
 556/1000 [===============>..............] - ETA: 3:20 - loss: 2.6918 - regression_loss: 2.0279 - classification_loss: 0.6639
 557/1000 [===============>..............] - ETA: 3:20 - loss: 2.6919 - regression_loss: 2.0285 - classification_loss: 0.6634
 558/1000 [===============>..............] - ETA: 3:20 - loss: 2.6957 - regression_loss: 2.0323 - classification_loss: 0.6633
 559/1000 [===============>..............] - ETA: 3:19 - loss: 2.6995 - regression_loss: 2.0345 - classification_loss: 0.6649
 560/1000 [===============>..............] - ETA: 3:19 - loss: 2.6946 - regression_loss: 2.0309 - classification_loss: 0.6638
 561/1000 [===============>..............] - ETA: 3:18 - loss: 2.6948 - regression_loss: 2.0313 - classification_loss: 0.6636
 562/1000 [===============>..............] - ETA: 3:18 - loss: 2.6970 - regression_loss: 2.0332 - classification_loss: 0.6638
 563/1000 [===============>..............] - ETA: 3:17 - loss: 2.7003 - regression_loss: 2.0343 - classification_loss: 0.6660
 564/1000 [===============>..............] - ETA: 3:17 - loss: 2.6992 - regression_loss: 2.0336 - classification_loss: 0.6656
 565/1000 [===============>..............] - ETA: 3:16 - loss: 2.6997 - regression_loss: 2.0345 - classification_loss: 0.6653
 566/1000 [===============>..............] - ETA: 3:16 - loss: 2.6989 - regression_loss: 2.0338 - classification_loss: 0.6651
 567/1000 [================>.............] - ETA: 3:15 - loss: 2.7009 - regression_loss: 2.0343 - classification_loss: 0.6666
 568/1000 [================>.............] - ETA: 3:15 - loss: 2.7036 - regression_loss: 2.0366 - classification_loss: 0.6670
 569/1000 [================>.............] - ETA: 3:15 - loss: 2.7051 - regression_loss: 2.0374 - classification_loss: 0.6678
 570/1000 [================>.............] - ETA: 3:14 - loss: 2.7082 - regression_loss: 2.0389 - classification_loss: 0.6693
 571/1000 [================>.............] - ETA: 3:14 - loss: 2.7112 - regression_loss: 2.0408 - classification_loss: 0.6704
 572/1000 [================>.............] - ETA: 3:13 - loss: 2.7123 - regression_loss: 2.0419 - classification_loss: 0.6704
 573/1000 [================>.............] - ETA: 3:13 - loss: 2.7076 - regression_loss: 2.0384 - classification_loss: 0.6692
 574/1000 [================>.............] - ETA: 3:12 - loss: 2.7162 - regression_loss: 2.0452 - classification_loss: 0.6710
 575/1000 [================>.............] - ETA: 3:12 - loss: 2.7181 - regression_loss: 2.0474 - classification_loss: 0.6707
 576/1000 [================>.............] - ETA: 3:11 - loss: 2.7134 - regression_loss: 2.0439 - classification_loss: 0.6695
 577/1000 [================>.............] - ETA: 3:11 - loss: 2.7087 - regression_loss: 2.0403 - classification_loss: 0.6684
 578/1000 [================>.............] - ETA: 3:10 - loss: 2.7088 - regression_loss: 2.0400 - classification_loss: 0.6688
 579/1000 [================>.............] - ETA: 3:10 - loss: 2.7082 - regression_loss: 2.0400 - classification_loss: 0.6682
 580/1000 [================>.............] - ETA: 3:10 - loss: 2.7076 - regression_loss: 2.0398 - classification_loss: 0.6678
 581/1000 [================>.............] - ETA: 3:09 - loss: 2.7077 - regression_loss: 2.0404 - classification_loss: 0.6673
 582/1000 [================>.............] - ETA: 3:09 - loss: 2.7030 - regression_loss: 2.0369 - classification_loss: 0.6661
 583/1000 [================>.............] - ETA: 3:08 - loss: 2.7021 - regression_loss: 2.0364 - classification_loss: 0.6656
 584/1000 [================>.............] - ETA: 3:08 - loss: 2.7021 - regression_loss: 2.0368 - classification_loss: 0.6653
 585/1000 [================>.............] - ETA: 3:07 - loss: 2.7002 - regression_loss: 2.0356 - classification_loss: 0.6646
 586/1000 [================>.............] - ETA: 3:07 - loss: 2.7012 - regression_loss: 2.0366 - classification_loss: 0.6646
 587/1000 [================>.............] - ETA: 3:06 - loss: 2.7030 - regression_loss: 2.0387 - classification_loss: 0.6644
 588/1000 [================>.............] - ETA: 3:06 - loss: 2.6985 - regression_loss: 2.0352 - classification_loss: 0.6633
 589/1000 [================>.............] - ETA: 3:05 - loss: 2.7038 - regression_loss: 2.0400 - classification_loss: 0.6637
 590/1000 [================>.............] - ETA: 3:05 - loss: 2.7063 - regression_loss: 2.0422 - classification_loss: 0.6641
 591/1000 [================>.............] - ETA: 3:05 - loss: 2.7017 - regression_loss: 2.0387 - classification_loss: 0.6630
 592/1000 [================>.............] - ETA: 3:04 - loss: 2.7001 - regression_loss: 2.0377 - classification_loss: 0.6624
 593/1000 [================>.............] - ETA: 3:04 - loss: 2.6955 - regression_loss: 2.0342 - classification_loss: 0.6613
 594/1000 [================>.............] - ETA: 3:03 - loss: 2.7027 - regression_loss: 2.0402 - classification_loss: 0.6625
 595/1000 [================>.............] - ETA: 3:03 - loss: 2.7081 - regression_loss: 2.0444 - classification_loss: 0.6637
 596/1000 [================>.............] - ETA: 3:02 - loss: 2.7099 - regression_loss: 2.0463 - classification_loss: 0.6636
 597/1000 [================>.............] - ETA: 3:02 - loss: 2.7053 - regression_loss: 2.0428 - classification_loss: 0.6625
 598/1000 [================>.............] - ETA: 3:01 - loss: 2.7051 - regression_loss: 2.0431 - classification_loss: 0.6619
 599/1000 [================>.............] - ETA: 3:01 - loss: 2.7052 - regression_loss: 2.0439 - classification_loss: 0.6612
 600/1000 [=================>............] - ETA: 3:01 - loss: 2.7054 - regression_loss: 2.0442 - classification_loss: 0.6612
 601/1000 [=================>............] - ETA: 3:00 - loss: 2.7034 - regression_loss: 2.0427 - classification_loss: 0.6606
 602/1000 [=================>............] - ETA: 3:00 - loss: 2.7037 - regression_loss: 2.0431 - classification_loss: 0.6606
 603/1000 [=================>............] - ETA: 2:59 - loss: 2.7033 - regression_loss: 2.0431 - classification_loss: 0.6603
 604/1000 [=================>............] - ETA: 2:59 - loss: 2.7041 - regression_loss: 2.0440 - classification_loss: 0.6601
 605/1000 [=================>............] - ETA: 2:58 - loss: 2.7056 - regression_loss: 2.0455 - classification_loss: 0.6601
 606/1000 [=================>............] - ETA: 2:58 - loss: 2.7057 - regression_loss: 2.0462 - classification_loss: 0.6595
 607/1000 [=================>............] - ETA: 2:57 - loss: 2.7067 - regression_loss: 2.0474 - classification_loss: 0.6593
 608/1000 [=================>............] - ETA: 2:57 - loss: 2.7078 - regression_loss: 2.0484 - classification_loss: 0.6593
 609/1000 [=================>............] - ETA: 2:56 - loss: 2.7101 - regression_loss: 2.0500 - classification_loss: 0.6600
 610/1000 [=================>............] - ETA: 2:56 - loss: 2.7113 - regression_loss: 2.0514 - classification_loss: 0.6599
 611/1000 [=================>............] - ETA: 2:56 - loss: 2.7103 - regression_loss: 2.0505 - classification_loss: 0.6598
 612/1000 [=================>............] - ETA: 2:55 - loss: 2.7059 - regression_loss: 2.0472 - classification_loss: 0.6587
 613/1000 [=================>............] - ETA: 2:55 - loss: 2.7048 - regression_loss: 2.0469 - classification_loss: 0.6579
 614/1000 [=================>............] - ETA: 2:54 - loss: 2.7078 - regression_loss: 2.0494 - classification_loss: 0.6584
 615/1000 [=================>............] - ETA: 2:54 - loss: 2.7084 - regression_loss: 2.0501 - classification_loss: 0.6583
 616/1000 [=================>............] - ETA: 2:53 - loss: 2.7079 - regression_loss: 2.0502 - classification_loss: 0.6577
 617/1000 [=================>............] - ETA: 2:53 - loss: 2.7089 - regression_loss: 2.0517 - classification_loss: 0.6573
 618/1000 [=================>............] - ETA: 2:52 - loss: 2.7110 - regression_loss: 2.0528 - classification_loss: 0.6583
 619/1000 [=================>............] - ETA: 2:52 - loss: 2.7124 - regression_loss: 2.0541 - classification_loss: 0.6583
 620/1000 [=================>............] - ETA: 2:51 - loss: 2.7119 - regression_loss: 2.0540 - classification_loss: 0.6579
 621/1000 [=================>............] - ETA: 2:51 - loss: 2.7198 - regression_loss: 2.0606 - classification_loss: 0.6592
 622/1000 [=================>............] - ETA: 2:51 - loss: 2.7154 - regression_loss: 2.0573 - classification_loss: 0.6582
 623/1000 [=================>............] - ETA: 2:50 - loss: 2.7111 - regression_loss: 2.0540 - classification_loss: 0.6571
 624/1000 [=================>............] - ETA: 2:50 - loss: 2.7146 - regression_loss: 2.0575 - classification_loss: 0.6571
 625/1000 [=================>............] - ETA: 2:49 - loss: 2.7130 - regression_loss: 2.0542 - classification_loss: 0.6587
 626/1000 [=================>............] - ETA: 2:49 - loss: 2.7086 - regression_loss: 2.0510 - classification_loss: 0.6577
 627/1000 [=================>............] - ETA: 2:48 - loss: 2.7127 - regression_loss: 2.0542 - classification_loss: 0.6585
 628/1000 [=================>............] - ETA: 2:48 - loss: 2.7143 - regression_loss: 2.0550 - classification_loss: 0.6593
 629/1000 [=================>............] - ETA: 2:47 - loss: 2.7168 - regression_loss: 2.0561 - classification_loss: 0.6606
 630/1000 [=================>............] - ETA: 2:47 - loss: 2.7218 - regression_loss: 2.0595 - classification_loss: 0.6623
 631/1000 [=================>............] - ETA: 2:46 - loss: 2.7256 - regression_loss: 2.0623 - classification_loss: 0.6633
 632/1000 [=================>............] - ETA: 2:46 - loss: 2.7259 - regression_loss: 2.0627 - classification_loss: 0.6632
 633/1000 [=================>............] - ETA: 2:46 - loss: 2.7264 - regression_loss: 2.0633 - classification_loss: 0.6632
 634/1000 [==================>...........] - ETA: 2:45 - loss: 2.7222 - regression_loss: 2.0600 - classification_loss: 0.6621
 635/1000 [==================>...........] - ETA: 2:45 - loss: 2.7179 - regression_loss: 2.0568 - classification_loss: 0.6611
 636/1000 [==================>...........] - ETA: 2:44 - loss: 2.7190 - regression_loss: 2.0579 - classification_loss: 0.6611
 637/1000 [==================>...........] - ETA: 2:44 - loss: 2.7200 - regression_loss: 2.0591 - classification_loss: 0.6609
 638/1000 [==================>...........] - ETA: 2:43 - loss: 2.7186 - regression_loss: 2.0582 - classification_loss: 0.6604
 639/1000 [==================>...........] - ETA: 2:43 - loss: 2.7144 - regression_loss: 2.0550 - classification_loss: 0.6594
 640/1000 [==================>...........] - ETA: 2:42 - loss: 2.7165 - regression_loss: 2.0572 - classification_loss: 0.6593
 641/1000 [==================>...........] - ETA: 2:42 - loss: 2.7123 - regression_loss: 2.0540 - classification_loss: 0.6583
 642/1000 [==================>...........] - ETA: 2:41 - loss: 2.7152 - regression_loss: 2.0558 - classification_loss: 0.6594
 643/1000 [==================>...........] - ETA: 2:41 - loss: 2.7183 - regression_loss: 2.0588 - classification_loss: 0.6595
 644/1000 [==================>...........] - ETA: 2:41 - loss: 2.7192 - regression_loss: 2.0596 - classification_loss: 0.6597
 645/1000 [==================>...........] - ETA: 2:40 - loss: 2.7193 - regression_loss: 2.0598 - classification_loss: 0.6595
 646/1000 [==================>...........] - ETA: 2:40 - loss: 2.7211 - regression_loss: 2.0615 - classification_loss: 0.6596
 647/1000 [==================>...........] - ETA: 2:39 - loss: 2.7238 - regression_loss: 2.0640 - classification_loss: 0.6599
 648/1000 [==================>...........] - ETA: 2:39 - loss: 2.7280 - regression_loss: 2.0681 - classification_loss: 0.6599
 649/1000 [==================>...........] - ETA: 2:38 - loss: 2.7272 - regression_loss: 2.0677 - classification_loss: 0.6595
 650/1000 [==================>...........] - ETA: 2:38 - loss: 2.7283 - regression_loss: 2.0686 - classification_loss: 0.6597
 651/1000 [==================>...........] - ETA: 2:37 - loss: 2.7283 - regression_loss: 2.0684 - classification_loss: 0.6599
 652/1000 [==================>...........] - ETA: 2:37 - loss: 2.7242 - regression_loss: 2.0653 - classification_loss: 0.6589
 653/1000 [==================>...........] - ETA: 2:37 - loss: 2.7276 - regression_loss: 2.0675 - classification_loss: 0.6601
 654/1000 [==================>...........] - ETA: 2:36 - loss: 2.7275 - regression_loss: 2.0678 - classification_loss: 0.6597
 655/1000 [==================>...........] - ETA: 2:36 - loss: 2.7259 - regression_loss: 2.0666 - classification_loss: 0.6593
 656/1000 [==================>...........] - ETA: 2:35 - loss: 2.7284 - regression_loss: 2.0676 - classification_loss: 0.6608
 657/1000 [==================>...........] - ETA: 2:35 - loss: 2.7293 - regression_loss: 2.0689 - classification_loss: 0.6604
 658/1000 [==================>...........] - ETA: 2:34 - loss: 2.7295 - regression_loss: 2.0698 - classification_loss: 0.6597
 659/1000 [==================>...........] - ETA: 2:34 - loss: 2.7298 - regression_loss: 2.0696 - classification_loss: 0.6601
 660/1000 [==================>...........] - ETA: 2:33 - loss: 2.7293 - regression_loss: 2.0695 - classification_loss: 0.6598
 661/1000 [==================>...........] - ETA: 2:33 - loss: 2.7325 - regression_loss: 2.0723 - classification_loss: 0.6602
 662/1000 [==================>...........] - ETA: 2:32 - loss: 2.7327 - regression_loss: 2.0728 - classification_loss: 0.6599
 663/1000 [==================>...........] - ETA: 2:32 - loss: 2.7292 - regression_loss: 2.0697 - classification_loss: 0.6596
 664/1000 [==================>...........] - ETA: 2:32 - loss: 2.7251 - regression_loss: 2.0665 - classification_loss: 0.6586
 665/1000 [==================>...........] - ETA: 2:31 - loss: 2.7210 - regression_loss: 2.0634 - classification_loss: 0.6576
 666/1000 [==================>...........] - ETA: 2:31 - loss: 2.7170 - regression_loss: 2.0603 - classification_loss: 0.6567
 667/1000 [===================>..........] - ETA: 2:30 - loss: 2.7179 - regression_loss: 2.0613 - classification_loss: 0.6566
 668/1000 [===================>..........] - ETA: 2:30 - loss: 2.7210 - regression_loss: 2.0636 - classification_loss: 0.6574
 669/1000 [===================>..........] - ETA: 2:29 - loss: 2.7214 - regression_loss: 2.0635 - classification_loss: 0.6579
 670/1000 [===================>..........] - ETA: 2:29 - loss: 2.7219 - regression_loss: 2.0637 - classification_loss: 0.6582
 671/1000 [===================>..........] - ETA: 2:28 - loss: 2.7178 - regression_loss: 2.0606 - classification_loss: 0.6572
 672/1000 [===================>..........] - ETA: 2:28 - loss: 2.7181 - regression_loss: 2.0609 - classification_loss: 0.6572
 673/1000 [===================>..........] - ETA: 2:27 - loss: 2.7221 - regression_loss: 2.0646 - classification_loss: 0.6575
 674/1000 [===================>..........] - ETA: 2:27 - loss: 2.7260 - regression_loss: 2.0664 - classification_loss: 0.6595
 675/1000 [===================>..........] - ETA: 2:27 - loss: 2.7219 - regression_loss: 2.0634 - classification_loss: 0.6586
 676/1000 [===================>..........] - ETA: 2:26 - loss: 2.7226 - regression_loss: 2.0645 - classification_loss: 0.6581
 677/1000 [===================>..........] - ETA: 2:26 - loss: 2.7247 - regression_loss: 2.0662 - classification_loss: 0.6585
 678/1000 [===================>..........] - ETA: 2:25 - loss: 2.7242 - regression_loss: 2.0658 - classification_loss: 0.6584
 679/1000 [===================>..........] - ETA: 2:25 - loss: 2.7246 - regression_loss: 2.0667 - classification_loss: 0.6580
 680/1000 [===================>..........] - ETA: 2:24 - loss: 2.7287 - regression_loss: 2.0695 - classification_loss: 0.6592
 681/1000 [===================>..........] - ETA: 2:24 - loss: 2.7247 - regression_loss: 2.0665 - classification_loss: 0.6582
 682/1000 [===================>..........] - ETA: 2:23 - loss: 2.7274 - regression_loss: 2.0683 - classification_loss: 0.6591
 683/1000 [===================>..........] - ETA: 2:23 - loss: 2.7303 - regression_loss: 2.0709 - classification_loss: 0.6594
 684/1000 [===================>..........] - ETA: 2:22 - loss: 2.7264 - regression_loss: 2.0679 - classification_loss: 0.6586
 685/1000 [===================>..........] - ETA: 2:22 - loss: 2.7294 - regression_loss: 2.0696 - classification_loss: 0.6598
 686/1000 [===================>..........] - ETA: 2:22 - loss: 2.7322 - regression_loss: 2.0711 - classification_loss: 0.6611
 687/1000 [===================>..........] - ETA: 2:21 - loss: 2.7343 - regression_loss: 2.0729 - classification_loss: 0.6614
 688/1000 [===================>..........] - ETA: 2:21 - loss: 2.7303 - regression_loss: 2.0699 - classification_loss: 0.6604
 689/1000 [===================>..........] - ETA: 2:20 - loss: 2.7299 - regression_loss: 2.0698 - classification_loss: 0.6600
 690/1000 [===================>..........] - ETA: 2:20 - loss: 2.7337 - regression_loss: 2.0725 - classification_loss: 0.6612
 691/1000 [===================>..........] - ETA: 2:19 - loss: 2.7373 - regression_loss: 2.0759 - classification_loss: 0.6615
 692/1000 [===================>..........] - ETA: 2:19 - loss: 2.7334 - regression_loss: 2.0729 - classification_loss: 0.6605
 693/1000 [===================>..........] - ETA: 2:18 - loss: 2.7364 - regression_loss: 2.0757 - classification_loss: 0.6607
 694/1000 [===================>..........] - ETA: 2:18 - loss: 2.7376 - regression_loss: 2.0765 - classification_loss: 0.6611
 695/1000 [===================>..........] - ETA: 2:18 - loss: 2.7396 - regression_loss: 2.0775 - classification_loss: 0.6621
 696/1000 [===================>..........] - ETA: 2:17 - loss: 2.7390 - regression_loss: 2.0774 - classification_loss: 0.6616
 697/1000 [===================>..........] - ETA: 2:17 - loss: 2.7351 - regression_loss: 2.0744 - classification_loss: 0.6606
 698/1000 [===================>..........] - ETA: 2:16 - loss: 2.7360 - regression_loss: 2.0755 - classification_loss: 0.6605
 699/1000 [===================>..........] - ETA: 2:16 - loss: 2.7376 - regression_loss: 2.0765 - classification_loss: 0.6611
 700/1000 [====================>.........] - ETA: 2:15 - loss: 2.7418 - regression_loss: 2.0798 - classification_loss: 0.6620
 701/1000 [====================>.........] - ETA: 2:15 - loss: 2.7430 - regression_loss: 2.0804 - classification_loss: 0.6625
 702/1000 [====================>.........] - ETA: 2:14 - loss: 2.7451 - regression_loss: 2.0818 - classification_loss: 0.6633
 703/1000 [====================>.........] - ETA: 2:14 - loss: 2.7458 - regression_loss: 2.0825 - classification_loss: 0.6633
 704/1000 [====================>.........] - ETA: 2:13 - loss: 2.7472 - regression_loss: 2.0840 - classification_loss: 0.6631
 705/1000 [====================>.........] - ETA: 2:13 - loss: 2.7433 - regression_loss: 2.0811 - classification_loss: 0.6622
 706/1000 [====================>.........] - ETA: 2:13 - loss: 2.7394 - regression_loss: 2.0781 - classification_loss: 0.6613
 707/1000 [====================>.........] - ETA: 2:12 - loss: 2.7415 - regression_loss: 2.0803 - classification_loss: 0.6612
 708/1000 [====================>.........] - ETA: 2:12 - loss: 2.7408 - regression_loss: 2.0795 - classification_loss: 0.6613
 709/1000 [====================>.........] - ETA: 2:11 - loss: 2.7396 - regression_loss: 2.0790 - classification_loss: 0.6607
 710/1000 [====================>.........] - ETA: 2:11 - loss: 2.7404 - regression_loss: 2.0802 - classification_loss: 0.6602
 711/1000 [====================>.........] - ETA: 2:10 - loss: 2.7365 - regression_loss: 2.0772 - classification_loss: 0.6593
 712/1000 [====================>.........] - ETA: 2:10 - loss: 2.7327 - regression_loss: 2.0743 - classification_loss: 0.6583
 713/1000 [====================>.........] - ETA: 2:09 - loss: 2.7355 - regression_loss: 2.0765 - classification_loss: 0.6590
 714/1000 [====================>.........] - ETA: 2:09 - loss: 2.7390 - regression_loss: 2.0790 - classification_loss: 0.6600
 715/1000 [====================>.........] - ETA: 2:08 - loss: 2.7408 - regression_loss: 2.0807 - classification_loss: 0.6601
 716/1000 [====================>.........] - ETA: 2:08 - loss: 2.7400 - regression_loss: 2.0806 - classification_loss: 0.6594
 717/1000 [====================>.........] - ETA: 2:08 - loss: 2.7411 - regression_loss: 2.0805 - classification_loss: 0.6606
 718/1000 [====================>.........] - ETA: 2:07 - loss: 2.7426 - regression_loss: 2.0825 - classification_loss: 0.6601
 719/1000 [====================>.........] - ETA: 2:07 - loss: 2.7456 - regression_loss: 2.0844 - classification_loss: 0.6612
 720/1000 [====================>.........] - ETA: 2:06 - loss: 2.7502 - regression_loss: 2.0875 - classification_loss: 0.6627
 721/1000 [====================>.........] - ETA: 2:06 - loss: 2.7514 - regression_loss: 2.0872 - classification_loss: 0.6642
 722/1000 [====================>.........] - ETA: 2:05 - loss: 2.7548 - regression_loss: 2.0903 - classification_loss: 0.6644
 723/1000 [====================>.........] - ETA: 2:05 - loss: 2.7596 - regression_loss: 2.0935 - classification_loss: 0.6661
 724/1000 [====================>.........] - ETA: 2:04 - loss: 2.7593 - regression_loss: 2.0933 - classification_loss: 0.6660
 725/1000 [====================>.........] - ETA: 2:04 - loss: 2.7598 - regression_loss: 2.0941 - classification_loss: 0.6657
 726/1000 [====================>.........] - ETA: 2:04 - loss: 2.7594 - regression_loss: 2.0943 - classification_loss: 0.6652
 727/1000 [====================>.........] - ETA: 2:03 - loss: 2.7584 - regression_loss: 2.0937 - classification_loss: 0.6647
 728/1000 [====================>.........] - ETA: 2:03 - loss: 2.7576 - regression_loss: 2.0933 - classification_loss: 0.6643
 729/1000 [====================>.........] - ETA: 2:02 - loss: 2.7564 - regression_loss: 2.0926 - classification_loss: 0.6638
 730/1000 [====================>.........] - ETA: 2:02 - loss: 2.7572 - regression_loss: 2.0933 - classification_loss: 0.6639
 731/1000 [====================>.........] - ETA: 2:01 - loss: 2.7567 - regression_loss: 2.0928 - classification_loss: 0.6639
 732/1000 [====================>.........] - ETA: 2:01 - loss: 2.7578 - regression_loss: 2.0941 - classification_loss: 0.6637
 733/1000 [====================>.........] - ETA: 2:00 - loss: 2.7578 - regression_loss: 2.0936 - classification_loss: 0.6643
 734/1000 [=====================>........] - ETA: 2:00 - loss: 2.7604 - regression_loss: 2.0947 - classification_loss: 0.6657
 735/1000 [=====================>........] - ETA: 1:59 - loss: 2.7629 - regression_loss: 2.0961 - classification_loss: 0.6668
 736/1000 [=====================>........] - ETA: 1:59 - loss: 2.7638 - regression_loss: 2.0964 - classification_loss: 0.6675
 737/1000 [=====================>........] - ETA: 1:59 - loss: 2.7674 - regression_loss: 2.0985 - classification_loss: 0.6688
 738/1000 [=====================>........] - ETA: 1:58 - loss: 2.7637 - regression_loss: 2.0957 - classification_loss: 0.6680
 739/1000 [=====================>........] - ETA: 1:58 - loss: 2.7599 - regression_loss: 2.0929 - classification_loss: 0.6671
 740/1000 [=====================>........] - ETA: 1:57 - loss: 2.7602 - regression_loss: 2.0935 - classification_loss: 0.6667
 741/1000 [=====================>........] - ETA: 1:57 - loss: 2.7619 - regression_loss: 2.0951 - classification_loss: 0.6668
 742/1000 [=====================>........] - ETA: 1:56 - loss: 2.7625 - regression_loss: 2.0956 - classification_loss: 0.6668
 743/1000 [=====================>........] - ETA: 1:56 - loss: 2.7649 - regression_loss: 2.0976 - classification_loss: 0.6672
 744/1000 [=====================>........] - ETA: 1:55 - loss: 2.7646 - regression_loss: 2.0979 - classification_loss: 0.6667
 745/1000 [=====================>........] - ETA: 1:55 - loss: 2.7609 - regression_loss: 2.0951 - classification_loss: 0.6659
 746/1000 [=====================>........] - ETA: 1:54 - loss: 2.7629 - regression_loss: 2.0965 - classification_loss: 0.6664
 747/1000 [=====================>........] - ETA: 1:54 - loss: 2.7629 - regression_loss: 2.0970 - classification_loss: 0.6659
 748/1000 [=====================>........] - ETA: 1:54 - loss: 2.7619 - regression_loss: 2.0966 - classification_loss: 0.6653
 749/1000 [=====================>........] - ETA: 1:53 - loss: 2.7624 - regression_loss: 2.0975 - classification_loss: 0.6649
 750/1000 [=====================>........] - ETA: 1:53 - loss: 2.7622 - regression_loss: 2.0974 - classification_loss: 0.6647
 751/1000 [=====================>........] - ETA: 1:52 - loss: 2.7629 - regression_loss: 2.0977 - classification_loss: 0.6652
 752/1000 [=====================>........] - ETA: 1:52 - loss: 2.7641 - regression_loss: 2.0990 - classification_loss: 0.6651
 753/1000 [=====================>........] - ETA: 1:51 - loss: 2.7604 - regression_loss: 2.0962 - classification_loss: 0.6642
 754/1000 [=====================>........] - ETA: 1:51 - loss: 2.7618 - regression_loss: 2.0976 - classification_loss: 0.6642
 755/1000 [=====================>........] - ETA: 1:50 - loss: 2.7581 - regression_loss: 2.0948 - classification_loss: 0.6633
 756/1000 [=====================>........] - ETA: 1:50 - loss: 2.7601 - regression_loss: 2.0963 - classification_loss: 0.6638
 757/1000 [=====================>........] - ETA: 1:49 - loss: 2.7611 - regression_loss: 2.0969 - classification_loss: 0.6642
 758/1000 [=====================>........] - ETA: 1:49 - loss: 2.7626 - regression_loss: 2.0972 - classification_loss: 0.6654
 759/1000 [=====================>........] - ETA: 1:49 - loss: 2.7605 - regression_loss: 2.0956 - classification_loss: 0.6649
 760/1000 [=====================>........] - ETA: 1:48 - loss: 2.7625 - regression_loss: 2.0974 - classification_loss: 0.6651
 761/1000 [=====================>........] - ETA: 1:48 - loss: 2.7632 - regression_loss: 2.0980 - classification_loss: 0.6652
 762/1000 [=====================>........] - ETA: 1:47 - loss: 2.7629 - regression_loss: 2.0975 - classification_loss: 0.6654
 763/1000 [=====================>........] - ETA: 1:47 - loss: 2.7616 - regression_loss: 2.0965 - classification_loss: 0.6651
 764/1000 [=====================>........] - ETA: 1:46 - loss: 2.7580 - regression_loss: 2.0938 - classification_loss: 0.6642
 765/1000 [=====================>........] - ETA: 1:46 - loss: 2.7585 - regression_loss: 2.0941 - classification_loss: 0.6643
 766/1000 [=====================>........] - ETA: 1:45 - loss: 2.7597 - regression_loss: 2.0957 - classification_loss: 0.6640
 767/1000 [======================>.......] - ETA: 1:45 - loss: 2.7609 - regression_loss: 2.0962 - classification_loss: 0.6647
 768/1000 [======================>.......] - ETA: 1:44 - loss: 2.7621 - regression_loss: 2.0975 - classification_loss: 0.6646
 769/1000 [======================>.......] - ETA: 1:44 - loss: 2.7585 - regression_loss: 2.0947 - classification_loss: 0.6638
 770/1000 [======================>.......] - ETA: 1:44 - loss: 2.7590 - regression_loss: 2.0953 - classification_loss: 0.6637
 771/1000 [======================>.......] - ETA: 1:43 - loss: 2.7595 - regression_loss: 2.0961 - classification_loss: 0.6634
 772/1000 [======================>.......] - ETA: 1:43 - loss: 2.7601 - regression_loss: 2.0966 - classification_loss: 0.6635
 773/1000 [======================>.......] - ETA: 1:42 - loss: 2.7594 - regression_loss: 2.0963 - classification_loss: 0.6631
 774/1000 [======================>.......] - ETA: 1:42 - loss: 2.7614 - regression_loss: 2.0981 - classification_loss: 0.6634
 775/1000 [======================>.......] - ETA: 1:41 - loss: 2.7599 - regression_loss: 2.0970 - classification_loss: 0.6629
 776/1000 [======================>.......] - ETA: 1:41 - loss: 2.7564 - regression_loss: 2.0943 - classification_loss: 0.6621
 777/1000 [======================>.......] - ETA: 1:40 - loss: 2.7574 - regression_loss: 2.0949 - classification_loss: 0.6625
 778/1000 [======================>.......] - ETA: 1:40 - loss: 2.7539 - regression_loss: 2.0923 - classification_loss: 0.6616
 779/1000 [======================>.......] - ETA: 1:39 - loss: 2.7546 - regression_loss: 2.0933 - classification_loss: 0.6613
 780/1000 [======================>.......] - ETA: 1:39 - loss: 2.7574 - regression_loss: 2.0958 - classification_loss: 0.6617
 781/1000 [======================>.......] - ETA: 1:39 - loss: 2.7603 - regression_loss: 2.0974 - classification_loss: 0.6629
 782/1000 [======================>.......] - ETA: 1:38 - loss: 2.7568 - regression_loss: 2.0947 - classification_loss: 0.6621
 783/1000 [======================>.......] - ETA: 1:38 - loss: 2.7592 - regression_loss: 2.0962 - classification_loss: 0.6630
 784/1000 [======================>.......] - ETA: 1:37 - loss: 2.7607 - regression_loss: 2.0965 - classification_loss: 0.6641
 785/1000 [======================>.......] - ETA: 1:37 - loss: 2.7592 - regression_loss: 2.0956 - classification_loss: 0.6636
 786/1000 [======================>.......] - ETA: 1:36 - loss: 2.7557 - regression_loss: 2.0930 - classification_loss: 0.6627
 787/1000 [======================>.......] - ETA: 1:36 - loss: 2.7522 - regression_loss: 2.0903 - classification_loss: 0.6619
 788/1000 [======================>.......] - ETA: 1:35 - loss: 2.7536 - regression_loss: 2.0917 - classification_loss: 0.6619
 789/1000 [======================>.......] - ETA: 1:35 - loss: 2.7532 - regression_loss: 2.0916 - classification_loss: 0.6616
 790/1000 [======================>.......] - ETA: 1:35 - loss: 2.7529 - regression_loss: 2.0916 - classification_loss: 0.6614
 791/1000 [======================>.......] - ETA: 1:34 - loss: 2.7570 - regression_loss: 2.0942 - classification_loss: 0.6628
 792/1000 [======================>.......] - ETA: 1:34 - loss: 2.7535 - regression_loss: 2.0916 - classification_loss: 0.6619
 793/1000 [======================>.......] - ETA: 1:33 - loss: 2.7500 - regression_loss: 2.0889 - classification_loss: 0.6611
 794/1000 [======================>.......] - ETA: 1:33 - loss: 2.7529 - regression_loss: 2.0914 - classification_loss: 0.6615
 795/1000 [======================>.......] - ETA: 1:32 - loss: 2.7533 - regression_loss: 2.0921 - classification_loss: 0.6612
 796/1000 [======================>.......] - ETA: 1:32 - loss: 2.7533 - regression_loss: 2.0927 - classification_loss: 0.6606
 797/1000 [======================>.......] - ETA: 1:31 - loss: 2.7548 - regression_loss: 2.0943 - classification_loss: 0.6605
 798/1000 [======================>.......] - ETA: 1:31 - loss: 2.7552 - regression_loss: 2.0947 - classification_loss: 0.6605
 799/1000 [======================>.......] - ETA: 1:30 - loss: 2.7577 - regression_loss: 2.0962 - classification_loss: 0.6615
 800/1000 [=======================>......] - ETA: 1:30 - loss: 2.7543 - regression_loss: 2.0936 - classification_loss: 0.6607
 801/1000 [=======================>......] - ETA: 1:30 - loss: 2.7532 - regression_loss: 2.0930 - classification_loss: 0.6601
 802/1000 [=======================>......] - ETA: 1:29 - loss: 2.7559 - regression_loss: 2.0943 - classification_loss: 0.6615
 803/1000 [=======================>......] - ETA: 1:29 - loss: 2.7564 - regression_loss: 2.0945 - classification_loss: 0.6620
 804/1000 [=======================>......] - ETA: 1:28 - loss: 2.7584 - regression_loss: 2.0956 - classification_loss: 0.6628
 805/1000 [=======================>......] - ETA: 1:28 - loss: 2.7599 - regression_loss: 2.0967 - classification_loss: 0.6631
 806/1000 [=======================>......] - ETA: 1:27 - loss: 2.7615 - regression_loss: 2.0973 - classification_loss: 0.6642
 807/1000 [=======================>......] - ETA: 1:27 - loss: 2.7621 - regression_loss: 2.0981 - classification_loss: 0.6639
 808/1000 [=======================>......] - ETA: 1:26 - loss: 2.7641 - regression_loss: 2.0995 - classification_loss: 0.6645
 809/1000 [=======================>......] - ETA: 1:26 - loss: 2.7639 - regression_loss: 2.0998 - classification_loss: 0.6641
 810/1000 [=======================>......] - ETA: 1:25 - loss: 2.7668 - regression_loss: 2.1016 - classification_loss: 0.6652
 811/1000 [=======================>......] - ETA: 1:25 - loss: 2.7660 - regression_loss: 2.1013 - classification_loss: 0.6647
 812/1000 [=======================>......] - ETA: 1:25 - loss: 2.7663 - regression_loss: 2.1015 - classification_loss: 0.6648
 813/1000 [=======================>......] - ETA: 1:24 - loss: 2.7671 - regression_loss: 2.1021 - classification_loss: 0.6650
 814/1000 [=======================>......] - ETA: 1:24 - loss: 2.7686 - regression_loss: 2.1032 - classification_loss: 0.6654
 815/1000 [=======================>......] - ETA: 1:23 - loss: 2.7681 - regression_loss: 2.1032 - classification_loss: 0.6649
 816/1000 [=======================>......] - ETA: 1:23 - loss: 2.7714 - regression_loss: 2.1054 - classification_loss: 0.6660
 817/1000 [=======================>......] - ETA: 1:22 - loss: 2.7744 - regression_loss: 2.1074 - classification_loss: 0.6670
 818/1000 [=======================>......] - ETA: 1:22 - loss: 2.7745 - regression_loss: 2.1074 - classification_loss: 0.6670
 819/1000 [=======================>......] - ETA: 1:21 - loss: 2.7746 - regression_loss: 2.1079 - classification_loss: 0.6667
 820/1000 [=======================>......] - ETA: 1:21 - loss: 2.7745 - regression_loss: 2.1082 - classification_loss: 0.6664
 821/1000 [=======================>......] - ETA: 1:21 - loss: 2.7755 - regression_loss: 2.1093 - classification_loss: 0.6661
 822/1000 [=======================>......] - ETA: 1:20 - loss: 2.7761 - regression_loss: 2.1099 - classification_loss: 0.6662
 823/1000 [=======================>......] - ETA: 1:20 - loss: 2.7766 - regression_loss: 2.1106 - classification_loss: 0.6661
 824/1000 [=======================>......] - ETA: 1:19 - loss: 2.7771 - regression_loss: 2.1113 - classification_loss: 0.6659
 825/1000 [=======================>......] - ETA: 1:19 - loss: 2.7783 - regression_loss: 2.1127 - classification_loss: 0.6656
 826/1000 [=======================>......] - ETA: 1:18 - loss: 2.7788 - regression_loss: 2.1138 - classification_loss: 0.6650
 827/1000 [=======================>......] - ETA: 1:18 - loss: 2.7755 - regression_loss: 2.1112 - classification_loss: 0.6643
 828/1000 [=======================>......] - ETA: 1:17 - loss: 2.7749 - regression_loss: 2.1110 - classification_loss: 0.6639
 829/1000 [=======================>......] - ETA: 1:17 - loss: 2.7715 - regression_loss: 2.1084 - classification_loss: 0.6631
 830/1000 [=======================>......] - ETA: 1:16 - loss: 2.7716 - regression_loss: 2.1084 - classification_loss: 0.6632
 831/1000 [=======================>......] - ETA: 1:16 - loss: 2.7717 - regression_loss: 2.1089 - classification_loss: 0.6628
 832/1000 [=======================>......] - ETA: 1:16 - loss: 2.7684 - regression_loss: 2.1064 - classification_loss: 0.6620
 833/1000 [=======================>......] - ETA: 1:15 - loss: 2.7713 - regression_loss: 2.1083 - classification_loss: 0.6630
 834/1000 [========================>.....] - ETA: 1:15 - loss: 2.7742 - regression_loss: 2.1103 - classification_loss: 0.6639
 835/1000 [========================>.....] - ETA: 1:14 - loss: 2.7752 - regression_loss: 2.1113 - classification_loss: 0.6639
 836/1000 [========================>.....] - ETA: 1:14 - loss: 2.7747 - regression_loss: 2.1110 - classification_loss: 0.6637
 837/1000 [========================>.....] - ETA: 1:13 - loss: 2.7731 - regression_loss: 2.1085 - classification_loss: 0.6646
 838/1000 [========================>.....] - ETA: 1:13 - loss: 2.7701 - regression_loss: 2.1060 - classification_loss: 0.6641
 839/1000 [========================>.....] - ETA: 1:12 - loss: 2.7716 - regression_loss: 2.1075 - classification_loss: 0.6640
 840/1000 [========================>.....] - ETA: 1:12 - loss: 2.7684 - regression_loss: 2.1050 - classification_loss: 0.6634
 841/1000 [========================>.....] - ETA: 1:11 - loss: 2.7698 - regression_loss: 2.1063 - classification_loss: 0.6635
 842/1000 [========================>.....] - ETA: 1:11 - loss: 2.7700 - regression_loss: 2.1067 - classification_loss: 0.6633
 843/1000 [========================>.....] - ETA: 1:11 - loss: 2.7721 - regression_loss: 2.1075 - classification_loss: 0.6646
 844/1000 [========================>.....] - ETA: 1:10 - loss: 2.7688 - regression_loss: 2.1050 - classification_loss: 0.6638
 845/1000 [========================>.....] - ETA: 1:10 - loss: 2.7655 - regression_loss: 2.1025 - classification_loss: 0.6630
 846/1000 [========================>.....] - ETA: 1:09 - loss: 2.7669 - regression_loss: 2.1032 - classification_loss: 0.6638
 847/1000 [========================>.....] - ETA: 1:09 - loss: 2.7636 - regression_loss: 2.1007 - classification_loss: 0.6630
 848/1000 [========================>.....] - ETA: 1:08 - loss: 2.7646 - regression_loss: 2.1018 - classification_loss: 0.6628
 849/1000 [========================>.....] - ETA: 1:08 - loss: 2.7664 - regression_loss: 2.1026 - classification_loss: 0.6639
 850/1000 [========================>.....] - ETA: 1:07 - loss: 2.7666 - regression_loss: 2.1025 - classification_loss: 0.6641
 851/1000 [========================>.....] - ETA: 1:07 - loss: 2.7692 - regression_loss: 2.1042 - classification_loss: 0.6650
 852/1000 [========================>.....] - ETA: 1:06 - loss: 2.7708 - regression_loss: 2.1056 - classification_loss: 0.6652
 853/1000 [========================>.....] - ETA: 1:06 - loss: 2.7707 - regression_loss: 2.1053 - classification_loss: 0.6653
 854/1000 [========================>.....] - ETA: 1:06 - loss: 2.7729 - regression_loss: 2.1075 - classification_loss: 0.6654
 855/1000 [========================>.....] - ETA: 1:05 - loss: 2.7728 - regression_loss: 2.1073 - classification_loss: 0.6655
 856/1000 [========================>.....] - ETA: 1:05 - loss: 2.7732 - regression_loss: 2.1078 - classification_loss: 0.6654
 857/1000 [========================>.....] - ETA: 1:04 - loss: 2.7700 - regression_loss: 2.1054 - classification_loss: 0.6646
 858/1000 [========================>.....] - ETA: 1:04 - loss: 2.7669 - regression_loss: 2.1029 - classification_loss: 0.6639
 859/1000 [========================>.....] - ETA: 1:03 - loss: 2.7696 - regression_loss: 2.1050 - classification_loss: 0.6646
 860/1000 [========================>.....] - ETA: 1:03 - loss: 2.7723 - regression_loss: 2.1072 - classification_loss: 0.6651
 861/1000 [========================>.....] - ETA: 1:02 - loss: 2.7741 - regression_loss: 2.1088 - classification_loss: 0.6652
 862/1000 [========================>.....] - ETA: 1:02 - loss: 2.7739 - regression_loss: 2.1086 - classification_loss: 0.6652
 863/1000 [========================>.....] - ETA: 1:02 - loss: 2.7737 - regression_loss: 2.1085 - classification_loss: 0.6652
 864/1000 [========================>.....] - ETA: 1:01 - loss: 2.7781 - regression_loss: 2.1113 - classification_loss: 0.6668
 865/1000 [========================>.....] - ETA: 1:01 - loss: 2.7795 - regression_loss: 2.1128 - classification_loss: 0.6667
 866/1000 [========================>.....] - ETA: 1:00 - loss: 2.7763 - regression_loss: 2.1103 - classification_loss: 0.6659
 867/1000 [=========================>....] - ETA: 1:00 - loss: 2.7773 - regression_loss: 2.1110 - classification_loss: 0.6663
 868/1000 [=========================>....] - ETA: 59s - loss: 2.7763 - regression_loss: 2.1104 - classification_loss: 0.6659 
 869/1000 [=========================>....] - ETA: 59s - loss: 2.7761 - regression_loss: 2.1101 - classification_loss: 0.6661
 870/1000 [=========================>....] - ETA: 58s - loss: 2.7761 - regression_loss: 2.1101 - classification_loss: 0.6660
 871/1000 [=========================>....] - ETA: 58s - loss: 2.7767 - regression_loss: 2.1108 - classification_loss: 0.6660
 872/1000 [=========================>....] - ETA: 57s - loss: 2.7737 - regression_loss: 2.1084 - classification_loss: 0.6653
 873/1000 [=========================>....] - ETA: 57s - loss: 2.7752 - regression_loss: 2.1095 - classification_loss: 0.6657
 874/1000 [=========================>....] - ETA: 57s - loss: 2.7761 - regression_loss: 2.1103 - classification_loss: 0.6658
 875/1000 [=========================>....] - ETA: 56s - loss: 2.7729 - regression_loss: 2.1078 - classification_loss: 0.6651
 876/1000 [=========================>....] - ETA: 56s - loss: 2.7732 - regression_loss: 2.1079 - classification_loss: 0.6653
 877/1000 [=========================>....] - ETA: 55s - loss: 2.7744 - regression_loss: 2.1091 - classification_loss: 0.6653
 878/1000 [=========================>....] - ETA: 55s - loss: 2.7747 - regression_loss: 2.1090 - classification_loss: 0.6657
 879/1000 [=========================>....] - ETA: 54s - loss: 2.7770 - regression_loss: 2.1105 - classification_loss: 0.6665
 880/1000 [=========================>....] - ETA: 54s - loss: 2.7738 - regression_loss: 2.1081 - classification_loss: 0.6657
 881/1000 [=========================>....] - ETA: 53s - loss: 2.7765 - regression_loss: 2.1098 - classification_loss: 0.6667
 882/1000 [=========================>....] - ETA: 53s - loss: 2.7733 - regression_loss: 2.1074 - classification_loss: 0.6659
 883/1000 [=========================>....] - ETA: 52s - loss: 2.7737 - regression_loss: 2.1076 - classification_loss: 0.6661
 884/1000 [=========================>....] - ETA: 52s - loss: 2.7706 - regression_loss: 2.1053 - classification_loss: 0.6653
 885/1000 [=========================>....] - ETA: 52s - loss: 2.7735 - regression_loss: 2.1073 - classification_loss: 0.6662
 886/1000 [=========================>....] - ETA: 51s - loss: 2.7734 - regression_loss: 2.1074 - classification_loss: 0.6660
 887/1000 [=========================>....] - ETA: 51s - loss: 2.7706 - regression_loss: 2.1050 - classification_loss: 0.6656
 888/1000 [=========================>....] - ETA: 50s - loss: 2.7715 - regression_loss: 2.1058 - classification_loss: 0.6657
 889/1000 [=========================>....] - ETA: 50s - loss: 2.7733 - regression_loss: 2.1066 - classification_loss: 0.6668
 890/1000 [=========================>....] - ETA: 49s - loss: 2.7740 - regression_loss: 2.1063 - classification_loss: 0.6677
 891/1000 [=========================>....] - ETA: 49s - loss: 2.7741 - regression_loss: 2.1063 - classification_loss: 0.6678
 892/1000 [=========================>....] - ETA: 48s - loss: 2.7710 - regression_loss: 2.1039 - classification_loss: 0.6671
 893/1000 [=========================>....] - ETA: 48s - loss: 2.7712 - regression_loss: 2.1041 - classification_loss: 0.6670
 894/1000 [=========================>....] - ETA: 47s - loss: 2.7681 - regression_loss: 2.1018 - classification_loss: 0.6663
 895/1000 [=========================>....] - ETA: 47s - loss: 2.7694 - regression_loss: 2.1027 - classification_loss: 0.6667
 896/1000 [=========================>....] - ETA: 47s - loss: 2.7706 - regression_loss: 2.1037 - classification_loss: 0.6669
 897/1000 [=========================>....] - ETA: 46s - loss: 2.7736 - regression_loss: 2.1056 - classification_loss: 0.6680
 898/1000 [=========================>....] - ETA: 46s - loss: 2.7750 - regression_loss: 2.1065 - classification_loss: 0.6685
 899/1000 [=========================>....] - ETA: 45s - loss: 2.7767 - regression_loss: 2.1074 - classification_loss: 0.6693
 900/1000 [==========================>...] - ETA: 45s - loss: 2.7760 - regression_loss: 2.1067 - classification_loss: 0.6693
 901/1000 [==========================>...] - ETA: 44s - loss: 2.7781 - regression_loss: 2.1084 - classification_loss: 0.6698
 902/1000 [==========================>...] - ETA: 44s - loss: 2.7791 - regression_loss: 2.1092 - classification_loss: 0.6699
 903/1000 [==========================>...] - ETA: 43s - loss: 2.7802 - regression_loss: 2.1098 - classification_loss: 0.6705
 904/1000 [==========================>...] - ETA: 43s - loss: 2.7773 - regression_loss: 2.1074 - classification_loss: 0.6698
 905/1000 [==========================>...] - ETA: 42s - loss: 2.7785 - regression_loss: 2.1081 - classification_loss: 0.6704
 906/1000 [==========================>...] - ETA: 42s - loss: 2.7754 - regression_loss: 2.1058 - classification_loss: 0.6697
 907/1000 [==========================>...] - ETA: 42s - loss: 2.7769 - regression_loss: 2.1072 - classification_loss: 0.6697
 908/1000 [==========================>...] - ETA: 41s - loss: 2.7792 - regression_loss: 2.1088 - classification_loss: 0.6704
 909/1000 [==========================>...] - ETA: 41s - loss: 2.7762 - regression_loss: 2.1065 - classification_loss: 0.6697
 910/1000 [==========================>...] - ETA: 40s - loss: 2.7772 - regression_loss: 2.1074 - classification_loss: 0.6698
 911/1000 [==========================>...] - ETA: 40s - loss: 2.7774 - regression_loss: 2.1076 - classification_loss: 0.6698
 912/1000 [==========================>...] - ETA: 39s - loss: 2.7773 - regression_loss: 2.1075 - classification_loss: 0.6698
 913/1000 [==========================>...] - ETA: 39s - loss: 2.7785 - regression_loss: 2.1088 - classification_loss: 0.6698
 914/1000 [==========================>...] - ETA: 38s - loss: 2.7784 - regression_loss: 2.1087 - classification_loss: 0.6697
 915/1000 [==========================>...] - ETA: 38s - loss: 2.7785 - regression_loss: 2.1089 - classification_loss: 0.6697
 916/1000 [==========================>...] - ETA: 38s - loss: 2.7788 - regression_loss: 2.1091 - classification_loss: 0.6697
 917/1000 [==========================>...] - ETA: 37s - loss: 2.7815 - regression_loss: 2.1115 - classification_loss: 0.6700
 918/1000 [==========================>...] - ETA: 37s - loss: 2.7831 - regression_loss: 2.1122 - classification_loss: 0.6709
 919/1000 [==========================>...] - ETA: 36s - loss: 2.7832 - regression_loss: 2.1124 - classification_loss: 0.6708
 920/1000 [==========================>...] - ETA: 36s - loss: 2.7845 - regression_loss: 2.1131 - classification_loss: 0.6714
 921/1000 [==========================>...] - ETA: 35s - loss: 2.7854 - regression_loss: 2.1140 - classification_loss: 0.6714
 922/1000 [==========================>...] - ETA: 35s - loss: 2.7825 - regression_loss: 2.1117 - classification_loss: 0.6708
 923/1000 [==========================>...] - ETA: 34s - loss: 2.7816 - regression_loss: 2.1113 - classification_loss: 0.6703
 924/1000 [==========================>...] - ETA: 34s - loss: 2.7834 - regression_loss: 2.1126 - classification_loss: 0.6709
 925/1000 [==========================>...] - ETA: 33s - loss: 2.7804 - regression_loss: 2.1103 - classification_loss: 0.6701
 926/1000 [==========================>...] - ETA: 33s - loss: 2.7817 - regression_loss: 2.1117 - classification_loss: 0.6700
 927/1000 [==========================>...] - ETA: 33s - loss: 2.7831 - regression_loss: 2.1127 - classification_loss: 0.6704
 928/1000 [==========================>...] - ETA: 32s - loss: 2.7802 - regression_loss: 2.1105 - classification_loss: 0.6697
 929/1000 [==========================>...] - ETA: 32s - loss: 2.7823 - regression_loss: 2.1117 - classification_loss: 0.6706
 930/1000 [==========================>...] - ETA: 31s - loss: 2.7836 - regression_loss: 2.1129 - classification_loss: 0.6707
 931/1000 [==========================>...] - ETA: 31s - loss: 2.7808 - regression_loss: 2.1106 - classification_loss: 0.6702
 932/1000 [==========================>...] - ETA: 30s - loss: 2.7806 - regression_loss: 2.1107 - classification_loss: 0.6699
 933/1000 [==========================>...] - ETA: 30s - loss: 2.7802 - regression_loss: 2.1106 - classification_loss: 0.6696
 934/1000 [===========================>..] - ETA: 29s - loss: 2.7772 - regression_loss: 2.1083 - classification_loss: 0.6689
 935/1000 [===========================>..] - ETA: 29s - loss: 2.7772 - regression_loss: 2.1085 - classification_loss: 0.6687
 936/1000 [===========================>..] - ETA: 28s - loss: 2.7795 - regression_loss: 2.1110 - classification_loss: 0.6686
 937/1000 [===========================>..] - ETA: 28s - loss: 2.7795 - regression_loss: 2.1109 - classification_loss: 0.6685
 938/1000 [===========================>..] - ETA: 28s - loss: 2.7800 - regression_loss: 2.1118 - classification_loss: 0.6682
 939/1000 [===========================>..] - ETA: 27s - loss: 2.7822 - regression_loss: 2.1141 - classification_loss: 0.6681
 940/1000 [===========================>..] - ETA: 27s - loss: 2.7793 - regression_loss: 2.1119 - classification_loss: 0.6674
 941/1000 [===========================>..] - ETA: 26s - loss: 2.7791 - regression_loss: 2.1121 - classification_loss: 0.6671
 942/1000 [===========================>..] - ETA: 26s - loss: 2.7794 - regression_loss: 2.1125 - classification_loss: 0.6669
 943/1000 [===========================>..] - ETA: 25s - loss: 2.7764 - regression_loss: 2.1103 - classification_loss: 0.6662
 944/1000 [===========================>..] - ETA: 25s - loss: 2.7763 - regression_loss: 2.1106 - classification_loss: 0.6657
 945/1000 [===========================>..] - ETA: 24s - loss: 2.7764 - regression_loss: 2.1106 - classification_loss: 0.6658
 946/1000 [===========================>..] - ETA: 24s - loss: 2.7735 - regression_loss: 2.1084 - classification_loss: 0.6651
 947/1000 [===========================>..] - ETA: 23s - loss: 2.7706 - regression_loss: 2.1061 - classification_loss: 0.6644
 948/1000 [===========================>..] - ETA: 23s - loss: 2.7717 - regression_loss: 2.1062 - classification_loss: 0.6655
 949/1000 [===========================>..] - ETA: 23s - loss: 2.7741 - regression_loss: 2.1079 - classification_loss: 0.6663
 950/1000 [===========================>..] - ETA: 22s - loss: 2.7777 - regression_loss: 2.1105 - classification_loss: 0.6672
 951/1000 [===========================>..] - ETA: 22s - loss: 2.7775 - regression_loss: 2.1102 - classification_loss: 0.6673
 952/1000 [===========================>..] - ETA: 21s - loss: 2.7780 - regression_loss: 2.1108 - classification_loss: 0.6672
 953/1000 [===========================>..] - ETA: 21s - loss: 2.7773 - regression_loss: 2.1101 - classification_loss: 0.6672
 954/1000 [===========================>..] - ETA: 20s - loss: 2.7772 - regression_loss: 2.1102 - classification_loss: 0.6670
 955/1000 [===========================>..] - ETA: 20s - loss: 2.7798 - regression_loss: 2.1118 - classification_loss: 0.6680
 956/1000 [===========================>..] - ETA: 19s - loss: 2.7804 - regression_loss: 2.1124 - classification_loss: 0.6680
 957/1000 [===========================>..] - ETA: 19s - loss: 2.7806 - regression_loss: 2.1126 - classification_loss: 0.6681
 958/1000 [===========================>..] - ETA: 19s - loss: 2.7777 - regression_loss: 2.1104 - classification_loss: 0.6674
 959/1000 [===========================>..] - ETA: 18s - loss: 2.7749 - regression_loss: 2.1082 - classification_loss: 0.6667
 960/1000 [===========================>..] - ETA: 18s - loss: 2.7754 - regression_loss: 2.1087 - classification_loss: 0.6666
 961/1000 [===========================>..] - ETA: 17s - loss: 2.7784 - regression_loss: 2.1100 - classification_loss: 0.6684
 962/1000 [===========================>..] - ETA: 17s - loss: 2.7785 - regression_loss: 2.1104 - classification_loss: 0.6681
 963/1000 [===========================>..] - ETA: 16s - loss: 2.7802 - regression_loss: 2.1111 - classification_loss: 0.6690
 964/1000 [===========================>..] - ETA: 16s - loss: 2.7801 - regression_loss: 2.1112 - classification_loss: 0.6689
 965/1000 [===========================>..] - ETA: 15s - loss: 2.7805 - regression_loss: 2.1119 - classification_loss: 0.6687
 966/1000 [===========================>..] - ETA: 15s - loss: 2.7777 - regression_loss: 2.1097 - classification_loss: 0.6680
 967/1000 [============================>.] - ETA: 14s - loss: 2.7786 - regression_loss: 2.1106 - classification_loss: 0.6679
 968/1000 [============================>.] - ETA: 14s - loss: 2.7794 - regression_loss: 2.1109 - classification_loss: 0.6685
 969/1000 [============================>.] - ETA: 14s - loss: 2.7798 - regression_loss: 2.1112 - classification_loss: 0.6686
 970/1000 [============================>.] - ETA: 13s - loss: 2.7769 - regression_loss: 2.1090 - classification_loss: 0.6679
 971/1000 [============================>.] - ETA: 13s - loss: 2.7792 - regression_loss: 2.1103 - classification_loss: 0.6690
 972/1000 [============================>.] - ETA: 12s - loss: 2.7781 - regression_loss: 2.1081 - classification_loss: 0.6700
 973/1000 [============================>.] - ETA: 12s - loss: 2.7778 - regression_loss: 2.1077 - classification_loss: 0.6702
 974/1000 [============================>.] - ETA: 11s - loss: 2.7781 - regression_loss: 2.1082 - classification_loss: 0.6700
 975/1000 [============================>.] - ETA: 11s - loss: 2.7781 - regression_loss: 2.1082 - classification_loss: 0.6698
 976/1000 [============================>.] - ETA: 10s - loss: 2.7795 - regression_loss: 2.1093 - classification_loss: 0.6702
 977/1000 [============================>.] - ETA: 10s - loss: 2.7804 - regression_loss: 2.1097 - classification_loss: 0.6706
 978/1000 [============================>.] - ETA: 9s - loss: 2.7800 - regression_loss: 2.1092 - classification_loss: 0.6708 
 979/1000 [============================>.] - ETA: 9s - loss: 2.7817 - regression_loss: 2.1109 - classification_loss: 0.6708
 980/1000 [============================>.] - ETA: 9s - loss: 2.7791 - regression_loss: 2.1088 - classification_loss: 0.6703
 981/1000 [============================>.] - ETA: 8s - loss: 2.7765 - regression_loss: 2.1066 - classification_loss: 0.6698
 982/1000 [============================>.] - ETA: 8s - loss: 2.7737 - regression_loss: 2.1045 - classification_loss: 0.6692
 983/1000 [============================>.] - ETA: 7s - loss: 2.7721 - regression_loss: 2.1023 - classification_loss: 0.6698
 984/1000 [============================>.] - ETA: 7s - loss: 2.7724 - regression_loss: 2.1028 - classification_loss: 0.6696
 985/1000 [============================>.] - ETA: 6s - loss: 2.7732 - regression_loss: 2.1034 - classification_loss: 0.6698
 986/1000 [============================>.] - ETA: 6s - loss: 2.7739 - regression_loss: 2.1043 - classification_loss: 0.6696
 987/1000 [============================>.] - ETA: 5s - loss: 2.7746 - regression_loss: 2.1049 - classification_loss: 0.6698
 988/1000 [============================>.] - ETA: 5s - loss: 2.7777 - regression_loss: 2.1077 - classification_loss: 0.6700
 989/1000 [============================>.] - ETA: 4s - loss: 2.7800 - regression_loss: 2.1097 - classification_loss: 0.6704
 990/1000 [============================>.] - ETA: 4s - loss: 2.7773 - regression_loss: 2.1076 - classification_loss: 0.6697
 991/1000 [============================>.] - ETA: 4s - loss: 2.7783 - regression_loss: 2.1087 - classification_loss: 0.6696
 992/1000 [============================>.] - ETA: 3s - loss: 2.7790 - regression_loss: 2.1093 - classification_loss: 0.6696
 993/1000 [============================>.] - ETA: 3s - loss: 2.7790 - regression_loss: 2.1093 - classification_loss: 0.6697
 994/1000 [============================>.] - ETA: 2s - loss: 2.7813 - regression_loss: 2.1108 - classification_loss: 0.6704
 995/1000 [============================>.] - ETA: 2s - loss: 2.7813 - regression_loss: 2.1109 - classification_loss: 0.6704
 996/1000 [============================>.] - ETA: 1s - loss: 2.7833 - regression_loss: 2.1117 - classification_loss: 0.6716
 997/1000 [============================>.] - ETA: 1s - loss: 2.7838 - regression_loss: 2.1119 - classification_loss: 0.6719
 998/1000 [============================>.] - ETA: 0s - loss: 2.7843 - regression_loss: 2.1124 - classification_loss: 0.6719
 999/1000 [============================>.] - ETA: 0s - loss: 2.7850 - regression_loss: 2.1127 - classification_loss: 0.6723
1000/1000 [==============================] - 453s 453ms/step - loss: 2.7875 - regression_loss: 2.1143 - classification_loss: 0.6733

Epoch 00015: saving model to ./snapshots/resnet50_csv_15.h5
0/2001/2002/2003/2004/2005/2006/2007/2008/2009/20010/20011/20012/20013/20014/20015/20016/20017/20018/20019/20020/20021/20022/20023/20024/20025/20026/20027/20028/20029/20030/20031/20032/20033/20034/20035/20036/20037/20038/20039/20040/20041/20042/20043/20044/20045/20046/20047/20048/20049/20050/20051/20052/20053/20054/20055/20056/20057/20058/20059/20060/20061/20062/20063/20064/20065/20066/20067/20068/20069/20070/20071/20072/20073/20074/20075/20076/20077/20078/20079/20080/20081/20082/20083/20084/20085/20086/20087/20088/20089/20090/20091/20092/20093/20094/20095/20096/20097/20098/20099/200100/200101/200102/200103/200104/200105/200106/200107/200108/200109/200110/200111/200112/200113/200114/200115/200116/200117/200118/200119/200120/200121/200122/200123/200124/200125/200126/200127/200128/200129/200130/200131/200132/200133/200134/200135/200136/200137/200138/200139/200140/200141/200142/200143/200144/200145/200146/200147/200148/200149/200150/200151/200152/200153/200154/200155/200156/200157/200158/200159/200160/200161/200162/200163/200164/200165/200166/200167/200168/200169/200170/200171/200172/200173/200174/200175/200176/200177/200178/200179/200180/200181/200182/200183/200184/200185/200186/200187/200188/200189/200190/200191/200192/200193/200194/200195/200196/200197/200198/200199/2000/2001/2002/2003/2004/2005/2006/2007/2008/2009/20010/20011/20012/20013/20014/20015/20016/20017/20018/20019/20020/20021/20022/20023/20024/20025/20026/20027/20028/20029/20030/20031/20032/20033/20034/20035/20036/20037/20038/20039/20040/20041/20042/20043/20044/20045/20046/20047/20048/20049/20050/20051/20052/20053/20054/20055/20056/20057/20058/20059/20060/20061/20062/20063/20064/20065/20066/20067/20068/20069/20070/20071/20072/20073/20074/20075/20076/20077/20078/20079/20080/20081/20082/20083/20084/20085/20086/20087/20088/20089/20090/20091/20092/20093/20094/20095/20096/20097/20098/20099/200100/200101/200102/200103/200104/200105/200106/200107/200108/200109/200110/200111/200112/200113/200114/200115/200116/200117/200118/200119/200120/200121/200122/200123/200124/200125/200126/200127/200128/200129/200130/200131/200132/200133/200134/200135/200136/200137/200138/200139/200140/200141/200142/200143/200144/200145/200146/200147/200148/200149/200150/200151/200152/200153/200154/200155/200156/200157/200158/200159/200160/200161/200162/200163/200164/200165/200166/200167/200168/200169/200170/200171/200172/200173/200174/200175/200176/200177/200178/200179/200180/200181/200182/200183/200184/200185/200186/200187/200188/200189/200190/200191/200192/200193/200194/200195/200196/200197/200198/200199/200

M 0.1536
N 0.0262
mAP: 0.0899
Epoch 16/30

   1/1000 [..............................] - ETA: 7:18 - loss: 4.9646 - regression_loss: 4.0790 - classification_loss: 0.8856
   2/1000 [..............................] - ETA: 7:23 - loss: 3.9718 - regression_loss: 3.1929 - classification_loss: 0.7789
   3/1000 [..............................] - ETA: 7:25 - loss: 3.8911 - regression_loss: 3.0526 - classification_loss: 0.8385
   4/1000 [..............................] - ETA: 7:26 - loss: 3.7302 - regression_loss: 2.9114 - classification_loss: 0.8188
   5/1000 [..............................] - ETA: 7:28 - loss: 2.9842 - regression_loss: 2.3291 - classification_loss: 0.6551
   6/1000 [..............................] - ETA: 7:27 - loss: 2.8944 - regression_loss: 2.2284 - classification_loss: 0.6660
   7/1000 [..............................] - ETA: 7:27 - loss: 3.0791 - regression_loss: 2.3080 - classification_loss: 0.7710
   8/1000 [..............................] - ETA: 7:27 - loss: 2.9973 - regression_loss: 2.2894 - classification_loss: 0.7079
   9/1000 [..............................] - ETA: 7:26 - loss: 3.0601 - regression_loss: 2.3293 - classification_loss: 0.7309
  10/1000 [..............................] - ETA: 7:25 - loss: 2.7679 - regression_loss: 2.0964 - classification_loss: 0.6715
  11/1000 [..............................] - ETA: 7:24 - loss: 2.9691 - regression_loss: 2.2757 - classification_loss: 0.6934
  12/1000 [..............................] - ETA: 7:24 - loss: 3.0110 - regression_loss: 2.3093 - classification_loss: 0.7016
  13/1000 [..............................] - ETA: 7:24 - loss: 3.0126 - regression_loss: 2.3017 - classification_loss: 0.7109
  14/1000 [..............................] - ETA: 7:24 - loss: 3.0850 - regression_loss: 2.3727 - classification_loss: 0.7124
  15/1000 [..............................] - ETA: 7:23 - loss: 2.8799 - regression_loss: 2.2145 - classification_loss: 0.6654
  16/1000 [..............................] - ETA: 7:23 - loss: 2.7004 - regression_loss: 2.0761 - classification_loss: 0.6243
  17/1000 [..............................] - ETA: 7:23 - loss: 2.5435 - regression_loss: 1.9540 - classification_loss: 0.5896
  18/1000 [..............................] - ETA: 7:23 - loss: 2.4049 - regression_loss: 1.8454 - classification_loss: 0.5595
  19/1000 [..............................] - ETA: 7:23 - loss: 2.5593 - regression_loss: 1.9881 - classification_loss: 0.5712
  20/1000 [..............................] - ETA: 7:22 - loss: 2.6153 - regression_loss: 2.0428 - classification_loss: 0.5725
  21/1000 [..............................] - ETA: 7:21 - loss: 2.7084 - regression_loss: 2.1164 - classification_loss: 0.5920
  22/1000 [..............................] - ETA: 7:21 - loss: 2.8928 - regression_loss: 2.2359 - classification_loss: 0.6569
  23/1000 [..............................] - ETA: 7:21 - loss: 2.8916 - regression_loss: 2.2337 - classification_loss: 0.6579
  24/1000 [..............................] - ETA: 7:20 - loss: 2.8739 - regression_loss: 2.2145 - classification_loss: 0.6595
  25/1000 [..............................] - ETA: 7:20 - loss: 2.8808 - regression_loss: 2.2267 - classification_loss: 0.6541
  26/1000 [..............................] - ETA: 7:19 - loss: 2.7700 - regression_loss: 2.1410 - classification_loss: 0.6290
  27/1000 [..............................] - ETA: 7:19 - loss: 2.7429 - regression_loss: 2.1167 - classification_loss: 0.6262
  28/1000 [..............................] - ETA: 7:18 - loss: 2.8047 - regression_loss: 2.1579 - classification_loss: 0.6468
  29/1000 [..............................] - ETA: 7:18 - loss: 2.7080 - regression_loss: 2.0835 - classification_loss: 0.6245
  30/1000 [..............................] - ETA: 7:17 - loss: 2.7615 - regression_loss: 2.1219 - classification_loss: 0.6397
  31/1000 [..............................] - ETA: 7:17 - loss: 2.7994 - regression_loss: 2.1512 - classification_loss: 0.6482
  32/1000 [..............................] - ETA: 7:17 - loss: 2.7963 - regression_loss: 2.1510 - classification_loss: 0.6453
  33/1000 [..............................] - ETA: 7:16 - loss: 2.7879 - regression_loss: 2.1395 - classification_loss: 0.6483
  34/1000 [>.............................] - ETA: 7:16 - loss: 2.7796 - regression_loss: 2.1307 - classification_loss: 0.6488
  35/1000 [>.............................] - ETA: 7:15 - loss: 2.9863 - regression_loss: 2.3050 - classification_loss: 0.6812
  36/1000 [>.............................] - ETA: 7:15 - loss: 2.9948 - regression_loss: 2.2934 - classification_loss: 0.7014
  37/1000 [>.............................] - ETA: 7:14 - loss: 2.9760 - regression_loss: 2.2833 - classification_loss: 0.6926
  38/1000 [>.............................] - ETA: 7:14 - loss: 2.9806 - regression_loss: 2.2848 - classification_loss: 0.6958
  39/1000 [>.............................] - ETA: 7:13 - loss: 2.9042 - regression_loss: 2.2262 - classification_loss: 0.6780
  40/1000 [>.............................] - ETA: 7:13 - loss: 2.9320 - regression_loss: 2.2472 - classification_loss: 0.6848
  41/1000 [>.............................] - ETA: 7:12 - loss: 2.9523 - regression_loss: 2.2631 - classification_loss: 0.6891
  42/1000 [>.............................] - ETA: 7:11 - loss: 2.8831 - regression_loss: 2.2093 - classification_loss: 0.6738
  43/1000 [>.............................] - ETA: 7:11 - loss: 2.8161 - regression_loss: 2.1579 - classification_loss: 0.6582
  44/1000 [>.............................] - ETA: 7:11 - loss: 2.8095 - regression_loss: 2.1507 - classification_loss: 0.6588
  45/1000 [>.............................] - ETA: 7:10 - loss: 2.7472 - regression_loss: 2.1029 - classification_loss: 0.6442
  46/1000 [>.............................] - ETA: 7:10 - loss: 2.7724 - regression_loss: 2.1224 - classification_loss: 0.6500
  47/1000 [>.............................] - ETA: 7:09 - loss: 2.7896 - regression_loss: 2.1333 - classification_loss: 0.6563
  48/1000 [>.............................] - ETA: 7:09 - loss: 2.7318 - regression_loss: 2.0889 - classification_loss: 0.6429
  49/1000 [>.............................] - ETA: 7:08 - loss: 2.7410 - regression_loss: 2.1048 - classification_loss: 0.6363
  50/1000 [>.............................] - ETA: 7:08 - loss: 2.6862 - regression_loss: 2.0627 - classification_loss: 0.6235
  51/1000 [>.............................] - ETA: 7:07 - loss: 2.7567 - regression_loss: 2.1232 - classification_loss: 0.6335
  52/1000 [>.............................] - ETA: 7:07 - loss: 2.7502 - regression_loss: 2.1156 - classification_loss: 0.6345
  53/1000 [>.............................] - ETA: 7:07 - loss: 2.7584 - regression_loss: 2.1267 - classification_loss: 0.6317
  54/1000 [>.............................] - ETA: 7:06 - loss: 2.7821 - regression_loss: 2.1480 - classification_loss: 0.6342
  55/1000 [>.............................] - ETA: 7:06 - loss: 2.7828 - regression_loss: 2.1493 - classification_loss: 0.6335
  56/1000 [>.............................] - ETA: 7:05 - loss: 2.7904 - regression_loss: 2.1551 - classification_loss: 0.6352
  57/1000 [>.............................] - ETA: 7:05 - loss: 2.7414 - regression_loss: 2.1173 - classification_loss: 0.6241
  58/1000 [>.............................] - ETA: 7:05 - loss: 2.6941 - regression_loss: 2.0808 - classification_loss: 0.6133
  59/1000 [>.............................] - ETA: 7:04 - loss: 2.7135 - regression_loss: 2.0994 - classification_loss: 0.6141
  60/1000 [>.............................] - ETA: 7:04 - loss: 2.7353 - regression_loss: 2.1104 - classification_loss: 0.6249
  61/1000 [>.............................] - ETA: 7:03 - loss: 2.7235 - regression_loss: 2.0979 - classification_loss: 0.6256
  62/1000 [>.............................] - ETA: 7:03 - loss: 2.7192 - regression_loss: 2.0925 - classification_loss: 0.6267
  63/1000 [>.............................] - ETA: 7:03 - loss: 2.7445 - regression_loss: 2.0993 - classification_loss: 0.6452
  64/1000 [>.............................] - ETA: 7:02 - loss: 2.7616 - regression_loss: 2.1172 - classification_loss: 0.6443
  65/1000 [>.............................] - ETA: 7:02 - loss: 2.7856 - regression_loss: 2.1241 - classification_loss: 0.6615
  66/1000 [>.............................] - ETA: 7:01 - loss: 2.8064 - regression_loss: 2.1254 - classification_loss: 0.6810
  67/1000 [=>............................] - ETA: 7:01 - loss: 2.7646 - regression_loss: 2.0937 - classification_loss: 0.6709
  68/1000 [=>............................] - ETA: 7:01 - loss: 2.7608 - regression_loss: 2.0924 - classification_loss: 0.6684
  69/1000 [=>............................] - ETA: 7:00 - loss: 2.7880 - regression_loss: 2.1082 - classification_loss: 0.6798
  70/1000 [=>............................] - ETA: 7:00 - loss: 2.7982 - regression_loss: 2.1142 - classification_loss: 0.6840
  71/1000 [=>............................] - ETA: 6:59 - loss: 2.8143 - regression_loss: 2.1312 - classification_loss: 0.6831
  72/1000 [=>............................] - ETA: 6:59 - loss: 2.8418 - regression_loss: 2.1476 - classification_loss: 0.6942
  73/1000 [=>............................] - ETA: 6:58 - loss: 2.8028 - regression_loss: 2.1182 - classification_loss: 0.6847
  74/1000 [=>............................] - ETA: 6:58 - loss: 2.8181 - regression_loss: 2.1226 - classification_loss: 0.6955
  75/1000 [=>............................] - ETA: 6:58 - loss: 2.8140 - regression_loss: 2.1240 - classification_loss: 0.6900
  76/1000 [=>............................] - ETA: 6:57 - loss: 2.8228 - regression_loss: 2.1349 - classification_loss: 0.6879
  77/1000 [=>............................] - ETA: 6:57 - loss: 2.8637 - regression_loss: 2.1556 - classification_loss: 0.7080
  78/1000 [=>............................] - ETA: 6:56 - loss: 2.8270 - regression_loss: 2.1280 - classification_loss: 0.6990
  79/1000 [=>............................] - ETA: 6:55 - loss: 2.7912 - regression_loss: 2.1011 - classification_loss: 0.6901
  80/1000 [=>............................] - ETA: 6:55 - loss: 2.8178 - regression_loss: 2.1283 - classification_loss: 0.6895
  81/1000 [=>............................] - ETA: 6:55 - loss: 2.8148 - regression_loss: 2.1278 - classification_loss: 0.6871
  82/1000 [=>............................] - ETA: 6:54 - loss: 2.7805 - regression_loss: 2.1018 - classification_loss: 0.6787
  83/1000 [=>............................] - ETA: 6:54 - loss: 2.7470 - regression_loss: 2.0765 - classification_loss: 0.6705
  84/1000 [=>............................] - ETA: 6:53 - loss: 2.7143 - regression_loss: 2.0518 - classification_loss: 0.6625
  85/1000 [=>............................] - ETA: 6:53 - loss: 2.6824 - regression_loss: 2.0277 - classification_loss: 0.6547
  86/1000 [=>............................] - ETA: 6:52 - loss: 2.6512 - regression_loss: 2.0041 - classification_loss: 0.6471
  87/1000 [=>............................] - ETA: 6:52 - loss: 2.6835 - regression_loss: 2.0189 - classification_loss: 0.6645
  88/1000 [=>............................] - ETA: 6:51 - loss: 2.6530 - regression_loss: 1.9960 - classification_loss: 0.6570
  89/1000 [=>............................] - ETA: 6:51 - loss: 2.6646 - regression_loss: 2.0105 - classification_loss: 0.6541
  90/1000 [=>............................] - ETA: 6:50 - loss: 2.6665 - regression_loss: 2.0135 - classification_loss: 0.6530
  91/1000 [=>............................] - ETA: 6:50 - loss: 2.6929 - regression_loss: 2.0260 - classification_loss: 0.6669
  92/1000 [=>............................] - ETA: 6:49 - loss: 2.6894 - regression_loss: 2.0252 - classification_loss: 0.6641
  93/1000 [=>............................] - ETA: 6:49 - loss: 2.7054 - regression_loss: 2.0418 - classification_loss: 0.6635
  94/1000 [=>............................] - ETA: 6:49 - loss: 2.7056 - regression_loss: 2.0397 - classification_loss: 0.6658
  95/1000 [=>............................] - ETA: 6:48 - loss: 2.6949 - regression_loss: 2.0339 - classification_loss: 0.6609
  96/1000 [=>............................] - ETA: 6:48 - loss: 2.7028 - regression_loss: 2.0439 - classification_loss: 0.6589
  97/1000 [=>............................] - ETA: 6:47 - loss: 2.7077 - regression_loss: 2.0516 - classification_loss: 0.6562
  98/1000 [=>............................] - ETA: 6:47 - loss: 2.6997 - regression_loss: 2.0480 - classification_loss: 0.6517
  99/1000 [=>............................] - ETA: 6:47 - loss: 2.7308 - regression_loss: 2.0686 - classification_loss: 0.6622
 100/1000 [==>...........................] - ETA: 6:46 - loss: 2.7304 - regression_loss: 2.0705 - classification_loss: 0.6599
 101/1000 [==>...........................] - ETA: 6:46 - loss: 2.7033 - regression_loss: 2.0500 - classification_loss: 0.6534
 102/1000 [==>...........................] - ETA: 6:45 - loss: 2.6768 - regression_loss: 2.0299 - classification_loss: 0.6469
 103/1000 [==>...........................] - ETA: 6:45 - loss: 2.6797 - regression_loss: 2.0326 - classification_loss: 0.6470
 104/1000 [==>...........................] - ETA: 6:44 - loss: 2.6559 - regression_loss: 2.0131 - classification_loss: 0.6428
 105/1000 [==>...........................] - ETA: 6:44 - loss: 2.6638 - regression_loss: 2.0180 - classification_loss: 0.6459
 106/1000 [==>...........................] - ETA: 6:43 - loss: 2.7032 - regression_loss: 2.0428 - classification_loss: 0.6604
 107/1000 [==>...........................] - ETA: 6:43 - loss: 2.7364 - regression_loss: 2.0596 - classification_loss: 0.6768
 108/1000 [==>...........................] - ETA: 6:43 - loss: 2.7574 - regression_loss: 2.0708 - classification_loss: 0.6865
 109/1000 [==>...........................] - ETA: 6:42 - loss: 2.7736 - regression_loss: 2.0764 - classification_loss: 0.6972
 110/1000 [==>...........................] - ETA: 6:42 - loss: 2.7748 - regression_loss: 2.0814 - classification_loss: 0.6934
 111/1000 [==>...........................] - ETA: 6:41 - loss: 2.7915 - regression_loss: 2.0908 - classification_loss: 0.7007
 112/1000 [==>...........................] - ETA: 6:41 - loss: 2.7779 - regression_loss: 2.0807 - classification_loss: 0.6973
 113/1000 [==>...........................] - ETA: 6:40 - loss: 2.7534 - regression_loss: 2.0623 - classification_loss: 0.6911
 114/1000 [==>...........................] - ETA: 6:40 - loss: 2.7548 - regression_loss: 2.0655 - classification_loss: 0.6893
 115/1000 [==>...........................] - ETA: 6:39 - loss: 2.7599 - regression_loss: 2.0730 - classification_loss: 0.6869
 116/1000 [==>...........................] - ETA: 6:39 - loss: 2.7525 - regression_loss: 2.0686 - classification_loss: 0.6839
 117/1000 [==>...........................] - ETA: 6:38 - loss: 2.7716 - regression_loss: 2.0798 - classification_loss: 0.6917
 118/1000 [==>...........................] - ETA: 6:38 - loss: 2.7481 - regression_loss: 2.0622 - classification_loss: 0.6859
 119/1000 [==>...........................] - ETA: 6:37 - loss: 2.7603 - regression_loss: 2.0678 - classification_loss: 0.6926
 120/1000 [==>...........................] - ETA: 6:37 - loss: 2.7728 - regression_loss: 2.0724 - classification_loss: 0.7004
 121/1000 [==>...........................] - ETA: 6:36 - loss: 2.7826 - regression_loss: 2.0820 - classification_loss: 0.7006
 122/1000 [==>...........................] - ETA: 6:36 - loss: 2.7821 - regression_loss: 2.0821 - classification_loss: 0.7000
 123/1000 [==>...........................] - ETA: 6:36 - loss: 2.7888 - regression_loss: 2.0911 - classification_loss: 0.6976
 124/1000 [==>...........................] - ETA: 6:35 - loss: 2.8081 - regression_loss: 2.1055 - classification_loss: 0.7026
 125/1000 [==>...........................] - ETA: 6:35 - loss: 2.8036 - regression_loss: 2.1056 - classification_loss: 0.6981
 126/1000 [==>...........................] - ETA: 6:34 - loss: 2.7981 - regression_loss: 2.1011 - classification_loss: 0.6970
 127/1000 [==>...........................] - ETA: 6:34 - loss: 2.8113 - regression_loss: 2.1107 - classification_loss: 0.7006
 128/1000 [==>...........................] - ETA: 6:33 - loss: 2.8177 - regression_loss: 2.1164 - classification_loss: 0.7013
 129/1000 [==>...........................] - ETA: 6:33 - loss: 2.8079 - regression_loss: 2.1094 - classification_loss: 0.6985
 130/1000 [==>...........................] - ETA: 6:32 - loss: 2.8216 - regression_loss: 2.1166 - classification_loss: 0.7050
 131/1000 [==>...........................] - ETA: 6:32 - loss: 2.8001 - regression_loss: 2.1005 - classification_loss: 0.6996
 132/1000 [==>...........................] - ETA: 6:32 - loss: 2.8066 - regression_loss: 2.1027 - classification_loss: 0.7039
 133/1000 [==>...........................] - ETA: 6:31 - loss: 2.8222 - regression_loss: 2.1131 - classification_loss: 0.7091
 134/1000 [===>..........................] - ETA: 6:31 - loss: 2.8012 - regression_loss: 2.0974 - classification_loss: 0.7038
 135/1000 [===>..........................] - ETA: 6:30 - loss: 2.8064 - regression_loss: 2.1008 - classification_loss: 0.7056
 136/1000 [===>..........................] - ETA: 6:30 - loss: 2.8157 - regression_loss: 2.1100 - classification_loss: 0.7057
 137/1000 [===>..........................] - ETA: 6:29 - loss: 2.8163 - regression_loss: 2.1093 - classification_loss: 0.7070
 138/1000 [===>..........................] - ETA: 6:29 - loss: 2.8172 - regression_loss: 2.1111 - classification_loss: 0.7061
 139/1000 [===>..........................] - ETA: 6:29 - loss: 2.8293 - regression_loss: 2.1217 - classification_loss: 0.7076
 140/1000 [===>..........................] - ETA: 6:28 - loss: 2.8284 - regression_loss: 2.1217 - classification_loss: 0.7067
 141/1000 [===>..........................] - ETA: 6:28 - loss: 2.8087 - regression_loss: 2.1067 - classification_loss: 0.7020
 142/1000 [===>..........................] - ETA: 6:27 - loss: 2.8030 - regression_loss: 2.1020 - classification_loss: 0.7010
 143/1000 [===>..........................] - ETA: 6:27 - loss: 2.7965 - regression_loss: 2.0989 - classification_loss: 0.6977
 144/1000 [===>..........................] - ETA: 6:27 - loss: 2.8019 - regression_loss: 2.1015 - classification_loss: 0.7003
 145/1000 [===>..........................] - ETA: 6:26 - loss: 2.8206 - regression_loss: 2.1144 - classification_loss: 0.7062
 146/1000 [===>..........................] - ETA: 6:25 - loss: 2.8256 - regression_loss: 2.1163 - classification_loss: 0.7093
 147/1000 [===>..........................] - ETA: 6:25 - loss: 2.8271 - regression_loss: 2.1185 - classification_loss: 0.7086
 148/1000 [===>..........................] - ETA: 6:24 - loss: 2.8293 - regression_loss: 2.1190 - classification_loss: 0.7103
 149/1000 [===>..........................] - ETA: 6:24 - loss: 2.8350 - regression_loss: 2.1249 - classification_loss: 0.7101
 150/1000 [===>..........................] - ETA: 6:24 - loss: 2.8511 - regression_loss: 2.1381 - classification_loss: 0.7130
 151/1000 [===>..........................] - ETA: 6:23 - loss: 2.8578 - regression_loss: 2.1461 - classification_loss: 0.7117
 152/1000 [===>..........................] - ETA: 6:23 - loss: 2.8607 - regression_loss: 2.1480 - classification_loss: 0.7127
 153/1000 [===>..........................] - ETA: 6:22 - loss: 2.8711 - regression_loss: 2.1525 - classification_loss: 0.7186
 154/1000 [===>..........................] - ETA: 6:22 - loss: 2.8682 - regression_loss: 2.1510 - classification_loss: 0.7172
 155/1000 [===>..........................] - ETA: 6:21 - loss: 2.8683 - regression_loss: 2.1526 - classification_loss: 0.7156
 156/1000 [===>..........................] - ETA: 6:21 - loss: 2.8838 - regression_loss: 2.1645 - classification_loss: 0.7194
 157/1000 [===>..........................] - ETA: 6:21 - loss: 2.8656 - regression_loss: 2.1507 - classification_loss: 0.7149
 158/1000 [===>..........................] - ETA: 6:20 - loss: 2.8630 - regression_loss: 2.1485 - classification_loss: 0.7144
 159/1000 [===>..........................] - ETA: 6:20 - loss: 2.8687 - regression_loss: 2.1547 - classification_loss: 0.7141
 160/1000 [===>..........................] - ETA: 6:19 - loss: 2.8667 - regression_loss: 2.1542 - classification_loss: 0.7125
 161/1000 [===>..........................] - ETA: 6:19 - loss: 2.8703 - regression_loss: 2.1584 - classification_loss: 0.7118
 162/1000 [===>..........................] - ETA: 6:18 - loss: 2.8525 - regression_loss: 2.1451 - classification_loss: 0.7074
 163/1000 [===>..........................] - ETA: 6:18 - loss: 2.8364 - regression_loss: 2.1320 - classification_loss: 0.7044
 164/1000 [===>..........................] - ETA: 6:17 - loss: 2.8447 - regression_loss: 2.1398 - classification_loss: 0.7050
 165/1000 [===>..........................] - ETA: 6:17 - loss: 2.8504 - regression_loss: 2.1452 - classification_loss: 0.7051
 166/1000 [===>..........................] - ETA: 6:17 - loss: 2.8338 - regression_loss: 2.1323 - classification_loss: 0.7014
 167/1000 [====>.........................] - ETA: 6:16 - loss: 2.8351 - regression_loss: 2.1351 - classification_loss: 0.7000
 168/1000 [====>.........................] - ETA: 6:16 - loss: 2.8192 - regression_loss: 2.1224 - classification_loss: 0.6968
 169/1000 [====>.........................] - ETA: 6:15 - loss: 2.8176 - regression_loss: 2.1229 - classification_loss: 0.6947
 170/1000 [====>.........................] - ETA: 6:15 - loss: 2.8010 - regression_loss: 2.1104 - classification_loss: 0.6906
 171/1000 [====>.........................] - ETA: 6:14 - loss: 2.8081 - regression_loss: 2.1149 - classification_loss: 0.6932
 172/1000 [====>.........................] - ETA: 6:14 - loss: 2.8082 - regression_loss: 2.1153 - classification_loss: 0.6928
 173/1000 [====>.........................] - ETA: 6:13 - loss: 2.7919 - regression_loss: 2.1031 - classification_loss: 0.6888
 174/1000 [====>.........................] - ETA: 6:13 - loss: 2.7909 - regression_loss: 2.1042 - classification_loss: 0.6867
 175/1000 [====>.........................] - ETA: 6:13 - loss: 2.7957 - regression_loss: 2.1100 - classification_loss: 0.6858
 176/1000 [====>.........................] - ETA: 6:12 - loss: 2.7924 - regression_loss: 2.1090 - classification_loss: 0.6834
 177/1000 [====>.........................] - ETA: 6:12 - loss: 2.7998 - regression_loss: 2.1130 - classification_loss: 0.6869
 178/1000 [====>.........................] - ETA: 6:11 - loss: 2.8122 - regression_loss: 2.1227 - classification_loss: 0.6895
 179/1000 [====>.........................] - ETA: 6:11 - loss: 2.8181 - regression_loss: 2.1264 - classification_loss: 0.6918
 180/1000 [====>.........................] - ETA: 6:10 - loss: 2.8310 - regression_loss: 2.1319 - classification_loss: 0.6991
 181/1000 [====>.........................] - ETA: 6:10 - loss: 2.8153 - regression_loss: 2.1201 - classification_loss: 0.6952
 182/1000 [====>.........................] - ETA: 6:09 - loss: 2.8162 - regression_loss: 2.1214 - classification_loss: 0.6949
 183/1000 [====>.........................] - ETA: 6:09 - loss: 2.8013 - regression_loss: 2.1098 - classification_loss: 0.6916
 184/1000 [====>.........................] - ETA: 6:09 - loss: 2.8078 - regression_loss: 2.1132 - classification_loss: 0.6946
 185/1000 [====>.........................] - ETA: 6:08 - loss: 2.8203 - regression_loss: 2.1217 - classification_loss: 0.6986
 186/1000 [====>.........................] - ETA: 6:08 - loss: 2.8053 - regression_loss: 2.1103 - classification_loss: 0.6950
 187/1000 [====>.........................] - ETA: 6:07 - loss: 2.8087 - regression_loss: 2.1153 - classification_loss: 0.6934
 188/1000 [====>.........................] - ETA: 6:07 - loss: 2.8091 - regression_loss: 2.1175 - classification_loss: 0.6916
 189/1000 [====>.........................] - ETA: 6:06 - loss: 2.7942 - regression_loss: 2.1063 - classification_loss: 0.6880
 190/1000 [====>.........................] - ETA: 6:06 - loss: 2.7916 - regression_loss: 2.1037 - classification_loss: 0.6879
 191/1000 [====>.........................] - ETA: 6:05 - loss: 2.7955 - regression_loss: 2.1056 - classification_loss: 0.6899
 192/1000 [====>.........................] - ETA: 6:05 - loss: 2.7958 - regression_loss: 2.1043 - classification_loss: 0.6915
 193/1000 [====>.........................] - ETA: 6:05 - loss: 2.7948 - regression_loss: 2.1054 - classification_loss: 0.6894
 194/1000 [====>.........................] - ETA: 6:04 - loss: 2.8002 - regression_loss: 2.1101 - classification_loss: 0.6900
 195/1000 [====>.........................] - ETA: 6:03 - loss: 2.8101 - regression_loss: 2.1171 - classification_loss: 0.6930
 196/1000 [====>.........................] - ETA: 6:03 - loss: 2.8184 - regression_loss: 2.1241 - classification_loss: 0.6943
 197/1000 [====>.........................] - ETA: 6:03 - loss: 2.8279 - regression_loss: 2.1285 - classification_loss: 0.6994
 198/1000 [====>.........................] - ETA: 6:02 - loss: 2.8417 - regression_loss: 2.1353 - classification_loss: 0.7064
 199/1000 [====>.........................] - ETA: 6:02 - loss: 2.8274 - regression_loss: 2.1245 - classification_loss: 0.7029
 200/1000 [=====>........................] - ETA: 6:01 - loss: 2.8133 - regression_loss: 2.1139 - classification_loss: 0.6994
 201/1000 [=====>........................] - ETA: 6:01 - loss: 2.8163 - regression_loss: 2.1148 - classification_loss: 0.7016
 202/1000 [=====>........................] - ETA: 6:00 - loss: 2.8237 - regression_loss: 2.1214 - classification_loss: 0.7023
 203/1000 [=====>........................] - ETA: 6:00 - loss: 2.8291 - regression_loss: 2.1259 - classification_loss: 0.7032
 204/1000 [=====>........................] - ETA: 5:59 - loss: 2.8259 - regression_loss: 2.1239 - classification_loss: 0.7020
 205/1000 [=====>........................] - ETA: 5:59 - loss: 2.8124 - regression_loss: 2.1135 - classification_loss: 0.6989
 206/1000 [=====>........................] - ETA: 5:59 - loss: 2.8235 - regression_loss: 2.1219 - classification_loss: 0.7016
 207/1000 [=====>........................] - ETA: 5:58 - loss: 2.8098 - regression_loss: 2.1116 - classification_loss: 0.6982
 208/1000 [=====>........................] - ETA: 5:58 - loss: 2.8183 - regression_loss: 2.1177 - classification_loss: 0.7006
 209/1000 [=====>........................] - ETA: 5:57 - loss: 2.8182 - regression_loss: 2.1167 - classification_loss: 0.7015
 210/1000 [=====>........................] - ETA: 5:57 - loss: 2.8230 - regression_loss: 2.1216 - classification_loss: 0.7014
 211/1000 [=====>........................] - ETA: 5:56 - loss: 2.8271 - regression_loss: 2.1255 - classification_loss: 0.7016
 212/1000 [=====>........................] - ETA: 5:56 - loss: 2.8138 - regression_loss: 2.1154 - classification_loss: 0.6983
 213/1000 [=====>........................] - ETA: 5:56 - loss: 2.8011 - regression_loss: 2.1055 - classification_loss: 0.6956
 214/1000 [=====>........................] - ETA: 5:55 - loss: 2.8028 - regression_loss: 2.1066 - classification_loss: 0.6961
 215/1000 [=====>........................] - ETA: 5:55 - loss: 2.8021 - regression_loss: 2.1047 - classification_loss: 0.6974
 216/1000 [=====>........................] - ETA: 5:54 - loss: 2.8037 - regression_loss: 2.1073 - classification_loss: 0.6964
 217/1000 [=====>........................] - ETA: 5:54 - loss: 2.8015 - regression_loss: 2.1049 - classification_loss: 0.6966
 218/1000 [=====>........................] - ETA: 5:53 - loss: 2.8090 - regression_loss: 2.1111 - classification_loss: 0.6979
 219/1000 [=====>........................] - ETA: 5:53 - loss: 2.8124 - regression_loss: 2.1141 - classification_loss: 0.6983
 220/1000 [=====>........................] - ETA: 5:52 - loss: 2.8086 - regression_loss: 2.1120 - classification_loss: 0.6967
 221/1000 [=====>........................] - ETA: 5:52 - loss: 2.8172 - regression_loss: 2.1193 - classification_loss: 0.6979
 222/1000 [=====>........................] - ETA: 5:52 - loss: 2.8238 - regression_loss: 2.1248 - classification_loss: 0.6990
 223/1000 [=====>........................] - ETA: 5:51 - loss: 2.8182 - regression_loss: 2.1214 - classification_loss: 0.6967
 224/1000 [=====>........................] - ETA: 5:51 - loss: 2.8056 - regression_loss: 2.1120 - classification_loss: 0.6936
 225/1000 [=====>........................] - ETA: 5:50 - loss: 2.8177 - regression_loss: 2.1224 - classification_loss: 0.6953
 226/1000 [=====>........................] - ETA: 5:50 - loss: 2.8197 - regression_loss: 2.1258 - classification_loss: 0.6939
 227/1000 [=====>........................] - ETA: 5:49 - loss: 2.8275 - regression_loss: 2.1303 - classification_loss: 0.6972
 228/1000 [=====>........................] - ETA: 5:49 - loss: 2.8278 - regression_loss: 2.1311 - classification_loss: 0.6967
 229/1000 [=====>........................] - ETA: 5:48 - loss: 2.8265 - regression_loss: 2.1312 - classification_loss: 0.6953
 230/1000 [=====>........................] - ETA: 5:48 - loss: 2.8239 - regression_loss: 2.1307 - classification_loss: 0.6933
 231/1000 [=====>........................] - ETA: 5:47 - loss: 2.8282 - regression_loss: 2.1348 - classification_loss: 0.6934
 232/1000 [=====>........................] - ETA: 5:47 - loss: 2.8329 - regression_loss: 2.1385 - classification_loss: 0.6944
 233/1000 [=====>........................] - ETA: 5:47 - loss: 2.8389 - regression_loss: 2.1433 - classification_loss: 0.6956
 234/1000 [======>.......................] - ETA: 5:46 - loss: 2.8397 - regression_loss: 2.1455 - classification_loss: 0.6942
 235/1000 [======>.......................] - ETA: 5:46 - loss: 2.8457 - regression_loss: 2.1480 - classification_loss: 0.6977
 236/1000 [======>.......................] - ETA: 5:45 - loss: 2.8449 - regression_loss: 2.1465 - classification_loss: 0.6984
 237/1000 [======>.......................] - ETA: 5:45 - loss: 2.8425 - regression_loss: 2.1438 - classification_loss: 0.6987
 238/1000 [======>.......................] - ETA: 5:44 - loss: 2.8412 - regression_loss: 2.1443 - classification_loss: 0.6969
 239/1000 [======>.......................] - ETA: 5:44 - loss: 2.8485 - regression_loss: 2.1469 - classification_loss: 0.7016
 240/1000 [======>.......................] - ETA: 5:43 - loss: 2.8480 - regression_loss: 2.1477 - classification_loss: 0.7002
 241/1000 [======>.......................] - ETA: 5:43 - loss: 2.8504 - regression_loss: 2.1517 - classification_loss: 0.6987
 242/1000 [======>.......................] - ETA: 5:43 - loss: 2.8471 - regression_loss: 2.1495 - classification_loss: 0.6976
 243/1000 [======>.......................] - ETA: 5:42 - loss: 2.8466 - regression_loss: 2.1500 - classification_loss: 0.6966
 244/1000 [======>.......................] - ETA: 5:42 - loss: 2.8470 - regression_loss: 2.1507 - classification_loss: 0.6963
 245/1000 [======>.......................] - ETA: 5:41 - loss: 2.8550 - regression_loss: 2.1575 - classification_loss: 0.6974
 246/1000 [======>.......................] - ETA: 5:41 - loss: 2.8561 - regression_loss: 2.1598 - classification_loss: 0.6963
 247/1000 [======>.......................] - ETA: 5:40 - loss: 2.8570 - regression_loss: 2.1611 - classification_loss: 0.6959
 248/1000 [======>.......................] - ETA: 5:40 - loss: 2.8588 - regression_loss: 2.1639 - classification_loss: 0.6948
 249/1000 [======>.......................] - ETA: 5:39 - loss: 2.8638 - regression_loss: 2.1675 - classification_loss: 0.6964
 250/1000 [======>.......................] - ETA: 5:39 - loss: 2.8643 - regression_loss: 2.1697 - classification_loss: 0.6947
 251/1000 [======>.......................] - ETA: 5:38 - loss: 2.8635 - regression_loss: 2.1686 - classification_loss: 0.6949
 252/1000 [======>.......................] - ETA: 5:38 - loss: 2.8633 - regression_loss: 2.1690 - classification_loss: 0.6943
 253/1000 [======>.......................] - ETA: 5:38 - loss: 2.8520 - regression_loss: 2.1605 - classification_loss: 0.6915
 254/1000 [======>.......................] - ETA: 5:37 - loss: 2.8565 - regression_loss: 2.1640 - classification_loss: 0.6925
 255/1000 [======>.......................] - ETA: 5:36 - loss: 2.8583 - regression_loss: 2.1655 - classification_loss: 0.6927
 256/1000 [======>.......................] - ETA: 5:36 - loss: 2.8576 - regression_loss: 2.1658 - classification_loss: 0.6918
 257/1000 [======>.......................] - ETA: 5:36 - loss: 2.8585 - regression_loss: 2.1677 - classification_loss: 0.6908
 258/1000 [======>.......................] - ETA: 5:35 - loss: 2.8578 - regression_loss: 2.1684 - classification_loss: 0.6893
 259/1000 [======>.......................] - ETA: 5:35 - loss: 2.8469 - regression_loss: 2.1601 - classification_loss: 0.6868
 260/1000 [======>.......................] - ETA: 5:34 - loss: 2.8448 - regression_loss: 2.1586 - classification_loss: 0.6863
 261/1000 [======>.......................] - ETA: 5:34 - loss: 2.8414 - regression_loss: 2.1565 - classification_loss: 0.6849
 262/1000 [======>.......................] - ETA: 5:33 - loss: 2.8451 - regression_loss: 2.1597 - classification_loss: 0.6855
 263/1000 [======>.......................] - ETA: 5:33 - loss: 2.8343 - regression_loss: 2.1514 - classification_loss: 0.6829
 264/1000 [======>.......................] - ETA: 5:32 - loss: 2.8365 - regression_loss: 2.1532 - classification_loss: 0.6833
 265/1000 [======>.......................] - ETA: 5:32 - loss: 2.8287 - regression_loss: 2.1451 - classification_loss: 0.6836
 266/1000 [======>.......................] - ETA: 5:32 - loss: 2.8259 - regression_loss: 2.1438 - classification_loss: 0.6821
 267/1000 [=======>......................] - ETA: 5:31 - loss: 2.8320 - regression_loss: 2.1473 - classification_loss: 0.6847
 268/1000 [=======>......................] - ETA: 5:31 - loss: 2.8215 - regression_loss: 2.1393 - classification_loss: 0.6822
 269/1000 [=======>......................] - ETA: 5:30 - loss: 2.8265 - regression_loss: 2.1428 - classification_loss: 0.6837
 270/1000 [=======>......................] - ETA: 5:30 - loss: 2.8304 - regression_loss: 2.1451 - classification_loss: 0.6853
 271/1000 [=======>......................] - ETA: 5:29 - loss: 2.8200 - regression_loss: 2.1372 - classification_loss: 0.6829
 272/1000 [=======>......................] - ETA: 5:29 - loss: 2.8191 - regression_loss: 2.1370 - classification_loss: 0.6821
 273/1000 [=======>......................] - ETA: 5:28 - loss: 2.8088 - regression_loss: 2.1292 - classification_loss: 0.6796
 274/1000 [=======>......................] - ETA: 5:28 - loss: 2.8152 - regression_loss: 2.1332 - classification_loss: 0.6820
 275/1000 [=======>......................] - ETA: 5:27 - loss: 2.8296 - regression_loss: 2.1451 - classification_loss: 0.6844
 276/1000 [=======>......................] - ETA: 5:27 - loss: 2.8193 - regression_loss: 2.1374 - classification_loss: 0.6819
 277/1000 [=======>......................] - ETA: 5:26 - loss: 2.8091 - regression_loss: 2.1296 - classification_loss: 0.6795
 278/1000 [=======>......................] - ETA: 5:26 - loss: 2.8107 - regression_loss: 2.1320 - classification_loss: 0.6787
 279/1000 [=======>......................] - ETA: 5:26 - loss: 2.8006 - regression_loss: 2.1244 - classification_loss: 0.6762
 280/1000 [=======>......................] - ETA: 5:25 - loss: 2.7906 - regression_loss: 2.1168 - classification_loss: 0.6738
 281/1000 [=======>......................] - ETA: 5:25 - loss: 2.7904 - regression_loss: 2.1170 - classification_loss: 0.6734
 282/1000 [=======>......................] - ETA: 5:24 - loss: 2.7941 - regression_loss: 2.1181 - classification_loss: 0.6760
 283/1000 [=======>......................] - ETA: 5:24 - loss: 2.7939 - regression_loss: 2.1172 - classification_loss: 0.6766
 284/1000 [=======>......................] - ETA: 5:23 - loss: 2.8007 - regression_loss: 2.1227 - classification_loss: 0.6781
 285/1000 [=======>......................] - ETA: 5:23 - loss: 2.8049 - regression_loss: 2.1265 - classification_loss: 0.6785
 286/1000 [=======>......................] - ETA: 5:22 - loss: 2.7952 - regression_loss: 2.1190 - classification_loss: 0.6761
 287/1000 [=======>......................] - ETA: 5:22 - loss: 2.7854 - regression_loss: 2.1117 - classification_loss: 0.6738
 288/1000 [=======>......................] - ETA: 5:22 - loss: 2.7899 - regression_loss: 2.1148 - classification_loss: 0.6751
 289/1000 [=======>......................] - ETA: 5:21 - loss: 2.7819 - regression_loss: 2.1075 - classification_loss: 0.6744
 290/1000 [=======>......................] - ETA: 5:21 - loss: 2.7723 - regression_loss: 2.1002 - classification_loss: 0.6721
 291/1000 [=======>......................] - ETA: 5:20 - loss: 2.7803 - regression_loss: 2.1052 - classification_loss: 0.6751
 292/1000 [=======>......................] - ETA: 5:20 - loss: 2.7904 - regression_loss: 2.1117 - classification_loss: 0.6787
 293/1000 [=======>......................] - ETA: 5:19 - loss: 2.7809 - regression_loss: 2.1045 - classification_loss: 0.6764
 294/1000 [=======>......................] - ETA: 5:19 - loss: 2.7865 - regression_loss: 2.1069 - classification_loss: 0.6796
 295/1000 [=======>......................] - ETA: 5:18 - loss: 2.7896 - regression_loss: 2.1095 - classification_loss: 0.6801
 296/1000 [=======>......................] - ETA: 5:18 - loss: 2.7875 - regression_loss: 2.1077 - classification_loss: 0.6799
 297/1000 [=======>......................] - ETA: 5:18 - loss: 2.7894 - regression_loss: 2.1094 - classification_loss: 0.6800
 298/1000 [=======>......................] - ETA: 5:17 - loss: 2.7942 - regression_loss: 2.1111 - classification_loss: 0.6831
 299/1000 [=======>......................] - ETA: 5:17 - loss: 2.8005 - regression_loss: 2.1149 - classification_loss: 0.6856
 300/1000 [========>.....................] - ETA: 5:16 - loss: 2.8087 - regression_loss: 2.1205 - classification_loss: 0.6883
 301/1000 [========>.....................] - ETA: 5:16 - loss: 2.8086 - regression_loss: 2.1212 - classification_loss: 0.6874
 302/1000 [========>.....................] - ETA: 5:15 - loss: 2.7993 - regression_loss: 2.1142 - classification_loss: 0.6852
 303/1000 [========>.....................] - ETA: 5:15 - loss: 2.8049 - regression_loss: 2.1161 - classification_loss: 0.6888
 304/1000 [========>.....................] - ETA: 5:14 - loss: 2.8133 - regression_loss: 2.1238 - classification_loss: 0.6895
 305/1000 [========>.....................] - ETA: 5:14 - loss: 2.8046 - regression_loss: 2.1169 - classification_loss: 0.6877
 306/1000 [========>.....................] - ETA: 5:13 - loss: 2.8027 - regression_loss: 2.1155 - classification_loss: 0.6872
 307/1000 [========>.....................] - ETA: 5:13 - loss: 2.8035 - regression_loss: 2.1152 - classification_loss: 0.6883
 308/1000 [========>.....................] - ETA: 5:13 - loss: 2.7944 - regression_loss: 2.1083 - classification_loss: 0.6861
 309/1000 [========>.....................] - ETA: 5:12 - loss: 2.7858 - regression_loss: 2.1015 - classification_loss: 0.6843
 310/1000 [========>.....................] - ETA: 5:12 - loss: 2.7846 - regression_loss: 2.1008 - classification_loss: 0.6838
 311/1000 [========>.....................] - ETA: 5:11 - loss: 2.7757 - regression_loss: 2.0941 - classification_loss: 0.6816
 312/1000 [========>.....................] - ETA: 5:11 - loss: 2.7668 - regression_loss: 2.0874 - classification_loss: 0.6794
 313/1000 [========>.....................] - ETA: 5:10 - loss: 2.7657 - regression_loss: 2.0858 - classification_loss: 0.6799
 314/1000 [========>.....................] - ETA: 5:10 - loss: 2.7670 - regression_loss: 2.0874 - classification_loss: 0.6796
 315/1000 [========>.....................] - ETA: 5:09 - loss: 2.7711 - regression_loss: 2.0899 - classification_loss: 0.6812
 316/1000 [========>.....................] - ETA: 5:09 - loss: 2.7795 - regression_loss: 2.0970 - classification_loss: 0.6825
 317/1000 [========>.....................] - ETA: 5:08 - loss: 2.7813 - regression_loss: 2.0981 - classification_loss: 0.6832
 318/1000 [========>.....................] - ETA: 5:08 - loss: 2.7831 - regression_loss: 2.0998 - classification_loss: 0.6832
 319/1000 [========>.....................] - ETA: 5:08 - loss: 2.7815 - regression_loss: 2.0981 - classification_loss: 0.6833
 320/1000 [========>.....................] - ETA: 5:07 - loss: 2.7823 - regression_loss: 2.0994 - classification_loss: 0.6829
 321/1000 [========>.....................] - ETA: 5:07 - loss: 2.7787 - regression_loss: 2.0968 - classification_loss: 0.6819
 322/1000 [========>.....................] - ETA: 5:06 - loss: 2.7765 - regression_loss: 2.0954 - classification_loss: 0.6811
 323/1000 [========>.....................] - ETA: 5:06 - loss: 2.7679 - regression_loss: 2.0890 - classification_loss: 0.6790
 324/1000 [========>.....................] - ETA: 5:05 - loss: 2.7660 - regression_loss: 2.0871 - classification_loss: 0.6789
 325/1000 [========>.....................] - ETA: 5:05 - loss: 2.7575 - regression_loss: 2.0807 - classification_loss: 0.6768
 326/1000 [========>.....................] - ETA: 5:04 - loss: 2.7609 - regression_loss: 2.0833 - classification_loss: 0.6775
 327/1000 [========>.....................] - ETA: 5:04 - loss: 2.7619 - regression_loss: 2.0846 - classification_loss: 0.6773
 328/1000 [========>.....................] - ETA: 5:03 - loss: 2.7647 - regression_loss: 2.0880 - classification_loss: 0.6767
 329/1000 [========>.....................] - ETA: 5:03 - loss: 2.7563 - regression_loss: 2.0816 - classification_loss: 0.6746
 330/1000 [========>.....................] - ETA: 5:03 - loss: 2.7479 - regression_loss: 2.0753 - classification_loss: 0.6726
 331/1000 [========>.....................] - ETA: 5:02 - loss: 2.7496 - regression_loss: 2.0756 - classification_loss: 0.6740
 332/1000 [========>.....................] - ETA: 5:02 - loss: 2.7514 - regression_loss: 2.0774 - classification_loss: 0.6740
 333/1000 [========>.....................] - ETA: 5:01 - loss: 2.7532 - regression_loss: 2.0801 - classification_loss: 0.6731
 334/1000 [=========>....................] - ETA: 5:01 - loss: 2.7548 - regression_loss: 2.0808 - classification_loss: 0.6740
 335/1000 [=========>....................] - ETA: 5:00 - loss: 2.7465 - regression_loss: 2.0745 - classification_loss: 0.6720
 336/1000 [=========>....................] - ETA: 5:00 - loss: 2.7468 - regression_loss: 2.0746 - classification_loss: 0.6722
 337/1000 [=========>....................] - ETA: 4:59 - loss: 2.7417 - regression_loss: 2.0684 - classification_loss: 0.6733
 338/1000 [=========>....................] - ETA: 4:59 - loss: 2.7443 - regression_loss: 2.0691 - classification_loss: 0.6752
 339/1000 [=========>....................] - ETA: 4:59 - loss: 2.7433 - regression_loss: 2.0693 - classification_loss: 0.6740
 340/1000 [=========>....................] - ETA: 4:58 - loss: 2.7473 - regression_loss: 2.0744 - classification_loss: 0.6729
 341/1000 [=========>....................] - ETA: 4:58 - loss: 2.7558 - regression_loss: 2.0803 - classification_loss: 0.6755
 342/1000 [=========>....................] - ETA: 4:57 - loss: 2.7584 - regression_loss: 2.0815 - classification_loss: 0.6769
 343/1000 [=========>....................] - ETA: 4:57 - loss: 2.7604 - regression_loss: 2.0837 - classification_loss: 0.6767
 344/1000 [=========>....................] - ETA: 4:56 - loss: 2.7597 - regression_loss: 2.0834 - classification_loss: 0.6763
 345/1000 [=========>....................] - ETA: 4:56 - loss: 2.7647 - regression_loss: 2.0839 - classification_loss: 0.6808
 346/1000 [=========>....................] - ETA: 4:55 - loss: 2.7642 - regression_loss: 2.0827 - classification_loss: 0.6814
 347/1000 [=========>....................] - ETA: 4:55 - loss: 2.7625 - regression_loss: 2.0819 - classification_loss: 0.6806
 348/1000 [=========>....................] - ETA: 4:54 - loss: 2.7661 - regression_loss: 2.0850 - classification_loss: 0.6811
 349/1000 [=========>....................] - ETA: 4:54 - loss: 2.7655 - regression_loss: 2.0857 - classification_loss: 0.6798
 350/1000 [=========>....................] - ETA: 4:54 - loss: 2.7582 - regression_loss: 2.0797 - classification_loss: 0.6784
 351/1000 [=========>....................] - ETA: 4:53 - loss: 2.7634 - regression_loss: 2.0824 - classification_loss: 0.6810
 352/1000 [=========>....................] - ETA: 4:53 - loss: 2.7574 - regression_loss: 2.0765 - classification_loss: 0.6810
 353/1000 [=========>....................] - ETA: 4:52 - loss: 2.7684 - regression_loss: 2.0843 - classification_loss: 0.6841
 354/1000 [=========>....................] - ETA: 4:52 - loss: 2.7641 - regression_loss: 2.0813 - classification_loss: 0.6828
 355/1000 [=========>....................] - ETA: 4:51 - loss: 2.7631 - regression_loss: 2.0809 - classification_loss: 0.6821
 356/1000 [=========>....................] - ETA: 4:51 - loss: 2.7553 - regression_loss: 2.0751 - classification_loss: 0.6802
 357/1000 [=========>....................] - ETA: 4:50 - loss: 2.7548 - regression_loss: 2.0748 - classification_loss: 0.6800
 358/1000 [=========>....................] - ETA: 4:50 - loss: 2.7471 - regression_loss: 2.0690 - classification_loss: 0.6781
 359/1000 [=========>....................] - ETA: 4:49 - loss: 2.7461 - regression_loss: 2.0688 - classification_loss: 0.6773
 360/1000 [=========>....................] - ETA: 4:49 - loss: 2.7493 - regression_loss: 2.0722 - classification_loss: 0.6771
 361/1000 [=========>....................] - ETA: 4:49 - loss: 2.7512 - regression_loss: 2.0744 - classification_loss: 0.6768
 362/1000 [=========>....................] - ETA: 4:48 - loss: 2.7521 - regression_loss: 2.0757 - classification_loss: 0.6763
 363/1000 [=========>....................] - ETA: 4:48 - loss: 2.7528 - regression_loss: 2.0766 - classification_loss: 0.6762
 364/1000 [=========>....................] - ETA: 4:47 - loss: 2.7453 - regression_loss: 2.0709 - classification_loss: 0.6744
 365/1000 [=========>....................] - ETA: 4:47 - loss: 2.7378 - regression_loss: 2.0652 - classification_loss: 0.6726
 366/1000 [=========>....................] - ETA: 4:46 - loss: 2.7303 - regression_loss: 2.0596 - classification_loss: 0.6708
 367/1000 [==========>...................] - ETA: 4:46 - loss: 2.7354 - regression_loss: 2.0636 - classification_loss: 0.6718
 368/1000 [==========>...................] - ETA: 4:45 - loss: 2.7349 - regression_loss: 2.0626 - classification_loss: 0.6723
 369/1000 [==========>...................] - ETA: 4:45 - loss: 2.7369 - regression_loss: 2.0627 - classification_loss: 0.6742
 370/1000 [==========>...................] - ETA: 4:45 - loss: 2.7400 - regression_loss: 2.0628 - classification_loss: 0.6771
 371/1000 [==========>...................] - ETA: 4:44 - loss: 2.7436 - regression_loss: 2.0653 - classification_loss: 0.6783
 372/1000 [==========>...................] - ETA: 4:44 - loss: 2.7456 - regression_loss: 2.0666 - classification_loss: 0.6791
 373/1000 [==========>...................] - ETA: 4:43 - loss: 2.7468 - regression_loss: 2.0663 - classification_loss: 0.6805
 374/1000 [==========>...................] - ETA: 4:43 - loss: 2.7461 - regression_loss: 2.0666 - classification_loss: 0.6795
 375/1000 [==========>...................] - ETA: 4:42 - loss: 2.7518 - regression_loss: 2.0712 - classification_loss: 0.6806
 376/1000 [==========>...................] - ETA: 4:42 - loss: 2.7522 - regression_loss: 2.0719 - classification_loss: 0.6803
 377/1000 [==========>...................] - ETA: 4:41 - loss: 2.7535 - regression_loss: 2.0726 - classification_loss: 0.6810
 378/1000 [==========>...................] - ETA: 4:41 - loss: 2.7560 - regression_loss: 2.0751 - classification_loss: 0.6809
 379/1000 [==========>...................] - ETA: 4:40 - loss: 2.7642 - regression_loss: 2.0798 - classification_loss: 0.6844
 380/1000 [==========>...................] - ETA: 4:40 - loss: 2.7673 - regression_loss: 2.0825 - classification_loss: 0.6848
 381/1000 [==========>...................] - ETA: 4:39 - loss: 2.7651 - regression_loss: 2.0815 - classification_loss: 0.6837
 382/1000 [==========>...................] - ETA: 4:39 - loss: 2.7680 - regression_loss: 2.0835 - classification_loss: 0.6845
 383/1000 [==========>...................] - ETA: 4:39 - loss: 2.7686 - regression_loss: 2.0849 - classification_loss: 0.6837
 384/1000 [==========>...................] - ETA: 4:38 - loss: 2.7695 - regression_loss: 2.0857 - classification_loss: 0.6837
 385/1000 [==========>...................] - ETA: 4:38 - loss: 2.7685 - regression_loss: 2.0848 - classification_loss: 0.6837
 386/1000 [==========>...................] - ETA: 4:37 - loss: 2.7693 - regression_loss: 2.0867 - classification_loss: 0.6827
 387/1000 [==========>...................] - ETA: 4:37 - loss: 2.7709 - regression_loss: 2.0883 - classification_loss: 0.6826
 388/1000 [==========>...................] - ETA: 4:36 - loss: 2.7743 - regression_loss: 2.0917 - classification_loss: 0.6826
 389/1000 [==========>...................] - ETA: 4:36 - loss: 2.7769 - regression_loss: 2.0941 - classification_loss: 0.6829
 390/1000 [==========>...................] - ETA: 4:35 - loss: 2.7786 - regression_loss: 2.0966 - classification_loss: 0.6819
 391/1000 [==========>...................] - ETA: 4:35 - loss: 2.7785 - regression_loss: 2.0971 - classification_loss: 0.6814
 392/1000 [==========>...................] - ETA: 4:35 - loss: 2.7778 - regression_loss: 2.0969 - classification_loss: 0.6809
 393/1000 [==========>...................] - ETA: 4:34 - loss: 2.7707 - regression_loss: 2.0915 - classification_loss: 0.6792
 394/1000 [==========>...................] - ETA: 4:34 - loss: 2.7639 - regression_loss: 2.0862 - classification_loss: 0.6777
 395/1000 [==========>...................] - ETA: 4:33 - loss: 2.7571 - regression_loss: 2.0810 - classification_loss: 0.6761
 396/1000 [==========>...................] - ETA: 4:33 - loss: 2.7503 - regression_loss: 2.0757 - classification_loss: 0.6746
 397/1000 [==========>...................] - ETA: 4:32 - loss: 2.7532 - regression_loss: 2.0780 - classification_loss: 0.6752
 398/1000 [==========>...................] - ETA: 4:32 - loss: 2.7544 - regression_loss: 2.0791 - classification_loss: 0.6753
 399/1000 [==========>...................] - ETA: 4:31 - loss: 2.7542 - regression_loss: 2.0797 - classification_loss: 0.6745
 400/1000 [===========>..................] - ETA: 4:31 - loss: 2.7571 - regression_loss: 2.0823 - classification_loss: 0.6748
 401/1000 [===========>..................] - ETA: 4:30 - loss: 2.7502 - regression_loss: 2.0771 - classification_loss: 0.6731
 402/1000 [===========>..................] - ETA: 4:30 - loss: 2.7534 - regression_loss: 2.0793 - classification_loss: 0.6741
 403/1000 [===========>..................] - ETA: 4:30 - loss: 2.7584 - regression_loss: 2.0838 - classification_loss: 0.6747
 404/1000 [===========>..................] - ETA: 4:29 - loss: 2.7623 - regression_loss: 2.0870 - classification_loss: 0.6753
 405/1000 [===========>..................] - ETA: 4:29 - loss: 2.7662 - regression_loss: 2.0905 - classification_loss: 0.6757
 406/1000 [===========>..................] - ETA: 4:28 - loss: 2.7679 - regression_loss: 2.0929 - classification_loss: 0.6749
 407/1000 [===========>..................] - ETA: 4:28 - loss: 2.7695 - regression_loss: 2.0929 - classification_loss: 0.6766
 408/1000 [===========>..................] - ETA: 4:27 - loss: 2.7738 - regression_loss: 2.0978 - classification_loss: 0.6761
 409/1000 [===========>..................] - ETA: 4:27 - loss: 2.7763 - regression_loss: 2.1007 - classification_loss: 0.6757
 410/1000 [===========>..................] - ETA: 4:26 - loss: 2.7792 - regression_loss: 2.1037 - classification_loss: 0.6754
 411/1000 [===========>..................] - ETA: 4:26 - loss: 2.7817 - regression_loss: 2.1063 - classification_loss: 0.6754
 412/1000 [===========>..................] - ETA: 4:26 - loss: 2.7749 - regression_loss: 2.1012 - classification_loss: 0.6737
 413/1000 [===========>..................] - ETA: 4:25 - loss: 2.7746 - regression_loss: 2.1017 - classification_loss: 0.6729
 414/1000 [===========>..................] - ETA: 4:25 - loss: 2.7740 - regression_loss: 2.1016 - classification_loss: 0.6724
 415/1000 [===========>..................] - ETA: 4:24 - loss: 2.7750 - regression_loss: 2.1025 - classification_loss: 0.6725
 416/1000 [===========>..................] - ETA: 4:24 - loss: 2.7760 - regression_loss: 2.1033 - classification_loss: 0.6727
 417/1000 [===========>..................] - ETA: 4:23 - loss: 2.7808 - regression_loss: 2.1063 - classification_loss: 0.6745
 418/1000 [===========>..................] - ETA: 4:23 - loss: 2.7806 - regression_loss: 2.1059 - classification_loss: 0.6747
 419/1000 [===========>..................] - ETA: 4:22 - loss: 2.7740 - regression_loss: 2.1009 - classification_loss: 0.6731
 420/1000 [===========>..................] - ETA: 4:22 - loss: 2.7753 - regression_loss: 2.1025 - classification_loss: 0.6728
 421/1000 [===========>..................] - ETA: 4:21 - loss: 2.7809 - regression_loss: 2.1060 - classification_loss: 0.6750
 422/1000 [===========>..................] - ETA: 4:21 - loss: 2.7801 - regression_loss: 2.1061 - classification_loss: 0.6741
 423/1000 [===========>..................] - ETA: 4:21 - loss: 2.7833 - regression_loss: 2.1084 - classification_loss: 0.6749
 424/1000 [===========>..................] - ETA: 4:20 - loss: 2.7876 - regression_loss: 2.1130 - classification_loss: 0.6747
 425/1000 [===========>..................] - ETA: 4:20 - loss: 2.7871 - regression_loss: 2.1128 - classification_loss: 0.6743
 426/1000 [===========>..................] - ETA: 4:19 - loss: 2.7914 - regression_loss: 2.1157 - classification_loss: 0.6756
 427/1000 [===========>..................] - ETA: 4:19 - loss: 2.7899 - regression_loss: 2.1149 - classification_loss: 0.6751
 428/1000 [===========>..................] - ETA: 4:18 - loss: 2.7834 - regression_loss: 2.1099 - classification_loss: 0.6735
 429/1000 [===========>..................] - ETA: 4:18 - loss: 2.7769 - regression_loss: 2.1050 - classification_loss: 0.6719
 430/1000 [===========>..................] - ETA: 4:17 - loss: 2.7824 - regression_loss: 2.1082 - classification_loss: 0.6743
 431/1000 [===========>..................] - ETA: 4:17 - loss: 2.7806 - regression_loss: 2.1073 - classification_loss: 0.6733
 432/1000 [===========>..................] - ETA: 4:16 - loss: 2.7852 - regression_loss: 2.1097 - classification_loss: 0.6755
 433/1000 [===========>..................] - ETA: 4:16 - loss: 2.7825 - regression_loss: 2.1078 - classification_loss: 0.6747
 434/1000 [============>.................] - ETA: 4:16 - loss: 2.7860 - regression_loss: 2.1103 - classification_loss: 0.6757
 435/1000 [============>.................] - ETA: 4:15 - loss: 2.7917 - regression_loss: 2.1161 - classification_loss: 0.6756
 436/1000 [============>.................] - ETA: 4:15 - loss: 2.7853 - regression_loss: 2.1113 - classification_loss: 0.6741
 437/1000 [============>.................] - ETA: 4:14 - loss: 2.7840 - regression_loss: 2.1108 - classification_loss: 0.6732
 438/1000 [============>.................] - ETA: 4:14 - loss: 2.7868 - regression_loss: 2.1127 - classification_loss: 0.6741
 439/1000 [============>.................] - ETA: 4:13 - loss: 2.7895 - regression_loss: 2.1150 - classification_loss: 0.6746
 440/1000 [============>.................] - ETA: 4:13 - loss: 2.7832 - regression_loss: 2.1102 - classification_loss: 0.6730
 441/1000 [============>.................] - ETA: 4:12 - loss: 2.7819 - regression_loss: 2.1093 - classification_loss: 0.6726
 442/1000 [============>.................] - ETA: 4:12 - loss: 2.7854 - regression_loss: 2.1109 - classification_loss: 0.6746
 443/1000 [============>.................] - ETA: 4:12 - loss: 2.7851 - regression_loss: 2.1115 - classification_loss: 0.6736
 444/1000 [============>.................] - ETA: 4:11 - loss: 2.7860 - regression_loss: 2.1115 - classification_loss: 0.6746
 445/1000 [============>.................] - ETA: 4:11 - loss: 2.7892 - regression_loss: 2.1143 - classification_loss: 0.6750
 446/1000 [============>.................] - ETA: 4:10 - loss: 2.7950 - regression_loss: 2.1184 - classification_loss: 0.6766
 447/1000 [============>.................] - ETA: 4:10 - loss: 2.7959 - regression_loss: 2.1201 - classification_loss: 0.6758
 448/1000 [============>.................] - ETA: 4:09 - loss: 2.7989 - regression_loss: 2.1216 - classification_loss: 0.6773
 449/1000 [============>.................] - ETA: 4:09 - loss: 2.7986 - regression_loss: 2.1218 - classification_loss: 0.6768
 450/1000 [============>.................] - ETA: 4:08 - loss: 2.7997 - regression_loss: 2.1223 - classification_loss: 0.6774
 451/1000 [============>.................] - ETA: 4:08 - loss: 2.8018 - regression_loss: 2.1240 - classification_loss: 0.6779
 452/1000 [============>.................] - ETA: 4:07 - loss: 2.8017 - regression_loss: 2.1243 - classification_loss: 0.6774
 453/1000 [============>.................] - ETA: 4:07 - loss: 2.7955 - regression_loss: 2.1196 - classification_loss: 0.6759
 454/1000 [============>.................] - ETA: 4:07 - loss: 2.7956 - regression_loss: 2.1204 - classification_loss: 0.6751
 455/1000 [============>.................] - ETA: 4:06 - loss: 2.7992 - regression_loss: 2.1240 - classification_loss: 0.6753
 456/1000 [============>.................] - ETA: 4:06 - loss: 2.8005 - regression_loss: 2.1255 - classification_loss: 0.6750
 457/1000 [============>.................] - ETA: 4:05 - loss: 2.8022 - regression_loss: 2.1277 - classification_loss: 0.6745
 458/1000 [============>.................] - ETA: 4:05 - loss: 2.8022 - regression_loss: 2.1284 - classification_loss: 0.6738
 459/1000 [============>.................] - ETA: 4:04 - loss: 2.8013 - regression_loss: 2.1283 - classification_loss: 0.6730
 460/1000 [============>.................] - ETA: 4:04 - loss: 2.8033 - regression_loss: 2.1308 - classification_loss: 0.6725
 461/1000 [============>.................] - ETA: 4:03 - loss: 2.7973 - regression_loss: 2.1262 - classification_loss: 0.6712
 462/1000 [============>.................] - ETA: 4:03 - loss: 2.7978 - regression_loss: 2.1266 - classification_loss: 0.6712
 463/1000 [============>.................] - ETA: 4:02 - loss: 2.7986 - regression_loss: 2.1273 - classification_loss: 0.6713
 464/1000 [============>.................] - ETA: 4:02 - loss: 2.8027 - regression_loss: 2.1299 - classification_loss: 0.6728
 465/1000 [============>.................] - ETA: 4:02 - loss: 2.8047 - regression_loss: 2.1314 - classification_loss: 0.6733
 466/1000 [============>.................] - ETA: 4:01 - loss: 2.8047 - regression_loss: 2.1319 - classification_loss: 0.6728
 467/1000 [=============>................] - ETA: 4:01 - loss: 2.8060 - regression_loss: 2.1336 - classification_loss: 0.6724
 468/1000 [=============>................] - ETA: 4:00 - loss: 2.8077 - regression_loss: 2.1357 - classification_loss: 0.6720
 469/1000 [=============>................] - ETA: 4:00 - loss: 2.8081 - regression_loss: 2.1362 - classification_loss: 0.6719
 470/1000 [=============>................] - ETA: 3:59 - loss: 2.8096 - regression_loss: 2.1375 - classification_loss: 0.6720
 471/1000 [=============>................] - ETA: 3:59 - loss: 2.8119 - regression_loss: 2.1403 - classification_loss: 0.6716
 472/1000 [=============>................] - ETA: 3:58 - loss: 2.8059 - regression_loss: 2.1358 - classification_loss: 0.6701
 473/1000 [=============>................] - ETA: 3:58 - loss: 2.8086 - regression_loss: 2.1374 - classification_loss: 0.6711
 474/1000 [=============>................] - ETA: 3:58 - loss: 2.8027 - regression_loss: 2.1329 - classification_loss: 0.6698
 475/1000 [=============>................] - ETA: 3:57 - loss: 2.7970 - regression_loss: 2.1284 - classification_loss: 0.6686
 476/1000 [=============>................] - ETA: 3:57 - loss: 2.7976 - regression_loss: 2.1289 - classification_loss: 0.6687
 477/1000 [=============>................] - ETA: 3:56 - loss: 2.8001 - regression_loss: 2.1311 - classification_loss: 0.6690
 478/1000 [=============>................] - ETA: 3:56 - loss: 2.8009 - regression_loss: 2.1320 - classification_loss: 0.6689
 479/1000 [=============>................] - ETA: 3:55 - loss: 2.7958 - regression_loss: 2.1276 - classification_loss: 0.6682
 480/1000 [=============>................] - ETA: 3:55 - loss: 2.7900 - regression_loss: 2.1231 - classification_loss: 0.6668
 481/1000 [=============>................] - ETA: 3:54 - loss: 2.7923 - regression_loss: 2.1246 - classification_loss: 0.6676
 482/1000 [=============>................] - ETA: 3:54 - loss: 2.7956 - regression_loss: 2.1260 - classification_loss: 0.6696
 483/1000 [=============>................] - ETA: 3:53 - loss: 2.7942 - regression_loss: 2.1253 - classification_loss: 0.6689
 484/1000 [=============>................] - ETA: 3:53 - loss: 2.7945 - regression_loss: 2.1260 - classification_loss: 0.6684
 485/1000 [=============>................] - ETA: 3:53 - loss: 2.7890 - regression_loss: 2.1217 - classification_loss: 0.6673
 486/1000 [=============>................] - ETA: 3:52 - loss: 2.7919 - regression_loss: 2.1241 - classification_loss: 0.6678
 487/1000 [=============>................] - ETA: 3:52 - loss: 2.7866 - regression_loss: 2.1197 - classification_loss: 0.6669
 488/1000 [=============>................] - ETA: 3:51 - loss: 2.7809 - regression_loss: 2.1154 - classification_loss: 0.6655
 489/1000 [=============>................] - ETA: 3:51 - loss: 2.7752 - regression_loss: 2.1111 - classification_loss: 0.6641
 490/1000 [=============>................] - ETA: 3:50 - loss: 2.7755 - regression_loss: 2.1120 - classification_loss: 0.6635
 491/1000 [=============>................] - ETA: 3:50 - loss: 2.7756 - regression_loss: 2.1122 - classification_loss: 0.6634
 492/1000 [=============>................] - ETA: 3:49 - loss: 2.7760 - regression_loss: 2.1130 - classification_loss: 0.6630
 493/1000 [=============>................] - ETA: 3:49 - loss: 2.7782 - regression_loss: 2.1148 - classification_loss: 0.6634
 494/1000 [=============>................] - ETA: 3:49 - loss: 2.7788 - regression_loss: 2.1153 - classification_loss: 0.6634
 495/1000 [=============>................] - ETA: 3:48 - loss: 2.7738 - regression_loss: 2.1111 - classification_loss: 0.6627
 496/1000 [=============>................] - ETA: 3:48 - loss: 2.7682 - regression_loss: 2.1068 - classification_loss: 0.6614
 497/1000 [=============>................] - ETA: 3:47 - loss: 2.7669 - regression_loss: 2.1056 - classification_loss: 0.6613
 498/1000 [=============>................] - ETA: 3:47 - loss: 2.7688 - regression_loss: 2.1073 - classification_loss: 0.6615
 499/1000 [=============>................] - ETA: 3:46 - loss: 2.7694 - regression_loss: 2.1084 - classification_loss: 0.6610
 500/1000 [==============>...............] - ETA: 3:46 - loss: 2.7692 - regression_loss: 2.1080 - classification_loss: 0.6612
 501/1000 [==============>...............] - ETA: 3:45 - loss: 2.7721 - regression_loss: 2.1093 - classification_loss: 0.6629
 502/1000 [==============>...............] - ETA: 3:45 - loss: 2.7758 - regression_loss: 2.1132 - classification_loss: 0.6626
 503/1000 [==============>...............] - ETA: 3:44 - loss: 2.7760 - regression_loss: 2.1142 - classification_loss: 0.6618
 504/1000 [==============>...............] - ETA: 3:44 - loss: 2.7814 - regression_loss: 2.1173 - classification_loss: 0.6641
 505/1000 [==============>...............] - ETA: 3:44 - loss: 2.7815 - regression_loss: 2.1178 - classification_loss: 0.6637
 506/1000 [==============>...............] - ETA: 3:43 - loss: 2.7840 - regression_loss: 2.1183 - classification_loss: 0.6657
 507/1000 [==============>...............] - ETA: 3:43 - loss: 2.7850 - regression_loss: 2.1197 - classification_loss: 0.6653
 508/1000 [==============>...............] - ETA: 3:42 - loss: 2.7904 - regression_loss: 2.1233 - classification_loss: 0.6671
 509/1000 [==============>...............] - ETA: 3:42 - loss: 2.7849 - regression_loss: 2.1192 - classification_loss: 0.6658
 510/1000 [==============>...............] - ETA: 3:41 - loss: 2.7795 - regression_loss: 2.1150 - classification_loss: 0.6645
 511/1000 [==============>...............] - ETA: 3:41 - loss: 2.7825 - regression_loss: 2.1173 - classification_loss: 0.6652
 512/1000 [==============>...............] - ETA: 3:40 - loss: 2.7858 - regression_loss: 2.1187 - classification_loss: 0.6671
 513/1000 [==============>...............] - ETA: 3:40 - loss: 2.7871 - regression_loss: 2.1201 - classification_loss: 0.6670
 514/1000 [==============>...............] - ETA: 3:39 - loss: 2.7903 - regression_loss: 2.1230 - classification_loss: 0.6673
 515/1000 [==============>...............] - ETA: 3:39 - loss: 2.7901 - regression_loss: 2.1225 - classification_loss: 0.6677
 516/1000 [==============>...............] - ETA: 3:39 - loss: 2.7847 - regression_loss: 2.1184 - classification_loss: 0.6664
 517/1000 [==============>...............] - ETA: 3:38 - loss: 2.7868 - regression_loss: 2.1203 - classification_loss: 0.6664
 518/1000 [==============>...............] - ETA: 3:38 - loss: 2.7880 - regression_loss: 2.1217 - classification_loss: 0.6663
 519/1000 [==============>...............] - ETA: 3:37 - loss: 2.7880 - regression_loss: 2.1215 - classification_loss: 0.6665
 520/1000 [==============>...............] - ETA: 3:37 - loss: 2.7869 - regression_loss: 2.1208 - classification_loss: 0.6660
 521/1000 [==============>...............] - ETA: 3:36 - loss: 2.7911 - regression_loss: 2.1232 - classification_loss: 0.6678
 522/1000 [==============>...............] - ETA: 3:36 - loss: 2.7957 - regression_loss: 2.1264 - classification_loss: 0.6693
 523/1000 [==============>...............] - ETA: 3:35 - loss: 2.7990 - regression_loss: 2.1274 - classification_loss: 0.6717
 524/1000 [==============>...............] - ETA: 3:35 - loss: 2.8001 - regression_loss: 2.1280 - classification_loss: 0.6722
 525/1000 [==============>...............] - ETA: 3:35 - loss: 2.7948 - regression_loss: 2.1239 - classification_loss: 0.6709
 526/1000 [==============>...............] - ETA: 3:34 - loss: 2.7956 - regression_loss: 2.1251 - classification_loss: 0.6705
 527/1000 [==============>...............] - ETA: 3:34 - loss: 2.8013 - regression_loss: 2.1293 - classification_loss: 0.6720
 528/1000 [==============>...............] - ETA: 3:33 - loss: 2.7992 - regression_loss: 2.1278 - classification_loss: 0.6713
 529/1000 [==============>...............] - ETA: 3:33 - loss: 2.8031 - regression_loss: 2.1313 - classification_loss: 0.6718
 530/1000 [==============>...............] - ETA: 3:32 - loss: 2.7978 - regression_loss: 2.1273 - classification_loss: 0.6705
 531/1000 [==============>...............] - ETA: 3:32 - loss: 2.7925 - regression_loss: 2.1233 - classification_loss: 0.6692
 532/1000 [==============>...............] - ETA: 3:31 - loss: 2.7873 - regression_loss: 2.1193 - classification_loss: 0.6680
 533/1000 [==============>...............] - ETA: 3:31 - loss: 2.7908 - regression_loss: 2.1212 - classification_loss: 0.6696
 534/1000 [===============>..............] - ETA: 3:30 - loss: 2.7856 - regression_loss: 2.1172 - classification_loss: 0.6684
 535/1000 [===============>..............] - ETA: 3:30 - loss: 2.7806 - regression_loss: 2.1133 - classification_loss: 0.6673
 536/1000 [===============>..............] - ETA: 3:29 - loss: 2.7813 - regression_loss: 2.1143 - classification_loss: 0.6670
 537/1000 [===============>..............] - ETA: 3:29 - loss: 2.7862 - regression_loss: 2.1187 - classification_loss: 0.6676
 538/1000 [===============>..............] - ETA: 3:29 - loss: 2.7852 - regression_loss: 2.1174 - classification_loss: 0.6678
 539/1000 [===============>..............] - ETA: 3:28 - loss: 2.7899 - regression_loss: 2.1203 - classification_loss: 0.6696
 540/1000 [===============>..............] - ETA: 3:28 - loss: 2.7938 - regression_loss: 2.1216 - classification_loss: 0.6722
 541/1000 [===============>..............] - ETA: 3:27 - loss: 2.7939 - regression_loss: 2.1225 - classification_loss: 0.6714
 542/1000 [===============>..............] - ETA: 3:27 - loss: 2.7940 - regression_loss: 2.1229 - classification_loss: 0.6712
 543/1000 [===============>..............] - ETA: 3:26 - loss: 2.7951 - regression_loss: 2.1235 - classification_loss: 0.6716
 544/1000 [===============>..............] - ETA: 3:26 - loss: 2.7965 - regression_loss: 2.1253 - classification_loss: 0.6712
 545/1000 [===============>..............] - ETA: 3:25 - loss: 2.7914 - regression_loss: 2.1214 - classification_loss: 0.6700
 546/1000 [===============>..............] - ETA: 3:25 - loss: 2.7934 - regression_loss: 2.1224 - classification_loss: 0.6710
 547/1000 [===============>..............] - ETA: 3:24 - loss: 2.7986 - regression_loss: 2.1272 - classification_loss: 0.6713
 548/1000 [===============>..............] - ETA: 3:24 - loss: 2.7996 - regression_loss: 2.1267 - classification_loss: 0.6729
 549/1000 [===============>..............] - ETA: 3:24 - loss: 2.7986 - regression_loss: 2.1263 - classification_loss: 0.6723
 550/1000 [===============>..............] - ETA: 3:23 - loss: 2.8024 - regression_loss: 2.1285 - classification_loss: 0.6739
 551/1000 [===============>..............] - ETA: 3:23 - loss: 2.8031 - regression_loss: 2.1285 - classification_loss: 0.6746
 552/1000 [===============>..............] - ETA: 3:22 - loss: 2.7980 - regression_loss: 2.1246 - classification_loss: 0.6734
 553/1000 [===============>..............] - ETA: 3:22 - loss: 2.7974 - regression_loss: 2.1243 - classification_loss: 0.6731
 554/1000 [===============>..............] - ETA: 3:21 - loss: 2.7924 - regression_loss: 2.1204 - classification_loss: 0.6719
 555/1000 [===============>..............] - ETA: 3:21 - loss: 2.7873 - regression_loss: 2.1166 - classification_loss: 0.6707
 556/1000 [===============>..............] - ETA: 3:20 - loss: 2.7879 - regression_loss: 2.1178 - classification_loss: 0.6701
 557/1000 [===============>..............] - ETA: 3:20 - loss: 2.7889 - regression_loss: 2.1173 - classification_loss: 0.6716
 558/1000 [===============>..............] - ETA: 3:19 - loss: 2.7889 - regression_loss: 2.1174 - classification_loss: 0.6714
 559/1000 [===============>..............] - ETA: 3:19 - loss: 2.7895 - regression_loss: 2.1188 - classification_loss: 0.6707
 560/1000 [===============>..............] - ETA: 3:19 - loss: 2.7960 - regression_loss: 2.1234 - classification_loss: 0.6726
 561/1000 [===============>..............] - ETA: 3:18 - loss: 2.7910 - regression_loss: 2.1196 - classification_loss: 0.6714
 562/1000 [===============>..............] - ETA: 3:18 - loss: 2.7912 - regression_loss: 2.1198 - classification_loss: 0.6714
 563/1000 [===============>..............] - ETA: 3:17 - loss: 2.7863 - regression_loss: 2.1161 - classification_loss: 0.6702
 564/1000 [===============>..............] - ETA: 3:17 - loss: 2.7866 - regression_loss: 2.1171 - classification_loss: 0.6695
 565/1000 [===============>..............] - ETA: 3:16 - loss: 2.7857 - regression_loss: 2.1169 - classification_loss: 0.6688
 566/1000 [===============>..............] - ETA: 3:16 - loss: 2.7849 - regression_loss: 2.1168 - classification_loss: 0.6681
 567/1000 [================>.............] - ETA: 3:15 - loss: 2.7858 - regression_loss: 2.1178 - classification_loss: 0.6680
 568/1000 [================>.............] - ETA: 3:15 - loss: 2.7809 - regression_loss: 2.1141 - classification_loss: 0.6668
 569/1000 [================>.............] - ETA: 3:15 - loss: 2.7820 - regression_loss: 2.1155 - classification_loss: 0.6666
 570/1000 [================>.............] - ETA: 3:14 - loss: 2.7848 - regression_loss: 2.1176 - classification_loss: 0.6672
 571/1000 [================>.............] - ETA: 3:14 - loss: 2.7857 - regression_loss: 2.1188 - classification_loss: 0.6669
 572/1000 [================>.............] - ETA: 3:13 - loss: 2.7833 - regression_loss: 2.1173 - classification_loss: 0.6661
 573/1000 [================>.............] - ETA: 3:13 - loss: 2.7785 - regression_loss: 2.1136 - classification_loss: 0.6649
 574/1000 [================>.............] - ETA: 3:12 - loss: 2.7778 - regression_loss: 2.1130 - classification_loss: 0.6649
 575/1000 [================>.............] - ETA: 3:12 - loss: 2.7817 - regression_loss: 2.1162 - classification_loss: 0.6655
 576/1000 [================>.............] - ETA: 3:11 - loss: 2.7769 - regression_loss: 2.1125 - classification_loss: 0.6644
 577/1000 [================>.............] - ETA: 3:11 - loss: 2.7800 - regression_loss: 2.1155 - classification_loss: 0.6645
 578/1000 [================>.............] - ETA: 3:10 - loss: 2.7752 - regression_loss: 2.1118 - classification_loss: 0.6634
 579/1000 [================>.............] - ETA: 3:10 - loss: 2.7754 - regression_loss: 2.1122 - classification_loss: 0.6632
 580/1000 [================>.............] - ETA: 3:10 - loss: 2.7767 - regression_loss: 2.1133 - classification_loss: 0.6634
 581/1000 [================>.............] - ETA: 3:09 - loss: 2.7784 - regression_loss: 2.1146 - classification_loss: 0.6638
 582/1000 [================>.............] - ETA: 3:09 - loss: 2.7787 - regression_loss: 2.1142 - classification_loss: 0.6644
 583/1000 [================>.............] - ETA: 3:08 - loss: 2.7766 - regression_loss: 2.1106 - classification_loss: 0.6660
 584/1000 [================>.............] - ETA: 3:08 - loss: 2.7799 - regression_loss: 2.1115 - classification_loss: 0.6684
 585/1000 [================>.............] - ETA: 3:07 - loss: 2.7752 - regression_loss: 2.1079 - classification_loss: 0.6673
 586/1000 [================>.............] - ETA: 3:07 - loss: 2.7779 - regression_loss: 2.1098 - classification_loss: 0.6681
 587/1000 [================>.............] - ETA: 3:06 - loss: 2.7731 - regression_loss: 2.1062 - classification_loss: 0.6669
 588/1000 [================>.............] - ETA: 3:06 - loss: 2.7734 - regression_loss: 2.1060 - classification_loss: 0.6674
 589/1000 [================>.............] - ETA: 3:05 - loss: 2.7722 - regression_loss: 2.1055 - classification_loss: 0.6667
 590/1000 [================>.............] - ETA: 3:05 - loss: 2.7721 - regression_loss: 2.1062 - classification_loss: 0.6659
 591/1000 [================>.............] - ETA: 3:05 - loss: 2.7726 - regression_loss: 2.1066 - classification_loss: 0.6660
 592/1000 [================>.............] - ETA: 3:04 - loss: 2.7679 - regression_loss: 2.1030 - classification_loss: 0.6649
 593/1000 [================>.............] - ETA: 3:04 - loss: 2.7686 - regression_loss: 2.1042 - classification_loss: 0.6645
 594/1000 [================>.............] - ETA: 3:03 - loss: 2.7734 - regression_loss: 2.1070 - classification_loss: 0.6664
 595/1000 [================>.............] - ETA: 3:03 - loss: 2.7734 - regression_loss: 2.1072 - classification_loss: 0.6661
 596/1000 [================>.............] - ETA: 3:02 - loss: 2.7753 - regression_loss: 2.1084 - classification_loss: 0.6669
 597/1000 [================>.............] - ETA: 3:02 - loss: 2.7781 - regression_loss: 2.1112 - classification_loss: 0.6669
 598/1000 [================>.............] - ETA: 3:01 - loss: 2.7828 - regression_loss: 2.1147 - classification_loss: 0.6682
 599/1000 [================>.............] - ETA: 3:01 - loss: 2.7857 - regression_loss: 2.1159 - classification_loss: 0.6698
 600/1000 [=================>............] - ETA: 3:00 - loss: 2.7851 - regression_loss: 2.1154 - classification_loss: 0.6697
 601/1000 [=================>............] - ETA: 3:00 - loss: 2.7843 - regression_loss: 2.1152 - classification_loss: 0.6690
 602/1000 [=================>............] - ETA: 3:00 - loss: 2.7844 - regression_loss: 2.1150 - classification_loss: 0.6694
 603/1000 [=================>............] - ETA: 2:59 - loss: 2.7861 - regression_loss: 2.1160 - classification_loss: 0.6701
 604/1000 [=================>............] - ETA: 2:59 - loss: 2.7867 - regression_loss: 2.1168 - classification_loss: 0.6699
 605/1000 [=================>............] - ETA: 2:58 - loss: 2.7894 - regression_loss: 2.1183 - classification_loss: 0.6711
 606/1000 [=================>............] - ETA: 2:58 - loss: 2.7875 - regression_loss: 2.1171 - classification_loss: 0.6704
 607/1000 [=================>............] - ETA: 2:57 - loss: 2.7829 - regression_loss: 2.1136 - classification_loss: 0.6693
 608/1000 [=================>............] - ETA: 2:57 - loss: 2.7825 - regression_loss: 2.1135 - classification_loss: 0.6689
 609/1000 [=================>............] - ETA: 2:56 - loss: 2.7815 - regression_loss: 2.1127 - classification_loss: 0.6688
 610/1000 [=================>............] - ETA: 2:56 - loss: 2.7819 - regression_loss: 2.1133 - classification_loss: 0.6686
 611/1000 [=================>............] - ETA: 2:56 - loss: 2.7837 - regression_loss: 2.1150 - classification_loss: 0.6687
 612/1000 [=================>............] - ETA: 2:55 - loss: 2.7859 - regression_loss: 2.1168 - classification_loss: 0.6691
 613/1000 [=================>............] - ETA: 2:55 - loss: 2.7863 - regression_loss: 2.1175 - classification_loss: 0.6687
 614/1000 [=================>............] - ETA: 2:54 - loss: 2.7873 - regression_loss: 2.1187 - classification_loss: 0.6685
 615/1000 [=================>............] - ETA: 2:54 - loss: 2.7896 - regression_loss: 2.1210 - classification_loss: 0.6686
 616/1000 [=================>............] - ETA: 2:53 - loss: 2.7850 - regression_loss: 2.1175 - classification_loss: 0.6675
 617/1000 [=================>............] - ETA: 2:53 - loss: 2.7834 - regression_loss: 2.1163 - classification_loss: 0.6671
 618/1000 [=================>............] - ETA: 2:52 - loss: 2.7851 - regression_loss: 2.1180 - classification_loss: 0.6671
 619/1000 [=================>............] - ETA: 2:52 - loss: 2.7887 - regression_loss: 2.1212 - classification_loss: 0.6675
 620/1000 [=================>............] - ETA: 2:51 - loss: 2.7885 - regression_loss: 2.1215 - classification_loss: 0.6670
 621/1000 [=================>............] - ETA: 2:51 - loss: 2.7891 - regression_loss: 2.1223 - classification_loss: 0.6667
 622/1000 [=================>............] - ETA: 2:51 - loss: 2.7863 - regression_loss: 2.1189 - classification_loss: 0.6673
 623/1000 [=================>............] - ETA: 2:50 - loss: 2.7843 - regression_loss: 2.1174 - classification_loss: 0.6669
 624/1000 [=================>............] - ETA: 2:50 - loss: 2.7837 - regression_loss: 2.1172 - classification_loss: 0.6664
 625/1000 [=================>............] - ETA: 2:49 - loss: 2.7869 - regression_loss: 2.1173 - classification_loss: 0.6696
 626/1000 [=================>............] - ETA: 2:49 - loss: 2.7890 - regression_loss: 2.1181 - classification_loss: 0.6709
 627/1000 [=================>............] - ETA: 2:48 - loss: 2.7890 - regression_loss: 2.1183 - classification_loss: 0.6706
 628/1000 [=================>............] - ETA: 2:48 - loss: 2.7913 - regression_loss: 2.1194 - classification_loss: 0.6719
 629/1000 [=================>............] - ETA: 2:47 - loss: 2.7910 - regression_loss: 2.1192 - classification_loss: 0.6718
 630/1000 [=================>............] - ETA: 2:47 - loss: 2.7916 - regression_loss: 2.1200 - classification_loss: 0.6715
 631/1000 [=================>............] - ETA: 2:46 - loss: 2.7936 - regression_loss: 2.1220 - classification_loss: 0.6716
 632/1000 [=================>............] - ETA: 2:46 - loss: 2.7923 - regression_loss: 2.1212 - classification_loss: 0.6711
 633/1000 [=================>............] - ETA: 2:46 - loss: 2.7879 - regression_loss: 2.1178 - classification_loss: 0.6701
 634/1000 [==================>...........] - ETA: 2:45 - loss: 2.7884 - regression_loss: 2.1183 - classification_loss: 0.6701
 635/1000 [==================>...........] - ETA: 2:45 - loss: 2.7888 - regression_loss: 2.1176 - classification_loss: 0.6712
 636/1000 [==================>...........] - ETA: 2:44 - loss: 2.7889 - regression_loss: 2.1174 - classification_loss: 0.6714
 637/1000 [==================>...........] - ETA: 2:44 - loss: 2.7897 - regression_loss: 2.1182 - classification_loss: 0.6715
 638/1000 [==================>...........] - ETA: 2:43 - loss: 2.7936 - regression_loss: 2.1213 - classification_loss: 0.6724
 639/1000 [==================>...........] - ETA: 2:43 - loss: 2.7974 - regression_loss: 2.1244 - classification_loss: 0.6730
 640/1000 [==================>...........] - ETA: 2:42 - loss: 2.7994 - regression_loss: 2.1263 - classification_loss: 0.6731
 641/1000 [==================>...........] - ETA: 2:42 - loss: 2.8017 - regression_loss: 2.1279 - classification_loss: 0.6738
 642/1000 [==================>...........] - ETA: 2:41 - loss: 2.8046 - regression_loss: 2.1305 - classification_loss: 0.6742
 643/1000 [==================>...........] - ETA: 2:41 - loss: 2.8048 - regression_loss: 2.1305 - classification_loss: 0.6743
 644/1000 [==================>...........] - ETA: 2:41 - loss: 2.8090 - regression_loss: 2.1333 - classification_loss: 0.6757
 645/1000 [==================>...........] - ETA: 2:40 - loss: 2.8047 - regression_loss: 2.1300 - classification_loss: 0.6747
 646/1000 [==================>...........] - ETA: 2:40 - loss: 2.8003 - regression_loss: 2.1267 - classification_loss: 0.6736
 647/1000 [==================>...........] - ETA: 2:39 - loss: 2.8001 - regression_loss: 2.1269 - classification_loss: 0.6732
 648/1000 [==================>...........] - ETA: 2:39 - loss: 2.8006 - regression_loss: 2.1273 - classification_loss: 0.6733
 649/1000 [==================>...........] - ETA: 2:38 - loss: 2.8013 - regression_loss: 2.1284 - classification_loss: 0.6729
 650/1000 [==================>...........] - ETA: 2:38 - loss: 2.8022 - regression_loss: 2.1294 - classification_loss: 0.6728
 651/1000 [==================>...........] - ETA: 2:37 - loss: 2.8015 - regression_loss: 2.1291 - classification_loss: 0.6723
 652/1000 [==================>...........] - ETA: 2:37 - loss: 2.7972 - regression_loss: 2.1259 - classification_loss: 0.6713
 653/1000 [==================>...........] - ETA: 2:37 - loss: 2.7987 - regression_loss: 2.1268 - classification_loss: 0.6719
 654/1000 [==================>...........] - ETA: 2:36 - loss: 2.8012 - regression_loss: 2.1288 - classification_loss: 0.6724
 655/1000 [==================>...........] - ETA: 2:36 - loss: 2.8031 - regression_loss: 2.1305 - classification_loss: 0.6726
 656/1000 [==================>...........] - ETA: 2:35 - loss: 2.8039 - regression_loss: 2.1317 - classification_loss: 0.6722
 657/1000 [==================>...........] - ETA: 2:35 - loss: 2.7997 - regression_loss: 2.1285 - classification_loss: 0.6712
 658/1000 [==================>...........] - ETA: 2:34 - loss: 2.7955 - regression_loss: 2.1252 - classification_loss: 0.6703
 659/1000 [==================>...........] - ETA: 2:34 - loss: 2.7912 - regression_loss: 2.1220 - classification_loss: 0.6693
 660/1000 [==================>...........] - ETA: 2:33 - loss: 2.7898 - regression_loss: 2.1210 - classification_loss: 0.6689
 661/1000 [==================>...........] - ETA: 2:33 - loss: 2.7922 - regression_loss: 2.1236 - classification_loss: 0.6687
 662/1000 [==================>...........] - ETA: 2:32 - loss: 2.7963 - regression_loss: 2.1255 - classification_loss: 0.6708
 663/1000 [==================>...........] - ETA: 2:32 - loss: 2.7972 - regression_loss: 2.1270 - classification_loss: 0.6702
 664/1000 [==================>...........] - ETA: 2:32 - loss: 2.8004 - regression_loss: 2.1301 - classification_loss: 0.6703
 665/1000 [==================>...........] - ETA: 2:31 - loss: 2.8027 - regression_loss: 2.1310 - classification_loss: 0.6717
 666/1000 [==================>...........] - ETA: 2:31 - loss: 2.8044 - regression_loss: 2.1333 - classification_loss: 0.6712
 667/1000 [===================>..........] - ETA: 2:30 - loss: 2.8038 - regression_loss: 2.1328 - classification_loss: 0.6710
 668/1000 [===================>..........] - ETA: 2:30 - loss: 2.8063 - regression_loss: 2.1351 - classification_loss: 0.6712
 669/1000 [===================>..........] - ETA: 2:29 - loss: 2.8107 - regression_loss: 2.1387 - classification_loss: 0.6721
 670/1000 [===================>..........] - ETA: 2:29 - loss: 2.8097 - regression_loss: 2.1382 - classification_loss: 0.6715
 671/1000 [===================>..........] - ETA: 2:28 - loss: 2.8095 - regression_loss: 2.1386 - classification_loss: 0.6710
 672/1000 [===================>..........] - ETA: 2:28 - loss: 2.8054 - regression_loss: 2.1354 - classification_loss: 0.6700
 673/1000 [===================>..........] - ETA: 2:27 - loss: 2.8041 - regression_loss: 2.1344 - classification_loss: 0.6697
 674/1000 [===================>..........] - ETA: 2:27 - loss: 2.8033 - regression_loss: 2.1341 - classification_loss: 0.6692
 675/1000 [===================>..........] - ETA: 2:27 - loss: 2.8026 - regression_loss: 2.1338 - classification_loss: 0.6688
 676/1000 [===================>..........] - ETA: 2:26 - loss: 2.7985 - regression_loss: 2.1306 - classification_loss: 0.6679
 677/1000 [===================>..........] - ETA: 2:26 - loss: 2.8003 - regression_loss: 2.1322 - classification_loss: 0.6680
 678/1000 [===================>..........] - ETA: 2:25 - loss: 2.7963 - regression_loss: 2.1291 - classification_loss: 0.6672
 679/1000 [===================>..........] - ETA: 2:25 - loss: 2.7923 - regression_loss: 2.1260 - classification_loss: 0.6663
 680/1000 [===================>..........] - ETA: 2:24 - loss: 2.7919 - regression_loss: 2.1258 - classification_loss: 0.6661
 681/1000 [===================>..........] - ETA: 2:24 - loss: 2.7878 - regression_loss: 2.1226 - classification_loss: 0.6651
 682/1000 [===================>..........] - ETA: 2:23 - loss: 2.7871 - regression_loss: 2.1222 - classification_loss: 0.6650
 683/1000 [===================>..........] - ETA: 2:23 - loss: 2.7850 - regression_loss: 2.1205 - classification_loss: 0.6645
 684/1000 [===================>..........] - ETA: 2:22 - loss: 2.7846 - regression_loss: 2.1202 - classification_loss: 0.6644
 685/1000 [===================>..........] - ETA: 2:22 - loss: 2.7854 - regression_loss: 2.1207 - classification_loss: 0.6647
 686/1000 [===================>..........] - ETA: 2:22 - loss: 2.7859 - regression_loss: 2.1206 - classification_loss: 0.6653
 687/1000 [===================>..........] - ETA: 2:21 - loss: 2.7856 - regression_loss: 2.1208 - classification_loss: 0.6648
 688/1000 [===================>..........] - ETA: 2:21 - loss: 2.7858 - regression_loss: 2.1211 - classification_loss: 0.6647
 689/1000 [===================>..........] - ETA: 2:20 - loss: 2.7891 - regression_loss: 2.1232 - classification_loss: 0.6659
 690/1000 [===================>..........] - ETA: 2:20 - loss: 2.7896 - regression_loss: 2.1239 - classification_loss: 0.6657
 691/1000 [===================>..........] - ETA: 2:19 - loss: 2.7856 - regression_loss: 2.1208 - classification_loss: 0.6648
 692/1000 [===================>..........] - ETA: 2:19 - loss: 2.7816 - regression_loss: 2.1178 - classification_loss: 0.6638
 693/1000 [===================>..........] - ETA: 2:18 - loss: 2.7823 - regression_loss: 2.1179 - classification_loss: 0.6644
 694/1000 [===================>..........] - ETA: 2:18 - loss: 2.7844 - regression_loss: 2.1199 - classification_loss: 0.6644
 695/1000 [===================>..........] - ETA: 2:18 - loss: 2.7861 - regression_loss: 2.1207 - classification_loss: 0.6653
 696/1000 [===================>..........] - ETA: 2:17 - loss: 2.7881 - regression_loss: 2.1218 - classification_loss: 0.6663
 697/1000 [===================>..........] - ETA: 2:17 - loss: 2.7841 - regression_loss: 2.1187 - classification_loss: 0.6653
 698/1000 [===================>..........] - ETA: 2:16 - loss: 2.7853 - regression_loss: 2.1194 - classification_loss: 0.6659
 699/1000 [===================>..........] - ETA: 2:16 - loss: 2.7814 - regression_loss: 2.1164 - classification_loss: 0.6650
 700/1000 [====================>.........] - ETA: 2:15 - loss: 2.7850 - regression_loss: 2.1199 - classification_loss: 0.6651
 701/1000 [====================>.........] - ETA: 2:15 - loss: 2.7845 - regression_loss: 2.1200 - classification_loss: 0.6645
 702/1000 [====================>.........] - ETA: 2:14 - loss: 2.7870 - regression_loss: 2.1215 - classification_loss: 0.6655
 703/1000 [====================>.........] - ETA: 2:14 - loss: 2.7851 - regression_loss: 2.1204 - classification_loss: 0.6648
 704/1000 [====================>.........] - ETA: 2:13 - loss: 2.7868 - regression_loss: 2.1217 - classification_loss: 0.6652
 705/1000 [====================>.........] - ETA: 2:13 - loss: 2.7891 - regression_loss: 2.1227 - classification_loss: 0.6663
 706/1000 [====================>.........] - ETA: 2:13 - loss: 2.7851 - regression_loss: 2.1197 - classification_loss: 0.6654
 707/1000 [====================>.........] - ETA: 2:12 - loss: 2.7857 - regression_loss: 2.1204 - classification_loss: 0.6653
 708/1000 [====================>.........] - ETA: 2:12 - loss: 2.7876 - regression_loss: 2.1225 - classification_loss: 0.6651
 709/1000 [====================>.........] - ETA: 2:11 - loss: 2.7870 - regression_loss: 2.1218 - classification_loss: 0.6652
 710/1000 [====================>.........] - ETA: 2:11 - loss: 2.7895 - regression_loss: 2.1239 - classification_loss: 0.6656
 711/1000 [====================>.........] - ETA: 2:10 - loss: 2.7935 - regression_loss: 2.1261 - classification_loss: 0.6674
 712/1000 [====================>.........] - ETA: 2:10 - loss: 2.7963 - regression_loss: 2.1279 - classification_loss: 0.6684
 713/1000 [====================>.........] - ETA: 2:09 - loss: 2.7998 - regression_loss: 2.1301 - classification_loss: 0.6697
 714/1000 [====================>.........] - ETA: 2:09 - loss: 2.7999 - regression_loss: 2.1301 - classification_loss: 0.6698
 715/1000 [====================>.........] - ETA: 2:08 - loss: 2.7960 - regression_loss: 2.1272 - classification_loss: 0.6688
 716/1000 [====================>.........] - ETA: 2:08 - loss: 2.7921 - regression_loss: 2.1242 - classification_loss: 0.6679
 717/1000 [====================>.........] - ETA: 2:08 - loss: 2.7905 - regression_loss: 2.1232 - classification_loss: 0.6673
 718/1000 [====================>.........] - ETA: 2:07 - loss: 2.7872 - regression_loss: 2.1202 - classification_loss: 0.6670
 719/1000 [====================>.........] - ETA: 2:07 - loss: 2.7862 - regression_loss: 2.1196 - classification_loss: 0.6666
 720/1000 [====================>.........] - ETA: 2:06 - loss: 2.7868 - regression_loss: 2.1203 - classification_loss: 0.6665
 721/1000 [====================>.........] - ETA: 2:06 - loss: 2.7849 - regression_loss: 2.1191 - classification_loss: 0.6658
 722/1000 [====================>.........] - ETA: 2:05 - loss: 2.7873 - regression_loss: 2.1201 - classification_loss: 0.6672
 723/1000 [====================>.........] - ETA: 2:05 - loss: 2.7869 - regression_loss: 2.1202 - classification_loss: 0.6667
 724/1000 [====================>.........] - ETA: 2:04 - loss: 2.7870 - regression_loss: 2.1204 - classification_loss: 0.6666
 725/1000 [====================>.........] - ETA: 2:04 - loss: 2.7867 - regression_loss: 2.1201 - classification_loss: 0.6666
 726/1000 [====================>.........] - ETA: 2:03 - loss: 2.7829 - regression_loss: 2.1172 - classification_loss: 0.6656
 727/1000 [====================>.........] - ETA: 2:03 - loss: 2.7843 - regression_loss: 2.1186 - classification_loss: 0.6657
 728/1000 [====================>.........] - ETA: 2:03 - loss: 2.7852 - regression_loss: 2.1192 - classification_loss: 0.6659
 729/1000 [====================>.........] - ETA: 2:02 - loss: 2.7848 - regression_loss: 2.1192 - classification_loss: 0.6656
 730/1000 [====================>.........] - ETA: 2:02 - loss: 2.7846 - regression_loss: 2.1195 - classification_loss: 0.6651
 731/1000 [====================>.........] - ETA: 2:01 - loss: 2.7845 - regression_loss: 2.1198 - classification_loss: 0.6646
 732/1000 [====================>.........] - ETA: 2:01 - loss: 2.7861 - regression_loss: 2.1216 - classification_loss: 0.6645
 733/1000 [====================>.........] - ETA: 2:00 - loss: 2.7870 - regression_loss: 2.1225 - classification_loss: 0.6645
 734/1000 [=====================>........] - ETA: 2:00 - loss: 2.7836 - regression_loss: 2.1197 - classification_loss: 0.6640
 735/1000 [=====================>........] - ETA: 1:59 - loss: 2.7841 - regression_loss: 2.1201 - classification_loss: 0.6641
 736/1000 [=====================>........] - ETA: 1:59 - loss: 2.7861 - regression_loss: 2.1215 - classification_loss: 0.6646
 737/1000 [=====================>........] - ETA: 1:59 - loss: 2.7851 - regression_loss: 2.1210 - classification_loss: 0.6641
 738/1000 [=====================>........] - ETA: 1:58 - loss: 2.7847 - regression_loss: 2.1209 - classification_loss: 0.6638
 739/1000 [=====================>........] - ETA: 1:58 - loss: 2.7841 - regression_loss: 2.1208 - classification_loss: 0.6633
 740/1000 [=====================>........] - ETA: 1:57 - loss: 2.7803 - regression_loss: 2.1179 - classification_loss: 0.6624
 741/1000 [=====================>........] - ETA: 1:57 - loss: 2.7802 - regression_loss: 2.1179 - classification_loss: 0.6623
 742/1000 [=====================>........] - ETA: 1:56 - loss: 2.7811 - regression_loss: 2.1185 - classification_loss: 0.6625
 743/1000 [=====================>........] - ETA: 1:56 - loss: 2.7802 - regression_loss: 2.1157 - classification_loss: 0.6645
 744/1000 [=====================>........] - ETA: 1:55 - loss: 2.7820 - regression_loss: 2.1178 - classification_loss: 0.6643
 745/1000 [=====================>........] - ETA: 1:55 - loss: 2.7783 - regression_loss: 2.1149 - classification_loss: 0.6634
 746/1000 [=====================>........] - ETA: 1:54 - loss: 2.7813 - regression_loss: 2.1164 - classification_loss: 0.6649
 747/1000 [=====================>........] - ETA: 1:54 - loss: 2.7811 - regression_loss: 2.1159 - classification_loss: 0.6652
 748/1000 [=====================>........] - ETA: 1:54 - loss: 2.7824 - regression_loss: 2.1172 - classification_loss: 0.6652
 749/1000 [=====================>........] - ETA: 1:53 - loss: 2.7788 - regression_loss: 2.1143 - classification_loss: 0.6645
 750/1000 [=====================>........] - ETA: 1:53 - loss: 2.7799 - regression_loss: 2.1144 - classification_loss: 0.6654
 751/1000 [=====================>........] - ETA: 1:52 - loss: 2.7762 - regression_loss: 2.1116 - classification_loss: 0.6645
 752/1000 [=====================>........] - ETA: 1:52 - loss: 2.7725 - regression_loss: 2.1088 - classification_loss: 0.6637
 753/1000 [=====================>........] - ETA: 1:51 - loss: 2.7688 - regression_loss: 2.1060 - classification_loss: 0.6628
 754/1000 [=====================>........] - ETA: 1:51 - loss: 2.7652 - regression_loss: 2.1032 - classification_loss: 0.6620
 755/1000 [=====================>........] - ETA: 1:50 - loss: 2.7672 - regression_loss: 2.1054 - classification_loss: 0.6618
 756/1000 [=====================>........] - ETA: 1:50 - loss: 2.7696 - regression_loss: 2.1076 - classification_loss: 0.6620
 757/1000 [=====================>........] - ETA: 1:49 - loss: 2.7726 - regression_loss: 2.1092 - classification_loss: 0.6634
 758/1000 [=====================>........] - ETA: 1:49 - loss: 2.7737 - regression_loss: 2.1097 - classification_loss: 0.6640
 759/1000 [=====================>........] - ETA: 1:49 - loss: 2.7765 - regression_loss: 2.1115 - classification_loss: 0.6650
 760/1000 [=====================>........] - ETA: 1:48 - loss: 2.7764 - regression_loss: 2.1120 - classification_loss: 0.6645
 761/1000 [=====================>........] - ETA: 1:48 - loss: 2.7776 - regression_loss: 2.1130 - classification_loss: 0.6646
 762/1000 [=====================>........] - ETA: 1:47 - loss: 2.7810 - regression_loss: 2.1149 - classification_loss: 0.6661
 763/1000 [=====================>........] - ETA: 1:47 - loss: 2.7854 - regression_loss: 2.1185 - classification_loss: 0.6668
 764/1000 [=====================>........] - ETA: 1:46 - loss: 2.7817 - regression_loss: 2.1157 - classification_loss: 0.6660
 765/1000 [=====================>........] - ETA: 1:46 - loss: 2.7846 - regression_loss: 2.1186 - classification_loss: 0.6660
 766/1000 [=====================>........] - ETA: 1:45 - loss: 2.7810 - regression_loss: 2.1159 - classification_loss: 0.6651
 767/1000 [======================>.......] - ETA: 1:45 - loss: 2.7845 - regression_loss: 2.1179 - classification_loss: 0.6666
 768/1000 [======================>.......] - ETA: 1:44 - loss: 2.7858 - regression_loss: 2.1191 - classification_loss: 0.6667
 769/1000 [======================>.......] - ETA: 1:44 - loss: 2.7852 - regression_loss: 2.1188 - classification_loss: 0.6664
 770/1000 [======================>.......] - ETA: 1:44 - loss: 2.7816 - regression_loss: 2.1161 - classification_loss: 0.6655
 771/1000 [======================>.......] - ETA: 1:43 - loss: 2.7819 - regression_loss: 2.1157 - classification_loss: 0.6662
 772/1000 [======================>.......] - ETA: 1:43 - loss: 2.7840 - regression_loss: 2.1174 - classification_loss: 0.6666
 773/1000 [======================>.......] - ETA: 1:42 - loss: 2.7841 - regression_loss: 2.1176 - classification_loss: 0.6665
 774/1000 [======================>.......] - ETA: 1:42 - loss: 2.7805 - regression_loss: 2.1149 - classification_loss: 0.6656
 775/1000 [======================>.......] - ETA: 1:41 - loss: 2.7789 - regression_loss: 2.1139 - classification_loss: 0.6651
 776/1000 [======================>.......] - ETA: 1:41 - loss: 2.7805 - regression_loss: 2.1149 - classification_loss: 0.6657
 777/1000 [======================>.......] - ETA: 1:40 - loss: 2.7819 - regression_loss: 2.1163 - classification_loss: 0.6656
 778/1000 [======================>.......] - ETA: 1:40 - loss: 2.7783 - regression_loss: 2.1136 - classification_loss: 0.6647
 779/1000 [======================>.......] - ETA: 1:40 - loss: 2.7776 - regression_loss: 2.1133 - classification_loss: 0.6643
 780/1000 [======================>.......] - ETA: 1:39 - loss: 2.7787 - regression_loss: 2.1131 - classification_loss: 0.6656
 781/1000 [======================>.......] - ETA: 1:39 - loss: 2.7786 - regression_loss: 2.1126 - classification_loss: 0.6660
 782/1000 [======================>.......] - ETA: 1:38 - loss: 2.7750 - regression_loss: 2.1099 - classification_loss: 0.6652
 783/1000 [======================>.......] - ETA: 1:38 - loss: 2.7753 - regression_loss: 2.1102 - classification_loss: 0.6651
 784/1000 [======================>.......] - ETA: 1:37 - loss: 2.7765 - regression_loss: 2.1118 - classification_loss: 0.6647
 785/1000 [======================>.......] - ETA: 1:37 - loss: 2.7730 - regression_loss: 2.1091 - classification_loss: 0.6638
 786/1000 [======================>.......] - ETA: 1:36 - loss: 2.7725 - regression_loss: 2.1091 - classification_loss: 0.6634
 787/1000 [======================>.......] - ETA: 1:36 - loss: 2.7748 - regression_loss: 2.1117 - classification_loss: 0.6631
 788/1000 [======================>.......] - ETA: 1:35 - loss: 2.7764 - regression_loss: 2.1120 - classification_loss: 0.6644
 789/1000 [======================>.......] - ETA: 1:35 - loss: 2.7754 - regression_loss: 2.1115 - classification_loss: 0.6639
 790/1000 [======================>.......] - ETA: 1:35 - loss: 2.7765 - regression_loss: 2.1123 - classification_loss: 0.6642
 791/1000 [======================>.......] - ETA: 1:34 - loss: 2.7730 - regression_loss: 2.1097 - classification_loss: 0.6633
 792/1000 [======================>.......] - ETA: 1:34 - loss: 2.7695 - regression_loss: 2.1070 - classification_loss: 0.6625
 793/1000 [======================>.......] - ETA: 1:33 - loss: 2.7729 - regression_loss: 2.1090 - classification_loss: 0.6639
 794/1000 [======================>.......] - ETA: 1:33 - loss: 2.7734 - regression_loss: 2.1092 - classification_loss: 0.6643
 795/1000 [======================>.......] - ETA: 1:32 - loss: 2.7749 - regression_loss: 2.1096 - classification_loss: 0.6654
 796/1000 [======================>.......] - ETA: 1:32 - loss: 2.7715 - regression_loss: 2.1069 - classification_loss: 0.6645
 797/1000 [======================>.......] - ETA: 1:31 - loss: 2.7714 - regression_loss: 2.1062 - classification_loss: 0.6652
 798/1000 [======================>.......] - ETA: 1:31 - loss: 2.7680 - regression_loss: 2.1036 - classification_loss: 0.6644
 799/1000 [======================>.......] - ETA: 1:30 - loss: 2.7678 - regression_loss: 2.1037 - classification_loss: 0.6640
 800/1000 [=======================>......] - ETA: 1:30 - loss: 2.7681 - regression_loss: 2.1039 - classification_loss: 0.6643
 801/1000 [=======================>......] - ETA: 1:30 - loss: 2.7710 - regression_loss: 2.1050 - classification_loss: 0.6660
 802/1000 [=======================>......] - ETA: 1:29 - loss: 2.7733 - regression_loss: 2.1069 - classification_loss: 0.6664
 803/1000 [=======================>......] - ETA: 1:29 - loss: 2.7699 - regression_loss: 2.1043 - classification_loss: 0.6656
 804/1000 [=======================>......] - ETA: 1:28 - loss: 2.7720 - regression_loss: 2.1054 - classification_loss: 0.6666
 805/1000 [=======================>......] - ETA: 1:28 - loss: 2.7741 - regression_loss: 2.1065 - classification_loss: 0.6676
 806/1000 [=======================>......] - ETA: 1:27 - loss: 2.7706 - regression_loss: 2.1038 - classification_loss: 0.6668
 807/1000 [=======================>......] - ETA: 1:27 - loss: 2.7674 - regression_loss: 2.1012 - classification_loss: 0.6662
 808/1000 [=======================>......] - ETA: 1:26 - loss: 2.7640 - regression_loss: 2.0986 - classification_loss: 0.6654
 809/1000 [=======================>......] - ETA: 1:26 - loss: 2.7654 - regression_loss: 2.0999 - classification_loss: 0.6655
 810/1000 [=======================>......] - ETA: 1:25 - loss: 2.7621 - regression_loss: 2.0973 - classification_loss: 0.6648
 811/1000 [=======================>......] - ETA: 1:25 - loss: 2.7609 - regression_loss: 2.0966 - classification_loss: 0.6643
 812/1000 [=======================>......] - ETA: 1:25 - loss: 2.7607 - regression_loss: 2.0969 - classification_loss: 0.6638
 813/1000 [=======================>......] - ETA: 1:24 - loss: 2.7614 - regression_loss: 2.0977 - classification_loss: 0.6637
 814/1000 [=======================>......] - ETA: 1:24 - loss: 2.7602 - regression_loss: 2.0970 - classification_loss: 0.6632
 815/1000 [=======================>......] - ETA: 1:23 - loss: 2.7611 - regression_loss: 2.0978 - classification_loss: 0.6634
 816/1000 [=======================>......] - ETA: 1:23 - loss: 2.7617 - regression_loss: 2.0986 - classification_loss: 0.6631
 817/1000 [=======================>......] - ETA: 1:22 - loss: 2.7583 - regression_loss: 2.0960 - classification_loss: 0.6623
 818/1000 [=======================>......] - ETA: 1:22 - loss: 2.7550 - regression_loss: 2.0935 - classification_loss: 0.6615
 819/1000 [=======================>......] - ETA: 1:21 - loss: 2.7568 - regression_loss: 2.0944 - classification_loss: 0.6625
 820/1000 [=======================>......] - ETA: 1:21 - loss: 2.7535 - regression_loss: 2.0918 - classification_loss: 0.6616
 821/1000 [=======================>......] - ETA: 1:21 - loss: 2.7553 - regression_loss: 2.0927 - classification_loss: 0.6626
 822/1000 [=======================>......] - ETA: 1:20 - loss: 2.7572 - regression_loss: 2.0944 - classification_loss: 0.6628
 823/1000 [=======================>......] - ETA: 1:20 - loss: 2.7562 - regression_loss: 2.0939 - classification_loss: 0.6622
 824/1000 [=======================>......] - ETA: 1:19 - loss: 2.7586 - regression_loss: 2.0958 - classification_loss: 0.6628
 825/1000 [=======================>......] - ETA: 1:19 - loss: 2.7597 - regression_loss: 2.0965 - classification_loss: 0.6632
 826/1000 [=======================>......] - ETA: 1:18 - loss: 2.7596 - regression_loss: 2.0969 - classification_loss: 0.6627
 827/1000 [=======================>......] - ETA: 1:18 - loss: 2.7563 - regression_loss: 2.0944 - classification_loss: 0.6619
 828/1000 [=======================>......] - ETA: 1:17 - loss: 2.7597 - regression_loss: 2.0974 - classification_loss: 0.6624
 829/1000 [=======================>......] - ETA: 1:17 - loss: 2.7564 - regression_loss: 2.0948 - classification_loss: 0.6616
 830/1000 [=======================>......] - ETA: 1:16 - loss: 2.7566 - regression_loss: 2.0944 - classification_loss: 0.6621
 831/1000 [=======================>......] - ETA: 1:16 - loss: 2.7589 - regression_loss: 2.0959 - classification_loss: 0.6630
 832/1000 [=======================>......] - ETA: 1:16 - loss: 2.7591 - regression_loss: 2.0963 - classification_loss: 0.6628
 833/1000 [=======================>......] - ETA: 1:15 - loss: 2.7598 - regression_loss: 2.0958 - classification_loss: 0.6641
 834/1000 [========================>.....] - ETA: 1:15 - loss: 2.7565 - regression_loss: 2.0933 - classification_loss: 0.6633
 835/1000 [========================>.....] - ETA: 1:14 - loss: 2.7603 - regression_loss: 2.0972 - classification_loss: 0.6631
 836/1000 [========================>.....] - ETA: 1:14 - loss: 2.7579 - regression_loss: 2.0947 - classification_loss: 0.6632
 837/1000 [========================>.....] - ETA: 1:13 - loss: 2.7546 - regression_loss: 2.0922 - classification_loss: 0.6624
 838/1000 [========================>.....] - ETA: 1:13 - loss: 2.7538 - regression_loss: 2.0915 - classification_loss: 0.6623
 839/1000 [========================>.....] - ETA: 1:12 - loss: 2.7533 - regression_loss: 2.0913 - classification_loss: 0.6620
 840/1000 [========================>.....] - ETA: 1:12 - loss: 2.7537 - regression_loss: 2.0920 - classification_loss: 0.6617
 841/1000 [========================>.....] - ETA: 1:11 - loss: 2.7554 - regression_loss: 2.0936 - classification_loss: 0.6617
 842/1000 [========================>.....] - ETA: 1:11 - loss: 2.7562 - regression_loss: 2.0935 - classification_loss: 0.6627
 843/1000 [========================>.....] - ETA: 1:11 - loss: 2.7544 - regression_loss: 2.0922 - classification_loss: 0.6622
 844/1000 [========================>.....] - ETA: 1:10 - loss: 2.7554 - regression_loss: 2.0933 - classification_loss: 0.6620
 845/1000 [========================>.....] - ETA: 1:10 - loss: 2.7568 - regression_loss: 2.0950 - classification_loss: 0.6618
 846/1000 [========================>.....] - ETA: 1:09 - loss: 2.7535 - regression_loss: 2.0925 - classification_loss: 0.6610
 847/1000 [========================>.....] - ETA: 1:09 - loss: 2.7566 - regression_loss: 2.0946 - classification_loss: 0.6619
 848/1000 [========================>.....] - ETA: 1:08 - loss: 2.7564 - regression_loss: 2.0949 - classification_loss: 0.6615
 849/1000 [========================>.....] - ETA: 1:08 - loss: 2.7598 - regression_loss: 2.0964 - classification_loss: 0.6634
 850/1000 [========================>.....] - ETA: 1:07 - loss: 2.7603 - regression_loss: 2.0975 - classification_loss: 0.6629
 851/1000 [========================>.....] - ETA: 1:07 - loss: 2.7620 - regression_loss: 2.0986 - classification_loss: 0.6634
 852/1000 [========================>.....] - ETA: 1:06 - loss: 2.7608 - regression_loss: 2.0978 - classification_loss: 0.6630
 853/1000 [========================>.....] - ETA: 1:06 - loss: 2.7576 - regression_loss: 2.0954 - classification_loss: 0.6622
 854/1000 [========================>.....] - ETA: 1:06 - loss: 2.7584 - regression_loss: 2.0965 - classification_loss: 0.6619
 855/1000 [========================>.....] - ETA: 1:05 - loss: 2.7552 - regression_loss: 2.0941 - classification_loss: 0.6612
 856/1000 [========================>.....] - ETA: 1:05 - loss: 2.7561 - regression_loss: 2.0953 - classification_loss: 0.6608
 857/1000 [========================>.....] - ETA: 1:04 - loss: 2.7582 - regression_loss: 2.0964 - classification_loss: 0.6618
 858/1000 [========================>.....] - ETA: 1:04 - loss: 2.7587 - regression_loss: 2.0971 - classification_loss: 0.6616
 859/1000 [========================>.....] - ETA: 1:03 - loss: 2.7555 - regression_loss: 2.0947 - classification_loss: 0.6608
 860/1000 [========================>.....] - ETA: 1:03 - loss: 2.7579 - regression_loss: 2.0961 - classification_loss: 0.6619
 861/1000 [========================>.....] - ETA: 1:02 - loss: 2.7578 - regression_loss: 2.0963 - classification_loss: 0.6615
 862/1000 [========================>.....] - ETA: 1:02 - loss: 2.7546 - regression_loss: 2.0939 - classification_loss: 0.6608
 863/1000 [========================>.....] - ETA: 1:02 - loss: 2.7554 - regression_loss: 2.0951 - classification_loss: 0.6603
 864/1000 [========================>.....] - ETA: 1:01 - loss: 2.7564 - regression_loss: 2.0960 - classification_loss: 0.6604
 865/1000 [========================>.....] - ETA: 1:01 - loss: 2.7570 - regression_loss: 2.0961 - classification_loss: 0.6609
 866/1000 [========================>.....] - ETA: 1:00 - loss: 2.7567 - regression_loss: 2.0961 - classification_loss: 0.6606
 867/1000 [=========================>....] - ETA: 1:00 - loss: 2.7572 - regression_loss: 2.0965 - classification_loss: 0.6606
 868/1000 [=========================>....] - ETA: 59s - loss: 2.7540 - regression_loss: 2.0941 - classification_loss: 0.6599 
 869/1000 [=========================>....] - ETA: 59s - loss: 2.7552 - regression_loss: 2.0951 - classification_loss: 0.6601
 870/1000 [=========================>....] - ETA: 58s - loss: 2.7554 - regression_loss: 2.0953 - classification_loss: 0.6601
 871/1000 [=========================>....] - ETA: 58s - loss: 2.7569 - regression_loss: 2.0961 - classification_loss: 0.6608
 872/1000 [=========================>....] - ETA: 57s - loss: 2.7578 - regression_loss: 2.0966 - classification_loss: 0.6612
 873/1000 [=========================>....] - ETA: 57s - loss: 2.7588 - regression_loss: 2.0974 - classification_loss: 0.6614
 874/1000 [=========================>....] - ETA: 57s - loss: 2.7592 - regression_loss: 2.0980 - classification_loss: 0.6612
 875/1000 [=========================>....] - ETA: 56s - loss: 2.7562 - regression_loss: 2.0956 - classification_loss: 0.6605
 876/1000 [=========================>....] - ETA: 56s - loss: 2.7566 - regression_loss: 2.0963 - classification_loss: 0.6602
 877/1000 [=========================>....] - ETA: 55s - loss: 2.7588 - regression_loss: 2.0981 - classification_loss: 0.6607
 878/1000 [=========================>....] - ETA: 55s - loss: 2.7598 - regression_loss: 2.0989 - classification_loss: 0.6609
 879/1000 [=========================>....] - ETA: 54s - loss: 2.7614 - regression_loss: 2.1004 - classification_loss: 0.6611
 880/1000 [=========================>....] - ETA: 54s - loss: 2.7614 - regression_loss: 2.1005 - classification_loss: 0.6609
 881/1000 [=========================>....] - ETA: 53s - loss: 2.7618 - regression_loss: 2.1012 - classification_loss: 0.6605
 882/1000 [=========================>....] - ETA: 53s - loss: 2.7586 - regression_loss: 2.0989 - classification_loss: 0.6598
 883/1000 [=========================>....] - ETA: 52s - loss: 2.7595 - regression_loss: 2.0999 - classification_loss: 0.6596
 884/1000 [=========================>....] - ETA: 52s - loss: 2.7621 - regression_loss: 2.1019 - classification_loss: 0.6602
 885/1000 [=========================>....] - ETA: 52s - loss: 2.7591 - regression_loss: 2.0995 - classification_loss: 0.6596
 886/1000 [=========================>....] - ETA: 51s - loss: 2.7607 - regression_loss: 2.1002 - classification_loss: 0.6605
 887/1000 [=========================>....] - ETA: 51s - loss: 2.7618 - regression_loss: 2.1000 - classification_loss: 0.6618
 888/1000 [=========================>....] - ETA: 50s - loss: 2.7616 - regression_loss: 2.1002 - classification_loss: 0.6614
 889/1000 [=========================>....] - ETA: 50s - loss: 2.7617 - regression_loss: 2.1003 - classification_loss: 0.6614
 890/1000 [=========================>....] - ETA: 49s - loss: 2.7626 - regression_loss: 2.1014 - classification_loss: 0.6612
 891/1000 [=========================>....] - ETA: 49s - loss: 2.7595 - regression_loss: 2.0990 - classification_loss: 0.6605
 892/1000 [=========================>....] - ETA: 48s - loss: 2.7629 - regression_loss: 2.1009 - classification_loss: 0.6620
 893/1000 [=========================>....] - ETA: 48s - loss: 2.7643 - regression_loss: 2.1015 - classification_loss: 0.6627
 894/1000 [=========================>....] - ETA: 47s - loss: 2.7643 - regression_loss: 2.1016 - classification_loss: 0.6627
 895/1000 [=========================>....] - ETA: 47s - loss: 2.7679 - regression_loss: 2.1034 - classification_loss: 0.6645
 896/1000 [=========================>....] - ETA: 47s - loss: 2.7648 - regression_loss: 2.1010 - classification_loss: 0.6638
 897/1000 [=========================>....] - ETA: 46s - loss: 2.7642 - regression_loss: 2.1004 - classification_loss: 0.6638
 898/1000 [=========================>....] - ETA: 46s - loss: 2.7651 - regression_loss: 2.1011 - classification_loss: 0.6640
 899/1000 [=========================>....] - ETA: 45s - loss: 2.7659 - regression_loss: 2.1017 - classification_loss: 0.6642
 900/1000 [==========================>...] - ETA: 45s - loss: 2.7681 - regression_loss: 2.1030 - classification_loss: 0.6651
 901/1000 [==========================>...] - ETA: 44s - loss: 2.7684 - regression_loss: 2.1036 - classification_loss: 0.6647
 902/1000 [==========================>...] - ETA: 44s - loss: 2.7654 - regression_loss: 2.1013 - classification_loss: 0.6641
 903/1000 [==========================>...] - ETA: 43s - loss: 2.7662 - regression_loss: 2.1023 - classification_loss: 0.6639
 904/1000 [==========================>...] - ETA: 43s - loss: 2.7631 - regression_loss: 2.1000 - classification_loss: 0.6632
 905/1000 [==========================>...] - ETA: 43s - loss: 2.7635 - regression_loss: 2.1001 - classification_loss: 0.6634
 906/1000 [==========================>...] - ETA: 42s - loss: 2.7652 - regression_loss: 2.1015 - classification_loss: 0.6637
 907/1000 [==========================>...] - ETA: 42s - loss: 2.7676 - regression_loss: 2.1029 - classification_loss: 0.6648
 908/1000 [==========================>...] - ETA: 41s - loss: 2.7682 - regression_loss: 2.1036 - classification_loss: 0.6646
 909/1000 [==========================>...] - ETA: 41s - loss: 2.7686 - regression_loss: 2.1043 - classification_loss: 0.6643
 910/1000 [==========================>...] - ETA: 40s - loss: 2.7689 - regression_loss: 2.1048 - classification_loss: 0.6641
 911/1000 [==========================>...] - ETA: 40s - loss: 2.7658 - regression_loss: 2.1025 - classification_loss: 0.6634
 912/1000 [==========================>...] - ETA: 39s - loss: 2.7628 - regression_loss: 2.1001 - classification_loss: 0.6626
 913/1000 [==========================>...] - ETA: 39s - loss: 2.7647 - regression_loss: 2.1019 - classification_loss: 0.6628
 914/1000 [==========================>...] - ETA: 38s - loss: 2.7651 - regression_loss: 2.1027 - classification_loss: 0.6624
 915/1000 [==========================>...] - ETA: 38s - loss: 2.7666 - regression_loss: 2.1036 - classification_loss: 0.6629
 916/1000 [==========================>...] - ETA: 38s - loss: 2.7697 - regression_loss: 2.1055 - classification_loss: 0.6642
 917/1000 [==========================>...] - ETA: 37s - loss: 2.7707 - regression_loss: 2.1067 - classification_loss: 0.6640
 918/1000 [==========================>...] - ETA: 37s - loss: 2.7709 - regression_loss: 2.1070 - classification_loss: 0.6639
 919/1000 [==========================>...] - ETA: 36s - loss: 2.7728 - regression_loss: 2.1087 - classification_loss: 0.6641
 920/1000 [==========================>...] - ETA: 36s - loss: 2.7734 - regression_loss: 2.1095 - classification_loss: 0.6639
 921/1000 [==========================>...] - ETA: 35s - loss: 2.7748 - regression_loss: 2.1106 - classification_loss: 0.6642
 922/1000 [==========================>...] - ETA: 35s - loss: 2.7768 - regression_loss: 2.1125 - classification_loss: 0.6644
 923/1000 [==========================>...] - ETA: 34s - loss: 2.7739 - regression_loss: 2.1102 - classification_loss: 0.6637
 924/1000 [==========================>...] - ETA: 34s - loss: 2.7765 - regression_loss: 2.1115 - classification_loss: 0.6651
 925/1000 [==========================>...] - ETA: 33s - loss: 2.7773 - regression_loss: 2.1122 - classification_loss: 0.6651
 926/1000 [==========================>...] - ETA: 33s - loss: 2.7743 - regression_loss: 2.1099 - classification_loss: 0.6644
 927/1000 [==========================>...] - ETA: 33s - loss: 2.7713 - regression_loss: 2.1076 - classification_loss: 0.6637
 928/1000 [==========================>...] - ETA: 32s - loss: 2.7683 - regression_loss: 2.1053 - classification_loss: 0.6629
 929/1000 [==========================>...] - ETA: 32s - loss: 2.7701 - regression_loss: 2.1069 - classification_loss: 0.6632
 930/1000 [==========================>...] - ETA: 31s - loss: 2.7671 - regression_loss: 2.1046 - classification_loss: 0.6625
 931/1000 [==========================>...] - ETA: 31s - loss: 2.7641 - regression_loss: 2.1024 - classification_loss: 0.6618
 932/1000 [==========================>...] - ETA: 30s - loss: 2.7642 - regression_loss: 2.1025 - classification_loss: 0.6616
 933/1000 [==========================>...] - ETA: 30s - loss: 2.7612 - regression_loss: 2.1003 - classification_loss: 0.6609
 934/1000 [===========================>..] - ETA: 29s - loss: 2.7622 - regression_loss: 2.1011 - classification_loss: 0.6611
 935/1000 [===========================>..] - ETA: 29s - loss: 2.7653 - regression_loss: 2.1032 - classification_loss: 0.6620
 936/1000 [===========================>..] - ETA: 28s - loss: 2.7662 - regression_loss: 2.1037 - classification_loss: 0.6625
 937/1000 [===========================>..] - ETA: 28s - loss: 2.7679 - regression_loss: 2.1045 - classification_loss: 0.6634
 938/1000 [===========================>..] - ETA: 28s - loss: 2.7684 - regression_loss: 2.1053 - classification_loss: 0.6631
 939/1000 [===========================>..] - ETA: 27s - loss: 2.7678 - regression_loss: 2.1044 - classification_loss: 0.6634
 940/1000 [===========================>..] - ETA: 27s - loss: 2.7723 - regression_loss: 2.1073 - classification_loss: 0.6650
 941/1000 [===========================>..] - ETA: 26s - loss: 2.7722 - regression_loss: 2.1070 - classification_loss: 0.6652
 942/1000 [===========================>..] - ETA: 26s - loss: 2.7734 - regression_loss: 2.1079 - classification_loss: 0.6655
 943/1000 [===========================>..] - ETA: 25s - loss: 2.7705 - regression_loss: 2.1057 - classification_loss: 0.6648
 944/1000 [===========================>..] - ETA: 25s - loss: 2.7723 - regression_loss: 2.1063 - classification_loss: 0.6660
 945/1000 [===========================>..] - ETA: 24s - loss: 2.7741 - regression_loss: 2.1085 - classification_loss: 0.6656
 946/1000 [===========================>..] - ETA: 24s - loss: 2.7752 - regression_loss: 2.1085 - classification_loss: 0.6667
 947/1000 [===========================>..] - ETA: 23s - loss: 2.7770 - regression_loss: 2.1103 - classification_loss: 0.6667
 948/1000 [===========================>..] - ETA: 23s - loss: 2.7741 - regression_loss: 2.1080 - classification_loss: 0.6660
 949/1000 [===========================>..] - ETA: 23s - loss: 2.7773 - regression_loss: 2.1104 - classification_loss: 0.6670
 950/1000 [===========================>..] - ETA: 22s - loss: 2.7777 - regression_loss: 2.1112 - classification_loss: 0.6665
 951/1000 [===========================>..] - ETA: 22s - loss: 2.7797 - regression_loss: 2.1128 - classification_loss: 0.6669
 952/1000 [===========================>..] - ETA: 21s - loss: 2.7802 - regression_loss: 2.1126 - classification_loss: 0.6676
 953/1000 [===========================>..] - ETA: 21s - loss: 2.7824 - regression_loss: 2.1146 - classification_loss: 0.6678
 954/1000 [===========================>..] - ETA: 20s - loss: 2.7820 - regression_loss: 2.1142 - classification_loss: 0.6678
 955/1000 [===========================>..] - ETA: 20s - loss: 2.7851 - regression_loss: 2.1174 - classification_loss: 0.6677
 956/1000 [===========================>..] - ETA: 19s - loss: 2.7822 - regression_loss: 2.1152 - classification_loss: 0.6670
 957/1000 [===========================>..] - ETA: 19s - loss: 2.7793 - regression_loss: 2.1130 - classification_loss: 0.6663
 958/1000 [===========================>..] - ETA: 19s - loss: 2.7814 - regression_loss: 2.1150 - classification_loss: 0.6664
 959/1000 [===========================>..] - ETA: 18s - loss: 2.7785 - regression_loss: 2.1128 - classification_loss: 0.6657
 960/1000 [===========================>..] - ETA: 18s - loss: 2.7791 - regression_loss: 2.1135 - classification_loss: 0.6656
 961/1000 [===========================>..] - ETA: 17s - loss: 2.7785 - regression_loss: 2.1133 - classification_loss: 0.6651
 962/1000 [===========================>..] - ETA: 17s - loss: 2.7793 - regression_loss: 2.1142 - classification_loss: 0.6651
 963/1000 [===========================>..] - ETA: 16s - loss: 2.7764 - regression_loss: 2.1120 - classification_loss: 0.6644
 964/1000 [===========================>..] - ETA: 16s - loss: 2.7775 - regression_loss: 2.1127 - classification_loss: 0.6648
 965/1000 [===========================>..] - ETA: 15s - loss: 2.7746 - regression_loss: 2.1105 - classification_loss: 0.6641
 966/1000 [===========================>..] - ETA: 15s - loss: 2.7756 - regression_loss: 2.1116 - classification_loss: 0.6640
 967/1000 [============================>.] - ETA: 14s - loss: 2.7754 - regression_loss: 2.1118 - classification_loss: 0.6636
 968/1000 [============================>.] - ETA: 14s - loss: 2.7761 - regression_loss: 2.1124 - classification_loss: 0.6637
 969/1000 [============================>.] - ETA: 14s - loss: 2.7765 - regression_loss: 2.1131 - classification_loss: 0.6634
 970/1000 [============================>.] - ETA: 13s - loss: 2.7737 - regression_loss: 2.1110 - classification_loss: 0.6627
 971/1000 [============================>.] - ETA: 13s - loss: 2.7746 - regression_loss: 2.1119 - classification_loss: 0.6626
 972/1000 [============================>.] - ETA: 12s - loss: 2.7717 - regression_loss: 2.1097 - classification_loss: 0.6620
 973/1000 [============================>.] - ETA: 12s - loss: 2.7737 - regression_loss: 2.1108 - classification_loss: 0.6629
 974/1000 [============================>.] - ETA: 11s - loss: 2.7709 - regression_loss: 2.1087 - classification_loss: 0.6622
 975/1000 [============================>.] - ETA: 11s - loss: 2.7681 - regression_loss: 2.1065 - classification_loss: 0.6616
 976/1000 [============================>.] - ETA: 10s - loss: 2.7653 - regression_loss: 2.1043 - classification_loss: 0.6609
 977/1000 [============================>.] - ETA: 10s - loss: 2.7668 - regression_loss: 2.1056 - classification_loss: 0.6612
 978/1000 [============================>.] - ETA: 9s - loss: 2.7694 - regression_loss: 2.1073 - classification_loss: 0.6621 
 979/1000 [============================>.] - ETA: 9s - loss: 2.7712 - regression_loss: 2.1080 - classification_loss: 0.6632
 980/1000 [============================>.] - ETA: 9s - loss: 2.7717 - regression_loss: 2.1087 - classification_loss: 0.6629
 981/1000 [============================>.] - ETA: 8s - loss: 2.7728 - regression_loss: 2.1098 - classification_loss: 0.6629
 982/1000 [============================>.] - ETA: 8s - loss: 2.7757 - regression_loss: 2.1113 - classification_loss: 0.6644
 983/1000 [============================>.] - ETA: 7s - loss: 2.7728 - regression_loss: 2.1091 - classification_loss: 0.6637
 984/1000 [============================>.] - ETA: 7s - loss: 2.7733 - regression_loss: 2.1095 - classification_loss: 0.6638
 985/1000 [============================>.] - ETA: 6s - loss: 2.7752 - regression_loss: 2.1106 - classification_loss: 0.6646
 986/1000 [============================>.] - ETA: 6s - loss: 2.7766 - regression_loss: 2.1105 - classification_loss: 0.6661
 987/1000 [============================>.] - ETA: 5s - loss: 2.7738 - regression_loss: 2.1084 - classification_loss: 0.6654
 988/1000 [============================>.] - ETA: 5s - loss: 2.7751 - regression_loss: 2.1098 - classification_loss: 0.6653
 989/1000 [============================>.] - ETA: 4s - loss: 2.7723 - regression_loss: 2.1077 - classification_loss: 0.6647
 990/1000 [============================>.] - ETA: 4s - loss: 2.7731 - regression_loss: 2.1084 - classification_loss: 0.6647
 991/1000 [============================>.] - ETA: 4s - loss: 2.7739 - regression_loss: 2.1090 - classification_loss: 0.6649
 992/1000 [============================>.] - ETA: 3s - loss: 2.7767 - regression_loss: 2.1105 - classification_loss: 0.6662
 993/1000 [============================>.] - ETA: 3s - loss: 2.7782 - regression_loss: 2.1107 - classification_loss: 0.6675
 994/1000 [============================>.] - ETA: 2s - loss: 2.7817 - regression_loss: 2.1125 - classification_loss: 0.6691
 995/1000 [============================>.] - ETA: 2s - loss: 2.7789 - regression_loss: 2.1104 - classification_loss: 0.6685
 996/1000 [============================>.] - ETA: 1s - loss: 2.7818 - regression_loss: 2.1121 - classification_loss: 0.6697
 997/1000 [============================>.] - ETA: 1s - loss: 2.7813 - regression_loss: 2.1119 - classification_loss: 0.6695
 998/1000 [============================>.] - ETA: 0s - loss: 2.7786 - regression_loss: 2.1097 - classification_loss: 0.6688
 999/1000 [============================>.] - ETA: 0s - loss: 2.7798 - regression_loss: 2.1104 - classification_loss: 0.6694
1000/1000 [==============================] - 453s 453ms/step - loss: 2.7816 - regression_loss: 2.1112 - classification_loss: 0.6703

Epoch 00016: saving model to ./snapshots/resnet50_csv_16.h5
0/2001/2002/2003/2004/2005/2006/2007/2008/2009/20010/20011/20012/20013/20014/20015/20016/20017/20018/20019/20020/20021/20022/20023/20024/20025/20026/20027/20028/20029/20030/20031/20032/20033/20034/20035/20036/20037/20038/20039/20040/20041/20042/20043/20044/20045/20046/20047/20048/20049/20050/20051/20052/20053/20054/20055/20056/20057/20058/20059/20060/20061/20062/20063/20064/20065/20066/20067/20068/20069/20070/20071/20072/20073/20074/20075/20076/20077/20078/20079/20080/20081/20082/20083/20084/20085/20086/20087/20088/20089/20090/20091/20092/20093/20094/20095/20096/20097/20098/20099/200100/200101/200102/200103/200104/200105/200106/200107/200108/200109/200110/200111/200112/200113/200114/200115/200116/200117/200118/200119/200120/200121/200122/200123/200124/200125/200126/200127/200128/200129/200130/200131/200132/200133/200134/200135/200136/200137/200138/200139/200140/200141/200142/200143/200144/200145/200146/200147/200148/200149/200150/200151/200152/200153/200154/200155/200156/200157/200158/200159/200160/200161/200162/200163/200164/200165/200166/200167/200168/200169/200170/200171/200172/200173/200174/200175/200176/200177/200178/200179/200180/200181/200182/200183/200184/200185/200186/200187/200188/200189/200190/200191/200192/200193/200194/200195/200196/200197/200198/200199/2000/2001/2002/2003/2004/2005/2006/2007/2008/2009/20010/20011/20012/20013/20014/20015/20016/20017/20018/20019/20020/20021/20022/20023/20024/20025/20026/20027/20028/20029/20030/20031/20032/20033/20034/20035/20036/20037/20038/20039/20040/20041/20042/20043/20044/20045/20046/20047/20048/20049/20050/20051/20052/20053/20054/20055/20056/20057/20058/20059/20060/20061/20062/20063/20064/20065/20066/20067/20068/20069/20070/20071/20072/20073/20074/20075/20076/20077/20078/20079/20080/20081/20082/20083/20084/20085/20086/20087/20088/20089/20090/20091/20092/20093/20094/20095/20096/20097/20098/20099/200100/200101/200102/200103/200104/200105/200106/200107/200108/200109/200110/200111/200112/200113/200114/200115/200116/200117/200118/200119/200120/200121/200122/200123/200124/200125/200126/200127/200128/200129/200130/200131/200132/200133/200134/200135/200136/200137/200138/200139/200140/200141/200142/200143/200144/200145/200146/200147/200148/200149/200150/200151/200152/200153/200154/200155/200156/200157/200158/200159/200160/200161/200162/200163/200164/200165/200166/200167/200168/200169/200170/200171/200172/200173/200174/200175/200176/200177/200178/200179/200180/200181/200182/200183/200184/200185/200186/200187/200188/200189/200190/200191/200192/200193/200194/200195/200196/200197/200198/200199/200

M 0.1206
N 0.0088
mAP: 0.0647
Epoch 17/30

   1/1000 [..............................] - ETA: 7:22 - loss: 2.7905 - regression_loss: 2.3394 - classification_loss: 0.4511
   2/1000 [..............................] - ETA: 7:26 - loss: 4.0192 - regression_loss: 3.1704 - classification_loss: 0.8488
   3/1000 [..............................] - ETA: 7:26 - loss: 3.5846 - regression_loss: 2.9519 - classification_loss: 0.6327
   4/1000 [..............................] - ETA: 7:31 - loss: 3.4746 - regression_loss: 2.8565 - classification_loss: 0.6181
   5/1000 [..............................] - ETA: 7:29 - loss: 3.8070 - regression_loss: 3.1029 - classification_loss: 0.7041
   6/1000 [..............................] - ETA: 7:30 - loss: 3.1727 - regression_loss: 2.5858 - classification_loss: 0.5870
   7/1000 [..............................] - ETA: 7:23 - loss: 3.1703 - regression_loss: 2.5744 - classification_loss: 0.5959
   8/1000 [..............................] - ETA: 7:22 - loss: 3.3259 - regression_loss: 2.6939 - classification_loss: 0.6320
   9/1000 [..............................] - ETA: 7:23 - loss: 3.4282 - regression_loss: 2.7768 - classification_loss: 0.6514
  10/1000 [..............................] - ETA: 7:23 - loss: 3.4176 - regression_loss: 2.7192 - classification_loss: 0.6984
  11/1000 [..............................] - ETA: 7:24 - loss: 3.4575 - regression_loss: 2.7577 - classification_loss: 0.6998
  12/1000 [..............................] - ETA: 7:23 - loss: 3.5505 - regression_loss: 2.8190 - classification_loss: 0.7315
  13/1000 [..............................] - ETA: 7:23 - loss: 3.2774 - regression_loss: 2.6022 - classification_loss: 0.6753
  14/1000 [..............................] - ETA: 7:22 - loss: 3.2040 - regression_loss: 2.5567 - classification_loss: 0.6473
  15/1000 [..............................] - ETA: 7:22 - loss: 3.1898 - regression_loss: 2.5672 - classification_loss: 0.6227
  16/1000 [..............................] - ETA: 7:22 - loss: 3.2694 - regression_loss: 2.6417 - classification_loss: 0.6277
  17/1000 [..............................] - ETA: 7:22 - loss: 3.3516 - regression_loss: 2.7294 - classification_loss: 0.6222
  18/1000 [..............................] - ETA: 7:22 - loss: 3.2835 - regression_loss: 2.6720 - classification_loss: 0.6115
  19/1000 [..............................] - ETA: 7:21 - loss: 3.2838 - regression_loss: 2.6791 - classification_loss: 0.6047
  20/1000 [..............................] - ETA: 7:20 - loss: 3.3492 - regression_loss: 2.7202 - classification_loss: 0.6291
  21/1000 [..............................] - ETA: 7:20 - loss: 3.3431 - regression_loss: 2.7265 - classification_loss: 0.6166
  22/1000 [..............................] - ETA: 7:20 - loss: 3.3449 - regression_loss: 2.7197 - classification_loss: 0.6251
  23/1000 [..............................] - ETA: 7:20 - loss: 3.4060 - regression_loss: 2.7267 - classification_loss: 0.6792
  24/1000 [..............................] - ETA: 7:20 - loss: 3.4543 - regression_loss: 2.7650 - classification_loss: 0.6893
  25/1000 [..............................] - ETA: 7:20 - loss: 3.3161 - regression_loss: 2.6544 - classification_loss: 0.6617
  26/1000 [..............................] - ETA: 7:19 - loss: 3.3542 - regression_loss: 2.6731 - classification_loss: 0.6811
  27/1000 [..............................] - ETA: 7:18 - loss: 3.4047 - regression_loss: 2.7072 - classification_loss: 0.6975
  28/1000 [..............................] - ETA: 7:18 - loss: 3.4020 - regression_loss: 2.7119 - classification_loss: 0.6901
  29/1000 [..............................] - ETA: 7:17 - loss: 3.4599 - regression_loss: 2.7414 - classification_loss: 0.7185
  30/1000 [..............................] - ETA: 7:17 - loss: 3.4919 - regression_loss: 2.7718 - classification_loss: 0.7201
  31/1000 [..............................] - ETA: 7:16 - loss: 3.5273 - regression_loss: 2.8024 - classification_loss: 0.7249
  32/1000 [..............................] - ETA: 7:16 - loss: 3.5337 - regression_loss: 2.8121 - classification_loss: 0.7216
  33/1000 [..............................] - ETA: 7:16 - loss: 3.5500 - regression_loss: 2.8353 - classification_loss: 0.7147
  34/1000 [>.............................] - ETA: 7:15 - loss: 3.5392 - regression_loss: 2.8191 - classification_loss: 0.7201
  35/1000 [>.............................] - ETA: 7:15 - loss: 3.5714 - regression_loss: 2.8325 - classification_loss: 0.7389
  36/1000 [>.............................] - ETA: 7:14 - loss: 3.4723 - regression_loss: 2.7539 - classification_loss: 0.7184
  37/1000 [>.............................] - ETA: 7:14 - loss: 3.4660 - regression_loss: 2.7508 - classification_loss: 0.7152
  38/1000 [>.............................] - ETA: 7:13 - loss: 3.4807 - regression_loss: 2.7658 - classification_loss: 0.7150
  39/1000 [>.............................] - ETA: 7:13 - loss: 3.4421 - regression_loss: 2.7376 - classification_loss: 0.7045
  40/1000 [>.............................] - ETA: 7:12 - loss: 3.4662 - regression_loss: 2.7428 - classification_loss: 0.7235
  41/1000 [>.............................] - ETA: 7:12 - loss: 3.4457 - regression_loss: 2.7319 - classification_loss: 0.7138
  42/1000 [>.............................] - ETA: 7:12 - loss: 3.4241 - regression_loss: 2.7089 - classification_loss: 0.7151
  43/1000 [>.............................] - ETA: 7:11 - loss: 3.3455 - regression_loss: 2.6459 - classification_loss: 0.6996
  44/1000 [>.............................] - ETA: 7:11 - loss: 3.3128 - regression_loss: 2.6233 - classification_loss: 0.6895
  45/1000 [>.............................] - ETA: 7:10 - loss: 3.2913 - regression_loss: 2.6089 - classification_loss: 0.6824
  46/1000 [>.............................] - ETA: 7:10 - loss: 3.2201 - regression_loss: 2.5522 - classification_loss: 0.6679
  47/1000 [>.............................] - ETA: 7:09 - loss: 3.2112 - regression_loss: 2.5480 - classification_loss: 0.6632
  48/1000 [>.............................] - ETA: 7:09 - loss: 3.2036 - regression_loss: 2.5318 - classification_loss: 0.6718
  49/1000 [>.............................] - ETA: 7:09 - loss: 3.2136 - regression_loss: 2.5493 - classification_loss: 0.6643
  50/1000 [>.............................] - ETA: 7:08 - loss: 3.1844 - regression_loss: 2.5283 - classification_loss: 0.6561
  51/1000 [>.............................] - ETA: 7:08 - loss: 3.1881 - regression_loss: 2.5372 - classification_loss: 0.6509
  52/1000 [>.............................] - ETA: 7:07 - loss: 3.1268 - regression_loss: 2.4884 - classification_loss: 0.6384
  53/1000 [>.............................] - ETA: 7:07 - loss: 3.1490 - regression_loss: 2.5047 - classification_loss: 0.6442
  54/1000 [>.............................] - ETA: 7:07 - loss: 3.1409 - regression_loss: 2.5022 - classification_loss: 0.6387
  55/1000 [>.............................] - ETA: 7:06 - loss: 3.1479 - regression_loss: 2.5140 - classification_loss: 0.6339
  56/1000 [>.............................] - ETA: 7:06 - loss: 3.1631 - regression_loss: 2.5316 - classification_loss: 0.6314
  57/1000 [>.............................] - ETA: 7:05 - loss: 3.1761 - regression_loss: 2.5450 - classification_loss: 0.6310
  58/1000 [>.............................] - ETA: 7:04 - loss: 3.2136 - regression_loss: 2.5818 - classification_loss: 0.6319
  59/1000 [>.............................] - ETA: 7:04 - loss: 3.2443 - regression_loss: 2.5965 - classification_loss: 0.6478
  60/1000 [>.............................] - ETA: 7:04 - loss: 3.2621 - regression_loss: 2.6140 - classification_loss: 0.6481
  61/1000 [>.............................] - ETA: 7:03 - loss: 3.2086 - regression_loss: 2.5711 - classification_loss: 0.6375
  62/1000 [>.............................] - ETA: 7:03 - loss: 3.2572 - regression_loss: 2.6120 - classification_loss: 0.6453
  63/1000 [>.............................] - ETA: 7:02 - loss: 3.2722 - regression_loss: 2.6200 - classification_loss: 0.6522
  64/1000 [>.............................] - ETA: 7:02 - loss: 3.2915 - regression_loss: 2.6411 - classification_loss: 0.6504
  65/1000 [>.............................] - ETA: 7:01 - loss: 3.2509 - regression_loss: 2.6005 - classification_loss: 0.6504
  66/1000 [>.............................] - ETA: 7:01 - loss: 3.2513 - regression_loss: 2.6047 - classification_loss: 0.6465
  67/1000 [=>............................] - ETA: 7:00 - loss: 3.2430 - regression_loss: 2.6020 - classification_loss: 0.6410
  68/1000 [=>............................] - ETA: 7:00 - loss: 3.2314 - regression_loss: 2.5961 - classification_loss: 0.6353
  69/1000 [=>............................] - ETA: 6:59 - loss: 3.2398 - regression_loss: 2.6075 - classification_loss: 0.6323
  70/1000 [=>............................] - ETA: 6:59 - loss: 3.2471 - regression_loss: 2.6121 - classification_loss: 0.6350
  71/1000 [=>............................] - ETA: 6:59 - loss: 3.2373 - regression_loss: 2.6029 - classification_loss: 0.6344
  72/1000 [=>............................] - ETA: 6:58 - loss: 3.2305 - regression_loss: 2.6012 - classification_loss: 0.6293
  73/1000 [=>............................] - ETA: 6:58 - loss: 3.2278 - regression_loss: 2.5944 - classification_loss: 0.6334
  74/1000 [=>............................] - ETA: 6:57 - loss: 3.2236 - regression_loss: 2.5873 - classification_loss: 0.6363
  75/1000 [=>............................] - ETA: 6:57 - loss: 3.2115 - regression_loss: 2.5803 - classification_loss: 0.6312
  76/1000 [=>............................] - ETA: 6:57 - loss: 3.2188 - regression_loss: 2.5911 - classification_loss: 0.6277
  77/1000 [=>............................] - ETA: 6:56 - loss: 3.2294 - regression_loss: 2.5884 - classification_loss: 0.6410
  78/1000 [=>............................] - ETA: 6:56 - loss: 3.2601 - regression_loss: 2.6018 - classification_loss: 0.6584
  79/1000 [=>............................] - ETA: 6:55 - loss: 3.2927 - regression_loss: 2.6282 - classification_loss: 0.6645
  80/1000 [=>............................] - ETA: 6:55 - loss: 3.2952 - regression_loss: 2.6326 - classification_loss: 0.6626
  81/1000 [=>............................] - ETA: 6:55 - loss: 3.2971 - regression_loss: 2.6370 - classification_loss: 0.6600
  82/1000 [=>............................] - ETA: 6:54 - loss: 3.2890 - regression_loss: 2.6314 - classification_loss: 0.6575
  83/1000 [=>............................] - ETA: 6:54 - loss: 3.3093 - regression_loss: 2.6348 - classification_loss: 0.6746
  84/1000 [=>............................] - ETA: 6:53 - loss: 3.3206 - regression_loss: 2.6375 - classification_loss: 0.6831
  85/1000 [=>............................] - ETA: 6:53 - loss: 3.3114 - regression_loss: 2.6065 - classification_loss: 0.7050
  86/1000 [=>............................] - ETA: 6:52 - loss: 3.3133 - regression_loss: 2.6079 - classification_loss: 0.7054
  87/1000 [=>............................] - ETA: 6:52 - loss: 3.3122 - regression_loss: 2.6070 - classification_loss: 0.7051
  88/1000 [=>............................] - ETA: 6:51 - loss: 3.3287 - regression_loss: 2.6215 - classification_loss: 0.7072
  89/1000 [=>............................] - ETA: 6:51 - loss: 3.3263 - regression_loss: 2.6228 - classification_loss: 0.7035
  90/1000 [=>............................] - ETA: 6:50 - loss: 3.3340 - regression_loss: 2.6322 - classification_loss: 0.7018
  91/1000 [=>............................] - ETA: 6:50 - loss: 3.3190 - regression_loss: 2.6221 - classification_loss: 0.6970
  92/1000 [=>............................] - ETA: 6:50 - loss: 3.2908 - regression_loss: 2.5936 - classification_loss: 0.6972
  93/1000 [=>............................] - ETA: 6:49 - loss: 3.3137 - regression_loss: 2.6158 - classification_loss: 0.6979
  94/1000 [=>............................] - ETA: 6:48 - loss: 3.3138 - regression_loss: 2.6197 - classification_loss: 0.6941
  95/1000 [=>............................] - ETA: 6:48 - loss: 3.3075 - regression_loss: 2.6159 - classification_loss: 0.6916
  96/1000 [=>............................] - ETA: 6:47 - loss: 3.3069 - regression_loss: 2.6164 - classification_loss: 0.6905
  97/1000 [=>............................] - ETA: 6:47 - loss: 3.2736 - regression_loss: 2.5894 - classification_loss: 0.6842
  98/1000 [=>............................] - ETA: 6:47 - loss: 3.2631 - regression_loss: 2.5816 - classification_loss: 0.6815
  99/1000 [=>............................] - ETA: 6:46 - loss: 3.2308 - regression_loss: 2.5555 - classification_loss: 0.6753
 100/1000 [==>...........................] - ETA: 6:46 - loss: 3.2489 - regression_loss: 2.5682 - classification_loss: 0.6807
 101/1000 [==>...........................] - ETA: 6:45 - loss: 3.2168 - regression_loss: 2.5428 - classification_loss: 0.6741
 102/1000 [==>...........................] - ETA: 6:45 - loss: 3.2326 - regression_loss: 2.5555 - classification_loss: 0.6770
 103/1000 [==>...........................] - ETA: 6:44 - loss: 3.2512 - regression_loss: 2.5681 - classification_loss: 0.6830
 104/1000 [==>...........................] - ETA: 6:44 - loss: 3.2203 - regression_loss: 2.5434 - classification_loss: 0.6768
 105/1000 [==>...........................] - ETA: 6:44 - loss: 3.2229 - regression_loss: 2.5482 - classification_loss: 0.6747
 106/1000 [==>...........................] - ETA: 6:43 - loss: 3.2398 - regression_loss: 2.5616 - classification_loss: 0.6782
 107/1000 [==>...........................] - ETA: 6:43 - loss: 3.2580 - regression_loss: 2.5723 - classification_loss: 0.6857
 108/1000 [==>...........................] - ETA: 6:42 - loss: 3.2572 - regression_loss: 2.5702 - classification_loss: 0.6870
 109/1000 [==>...........................] - ETA: 6:42 - loss: 3.2661 - regression_loss: 2.5741 - classification_loss: 0.6920
 110/1000 [==>...........................] - ETA: 6:41 - loss: 3.2663 - regression_loss: 2.5775 - classification_loss: 0.6889
 111/1000 [==>...........................] - ETA: 6:41 - loss: 3.2374 - regression_loss: 2.5543 - classification_loss: 0.6832
 112/1000 [==>...........................] - ETA: 6:40 - loss: 3.2492 - regression_loss: 2.5641 - classification_loss: 0.6851
 113/1000 [==>...........................] - ETA: 6:40 - loss: 3.2507 - regression_loss: 2.5617 - classification_loss: 0.6890
 114/1000 [==>...........................] - ETA: 6:40 - loss: 3.2443 - regression_loss: 2.5553 - classification_loss: 0.6890
 115/1000 [==>...........................] - ETA: 6:39 - loss: 3.2387 - regression_loss: 2.5510 - classification_loss: 0.6877
 116/1000 [==>...........................] - ETA: 6:39 - loss: 3.2396 - regression_loss: 2.5506 - classification_loss: 0.6890
 117/1000 [==>...........................] - ETA: 6:38 - loss: 3.2493 - regression_loss: 2.5550 - classification_loss: 0.6943
 118/1000 [==>...........................] - ETA: 6:38 - loss: 3.2481 - regression_loss: 2.5544 - classification_loss: 0.6937
 119/1000 [==>...........................] - ETA: 6:37 - loss: 3.2494 - regression_loss: 2.5556 - classification_loss: 0.6938
 120/1000 [==>...........................] - ETA: 6:37 - loss: 3.2231 - regression_loss: 2.5343 - classification_loss: 0.6888
 121/1000 [==>...........................] - ETA: 6:37 - loss: 3.2205 - regression_loss: 2.5299 - classification_loss: 0.6906
 122/1000 [==>...........................] - ETA: 6:36 - loss: 3.1942 - regression_loss: 2.5092 - classification_loss: 0.6850
 123/1000 [==>...........................] - ETA: 6:36 - loss: 3.2099 - regression_loss: 2.5205 - classification_loss: 0.6894
 124/1000 [==>...........................] - ETA: 6:35 - loss: 3.1841 - regression_loss: 2.5002 - classification_loss: 0.6838
 125/1000 [==>...........................] - ETA: 6:35 - loss: 3.1876 - regression_loss: 2.5026 - classification_loss: 0.6849
 126/1000 [==>...........................] - ETA: 6:34 - loss: 3.1864 - regression_loss: 2.5031 - classification_loss: 0.6834
 127/1000 [==>...........................] - ETA: 6:34 - loss: 3.1983 - regression_loss: 2.5141 - classification_loss: 0.6842
 128/1000 [==>...........................] - ETA: 6:33 - loss: 3.1993 - regression_loss: 2.5145 - classification_loss: 0.6848
 129/1000 [==>...........................] - ETA: 6:33 - loss: 3.2113 - regression_loss: 2.5254 - classification_loss: 0.6859
 130/1000 [==>...........................] - ETA: 6:33 - loss: 3.1876 - regression_loss: 2.5060 - classification_loss: 0.6816
 131/1000 [==>...........................] - ETA: 6:32 - loss: 3.1632 - regression_loss: 2.4868 - classification_loss: 0.6764
 132/1000 [==>...........................] - ETA: 6:32 - loss: 3.1393 - regression_loss: 2.4680 - classification_loss: 0.6713
 133/1000 [==>...........................] - ETA: 6:31 - loss: 3.1402 - regression_loss: 2.4670 - classification_loss: 0.6732
 134/1000 [===>..........................] - ETA: 6:31 - loss: 3.1314 - regression_loss: 2.4589 - classification_loss: 0.6725
 135/1000 [===>..........................] - ETA: 6:30 - loss: 3.1485 - regression_loss: 2.4722 - classification_loss: 0.6762
 136/1000 [===>..........................] - ETA: 6:30 - loss: 3.1601 - regression_loss: 2.4791 - classification_loss: 0.6810
 137/1000 [===>..........................] - ETA: 6:29 - loss: 3.1684 - regression_loss: 2.4837 - classification_loss: 0.6847
 138/1000 [===>..........................] - ETA: 6:29 - loss: 3.1714 - regression_loss: 2.4822 - classification_loss: 0.6892
 139/1000 [===>..........................] - ETA: 6:28 - loss: 3.1830 - regression_loss: 2.4871 - classification_loss: 0.6959
 140/1000 [===>..........................] - ETA: 6:28 - loss: 3.2014 - regression_loss: 2.5037 - classification_loss: 0.6977
 141/1000 [===>..........................] - ETA: 6:28 - loss: 3.1948 - regression_loss: 2.4993 - classification_loss: 0.6955
 142/1000 [===>..........................] - ETA: 6:27 - loss: 3.2200 - regression_loss: 2.5178 - classification_loss: 0.7022
 143/1000 [===>..........................] - ETA: 6:27 - loss: 3.2262 - regression_loss: 2.5218 - classification_loss: 0.7044
 144/1000 [===>..........................] - ETA: 6:26 - loss: 3.2204 - regression_loss: 2.5168 - classification_loss: 0.7036
 145/1000 [===>..........................] - ETA: 6:26 - loss: 3.2267 - regression_loss: 2.5171 - classification_loss: 0.7097
 146/1000 [===>..........................] - ETA: 6:25 - loss: 3.2261 - regression_loss: 2.5189 - classification_loss: 0.7072
 147/1000 [===>..........................] - ETA: 6:25 - loss: 3.2280 - regression_loss: 2.5201 - classification_loss: 0.7079
 148/1000 [===>..........................] - ETA: 6:24 - loss: 3.2391 - regression_loss: 2.5255 - classification_loss: 0.7136
 149/1000 [===>..........................] - ETA: 6:24 - loss: 3.2466 - regression_loss: 2.5308 - classification_loss: 0.7159
 150/1000 [===>..........................] - ETA: 6:24 - loss: 3.2250 - regression_loss: 2.5139 - classification_loss: 0.7111
 151/1000 [===>..........................] - ETA: 6:23 - loss: 3.2255 - regression_loss: 2.5156 - classification_loss: 0.7100
 152/1000 [===>..........................] - ETA: 6:23 - loss: 3.2318 - regression_loss: 2.5228 - classification_loss: 0.7090
 153/1000 [===>..........................] - ETA: 6:22 - loss: 3.2106 - regression_loss: 2.5063 - classification_loss: 0.7043
 154/1000 [===>..........................] - ETA: 6:22 - loss: 3.2097 - regression_loss: 2.5079 - classification_loss: 0.7017
 155/1000 [===>..........................] - ETA: 6:21 - loss: 3.2139 - regression_loss: 2.5091 - classification_loss: 0.7048
 156/1000 [===>..........................] - ETA: 6:21 - loss: 3.2226 - regression_loss: 2.5151 - classification_loss: 0.7075
 157/1000 [===>..........................] - ETA: 6:20 - loss: 3.2218 - regression_loss: 2.5172 - classification_loss: 0.7047
 158/1000 [===>..........................] - ETA: 6:20 - loss: 3.2210 - regression_loss: 2.5182 - classification_loss: 0.7028
 159/1000 [===>..........................] - ETA: 6:20 - loss: 3.2007 - regression_loss: 2.5023 - classification_loss: 0.6984
 160/1000 [===>..........................] - ETA: 6:19 - loss: 3.2080 - regression_loss: 2.5089 - classification_loss: 0.6992
 161/1000 [===>..........................] - ETA: 6:19 - loss: 3.2213 - regression_loss: 2.5155 - classification_loss: 0.7058
 162/1000 [===>..........................] - ETA: 6:18 - loss: 3.2512 - regression_loss: 2.5425 - classification_loss: 0.7087
 163/1000 [===>..........................] - ETA: 6:18 - loss: 3.2583 - regression_loss: 2.5490 - classification_loss: 0.7092
 164/1000 [===>..........................] - ETA: 6:17 - loss: 3.2721 - regression_loss: 2.5567 - classification_loss: 0.7154
 165/1000 [===>..........................] - ETA: 6:17 - loss: 3.2785 - regression_loss: 2.5568 - classification_loss: 0.7217
 166/1000 [===>..........................] - ETA: 6:16 - loss: 3.2784 - regression_loss: 2.5588 - classification_loss: 0.7196
 167/1000 [====>.........................] - ETA: 6:16 - loss: 3.2774 - regression_loss: 2.5579 - classification_loss: 0.7195
 168/1000 [====>.........................] - ETA: 6:15 - loss: 3.2771 - regression_loss: 2.5594 - classification_loss: 0.7177
 169/1000 [====>.........................] - ETA: 6:15 - loss: 3.2741 - regression_loss: 2.5578 - classification_loss: 0.7162
 170/1000 [====>.........................] - ETA: 6:15 - loss: 3.2776 - regression_loss: 2.5608 - classification_loss: 0.7167
 171/1000 [====>.........................] - ETA: 6:14 - loss: 3.2584 - regression_loss: 2.5459 - classification_loss: 0.7125
 172/1000 [====>.........................] - ETA: 6:14 - loss: 3.2395 - regression_loss: 2.5311 - classification_loss: 0.7084
 173/1000 [====>.........................] - ETA: 6:13 - loss: 3.2425 - regression_loss: 2.5343 - classification_loss: 0.7082
 174/1000 [====>.........................] - ETA: 6:13 - loss: 3.2391 - regression_loss: 2.5330 - classification_loss: 0.7061
 175/1000 [====>.........................] - ETA: 6:12 - loss: 3.2207 - regression_loss: 2.5186 - classification_loss: 0.7022
 176/1000 [====>.........................] - ETA: 6:12 - loss: 3.2024 - regression_loss: 2.5043 - classification_loss: 0.6982
 177/1000 [====>.........................] - ETA: 6:11 - loss: 3.1844 - regression_loss: 2.4901 - classification_loss: 0.6943
 178/1000 [====>.........................] - ETA: 6:11 - loss: 3.1667 - regression_loss: 2.4761 - classification_loss: 0.6905
 179/1000 [====>.........................] - ETA: 6:11 - loss: 3.1632 - regression_loss: 2.4744 - classification_loss: 0.6888
 180/1000 [====>.........................] - ETA: 6:10 - loss: 3.1624 - regression_loss: 2.4730 - classification_loss: 0.6895
 181/1000 [====>.........................] - ETA: 6:10 - loss: 3.1576 - regression_loss: 2.4696 - classification_loss: 0.6880
 182/1000 [====>.........................] - ETA: 6:09 - loss: 3.1677 - regression_loss: 2.4784 - classification_loss: 0.6893
 183/1000 [====>.........................] - ETA: 6:09 - loss: 3.1663 - regression_loss: 2.4765 - classification_loss: 0.6898
 184/1000 [====>.........................] - ETA: 6:08 - loss: 3.1637 - regression_loss: 2.4760 - classification_loss: 0.6877
 185/1000 [====>.........................] - ETA: 6:08 - loss: 3.1709 - regression_loss: 2.4781 - classification_loss: 0.6928
 186/1000 [====>.........................] - ETA: 6:07 - loss: 3.1539 - regression_loss: 2.4648 - classification_loss: 0.6890
 187/1000 [====>.........................] - ETA: 6:07 - loss: 3.1370 - regression_loss: 2.4516 - classification_loss: 0.6854
 188/1000 [====>.........................] - ETA: 6:06 - loss: 3.1311 - regression_loss: 2.4473 - classification_loss: 0.6838
 189/1000 [====>.........................] - ETA: 6:06 - loss: 3.1145 - regression_loss: 2.4344 - classification_loss: 0.6802
 190/1000 [====>.........................] - ETA: 6:06 - loss: 3.1137 - regression_loss: 2.4329 - classification_loss: 0.6807
 191/1000 [====>.........................] - ETA: 6:05 - loss: 3.1294 - regression_loss: 2.4401 - classification_loss: 0.6893
 192/1000 [====>.........................] - ETA: 6:05 - loss: 3.1298 - regression_loss: 2.4413 - classification_loss: 0.6885
 193/1000 [====>.........................] - ETA: 6:04 - loss: 3.1375 - regression_loss: 2.4434 - classification_loss: 0.6941
 194/1000 [====>.........................] - ETA: 6:04 - loss: 3.1445 - regression_loss: 2.4448 - classification_loss: 0.6997
 195/1000 [====>.........................] - ETA: 6:03 - loss: 3.1504 - regression_loss: 2.4457 - classification_loss: 0.7047
 196/1000 [====>.........................] - ETA: 6:03 - loss: 3.1343 - regression_loss: 2.4332 - classification_loss: 0.7011
 197/1000 [====>.........................] - ETA: 6:02 - loss: 3.1184 - regression_loss: 2.4209 - classification_loss: 0.6975
 198/1000 [====>.........................] - ETA: 6:02 - loss: 3.1026 - regression_loss: 2.4086 - classification_loss: 0.6940
 199/1000 [====>.........................] - ETA: 6:02 - loss: 3.1074 - regression_loss: 2.4094 - classification_loss: 0.6980
 200/1000 [=====>........................] - ETA: 6:01 - loss: 3.0919 - regression_loss: 2.3973 - classification_loss: 0.6945
 201/1000 [=====>........................] - ETA: 6:01 - loss: 3.0921 - regression_loss: 2.3972 - classification_loss: 0.6949
 202/1000 [=====>........................] - ETA: 6:00 - loss: 3.0938 - regression_loss: 2.3985 - classification_loss: 0.6952
 203/1000 [=====>........................] - ETA: 6:00 - loss: 3.1037 - regression_loss: 2.4033 - classification_loss: 0.7005
 204/1000 [=====>........................] - ETA: 5:59 - loss: 3.0991 - regression_loss: 2.4007 - classification_loss: 0.6984
 205/1000 [=====>........................] - ETA: 5:59 - loss: 3.1007 - regression_loss: 2.3967 - classification_loss: 0.7040
 206/1000 [=====>........................] - ETA: 5:58 - loss: 3.1075 - regression_loss: 2.4030 - classification_loss: 0.7045
 207/1000 [=====>........................] - ETA: 5:58 - loss: 3.1186 - regression_loss: 2.4076 - classification_loss: 0.7110
 208/1000 [=====>........................] - ETA: 5:57 - loss: 3.1190 - regression_loss: 2.4080 - classification_loss: 0.7110
 209/1000 [=====>........................] - ETA: 5:57 - loss: 3.1043 - regression_loss: 2.3965 - classification_loss: 0.7078
 210/1000 [=====>........................] - ETA: 5:57 - loss: 3.0899 - regression_loss: 2.3851 - classification_loss: 0.7048
 211/1000 [=====>........................] - ETA: 5:56 - loss: 3.0867 - regression_loss: 2.3839 - classification_loss: 0.7029
 212/1000 [=====>........................] - ETA: 5:56 - loss: 3.0827 - regression_loss: 2.3809 - classification_loss: 0.7018
 213/1000 [=====>........................] - ETA: 5:55 - loss: 3.0782 - regression_loss: 2.3760 - classification_loss: 0.7022
 214/1000 [=====>........................] - ETA: 5:55 - loss: 3.0765 - regression_loss: 2.3745 - classification_loss: 0.7020
 215/1000 [=====>........................] - ETA: 5:54 - loss: 3.0801 - regression_loss: 2.3778 - classification_loss: 0.7023
 216/1000 [=====>........................] - ETA: 5:54 - loss: 3.0758 - regression_loss: 2.3754 - classification_loss: 0.7004
 217/1000 [=====>........................] - ETA: 5:54 - loss: 3.0616 - regression_loss: 2.3645 - classification_loss: 0.6972
 218/1000 [=====>........................] - ETA: 5:53 - loss: 3.0570 - regression_loss: 2.3611 - classification_loss: 0.6959
 219/1000 [=====>........................] - ETA: 5:53 - loss: 3.0625 - regression_loss: 2.3664 - classification_loss: 0.6961
 220/1000 [=====>........................] - ETA: 5:52 - loss: 3.0617 - regression_loss: 2.3649 - classification_loss: 0.6968
 221/1000 [=====>........................] - ETA: 5:52 - loss: 3.0559 - regression_loss: 2.3607 - classification_loss: 0.6952
 222/1000 [=====>........................] - ETA: 5:51 - loss: 3.0570 - regression_loss: 2.3606 - classification_loss: 0.6964
 223/1000 [=====>........................] - ETA: 5:51 - loss: 3.0580 - regression_loss: 2.3615 - classification_loss: 0.6964
 224/1000 [=====>........................] - ETA: 5:50 - loss: 3.0589 - regression_loss: 2.3617 - classification_loss: 0.6973
 225/1000 [=====>........................] - ETA: 5:50 - loss: 3.0647 - regression_loss: 2.3635 - classification_loss: 0.7011
 226/1000 [=====>........................] - ETA: 5:49 - loss: 3.0601 - regression_loss: 2.3597 - classification_loss: 0.7004
 227/1000 [=====>........................] - ETA: 5:49 - loss: 3.0602 - regression_loss: 2.3595 - classification_loss: 0.7007
 228/1000 [=====>........................] - ETA: 5:49 - loss: 3.0713 - regression_loss: 2.3670 - classification_loss: 0.7043
 229/1000 [=====>........................] - ETA: 5:48 - loss: 3.0767 - regression_loss: 2.3713 - classification_loss: 0.7054
 230/1000 [=====>........................] - ETA: 5:48 - loss: 3.0784 - regression_loss: 2.3724 - classification_loss: 0.7060
 231/1000 [=====>........................] - ETA: 5:47 - loss: 3.0791 - regression_loss: 2.3715 - classification_loss: 0.7075
 232/1000 [=====>........................] - ETA: 5:47 - loss: 3.0791 - regression_loss: 2.3719 - classification_loss: 0.7072
 233/1000 [=====>........................] - ETA: 5:46 - loss: 3.0683 - regression_loss: 2.3618 - classification_loss: 0.7065
 234/1000 [======>.......................] - ETA: 5:46 - loss: 3.0552 - regression_loss: 2.3517 - classification_loss: 0.7035
 235/1000 [======>.......................] - ETA: 5:45 - loss: 3.0574 - regression_loss: 2.3535 - classification_loss: 0.7039
 236/1000 [======>.......................] - ETA: 5:45 - loss: 3.0591 - regression_loss: 2.3556 - classification_loss: 0.7034
 237/1000 [======>.......................] - ETA: 5:45 - loss: 3.0598 - regression_loss: 2.3574 - classification_loss: 0.7024
 238/1000 [======>.......................] - ETA: 5:44 - loss: 3.0470 - regression_loss: 2.3475 - classification_loss: 0.6996
 239/1000 [======>.......................] - ETA: 5:44 - loss: 3.0504 - regression_loss: 2.3501 - classification_loss: 0.7003
 240/1000 [======>.......................] - ETA: 5:43 - loss: 3.0522 - regression_loss: 2.3511 - classification_loss: 0.7011
 241/1000 [======>.......................] - ETA: 5:43 - loss: 3.0396 - regression_loss: 2.3413 - classification_loss: 0.6983
 242/1000 [======>.......................] - ETA: 5:42 - loss: 3.0438 - regression_loss: 2.3436 - classification_loss: 0.7002
 243/1000 [======>.......................] - ETA: 5:42 - loss: 3.0510 - regression_loss: 2.3492 - classification_loss: 0.7018
 244/1000 [======>.......................] - ETA: 5:41 - loss: 3.0512 - regression_loss: 2.3498 - classification_loss: 0.7014
 245/1000 [======>.......................] - ETA: 5:41 - loss: 3.0489 - regression_loss: 2.3478 - classification_loss: 0.7011
 246/1000 [======>.......................] - ETA: 5:40 - loss: 3.0491 - regression_loss: 2.3482 - classification_loss: 0.7009
 247/1000 [======>.......................] - ETA: 5:40 - loss: 3.0476 - regression_loss: 2.3480 - classification_loss: 0.6996
 248/1000 [======>.......................] - ETA: 5:40 - loss: 3.0353 - regression_loss: 2.3385 - classification_loss: 0.6967
 249/1000 [======>.......................] - ETA: 5:39 - loss: 3.0368 - regression_loss: 2.3393 - classification_loss: 0.6974
 250/1000 [======>.......................] - ETA: 5:39 - loss: 3.0424 - regression_loss: 2.3441 - classification_loss: 0.6983
 251/1000 [======>.......................] - ETA: 5:38 - loss: 3.0431 - regression_loss: 2.3452 - classification_loss: 0.6979
 252/1000 [======>.......................] - ETA: 5:38 - loss: 3.0310 - regression_loss: 2.3359 - classification_loss: 0.6951
 253/1000 [======>.......................] - ETA: 5:37 - loss: 3.0190 - regression_loss: 2.3267 - classification_loss: 0.6924
 254/1000 [======>.......................] - ETA: 5:37 - loss: 3.0273 - regression_loss: 2.3315 - classification_loss: 0.6958
 255/1000 [======>.......................] - ETA: 5:36 - loss: 3.0343 - regression_loss: 2.3358 - classification_loss: 0.6985
 256/1000 [======>.......................] - ETA: 5:36 - loss: 3.0435 - regression_loss: 2.3432 - classification_loss: 0.7003
 257/1000 [======>.......................] - ETA: 5:36 - loss: 3.0316 - regression_loss: 2.3341 - classification_loss: 0.6976
 258/1000 [======>.......................] - ETA: 5:35 - loss: 3.0371 - regression_loss: 2.3367 - classification_loss: 0.7004
 259/1000 [======>.......................] - ETA: 5:35 - loss: 3.0253 - regression_loss: 2.3276 - classification_loss: 0.6977
 260/1000 [======>.......................] - ETA: 5:34 - loss: 3.0312 - regression_loss: 2.3319 - classification_loss: 0.6994
 261/1000 [======>.......................] - ETA: 5:34 - loss: 3.0331 - regression_loss: 2.3339 - classification_loss: 0.6992
 262/1000 [======>.......................] - ETA: 5:33 - loss: 3.0381 - regression_loss: 2.3380 - classification_loss: 0.7001
 263/1000 [======>.......................] - ETA: 5:33 - loss: 3.0266 - regression_loss: 2.3291 - classification_loss: 0.6974
 264/1000 [======>.......................] - ETA: 5:32 - loss: 3.0331 - regression_loss: 2.3327 - classification_loss: 0.7004
 265/1000 [======>.......................] - ETA: 5:32 - loss: 3.0217 - regression_loss: 2.3239 - classification_loss: 0.6977
 266/1000 [======>.......................] - ETA: 5:31 - loss: 3.0269 - regression_loss: 2.3266 - classification_loss: 0.7003
 267/1000 [=======>......................] - ETA: 5:31 - loss: 3.0314 - regression_loss: 2.3315 - classification_loss: 0.6999
 268/1000 [=======>......................] - ETA: 5:31 - loss: 3.0394 - regression_loss: 2.3349 - classification_loss: 0.7046
 269/1000 [=======>......................] - ETA: 5:30 - loss: 3.0440 - regression_loss: 2.3363 - classification_loss: 0.7077
 270/1000 [=======>......................] - ETA: 5:30 - loss: 3.0449 - regression_loss: 2.3371 - classification_loss: 0.7078
 271/1000 [=======>......................] - ETA: 5:29 - loss: 3.0505 - regression_loss: 2.3398 - classification_loss: 0.7107
 272/1000 [=======>......................] - ETA: 5:29 - loss: 3.0577 - regression_loss: 2.3432 - classification_loss: 0.7145
 273/1000 [=======>......................] - ETA: 5:28 - loss: 3.0552 - regression_loss: 2.3409 - classification_loss: 0.7143
 274/1000 [=======>......................] - ETA: 5:28 - loss: 3.0545 - regression_loss: 2.3413 - classification_loss: 0.7132
 275/1000 [=======>......................] - ETA: 5:27 - loss: 3.0535 - regression_loss: 2.3381 - classification_loss: 0.7154
 276/1000 [=======>......................] - ETA: 5:27 - loss: 3.0508 - regression_loss: 2.3364 - classification_loss: 0.7144
 277/1000 [=======>......................] - ETA: 5:26 - loss: 3.0468 - regression_loss: 2.3331 - classification_loss: 0.7138
 278/1000 [=======>......................] - ETA: 5:26 - loss: 3.0528 - regression_loss: 2.3377 - classification_loss: 0.7151
 279/1000 [=======>......................] - ETA: 5:25 - loss: 3.0553 - regression_loss: 2.3391 - classification_loss: 0.7162
 280/1000 [=======>......................] - ETA: 5:25 - loss: 3.0543 - regression_loss: 2.3369 - classification_loss: 0.7174
 281/1000 [=======>......................] - ETA: 5:25 - loss: 3.0519 - regression_loss: 2.3352 - classification_loss: 0.7166
 282/1000 [=======>......................] - ETA: 5:24 - loss: 3.0583 - regression_loss: 2.3388 - classification_loss: 0.7195
 283/1000 [=======>......................] - ETA: 5:24 - loss: 3.0630 - regression_loss: 2.3428 - classification_loss: 0.7202
 284/1000 [=======>......................] - ETA: 5:23 - loss: 3.0619 - regression_loss: 2.3429 - classification_loss: 0.7190
 285/1000 [=======>......................] - ETA: 5:23 - loss: 3.0511 - regression_loss: 2.3347 - classification_loss: 0.7165
 286/1000 [=======>......................] - ETA: 5:22 - loss: 3.0545 - regression_loss: 2.3379 - classification_loss: 0.7166
 287/1000 [=======>......................] - ETA: 5:22 - loss: 3.0598 - regression_loss: 2.3421 - classification_loss: 0.7177
 288/1000 [=======>......................] - ETA: 5:21 - loss: 3.0492 - regression_loss: 2.3340 - classification_loss: 0.7152
 289/1000 [=======>......................] - ETA: 5:21 - loss: 3.0433 - regression_loss: 2.3299 - classification_loss: 0.7134
 290/1000 [=======>......................] - ETA: 5:21 - loss: 3.0416 - regression_loss: 2.3284 - classification_loss: 0.7133
 291/1000 [=======>......................] - ETA: 5:20 - loss: 3.0312 - regression_loss: 2.3204 - classification_loss: 0.7108
 292/1000 [=======>......................] - ETA: 5:20 - loss: 3.0284 - regression_loss: 2.3182 - classification_loss: 0.7103
 293/1000 [=======>......................] - ETA: 5:19 - loss: 3.0343 - regression_loss: 2.3223 - classification_loss: 0.7120
 294/1000 [=======>......................] - ETA: 5:19 - loss: 3.0319 - regression_loss: 2.3213 - classification_loss: 0.7107
 295/1000 [=======>......................] - ETA: 5:18 - loss: 3.0345 - regression_loss: 2.3235 - classification_loss: 0.7111
 296/1000 [=======>......................] - ETA: 5:18 - loss: 3.0368 - regression_loss: 2.3252 - classification_loss: 0.7116
 297/1000 [=======>......................] - ETA: 5:17 - loss: 3.0358 - regression_loss: 2.3244 - classification_loss: 0.7114
 298/1000 [=======>......................] - ETA: 5:17 - loss: 3.0322 - regression_loss: 2.3221 - classification_loss: 0.7101
 299/1000 [=======>......................] - ETA: 5:17 - loss: 3.0364 - regression_loss: 2.3269 - classification_loss: 0.7095
 300/1000 [========>.....................] - ETA: 5:16 - loss: 3.0333 - regression_loss: 2.3252 - classification_loss: 0.7081
 301/1000 [========>.....................] - ETA: 5:16 - loss: 3.0311 - regression_loss: 2.3237 - classification_loss: 0.7074
 302/1000 [========>.....................] - ETA: 5:15 - loss: 3.0424 - regression_loss: 2.3351 - classification_loss: 0.7073
 303/1000 [========>.....................] - ETA: 5:15 - loss: 3.0460 - regression_loss: 2.3383 - classification_loss: 0.7077
 304/1000 [========>.....................] - ETA: 5:14 - loss: 3.0501 - regression_loss: 2.3429 - classification_loss: 0.7072
 305/1000 [========>.....................] - ETA: 5:14 - loss: 3.0723 - regression_loss: 2.3352 - classification_loss: 0.7371
 306/1000 [========>.....................] - ETA: 5:13 - loss: 3.0700 - regression_loss: 2.3341 - classification_loss: 0.7359
 307/1000 [========>.....................] - ETA: 5:13 - loss: 3.0726 - regression_loss: 2.3371 - classification_loss: 0.7354
 308/1000 [========>.....................] - ETA: 5:12 - loss: 3.0806 - regression_loss: 2.3445 - classification_loss: 0.7361
 309/1000 [========>.....................] - ETA: 5:12 - loss: 3.0850 - regression_loss: 2.3480 - classification_loss: 0.7371
 310/1000 [========>.....................] - ETA: 5:12 - loss: 3.0752 - regression_loss: 2.3404 - classification_loss: 0.7348
 311/1000 [========>.....................] - ETA: 5:11 - loss: 3.1021 - regression_loss: 2.3448 - classification_loss: 0.7573
 312/1000 [========>.....................] - ETA: 5:11 - loss: 3.1078 - regression_loss: 2.3495 - classification_loss: 0.7583
 313/1000 [========>.....................] - ETA: 5:10 - loss: 3.1177 - regression_loss: 2.3576 - classification_loss: 0.7602
 314/1000 [========>.....................] - ETA: 5:10 - loss: 3.1166 - regression_loss: 2.3571 - classification_loss: 0.7595
 315/1000 [========>.....................] - ETA: 5:09 - loss: 3.1077 - regression_loss: 2.3497 - classification_loss: 0.7581
 316/1000 [========>.....................] - ETA: 5:09 - loss: 3.1040 - regression_loss: 2.3468 - classification_loss: 0.7571
 317/1000 [========>.....................] - ETA: 5:08 - loss: 3.1094 - regression_loss: 2.3518 - classification_loss: 0.7576
 318/1000 [========>.....................] - ETA: 5:08 - loss: 3.1118 - regression_loss: 2.3547 - classification_loss: 0.7571
 319/1000 [========>.....................] - ETA: 5:07 - loss: 3.1121 - regression_loss: 2.3540 - classification_loss: 0.7581
 320/1000 [========>.....................] - ETA: 5:07 - loss: 3.1093 - regression_loss: 2.3524 - classification_loss: 0.7569
 321/1000 [========>.....................] - ETA: 5:07 - loss: 3.0996 - regression_loss: 2.3450 - classification_loss: 0.7546
 322/1000 [========>.....................] - ETA: 5:06 - loss: 3.1033 - regression_loss: 2.3482 - classification_loss: 0.7550
 323/1000 [========>.....................] - ETA: 5:06 - loss: 3.1048 - regression_loss: 2.3495 - classification_loss: 0.7553
 324/1000 [========>.....................] - ETA: 5:05 - loss: 3.1046 - regression_loss: 2.3495 - classification_loss: 0.7550
 325/1000 [========>.....................] - ETA: 5:05 - loss: 3.1088 - regression_loss: 2.3530 - classification_loss: 0.7558
 326/1000 [========>.....................] - ETA: 5:04 - loss: 3.1098 - regression_loss: 2.3538 - classification_loss: 0.7560
 327/1000 [========>.....................] - ETA: 5:04 - loss: 3.1089 - regression_loss: 2.3528 - classification_loss: 0.7561
 328/1000 [========>.....................] - ETA: 5:03 - loss: 3.0995 - regression_loss: 2.3457 - classification_loss: 0.7538
 329/1000 [========>.....................] - ETA: 5:03 - loss: 3.0978 - regression_loss: 2.3450 - classification_loss: 0.7527
 330/1000 [========>.....................] - ETA: 5:03 - loss: 3.1027 - regression_loss: 2.3485 - classification_loss: 0.7542
 331/1000 [========>.....................] - ETA: 5:02 - loss: 3.1027 - regression_loss: 2.3483 - classification_loss: 0.7544
 332/1000 [========>.....................] - ETA: 5:02 - loss: 3.0997 - regression_loss: 2.3456 - classification_loss: 0.7541
 333/1000 [========>.....................] - ETA: 5:01 - loss: 3.0907 - regression_loss: 2.3386 - classification_loss: 0.7522
 334/1000 [=========>....................] - ETA: 5:01 - loss: 3.0887 - regression_loss: 2.3376 - classification_loss: 0.7510
 335/1000 [=========>....................] - ETA: 5:00 - loss: 3.0801 - regression_loss: 2.3307 - classification_loss: 0.7494
 336/1000 [=========>....................] - ETA: 5:00 - loss: 3.0855 - regression_loss: 2.3354 - classification_loss: 0.7501
 337/1000 [=========>....................] - ETA: 4:59 - loss: 3.0879 - regression_loss: 2.3375 - classification_loss: 0.7504
 338/1000 [=========>....................] - ETA: 4:59 - loss: 3.0902 - regression_loss: 2.3396 - classification_loss: 0.7506
 339/1000 [=========>....................] - ETA: 4:58 - loss: 3.0811 - regression_loss: 2.3327 - classification_loss: 0.7484
 340/1000 [=========>....................] - ETA: 4:58 - loss: 3.0786 - regression_loss: 2.3310 - classification_loss: 0.7476
 341/1000 [=========>....................] - ETA: 4:58 - loss: 3.0766 - regression_loss: 2.3291 - classification_loss: 0.7475
 342/1000 [=========>....................] - ETA: 4:57 - loss: 3.0786 - regression_loss: 2.3313 - classification_loss: 0.7474
 343/1000 [=========>....................] - ETA: 4:57 - loss: 3.0850 - regression_loss: 2.3370 - classification_loss: 0.7480
 344/1000 [=========>....................] - ETA: 4:56 - loss: 3.0878 - regression_loss: 2.3405 - classification_loss: 0.7474
 345/1000 [=========>....................] - ETA: 4:56 - loss: 3.0789 - regression_loss: 2.3337 - classification_loss: 0.7452
 346/1000 [=========>....................] - ETA: 4:55 - loss: 3.0825 - regression_loss: 2.3354 - classification_loss: 0.7472
 347/1000 [=========>....................] - ETA: 4:55 - loss: 3.0832 - regression_loss: 2.3368 - classification_loss: 0.7464
 348/1000 [=========>....................] - ETA: 4:54 - loss: 3.0802 - regression_loss: 2.3343 - classification_loss: 0.7459
 349/1000 [=========>....................] - ETA: 4:54 - loss: 3.0821 - regression_loss: 2.3366 - classification_loss: 0.7455
 350/1000 [=========>....................] - ETA: 4:53 - loss: 3.0831 - regression_loss: 2.3377 - classification_loss: 0.7454
 351/1000 [=========>....................] - ETA: 4:53 - loss: 3.0799 - regression_loss: 2.3355 - classification_loss: 0.7444
 352/1000 [=========>....................] - ETA: 4:53 - loss: 3.0817 - regression_loss: 2.3371 - classification_loss: 0.7446
 353/1000 [=========>....................] - ETA: 4:52 - loss: 3.0806 - regression_loss: 2.3358 - classification_loss: 0.7448
 354/1000 [=========>....................] - ETA: 4:52 - loss: 3.0845 - regression_loss: 2.3399 - classification_loss: 0.7446
 355/1000 [=========>....................] - ETA: 4:51 - loss: 3.0808 - regression_loss: 2.3373 - classification_loss: 0.7434
 356/1000 [=========>....................] - ETA: 4:51 - loss: 3.0789 - regression_loss: 2.3350 - classification_loss: 0.7439
 357/1000 [=========>....................] - ETA: 4:50 - loss: 3.0703 - regression_loss: 2.3284 - classification_loss: 0.7418
 358/1000 [=========>....................] - ETA: 4:50 - loss: 3.0695 - regression_loss: 2.3288 - classification_loss: 0.7407
 359/1000 [=========>....................] - ETA: 4:49 - loss: 3.0734 - regression_loss: 2.3322 - classification_loss: 0.7412
 360/1000 [=========>....................] - ETA: 4:49 - loss: 3.0710 - regression_loss: 2.3304 - classification_loss: 0.7406
 361/1000 [=========>....................] - ETA: 4:49 - loss: 3.0723 - regression_loss: 2.3317 - classification_loss: 0.7406
 362/1000 [=========>....................] - ETA: 4:48 - loss: 3.0732 - regression_loss: 2.3326 - classification_loss: 0.7406
 363/1000 [=========>....................] - ETA: 4:48 - loss: 3.0727 - regression_loss: 2.3334 - classification_loss: 0.7393
 364/1000 [=========>....................] - ETA: 4:47 - loss: 3.0646 - regression_loss: 2.3270 - classification_loss: 0.7377
 365/1000 [=========>....................] - ETA: 4:47 - loss: 3.0620 - regression_loss: 2.3254 - classification_loss: 0.7366
 366/1000 [=========>....................] - ETA: 4:46 - loss: 3.0586 - regression_loss: 2.3232 - classification_loss: 0.7354
 367/1000 [==========>...................] - ETA: 4:46 - loss: 3.0620 - regression_loss: 2.3260 - classification_loss: 0.7360
 368/1000 [==========>...................] - ETA: 4:45 - loss: 3.0635 - regression_loss: 2.3282 - classification_loss: 0.7354
 369/1000 [==========>...................] - ETA: 4:45 - loss: 3.0653 - regression_loss: 2.3306 - classification_loss: 0.7347
 370/1000 [==========>...................] - ETA: 4:45 - loss: 3.0658 - regression_loss: 2.3319 - classification_loss: 0.7338
 371/1000 [==========>...................] - ETA: 4:44 - loss: 3.0579 - regression_loss: 2.3257 - classification_loss: 0.7323
 372/1000 [==========>...................] - ETA: 4:44 - loss: 3.0565 - regression_loss: 2.3246 - classification_loss: 0.7320
 373/1000 [==========>...................] - ETA: 4:43 - loss: 3.0560 - regression_loss: 2.3239 - classification_loss: 0.7321
 374/1000 [==========>...................] - ETA: 4:43 - loss: 3.0484 - regression_loss: 2.3177 - classification_loss: 0.7307
 375/1000 [==========>...................] - ETA: 4:42 - loss: 3.0404 - regression_loss: 2.3116 - classification_loss: 0.7289
 376/1000 [==========>...................] - ETA: 4:42 - loss: 3.0382 - regression_loss: 2.3101 - classification_loss: 0.7282
 377/1000 [==========>...................] - ETA: 4:41 - loss: 3.0408 - regression_loss: 2.3120 - classification_loss: 0.7288
 378/1000 [==========>...................] - ETA: 4:41 - loss: 3.0448 - regression_loss: 2.3143 - classification_loss: 0.7305
 379/1000 [==========>...................] - ETA: 4:40 - loss: 3.0370 - regression_loss: 2.3082 - classification_loss: 0.7288
 380/1000 [==========>...................] - ETA: 4:40 - loss: 3.0375 - regression_loss: 2.3090 - classification_loss: 0.7285
 381/1000 [==========>...................] - ETA: 4:40 - loss: 3.0295 - regression_loss: 2.3029 - classification_loss: 0.7266
 382/1000 [==========>...................] - ETA: 4:39 - loss: 3.0339 - regression_loss: 2.3055 - classification_loss: 0.7284
 383/1000 [==========>...................] - ETA: 4:39 - loss: 3.0304 - regression_loss: 2.2995 - classification_loss: 0.7309
 384/1000 [==========>...................] - ETA: 4:38 - loss: 3.0305 - regression_loss: 2.2993 - classification_loss: 0.7312
 385/1000 [==========>...................] - ETA: 4:38 - loss: 3.0302 - regression_loss: 2.3000 - classification_loss: 0.7302
 386/1000 [==========>...................] - ETA: 4:37 - loss: 3.0350 - regression_loss: 2.3017 - classification_loss: 0.7333
 387/1000 [==========>...................] - ETA: 4:37 - loss: 3.0343 - regression_loss: 2.3015 - classification_loss: 0.7328
 388/1000 [==========>...................] - ETA: 4:36 - loss: 3.0266 - regression_loss: 2.2956 - classification_loss: 0.7310
 389/1000 [==========>...................] - ETA: 4:36 - loss: 3.0318 - regression_loss: 2.2983 - classification_loss: 0.7336
 390/1000 [==========>...................] - ETA: 4:36 - loss: 3.0241 - regression_loss: 2.2924 - classification_loss: 0.7317
 391/1000 [==========>...................] - ETA: 4:35 - loss: 3.0276 - regression_loss: 2.2942 - classification_loss: 0.7334
 392/1000 [==========>...................] - ETA: 4:35 - loss: 3.0284 - regression_loss: 2.2939 - classification_loss: 0.7345
 393/1000 [==========>...................] - ETA: 4:34 - loss: 3.0296 - regression_loss: 2.2930 - classification_loss: 0.7366
 394/1000 [==========>...................] - ETA: 4:34 - loss: 3.0318 - regression_loss: 2.2934 - classification_loss: 0.7384
 395/1000 [==========>...................] - ETA: 4:33 - loss: 3.0340 - regression_loss: 2.2960 - classification_loss: 0.7380
 396/1000 [==========>...................] - ETA: 4:33 - loss: 3.0369 - regression_loss: 2.2981 - classification_loss: 0.7389
 397/1000 [==========>...................] - ETA: 4:32 - loss: 3.0356 - regression_loss: 2.2976 - classification_loss: 0.7380
 398/1000 [==========>...................] - ETA: 4:32 - loss: 3.0317 - regression_loss: 2.2948 - classification_loss: 0.7369
 399/1000 [==========>...................] - ETA: 4:32 - loss: 3.0324 - regression_loss: 2.2957 - classification_loss: 0.7368
 400/1000 [===========>..................] - ETA: 4:31 - loss: 3.0372 - regression_loss: 2.2986 - classification_loss: 0.7386
 401/1000 [===========>..................] - ETA: 4:31 - loss: 3.0387 - regression_loss: 2.3003 - classification_loss: 0.7385
 402/1000 [===========>..................] - ETA: 4:30 - loss: 3.0382 - regression_loss: 2.3004 - classification_loss: 0.7378
 403/1000 [===========>..................] - ETA: 4:30 - loss: 3.0374 - regression_loss: 2.3001 - classification_loss: 0.7373
 404/1000 [===========>..................] - ETA: 4:29 - loss: 3.0370 - regression_loss: 2.3004 - classification_loss: 0.7366
 405/1000 [===========>..................] - ETA: 4:29 - loss: 3.0357 - regression_loss: 2.2995 - classification_loss: 0.7362
 406/1000 [===========>..................] - ETA: 4:28 - loss: 3.0349 - regression_loss: 2.2988 - classification_loss: 0.7361
 407/1000 [===========>..................] - ETA: 4:28 - loss: 3.0275 - regression_loss: 2.2931 - classification_loss: 0.7343
 408/1000 [===========>..................] - ETA: 4:27 - loss: 3.0201 - regression_loss: 2.2875 - classification_loss: 0.7325
 409/1000 [===========>..................] - ETA: 4:27 - loss: 3.0145 - regression_loss: 2.2819 - classification_loss: 0.7326
 410/1000 [===========>..................] - ETA: 4:27 - loss: 3.0198 - regression_loss: 2.2858 - classification_loss: 0.7340
 411/1000 [===========>..................] - ETA: 4:26 - loss: 3.0125 - regression_loss: 2.2802 - classification_loss: 0.7323
 412/1000 [===========>..................] - ETA: 4:26 - loss: 3.0111 - regression_loss: 2.2789 - classification_loss: 0.7322
 413/1000 [===========>..................] - ETA: 4:25 - loss: 3.0087 - regression_loss: 2.2768 - classification_loss: 0.7320
 414/1000 [===========>..................] - ETA: 4:25 - loss: 3.0109 - regression_loss: 2.2780 - classification_loss: 0.7329
 415/1000 [===========>..................] - ETA: 4:24 - loss: 3.0113 - regression_loss: 2.2791 - classification_loss: 0.7322
 416/1000 [===========>..................] - ETA: 4:24 - loss: 3.0042 - regression_loss: 2.2736 - classification_loss: 0.7306
 417/1000 [===========>..................] - ETA: 4:23 - loss: 2.9970 - regression_loss: 2.2682 - classification_loss: 0.7289
 418/1000 [===========>..................] - ETA: 4:23 - loss: 2.9899 - regression_loss: 2.2627 - classification_loss: 0.7271
 419/1000 [===========>..................] - ETA: 4:22 - loss: 2.9913 - regression_loss: 2.2639 - classification_loss: 0.7274
 420/1000 [===========>..................] - ETA: 4:22 - loss: 2.9842 - regression_loss: 2.2585 - classification_loss: 0.7257
 421/1000 [===========>..................] - ETA: 4:22 - loss: 2.9772 - regression_loss: 2.2532 - classification_loss: 0.7240
 422/1000 [===========>..................] - ETA: 4:21 - loss: 2.9792 - regression_loss: 2.2560 - classification_loss: 0.7232
 423/1000 [===========>..................] - ETA: 4:21 - loss: 2.9833 - regression_loss: 2.2581 - classification_loss: 0.7251
 424/1000 [===========>..................] - ETA: 4:20 - loss: 2.9762 - regression_loss: 2.2528 - classification_loss: 0.7234
 425/1000 [===========>..................] - ETA: 4:20 - loss: 2.9786 - regression_loss: 2.2543 - classification_loss: 0.7243
 426/1000 [===========>..................] - ETA: 4:19 - loss: 2.9838 - regression_loss: 2.2568 - classification_loss: 0.7271
 427/1000 [===========>..................] - ETA: 4:19 - loss: 2.9845 - regression_loss: 2.2581 - classification_loss: 0.7264
 428/1000 [===========>..................] - ETA: 4:18 - loss: 2.9910 - regression_loss: 2.2619 - classification_loss: 0.7291
 429/1000 [===========>..................] - ETA: 4:18 - loss: 2.9916 - regression_loss: 2.2607 - classification_loss: 0.7309
 430/1000 [===========>..................] - ETA: 4:17 - loss: 2.9935 - regression_loss: 2.2614 - classification_loss: 0.7321
 431/1000 [===========>..................] - ETA: 4:17 - loss: 2.9962 - regression_loss: 2.2639 - classification_loss: 0.7323
 432/1000 [===========>..................] - ETA: 4:17 - loss: 2.9980 - regression_loss: 2.2639 - classification_loss: 0.7341
 433/1000 [===========>..................] - ETA: 4:16 - loss: 3.0041 - regression_loss: 2.2670 - classification_loss: 0.7371
 434/1000 [============>.................] - ETA: 4:16 - loss: 3.0078 - regression_loss: 2.2704 - classification_loss: 0.7374
 435/1000 [============>.................] - ETA: 4:15 - loss: 3.0009 - regression_loss: 2.2652 - classification_loss: 0.7357
 436/1000 [============>.................] - ETA: 4:15 - loss: 3.0022 - regression_loss: 2.2666 - classification_loss: 0.7356
 437/1000 [============>.................] - ETA: 4:14 - loss: 3.0029 - regression_loss: 2.2669 - classification_loss: 0.7360
 438/1000 [============>.................] - ETA: 4:14 - loss: 2.9961 - regression_loss: 2.2618 - classification_loss: 0.7343
 439/1000 [============>.................] - ETA: 4:13 - loss: 3.0015 - regression_loss: 2.2645 - classification_loss: 0.7370
 440/1000 [============>.................] - ETA: 4:13 - loss: 2.9947 - regression_loss: 2.2593 - classification_loss: 0.7354
 441/1000 [============>.................] - ETA: 4:12 - loss: 2.9932 - regression_loss: 2.2584 - classification_loss: 0.7348
 442/1000 [============>.................] - ETA: 4:12 - loss: 2.9924 - regression_loss: 2.2580 - classification_loss: 0.7344
 443/1000 [============>.................] - ETA: 4:12 - loss: 2.9857 - regression_loss: 2.2529 - classification_loss: 0.7328
 444/1000 [============>.................] - ETA: 4:11 - loss: 2.9855 - regression_loss: 2.2530 - classification_loss: 0.7325
 445/1000 [============>.................] - ETA: 4:11 - loss: 2.9788 - regression_loss: 2.2479 - classification_loss: 0.7309
 446/1000 [============>.................] - ETA: 4:10 - loss: 2.9800 - regression_loss: 2.2493 - classification_loss: 0.7306
 447/1000 [============>.................] - ETA: 4:10 - loss: 2.9828 - regression_loss: 2.2500 - classification_loss: 0.7328
 448/1000 [============>.................] - ETA: 4:09 - loss: 2.9837 - regression_loss: 2.2507 - classification_loss: 0.7331
 449/1000 [============>.................] - ETA: 4:09 - loss: 2.9840 - regression_loss: 2.2516 - classification_loss: 0.7325
 450/1000 [============>.................] - ETA: 4:08 - loss: 2.9855 - regression_loss: 2.2534 - classification_loss: 0.7321
 451/1000 [============>.................] - ETA: 4:08 - loss: 2.9854 - regression_loss: 2.2538 - classification_loss: 0.7316
 452/1000 [============>.................] - ETA: 4:07 - loss: 2.9847 - regression_loss: 2.2523 - classification_loss: 0.7324
 453/1000 [============>.................] - ETA: 4:07 - loss: 2.9833 - regression_loss: 2.2520 - classification_loss: 0.7313
 454/1000 [============>.................] - ETA: 4:07 - loss: 2.9767 - regression_loss: 2.2470 - classification_loss: 0.7296
 455/1000 [============>.................] - ETA: 4:06 - loss: 2.9745 - regression_loss: 2.2458 - classification_loss: 0.7287
 456/1000 [============>.................] - ETA: 4:06 - loss: 2.9790 - regression_loss: 2.2511 - classification_loss: 0.7279
 457/1000 [============>.................] - ETA: 4:05 - loss: 2.9725 - regression_loss: 2.2462 - classification_loss: 0.7263
 458/1000 [============>.................] - ETA: 4:05 - loss: 2.9660 - regression_loss: 2.2413 - classification_loss: 0.7247
 459/1000 [============>.................] - ETA: 4:04 - loss: 2.9682 - regression_loss: 2.2425 - classification_loss: 0.7257
 460/1000 [============>.................] - ETA: 4:04 - loss: 2.9665 - regression_loss: 2.2413 - classification_loss: 0.7252
 461/1000 [============>.................] - ETA: 4:03 - loss: 2.9601 - regression_loss: 2.2365 - classification_loss: 0.7236
 462/1000 [============>.................] - ETA: 4:03 - loss: 2.9537 - regression_loss: 2.2316 - classification_loss: 0.7221
 463/1000 [============>.................] - ETA: 4:03 - loss: 2.9623 - regression_loss: 2.2364 - classification_loss: 0.7259
 464/1000 [============>.................] - ETA: 4:02 - loss: 2.9607 - regression_loss: 2.2355 - classification_loss: 0.7252
 465/1000 [============>.................] - ETA: 4:02 - loss: 2.9647 - regression_loss: 2.2371 - classification_loss: 0.7276
 466/1000 [============>.................] - ETA: 4:01 - loss: 2.9644 - regression_loss: 2.2370 - classification_loss: 0.7274
 467/1000 [=============>................] - ETA: 4:01 - loss: 2.9643 - regression_loss: 2.2362 - classification_loss: 0.7281
 468/1000 [=============>................] - ETA: 4:00 - loss: 2.9680 - regression_loss: 2.2379 - classification_loss: 0.7301
 469/1000 [=============>................] - ETA: 4:00 - loss: 2.9681 - regression_loss: 2.2381 - classification_loss: 0.7300
 470/1000 [=============>................] - ETA: 3:59 - loss: 2.9617 - regression_loss: 2.2333 - classification_loss: 0.7284
 471/1000 [=============>................] - ETA: 3:59 - loss: 2.9609 - regression_loss: 2.2333 - classification_loss: 0.7276
 472/1000 [=============>................] - ETA: 3:58 - loss: 2.9604 - regression_loss: 2.2336 - classification_loss: 0.7267
 473/1000 [=============>................] - ETA: 3:58 - loss: 2.9583 - regression_loss: 2.2324 - classification_loss: 0.7259
 474/1000 [=============>................] - ETA: 3:58 - loss: 2.9522 - regression_loss: 2.2277 - classification_loss: 0.7245
 475/1000 [=============>................] - ETA: 3:57 - loss: 2.9601 - regression_loss: 2.2350 - classification_loss: 0.7251
 476/1000 [=============>................] - ETA: 3:57 - loss: 2.9607 - regression_loss: 2.2358 - classification_loss: 0.7249
 477/1000 [=============>................] - ETA: 3:56 - loss: 2.9610 - regression_loss: 2.2362 - classification_loss: 0.7248
 478/1000 [=============>................] - ETA: 3:56 - loss: 2.9612 - regression_loss: 2.2369 - classification_loss: 0.7244
 479/1000 [=============>................] - ETA: 3:55 - loss: 2.9599 - regression_loss: 2.2359 - classification_loss: 0.7240
 480/1000 [=============>................] - ETA: 3:55 - loss: 2.9591 - regression_loss: 2.2358 - classification_loss: 0.7233
 481/1000 [=============>................] - ETA: 3:54 - loss: 2.9648 - regression_loss: 2.2393 - classification_loss: 0.7255
 482/1000 [=============>................] - ETA: 3:54 - loss: 2.9644 - regression_loss: 2.2387 - classification_loss: 0.7258
 483/1000 [=============>................] - ETA: 3:53 - loss: 2.9637 - regression_loss: 2.2377 - classification_loss: 0.7260
 484/1000 [=============>................] - ETA: 3:53 - loss: 2.9657 - regression_loss: 2.2399 - classification_loss: 0.7259
 485/1000 [=============>................] - ETA: 3:53 - loss: 2.9694 - regression_loss: 2.2418 - classification_loss: 0.7276
 486/1000 [=============>................] - ETA: 3:52 - loss: 2.9708 - regression_loss: 2.2439 - classification_loss: 0.7268
 487/1000 [=============>................] - ETA: 3:52 - loss: 2.9707 - regression_loss: 2.2447 - classification_loss: 0.7261
 488/1000 [=============>................] - ETA: 3:51 - loss: 2.9725 - regression_loss: 2.2463 - classification_loss: 0.7262
 489/1000 [=============>................] - ETA: 3:51 - loss: 2.9732 - regression_loss: 2.2476 - classification_loss: 0.7256
 490/1000 [=============>................] - ETA: 3:50 - loss: 2.9724 - regression_loss: 2.2475 - classification_loss: 0.7249
 491/1000 [=============>................] - ETA: 3:50 - loss: 2.9664 - regression_loss: 2.2429 - classification_loss: 0.7234
 492/1000 [=============>................] - ETA: 3:49 - loss: 2.9677 - regression_loss: 2.2437 - classification_loss: 0.7240
 493/1000 [=============>................] - ETA: 3:49 - loss: 2.9691 - regression_loss: 2.2448 - classification_loss: 0.7243
 494/1000 [=============>................] - ETA: 3:49 - loss: 2.9689 - regression_loss: 2.2449 - classification_loss: 0.7240
 495/1000 [=============>................] - ETA: 3:48 - loss: 2.9720 - regression_loss: 2.2478 - classification_loss: 0.7242
 496/1000 [=============>................] - ETA: 3:48 - loss: 2.9729 - regression_loss: 2.2484 - classification_loss: 0.7245
 497/1000 [=============>................] - ETA: 3:47 - loss: 2.9720 - regression_loss: 2.2480 - classification_loss: 0.7240
 498/1000 [=============>................] - ETA: 3:47 - loss: 2.9741 - regression_loss: 2.2485 - classification_loss: 0.7255
 499/1000 [=============>................] - ETA: 3:46 - loss: 2.9738 - regression_loss: 2.2486 - classification_loss: 0.7252
 500/1000 [==============>...............] - ETA: 3:46 - loss: 2.9683 - regression_loss: 2.2441 - classification_loss: 0.7242
 501/1000 [==============>...............] - ETA: 3:45 - loss: 2.9672 - regression_loss: 2.2440 - classification_loss: 0.7232
 502/1000 [==============>...............] - ETA: 3:45 - loss: 2.9655 - regression_loss: 2.2429 - classification_loss: 0.7226
 503/1000 [==============>...............] - ETA: 3:44 - loss: 2.9650 - regression_loss: 2.2429 - classification_loss: 0.7221
 504/1000 [==============>...............] - ETA: 3:44 - loss: 2.9591 - regression_loss: 2.2384 - classification_loss: 0.7207
 505/1000 [==============>...............] - ETA: 3:44 - loss: 2.9575 - regression_loss: 2.2377 - classification_loss: 0.7199
 506/1000 [==============>...............] - ETA: 3:43 - loss: 2.9622 - regression_loss: 2.2422 - classification_loss: 0.7201
 507/1000 [==============>...............] - ETA: 3:43 - loss: 2.9636 - regression_loss: 2.2436 - classification_loss: 0.7200
 508/1000 [==============>...............] - ETA: 3:42 - loss: 2.9612 - regression_loss: 2.2420 - classification_loss: 0.7192
 509/1000 [==============>...............] - ETA: 3:42 - loss: 2.9615 - regression_loss: 2.2428 - classification_loss: 0.7186
 510/1000 [==============>...............] - ETA: 3:41 - loss: 2.9597 - regression_loss: 2.2420 - classification_loss: 0.7178
 511/1000 [==============>...............] - ETA: 3:41 - loss: 2.9618 - regression_loss: 2.2438 - classification_loss: 0.7179
 512/1000 [==============>...............] - ETA: 3:40 - loss: 2.9612 - regression_loss: 2.2439 - classification_loss: 0.7173
 513/1000 [==============>...............] - ETA: 3:40 - loss: 2.9641 - regression_loss: 2.2465 - classification_loss: 0.7176
 514/1000 [==============>...............] - ETA: 3:39 - loss: 2.9649 - regression_loss: 2.2475 - classification_loss: 0.7173
 515/1000 [==============>...............] - ETA: 3:39 - loss: 2.9591 - regression_loss: 2.2432 - classification_loss: 0.7160
 516/1000 [==============>...............] - ETA: 3:39 - loss: 2.9541 - regression_loss: 2.2388 - classification_loss: 0.7153
 517/1000 [==============>...............] - ETA: 3:38 - loss: 2.9524 - regression_loss: 2.2378 - classification_loss: 0.7146
 518/1000 [==============>...............] - ETA: 3:38 - loss: 2.9533 - regression_loss: 2.2385 - classification_loss: 0.7148
 519/1000 [==============>...............] - ETA: 3:37 - loss: 2.9476 - regression_loss: 2.2342 - classification_loss: 0.7134
 520/1000 [==============>...............] - ETA: 3:37 - loss: 2.9489 - regression_loss: 2.2357 - classification_loss: 0.7132
 521/1000 [==============>...............] - ETA: 3:36 - loss: 2.9499 - regression_loss: 2.2370 - classification_loss: 0.7129
 522/1000 [==============>...............] - ETA: 3:36 - loss: 2.9523 - regression_loss: 2.2385 - classification_loss: 0.7139
 523/1000 [==============>...............] - ETA: 3:35 - loss: 2.9467 - regression_loss: 2.2342 - classification_loss: 0.7125
 524/1000 [==============>...............] - ETA: 3:35 - loss: 2.9479 - regression_loss: 2.2360 - classification_loss: 0.7119
 525/1000 [==============>...............] - ETA: 3:34 - loss: 2.9494 - regression_loss: 2.2381 - classification_loss: 0.7113
 526/1000 [==============>...............] - ETA: 3:34 - loss: 2.9438 - regression_loss: 2.2339 - classification_loss: 0.7099
 527/1000 [==============>...............] - ETA: 3:34 - loss: 2.9444 - regression_loss: 2.2348 - classification_loss: 0.7095
 528/1000 [==============>...............] - ETA: 3:33 - loss: 2.9476 - regression_loss: 2.2369 - classification_loss: 0.7107
 529/1000 [==============>...............] - ETA: 3:33 - loss: 2.9477 - regression_loss: 2.2371 - classification_loss: 0.7106
 530/1000 [==============>...............] - ETA: 3:32 - loss: 2.9492 - regression_loss: 2.2385 - classification_loss: 0.7106
 531/1000 [==============>...............] - ETA: 3:32 - loss: 2.9437 - regression_loss: 2.2343 - classification_loss: 0.7093
 532/1000 [==============>...............] - ETA: 3:31 - loss: 2.9381 - regression_loss: 2.2301 - classification_loss: 0.7080
 533/1000 [==============>...............] - ETA: 3:31 - loss: 2.9392 - regression_loss: 2.2308 - classification_loss: 0.7084
 534/1000 [===============>..............] - ETA: 3:30 - loss: 2.9337 - regression_loss: 2.2266 - classification_loss: 0.7071
 535/1000 [===============>..............] - ETA: 3:30 - loss: 2.9282 - regression_loss: 2.2224 - classification_loss: 0.7058
 536/1000 [===============>..............] - ETA: 3:30 - loss: 2.9265 - regression_loss: 2.2215 - classification_loss: 0.7049
 537/1000 [===============>..............] - ETA: 3:29 - loss: 2.9253 - regression_loss: 2.2212 - classification_loss: 0.7041
 538/1000 [===============>..............] - ETA: 3:29 - loss: 2.9200 - regression_loss: 2.2171 - classification_loss: 0.7029
 539/1000 [===============>..............] - ETA: 3:28 - loss: 2.9146 - regression_loss: 2.2130 - classification_loss: 0.7016
 540/1000 [===============>..............] - ETA: 3:28 - loss: 2.9136 - regression_loss: 2.2119 - classification_loss: 0.7017
 541/1000 [===============>..............] - ETA: 3:27 - loss: 2.9082 - regression_loss: 2.2078 - classification_loss: 0.7004
 542/1000 [===============>..............] - ETA: 3:27 - loss: 2.9101 - regression_loss: 2.2097 - classification_loss: 0.7004
 543/1000 [===============>..............] - ETA: 3:26 - loss: 2.9098 - regression_loss: 2.2101 - classification_loss: 0.6997
 544/1000 [===============>..............] - ETA: 3:26 - loss: 2.9168 - regression_loss: 2.2145 - classification_loss: 0.7023
 545/1000 [===============>..............] - ETA: 3:25 - loss: 2.9170 - regression_loss: 2.2151 - classification_loss: 0.7019
 546/1000 [===============>..............] - ETA: 3:25 - loss: 2.9116 - regression_loss: 2.2111 - classification_loss: 0.7006
 547/1000 [===============>..............] - ETA: 3:25 - loss: 2.9122 - regression_loss: 2.2118 - classification_loss: 0.7004
 548/1000 [===============>..............] - ETA: 3:24 - loss: 2.9116 - regression_loss: 2.2117 - classification_loss: 0.6999
 549/1000 [===============>..............] - ETA: 3:24 - loss: 2.9114 - regression_loss: 2.2113 - classification_loss: 0.7000
 550/1000 [===============>..............] - ETA: 3:23 - loss: 2.9061 - regression_loss: 2.2073 - classification_loss: 0.6988
 551/1000 [===============>..............] - ETA: 3:23 - loss: 2.9059 - regression_loss: 2.2075 - classification_loss: 0.6984
 552/1000 [===============>..............] - ETA: 3:22 - loss: 2.9074 - regression_loss: 2.2093 - classification_loss: 0.6981
 553/1000 [===============>..............] - ETA: 3:22 - loss: 2.9108 - regression_loss: 2.2108 - classification_loss: 0.7000
 554/1000 [===============>..............] - ETA: 3:21 - loss: 2.9120 - regression_loss: 2.2122 - classification_loss: 0.6997
 555/1000 [===============>..............] - ETA: 3:21 - loss: 2.9100 - regression_loss: 2.2111 - classification_loss: 0.6990
 556/1000 [===============>..............] - ETA: 3:21 - loss: 2.9048 - regression_loss: 2.2071 - classification_loss: 0.6977
 557/1000 [===============>..............] - ETA: 3:20 - loss: 2.9060 - regression_loss: 2.2087 - classification_loss: 0.6974
 558/1000 [===============>..............] - ETA: 3:20 - loss: 2.9047 - regression_loss: 2.2079 - classification_loss: 0.6968
 559/1000 [===============>..............] - ETA: 3:19 - loss: 2.9066 - regression_loss: 2.2078 - classification_loss: 0.6988
 560/1000 [===============>..............] - ETA: 3:19 - loss: 2.9084 - regression_loss: 2.2078 - classification_loss: 0.7007
 561/1000 [===============>..............] - ETA: 3:18 - loss: 2.9105 - regression_loss: 2.2084 - classification_loss: 0.7021
 562/1000 [===============>..............] - ETA: 3:18 - loss: 2.9104 - regression_loss: 2.2088 - classification_loss: 0.7016
 563/1000 [===============>..............] - ETA: 3:17 - loss: 2.9131 - regression_loss: 2.2112 - classification_loss: 0.7019
 564/1000 [===============>..............] - ETA: 3:17 - loss: 2.9139 - regression_loss: 2.2114 - classification_loss: 0.7024
 565/1000 [===============>..............] - ETA: 3:16 - loss: 2.9087 - regression_loss: 2.2075 - classification_loss: 0.7012
 566/1000 [===============>..............] - ETA: 3:16 - loss: 2.9055 - regression_loss: 2.2036 - classification_loss: 0.7019
 567/1000 [================>.............] - ETA: 3:16 - loss: 2.9102 - regression_loss: 2.2060 - classification_loss: 0.7042
 568/1000 [================>.............] - ETA: 3:15 - loss: 2.9124 - regression_loss: 2.2071 - classification_loss: 0.7053
 569/1000 [================>.............] - ETA: 3:15 - loss: 2.9084 - regression_loss: 2.2032 - classification_loss: 0.7053
 570/1000 [================>.............] - ETA: 3:14 - loss: 2.9033 - regression_loss: 2.1993 - classification_loss: 0.7040
 571/1000 [================>.............] - ETA: 3:14 - loss: 2.9063 - regression_loss: 2.2021 - classification_loss: 0.7042
 572/1000 [================>.............] - ETA: 3:13 - loss: 2.9095 - regression_loss: 2.2044 - classification_loss: 0.7051
 573/1000 [================>.............] - ETA: 3:13 - loss: 2.9135 - regression_loss: 2.2065 - classification_loss: 0.7070
 574/1000 [================>.............] - ETA: 3:12 - loss: 2.9084 - regression_loss: 2.2026 - classification_loss: 0.7058
 575/1000 [================>.............] - ETA: 3:12 - loss: 2.9098 - regression_loss: 2.2038 - classification_loss: 0.7060
 576/1000 [================>.............] - ETA: 3:11 - loss: 2.9127 - regression_loss: 2.2051 - classification_loss: 0.7076
 577/1000 [================>.............] - ETA: 3:11 - loss: 2.9137 - regression_loss: 2.2065 - classification_loss: 0.7072
 578/1000 [================>.............] - ETA: 3:11 - loss: 2.9159 - regression_loss: 2.2091 - classification_loss: 0.7068
 579/1000 [================>.............] - ETA: 3:10 - loss: 2.9159 - regression_loss: 2.2082 - classification_loss: 0.7076
 580/1000 [================>.............] - ETA: 3:10 - loss: 2.9109 - regression_loss: 2.2044 - classification_loss: 0.7065
 581/1000 [================>.............] - ETA: 3:09 - loss: 2.9099 - regression_loss: 2.2036 - classification_loss: 0.7063
 582/1000 [================>.............] - ETA: 3:09 - loss: 2.9114 - regression_loss: 2.2048 - classification_loss: 0.7066
 583/1000 [================>.............] - ETA: 3:08 - loss: 2.9129 - regression_loss: 2.2064 - classification_loss: 0.7065
 584/1000 [================>.............] - ETA: 3:08 - loss: 2.9155 - regression_loss: 2.2081 - classification_loss: 0.7075
 585/1000 [================>.............] - ETA: 3:07 - loss: 2.9198 - regression_loss: 2.2108 - classification_loss: 0.7090
 586/1000 [================>.............] - ETA: 3:07 - loss: 2.9213 - regression_loss: 2.2123 - classification_loss: 0.7090
 587/1000 [================>.............] - ETA: 3:06 - loss: 2.9202 - regression_loss: 2.2118 - classification_loss: 0.7084
 588/1000 [================>.............] - ETA: 3:06 - loss: 2.9220 - regression_loss: 2.2135 - classification_loss: 0.7084
 589/1000 [================>.............] - ETA: 3:06 - loss: 2.9230 - regression_loss: 2.2143 - classification_loss: 0.7087
 590/1000 [================>.............] - ETA: 3:05 - loss: 2.9181 - regression_loss: 2.2105 - classification_loss: 0.7075
 591/1000 [================>.............] - ETA: 3:05 - loss: 2.9198 - regression_loss: 2.2117 - classification_loss: 0.7081
 592/1000 [================>.............] - ETA: 3:04 - loss: 2.9219 - regression_loss: 2.2134 - classification_loss: 0.7085
 593/1000 [================>.............] - ETA: 3:04 - loss: 2.9232 - regression_loss: 2.2146 - classification_loss: 0.7087
 594/1000 [================>.............] - ETA: 3:03 - loss: 2.9232 - regression_loss: 2.2150 - classification_loss: 0.7081
 595/1000 [================>.............] - ETA: 3:03 - loss: 2.9183 - regression_loss: 2.2113 - classification_loss: 0.7070
 596/1000 [================>.............] - ETA: 3:02 - loss: 2.9158 - regression_loss: 2.2076 - classification_loss: 0.7082
 597/1000 [================>.............] - ETA: 3:02 - loss: 2.9204 - regression_loss: 2.2105 - classification_loss: 0.7099
 598/1000 [================>.............] - ETA: 3:01 - loss: 2.9223 - regression_loss: 2.2124 - classification_loss: 0.7099
 599/1000 [================>.............] - ETA: 3:01 - loss: 2.9260 - regression_loss: 2.2152 - classification_loss: 0.7108
 600/1000 [=================>............] - ETA: 3:01 - loss: 2.9211 - regression_loss: 2.2116 - classification_loss: 0.7096
 601/1000 [=================>............] - ETA: 3:00 - loss: 2.9213 - regression_loss: 2.2114 - classification_loss: 0.7099
 602/1000 [=================>............] - ETA: 3:00 - loss: 2.9209 - regression_loss: 2.2118 - classification_loss: 0.7091
 603/1000 [=================>............] - ETA: 2:59 - loss: 2.9161 - regression_loss: 2.2082 - classification_loss: 0.7079
 604/1000 [=================>............] - ETA: 2:59 - loss: 2.9197 - regression_loss: 2.2114 - classification_loss: 0.7083
 605/1000 [=================>............] - ETA: 2:58 - loss: 2.9149 - regression_loss: 2.2078 - classification_loss: 0.7071
 606/1000 [=================>............] - ETA: 2:58 - loss: 2.9165 - regression_loss: 2.2098 - classification_loss: 0.7067
 607/1000 [=================>............] - ETA: 2:57 - loss: 2.9200 - regression_loss: 2.2128 - classification_loss: 0.7072
 608/1000 [=================>............] - ETA: 2:57 - loss: 2.9211 - regression_loss: 2.2135 - classification_loss: 0.7076
 609/1000 [=================>............] - ETA: 2:57 - loss: 2.9163 - regression_loss: 2.2099 - classification_loss: 0.7064
 610/1000 [=================>............] - ETA: 2:56 - loss: 2.9116 - regression_loss: 2.2063 - classification_loss: 0.7053
 611/1000 [=================>............] - ETA: 2:56 - loss: 2.9114 - regression_loss: 2.2066 - classification_loss: 0.7048
 612/1000 [=================>............] - ETA: 2:55 - loss: 2.9149 - regression_loss: 2.2096 - classification_loss: 0.7053
 613/1000 [=================>............] - ETA: 2:55 - loss: 2.9101 - regression_loss: 2.2060 - classification_loss: 0.7041
 614/1000 [=================>............] - ETA: 2:54 - loss: 2.9113 - regression_loss: 2.2071 - classification_loss: 0.7042
 615/1000 [=================>............] - ETA: 2:54 - loss: 2.9103 - regression_loss: 2.2066 - classification_loss: 0.7037
 616/1000 [=================>............] - ETA: 2:53 - loss: 2.9126 - regression_loss: 2.2074 - classification_loss: 0.7052
 617/1000 [=================>............] - ETA: 2:53 - loss: 2.9149 - regression_loss: 2.2087 - classification_loss: 0.7062
 618/1000 [=================>............] - ETA: 2:52 - loss: 2.9160 - regression_loss: 2.2087 - classification_loss: 0.7073
 619/1000 [=================>............] - ETA: 2:52 - loss: 2.9232 - regression_loss: 2.2136 - classification_loss: 0.7096
 620/1000 [=================>............] - ETA: 2:52 - loss: 2.9275 - regression_loss: 2.2167 - classification_loss: 0.7108
 621/1000 [=================>............] - ETA: 2:51 - loss: 2.9228 - regression_loss: 2.2132 - classification_loss: 0.7097
 622/1000 [=================>............] - ETA: 2:51 - loss: 2.9250 - regression_loss: 2.2139 - classification_loss: 0.7110
 623/1000 [=================>............] - ETA: 2:50 - loss: 2.9269 - regression_loss: 2.2161 - classification_loss: 0.7108
 624/1000 [=================>............] - ETA: 2:50 - loss: 2.9274 - regression_loss: 2.2165 - classification_loss: 0.7109
 625/1000 [=================>............] - ETA: 2:49 - loss: 2.9297 - regression_loss: 2.2186 - classification_loss: 0.7111
 626/1000 [=================>............] - ETA: 2:49 - loss: 2.9288 - regression_loss: 2.2181 - classification_loss: 0.7107
 627/1000 [=================>............] - ETA: 2:48 - loss: 2.9277 - regression_loss: 2.2176 - classification_loss: 0.7101
 628/1000 [=================>............] - ETA: 2:48 - loss: 2.9278 - regression_loss: 2.2179 - classification_loss: 0.7100
 629/1000 [=================>............] - ETA: 2:47 - loss: 2.9286 - regression_loss: 2.2193 - classification_loss: 0.7094
 630/1000 [=================>............] - ETA: 2:47 - loss: 2.9285 - regression_loss: 2.2195 - classification_loss: 0.7090
 631/1000 [=================>............] - ETA: 2:47 - loss: 2.9289 - regression_loss: 2.2206 - classification_loss: 0.7084
 632/1000 [=================>............] - ETA: 2:46 - loss: 2.9297 - regression_loss: 2.2219 - classification_loss: 0.7079
 633/1000 [=================>............] - ETA: 2:46 - loss: 2.9251 - regression_loss: 2.2183 - classification_loss: 0.7067
 634/1000 [==================>...........] - ETA: 2:45 - loss: 2.9205 - regression_loss: 2.2148 - classification_loss: 0.7056
 635/1000 [==================>...........] - ETA: 2:45 - loss: 2.9215 - regression_loss: 2.2161 - classification_loss: 0.7054
 636/1000 [==================>...........] - ETA: 2:44 - loss: 2.9169 - regression_loss: 2.2126 - classification_loss: 0.7043
 637/1000 [==================>...........] - ETA: 2:44 - loss: 2.9183 - regression_loss: 2.2133 - classification_loss: 0.7050
 638/1000 [==================>...........] - ETA: 2:43 - loss: 2.9138 - regression_loss: 2.2098 - classification_loss: 0.7039
 639/1000 [==================>...........] - ETA: 2:43 - loss: 2.9166 - regression_loss: 2.2113 - classification_loss: 0.7054
 640/1000 [==================>...........] - ETA: 2:42 - loss: 2.9155 - regression_loss: 2.2101 - classification_loss: 0.7054
 641/1000 [==================>...........] - ETA: 2:42 - loss: 2.9142 - regression_loss: 2.2089 - classification_loss: 0.7052
 642/1000 [==================>...........] - ETA: 2:42 - loss: 2.9139 - regression_loss: 2.2087 - classification_loss: 0.7052
 643/1000 [==================>...........] - ETA: 2:41 - loss: 2.9166 - regression_loss: 2.2114 - classification_loss: 0.7052
 644/1000 [==================>...........] - ETA: 2:41 - loss: 2.9173 - regression_loss: 2.2121 - classification_loss: 0.7052
 645/1000 [==================>...........] - ETA: 2:40 - loss: 2.9185 - regression_loss: 2.2131 - classification_loss: 0.7054
 646/1000 [==================>...........] - ETA: 2:40 - loss: 2.9140 - regression_loss: 2.2096 - classification_loss: 0.7044
 647/1000 [==================>...........] - ETA: 2:39 - loss: 2.9136 - regression_loss: 2.2088 - classification_loss: 0.7047
 648/1000 [==================>...........] - ETA: 2:39 - loss: 2.9134 - regression_loss: 2.2088 - classification_loss: 0.7045
 649/1000 [==================>...........] - ETA: 2:38 - loss: 2.9143 - regression_loss: 2.2090 - classification_loss: 0.7053
 650/1000 [==================>...........] - ETA: 2:38 - loss: 2.9191 - regression_loss: 2.2126 - classification_loss: 0.7065
 651/1000 [==================>...........] - ETA: 2:37 - loss: 2.9197 - regression_loss: 2.2132 - classification_loss: 0.7065
 652/1000 [==================>...........] - ETA: 2:37 - loss: 2.9195 - regression_loss: 2.2135 - classification_loss: 0.7060
 653/1000 [==================>...........] - ETA: 2:37 - loss: 2.9197 - regression_loss: 2.2138 - classification_loss: 0.7059
 654/1000 [==================>...........] - ETA: 2:36 - loss: 2.9192 - regression_loss: 2.2131 - classification_loss: 0.7062
 655/1000 [==================>...........] - ETA: 2:36 - loss: 2.9192 - regression_loss: 2.2135 - classification_loss: 0.7058
 656/1000 [==================>...........] - ETA: 2:35 - loss: 2.9224 - regression_loss: 2.2154 - classification_loss: 0.7069
 657/1000 [==================>...........] - ETA: 2:35 - loss: 2.9230 - regression_loss: 2.2161 - classification_loss: 0.7068
 658/1000 [==================>...........] - ETA: 2:34 - loss: 2.9223 - regression_loss: 2.2160 - classification_loss: 0.7063
 659/1000 [==================>...........] - ETA: 2:34 - loss: 2.9178 - regression_loss: 2.2126 - classification_loss: 0.7052
 660/1000 [==================>...........] - ETA: 2:33 - loss: 2.9184 - regression_loss: 2.2126 - classification_loss: 0.7057
 661/1000 [==================>...........] - ETA: 2:33 - loss: 2.9140 - regression_loss: 2.2093 - classification_loss: 0.7047
 662/1000 [==================>...........] - ETA: 2:33 - loss: 2.9126 - regression_loss: 2.2086 - classification_loss: 0.7040
 663/1000 [==================>...........] - ETA: 2:32 - loss: 2.9128 - regression_loss: 2.2088 - classification_loss: 0.7040
 664/1000 [==================>...........] - ETA: 2:32 - loss: 2.9162 - regression_loss: 2.2113 - classification_loss: 0.7049
 665/1000 [==================>...........] - ETA: 2:31 - loss: 2.9196 - regression_loss: 2.2139 - classification_loss: 0.7057
 666/1000 [==================>...........] - ETA: 2:31 - loss: 2.9187 - regression_loss: 2.2133 - classification_loss: 0.7054
 667/1000 [===================>..........] - ETA: 2:30 - loss: 2.9184 - regression_loss: 2.2134 - classification_loss: 0.7050
 668/1000 [===================>..........] - ETA: 2:30 - loss: 2.9182 - regression_loss: 2.2138 - classification_loss: 0.7045
 669/1000 [===================>..........] - ETA: 2:29 - loss: 2.9218 - regression_loss: 2.2153 - classification_loss: 0.7065
 670/1000 [===================>..........] - ETA: 2:29 - loss: 2.9174 - regression_loss: 2.2120 - classification_loss: 0.7055
 671/1000 [===================>..........] - ETA: 2:28 - loss: 2.9131 - regression_loss: 2.2087 - classification_loss: 0.7044
 672/1000 [===================>..........] - ETA: 2:28 - loss: 2.9140 - regression_loss: 2.2082 - classification_loss: 0.7058
 673/1000 [===================>..........] - ETA: 2:27 - loss: 2.9131 - regression_loss: 2.2078 - classification_loss: 0.7053
 674/1000 [===================>..........] - ETA: 2:27 - loss: 2.9110 - regression_loss: 2.2066 - classification_loss: 0.7045
 675/1000 [===================>..........] - ETA: 2:27 - loss: 2.9113 - regression_loss: 2.2069 - classification_loss: 0.7045
 676/1000 [===================>..........] - ETA: 2:26 - loss: 2.9116 - regression_loss: 2.2076 - classification_loss: 0.7040
 677/1000 [===================>..........] - ETA: 2:26 - loss: 2.9142 - regression_loss: 2.2098 - classification_loss: 0.7045
 678/1000 [===================>..........] - ETA: 2:25 - loss: 2.9138 - regression_loss: 2.2098 - classification_loss: 0.7040
 679/1000 [===================>..........] - ETA: 2:25 - loss: 2.9150 - regression_loss: 2.2110 - classification_loss: 0.7039
 680/1000 [===================>..........] - ETA: 2:24 - loss: 2.9144 - regression_loss: 2.2110 - classification_loss: 0.7034
 681/1000 [===================>..........] - ETA: 2:24 - loss: 2.9127 - regression_loss: 2.2098 - classification_loss: 0.7029
 682/1000 [===================>..........] - ETA: 2:23 - loss: 2.9142 - regression_loss: 2.2106 - classification_loss: 0.7037
 683/1000 [===================>..........] - ETA: 2:23 - loss: 2.9142 - regression_loss: 2.2108 - classification_loss: 0.7035
 684/1000 [===================>..........] - ETA: 2:23 - loss: 2.9150 - regression_loss: 2.2103 - classification_loss: 0.7048
 685/1000 [===================>..........] - ETA: 2:22 - loss: 2.9146 - regression_loss: 2.2100 - classification_loss: 0.7046
 686/1000 [===================>..........] - ETA: 2:22 - loss: 2.9152 - regression_loss: 2.2109 - classification_loss: 0.7043
 687/1000 [===================>..........] - ETA: 2:21 - loss: 2.9156 - regression_loss: 2.2115 - classification_loss: 0.7041
 688/1000 [===================>..........] - ETA: 2:21 - loss: 2.9160 - regression_loss: 2.2126 - classification_loss: 0.7034
 689/1000 [===================>..........] - ETA: 2:20 - loss: 2.9149 - regression_loss: 2.2121 - classification_loss: 0.7027
 690/1000 [===================>..........] - ETA: 2:20 - loss: 2.9157 - regression_loss: 2.2134 - classification_loss: 0.7023
 691/1000 [===================>..........] - ETA: 2:19 - loss: 2.9157 - regression_loss: 2.2136 - classification_loss: 0.7022
 692/1000 [===================>..........] - ETA: 2:19 - loss: 2.9184 - regression_loss: 2.2155 - classification_loss: 0.7029
 693/1000 [===================>..........] - ETA: 2:18 - loss: 2.9201 - regression_loss: 2.2160 - classification_loss: 0.7041
 694/1000 [===================>..........] - ETA: 2:18 - loss: 2.9205 - regression_loss: 2.2167 - classification_loss: 0.7038
 695/1000 [===================>..........] - ETA: 2:18 - loss: 2.9231 - regression_loss: 2.2177 - classification_loss: 0.7053
 696/1000 [===================>..........] - ETA: 2:17 - loss: 2.9233 - regression_loss: 2.2183 - classification_loss: 0.7050
 697/1000 [===================>..........] - ETA: 2:17 - loss: 2.9223 - regression_loss: 2.2176 - classification_loss: 0.7047
 698/1000 [===================>..........] - ETA: 2:16 - loss: 2.9181 - regression_loss: 2.2144 - classification_loss: 0.7037
 699/1000 [===================>..........] - ETA: 2:16 - loss: 2.9190 - regression_loss: 2.2156 - classification_loss: 0.7035
 700/1000 [====================>.........] - ETA: 2:15 - loss: 2.9149 - regression_loss: 2.2124 - classification_loss: 0.7025
 701/1000 [====================>.........] - ETA: 2:15 - loss: 2.9181 - regression_loss: 2.2151 - classification_loss: 0.7030
 702/1000 [====================>.........] - ETA: 2:14 - loss: 2.9140 - regression_loss: 2.2120 - classification_loss: 0.7020
 703/1000 [====================>.........] - ETA: 2:14 - loss: 2.9134 - regression_loss: 2.2115 - classification_loss: 0.7018
 704/1000 [====================>.........] - ETA: 2:13 - loss: 2.9162 - regression_loss: 2.2138 - classification_loss: 0.7024
 705/1000 [====================>.........] - ETA: 2:13 - loss: 2.9191 - regression_loss: 2.2169 - classification_loss: 0.7022
 706/1000 [====================>.........] - ETA: 2:13 - loss: 2.9218 - regression_loss: 2.2196 - classification_loss: 0.7022
 707/1000 [====================>.........] - ETA: 2:12 - loss: 2.9250 - regression_loss: 2.2216 - classification_loss: 0.7033
 708/1000 [====================>.........] - ETA: 2:12 - loss: 2.9209 - regression_loss: 2.2185 - classification_loss: 0.7024
 709/1000 [====================>.........] - ETA: 2:11 - loss: 2.9220 - regression_loss: 2.2197 - classification_loss: 0.7022
 710/1000 [====================>.........] - ETA: 2:11 - loss: 2.9207 - regression_loss: 2.2191 - classification_loss: 0.7016
 711/1000 [====================>.........] - ETA: 2:10 - loss: 2.9216 - regression_loss: 2.2201 - classification_loss: 0.7014
 712/1000 [====================>.........] - ETA: 2:10 - loss: 2.9219 - regression_loss: 2.2207 - classification_loss: 0.7012
 713/1000 [====================>.........] - ETA: 2:09 - loss: 2.9178 - regression_loss: 2.2176 - classification_loss: 0.7002
 714/1000 [====================>.........] - ETA: 2:09 - loss: 2.9137 - regression_loss: 2.2145 - classification_loss: 0.6993
 715/1000 [====================>.........] - ETA: 2:08 - loss: 2.9150 - regression_loss: 2.2159 - classification_loss: 0.6991
 716/1000 [====================>.........] - ETA: 2:08 - loss: 2.9133 - regression_loss: 2.2146 - classification_loss: 0.6987
 717/1000 [====================>.........] - ETA: 2:08 - loss: 2.9129 - regression_loss: 2.2145 - classification_loss: 0.6983
 718/1000 [====================>.........] - ETA: 2:07 - loss: 2.9137 - regression_loss: 2.2154 - classification_loss: 0.6983
 719/1000 [====================>.........] - ETA: 2:07 - loss: 2.9160 - regression_loss: 2.2174 - classification_loss: 0.6986
 720/1000 [====================>.........] - ETA: 2:06 - loss: 2.9168 - regression_loss: 2.2183 - classification_loss: 0.6985
 721/1000 [====================>.........] - ETA: 2:06 - loss: 2.9152 - regression_loss: 2.2173 - classification_loss: 0.6980
 722/1000 [====================>.........] - ETA: 2:05 - loss: 2.9153 - regression_loss: 2.2174 - classification_loss: 0.6979
 723/1000 [====================>.........] - ETA: 2:05 - loss: 2.9151 - regression_loss: 2.2173 - classification_loss: 0.6977
 724/1000 [====================>.........] - ETA: 2:04 - loss: 2.9161 - regression_loss: 2.2173 - classification_loss: 0.6988
 725/1000 [====================>.........] - ETA: 2:04 - loss: 2.9183 - regression_loss: 2.2193 - classification_loss: 0.6989
 726/1000 [====================>.........] - ETA: 2:03 - loss: 2.9175 - regression_loss: 2.2191 - classification_loss: 0.6983
 727/1000 [====================>.........] - ETA: 2:03 - loss: 2.9134 - regression_loss: 2.2161 - classification_loss: 0.6974
 728/1000 [====================>.........] - ETA: 2:03 - loss: 2.9136 - regression_loss: 2.2166 - classification_loss: 0.6970
 729/1000 [====================>.........] - ETA: 2:02 - loss: 2.9151 - regression_loss: 2.2184 - classification_loss: 0.6967
 730/1000 [====================>.........] - ETA: 2:02 - loss: 2.9111 - regression_loss: 2.2154 - classification_loss: 0.6957
 731/1000 [====================>.........] - ETA: 2:01 - loss: 2.9116 - regression_loss: 2.2154 - classification_loss: 0.6961
 732/1000 [====================>.........] - ETA: 2:01 - loss: 2.9127 - regression_loss: 2.2169 - classification_loss: 0.6958
 733/1000 [====================>.........] - ETA: 2:00 - loss: 2.9121 - regression_loss: 2.2167 - classification_loss: 0.6954
 734/1000 [=====================>........] - ETA: 2:00 - loss: 2.9081 - regression_loss: 2.2137 - classification_loss: 0.6944
 735/1000 [=====================>........] - ETA: 1:59 - loss: 2.9087 - regression_loss: 2.2142 - classification_loss: 0.6945
 736/1000 [=====================>........] - ETA: 1:59 - loss: 2.9048 - regression_loss: 2.2112 - classification_loss: 0.6936
 737/1000 [=====================>........] - ETA: 1:58 - loss: 2.9044 - regression_loss: 2.2112 - classification_loss: 0.6932
 738/1000 [=====================>........] - ETA: 1:58 - loss: 2.9039 - regression_loss: 2.2113 - classification_loss: 0.6926
 739/1000 [=====================>........] - ETA: 1:58 - loss: 2.9060 - regression_loss: 2.2126 - classification_loss: 0.6935
 740/1000 [=====================>........] - ETA: 1:57 - loss: 2.9023 - regression_loss: 2.2096 - classification_loss: 0.6927
 741/1000 [=====================>........] - ETA: 1:57 - loss: 2.9041 - regression_loss: 2.2107 - classification_loss: 0.6934
 742/1000 [=====================>........] - ETA: 1:56 - loss: 2.9002 - regression_loss: 2.2077 - classification_loss: 0.6925
 743/1000 [=====================>........] - ETA: 1:56 - loss: 2.8963 - regression_loss: 2.2047 - classification_loss: 0.6916
 744/1000 [=====================>........] - ETA: 1:55 - loss: 2.8955 - regression_loss: 2.2045 - classification_loss: 0.6910
 745/1000 [=====================>........] - ETA: 1:55 - loss: 2.8957 - regression_loss: 2.2049 - classification_loss: 0.6907
 746/1000 [=====================>........] - ETA: 1:54 - loss: 2.8918 - regression_loss: 2.2020 - classification_loss: 0.6898
 747/1000 [=====================>........] - ETA: 1:54 - loss: 2.8940 - regression_loss: 2.2029 - classification_loss: 0.6911
 748/1000 [=====================>........] - ETA: 1:53 - loss: 2.8948 - regression_loss: 2.2042 - classification_loss: 0.6906
 749/1000 [=====================>........] - ETA: 1:53 - loss: 2.8958 - regression_loss: 2.2053 - classification_loss: 0.6905
 750/1000 [=====================>........] - ETA: 1:53 - loss: 2.8988 - regression_loss: 2.2069 - classification_loss: 0.6919
 751/1000 [=====================>........] - ETA: 1:52 - loss: 2.8993 - regression_loss: 2.2079 - classification_loss: 0.6914
 752/1000 [=====================>........] - ETA: 1:52 - loss: 2.8993 - regression_loss: 2.2079 - classification_loss: 0.6913
 753/1000 [=====================>........] - ETA: 1:51 - loss: 2.8987 - regression_loss: 2.2075 - classification_loss: 0.6911
 754/1000 [=====================>........] - ETA: 1:51 - loss: 2.8949 - regression_loss: 2.2046 - classification_loss: 0.6903
 755/1000 [=====================>........] - ETA: 1:50 - loss: 2.8967 - regression_loss: 2.2050 - classification_loss: 0.6917
 756/1000 [=====================>........] - ETA: 1:50 - loss: 2.8992 - regression_loss: 2.2074 - classification_loss: 0.6918
 757/1000 [=====================>........] - ETA: 1:49 - loss: 2.9029 - regression_loss: 2.2097 - classification_loss: 0.6932
 758/1000 [=====================>........] - ETA: 1:49 - loss: 2.8991 - regression_loss: 2.2068 - classification_loss: 0.6923
 759/1000 [=====================>........] - ETA: 1:48 - loss: 2.9005 - regression_loss: 2.2078 - classification_loss: 0.6926
 760/1000 [=====================>........] - ETA: 1:48 - loss: 2.8967 - regression_loss: 2.2049 - classification_loss: 0.6917
 761/1000 [=====================>........] - ETA: 1:48 - loss: 2.8929 - regression_loss: 2.2020 - classification_loss: 0.6908
 762/1000 [=====================>........] - ETA: 1:47 - loss: 2.8936 - regression_loss: 2.2031 - classification_loss: 0.6905
 763/1000 [=====================>........] - ETA: 1:47 - loss: 2.8952 - regression_loss: 2.2034 - classification_loss: 0.6917
 764/1000 [=====================>........] - ETA: 1:46 - loss: 2.8943 - regression_loss: 2.2029 - classification_loss: 0.6914
 765/1000 [=====================>........] - ETA: 1:46 - loss: 2.8937 - regression_loss: 2.2024 - classification_loss: 0.6913
 766/1000 [=====================>........] - ETA: 1:45 - loss: 2.8899 - regression_loss: 2.1995 - classification_loss: 0.6904
 767/1000 [======================>.......] - ETA: 1:45 - loss: 2.8861 - regression_loss: 2.1966 - classification_loss: 0.6895
 768/1000 [======================>.......] - ETA: 1:44 - loss: 2.8840 - regression_loss: 2.1952 - classification_loss: 0.6888
 769/1000 [======================>.......] - ETA: 1:44 - loss: 2.8854 - regression_loss: 2.1960 - classification_loss: 0.6894
 770/1000 [======================>.......] - ETA: 1:44 - loss: 2.8866 - regression_loss: 2.1965 - classification_loss: 0.6901
 771/1000 [======================>.......] - ETA: 1:43 - loss: 2.8829 - regression_loss: 2.1937 - classification_loss: 0.6892
 772/1000 [======================>.......] - ETA: 1:43 - loss: 2.8834 - regression_loss: 2.1943 - classification_loss: 0.6890
 773/1000 [======================>.......] - ETA: 1:42 - loss: 2.8858 - regression_loss: 2.1951 - classification_loss: 0.6907
 774/1000 [======================>.......] - ETA: 1:42 - loss: 2.8878 - regression_loss: 2.1959 - classification_loss: 0.6919
 775/1000 [======================>.......] - ETA: 1:41 - loss: 2.8880 - regression_loss: 2.1963 - classification_loss: 0.6917
 776/1000 [======================>.......] - ETA: 1:41 - loss: 2.8905 - regression_loss: 2.1972 - classification_loss: 0.6933
 777/1000 [======================>.......] - ETA: 1:40 - loss: 2.8868 - regression_loss: 2.1944 - classification_loss: 0.6924
 778/1000 [======================>.......] - ETA: 1:40 - loss: 2.8900 - regression_loss: 2.1964 - classification_loss: 0.6936
 779/1000 [======================>.......] - ETA: 1:39 - loss: 2.8921 - regression_loss: 2.1981 - classification_loss: 0.6940
 780/1000 [======================>.......] - ETA: 1:39 - loss: 2.8948 - regression_loss: 2.1993 - classification_loss: 0.6956
 781/1000 [======================>.......] - ETA: 1:39 - loss: 2.8967 - regression_loss: 2.2007 - classification_loss: 0.6960
 782/1000 [======================>.......] - ETA: 1:38 - loss: 2.8957 - regression_loss: 2.2002 - classification_loss: 0.6955
 783/1000 [======================>.......] - ETA: 1:38 - loss: 2.8999 - regression_loss: 2.2027 - classification_loss: 0.6972
 784/1000 [======================>.......] - ETA: 1:37 - loss: 2.8995 - regression_loss: 2.2025 - classification_loss: 0.6970
 785/1000 [======================>.......] - ETA: 1:37 - loss: 2.9008 - regression_loss: 2.2036 - classification_loss: 0.6972
 786/1000 [======================>.......] - ETA: 1:36 - loss: 2.9037 - regression_loss: 2.2008 - classification_loss: 0.7030
 787/1000 [======================>.......] - ETA: 1:36 - loss: 2.9069 - regression_loss: 2.2035 - classification_loss: 0.7035
 788/1000 [======================>.......] - ETA: 1:35 - loss: 2.9078 - regression_loss: 2.2045 - classification_loss: 0.7033
 789/1000 [======================>.......] - ETA: 1:35 - loss: 2.9097 - regression_loss: 2.2053 - classification_loss: 0.7043
 790/1000 [======================>.......] - ETA: 1:34 - loss: 2.9132 - regression_loss: 2.2076 - classification_loss: 0.7056
 791/1000 [======================>.......] - ETA: 1:34 - loss: 2.9130 - regression_loss: 2.2079 - classification_loss: 0.7052
 792/1000 [======================>.......] - ETA: 1:34 - loss: 2.9136 - regression_loss: 2.2087 - classification_loss: 0.7048
 793/1000 [======================>.......] - ETA: 1:33 - loss: 2.9146 - regression_loss: 2.2099 - classification_loss: 0.7047
 794/1000 [======================>.......] - ETA: 1:33 - loss: 2.9164 - regression_loss: 2.2115 - classification_loss: 0.7049
 795/1000 [======================>.......] - ETA: 1:32 - loss: 2.9167 - regression_loss: 2.2116 - classification_loss: 0.7051
 796/1000 [======================>.......] - ETA: 1:32 - loss: 2.9178 - regression_loss: 2.2122 - classification_loss: 0.7055
 797/1000 [======================>.......] - ETA: 1:31 - loss: 2.9173 - regression_loss: 2.2123 - classification_loss: 0.7049
 798/1000 [======================>.......] - ETA: 1:31 - loss: 2.9178 - regression_loss: 2.2122 - classification_loss: 0.7055
 799/1000 [======================>.......] - ETA: 1:30 - loss: 2.9193 - regression_loss: 2.2130 - classification_loss: 0.7063
 800/1000 [=======================>......] - ETA: 1:30 - loss: 2.9157 - regression_loss: 2.2102 - classification_loss: 0.7055
 801/1000 [=======================>......] - ETA: 1:29 - loss: 2.9178 - regression_loss: 2.2123 - classification_loss: 0.7055
 802/1000 [=======================>......] - ETA: 1:29 - loss: 2.9180 - regression_loss: 2.2126 - classification_loss: 0.7054
 803/1000 [=======================>......] - ETA: 1:29 - loss: 2.9176 - regression_loss: 2.2124 - classification_loss: 0.7052
 804/1000 [=======================>......] - ETA: 1:28 - loss: 2.9175 - regression_loss: 2.2127 - classification_loss: 0.7048
 805/1000 [=======================>......] - ETA: 1:28 - loss: 2.9179 - regression_loss: 2.2136 - classification_loss: 0.7042
 806/1000 [=======================>......] - ETA: 1:27 - loss: 2.9199 - regression_loss: 2.2150 - classification_loss: 0.7050
 807/1000 [=======================>......] - ETA: 1:27 - loss: 2.9227 - regression_loss: 2.2169 - classification_loss: 0.7058
 808/1000 [=======================>......] - ETA: 1:26 - loss: 2.9224 - regression_loss: 2.2168 - classification_loss: 0.7056
 809/1000 [=======================>......] - ETA: 1:26 - loss: 2.9189 - regression_loss: 2.2140 - classification_loss: 0.7049
 810/1000 [=======================>......] - ETA: 1:25 - loss: 2.9210 - regression_loss: 2.2149 - classification_loss: 0.7061
 811/1000 [=======================>......] - ETA: 1:25 - loss: 2.9206 - regression_loss: 2.2150 - classification_loss: 0.7056
 812/1000 [=======================>......] - ETA: 1:25 - loss: 2.9223 - regression_loss: 2.2170 - classification_loss: 0.7053
 813/1000 [=======================>......] - ETA: 1:24 - loss: 2.9187 - regression_loss: 2.2143 - classification_loss: 0.7044
 814/1000 [=======================>......] - ETA: 1:24 - loss: 2.9169 - regression_loss: 2.2131 - classification_loss: 0.7038
 815/1000 [=======================>......] - ETA: 1:23 - loss: 2.9133 - regression_loss: 2.2104 - classification_loss: 0.7029
 816/1000 [=======================>......] - ETA: 1:23 - loss: 2.9139 - regression_loss: 2.2114 - classification_loss: 0.7026
 817/1000 [=======================>......] - ETA: 1:22 - loss: 2.9155 - regression_loss: 2.2127 - classification_loss: 0.7028
 818/1000 [=======================>......] - ETA: 1:22 - loss: 2.9157 - regression_loss: 2.2131 - classification_loss: 0.7026
 819/1000 [=======================>......] - ETA: 1:21 - loss: 2.9121 - regression_loss: 2.2104 - classification_loss: 0.7017
 820/1000 [=======================>......] - ETA: 1:21 - loss: 2.9086 - regression_loss: 2.2077 - classification_loss: 0.7009
 821/1000 [=======================>......] - ETA: 1:20 - loss: 2.9077 - regression_loss: 2.2073 - classification_loss: 0.7004
 822/1000 [=======================>......] - ETA: 1:20 - loss: 2.9070 - regression_loss: 2.2066 - classification_loss: 0.7004
 823/1000 [=======================>......] - ETA: 1:20 - loss: 2.9035 - regression_loss: 2.2040 - classification_loss: 0.6995
 824/1000 [=======================>......] - ETA: 1:19 - loss: 2.9000 - regression_loss: 2.2013 - classification_loss: 0.6987
 825/1000 [=======================>......] - ETA: 1:19 - loss: 2.8993 - regression_loss: 2.2011 - classification_loss: 0.6982
 826/1000 [=======================>......] - ETA: 1:18 - loss: 2.8958 - regression_loss: 2.1984 - classification_loss: 0.6973
 827/1000 [=======================>......] - ETA: 1:18 - loss: 2.8923 - regression_loss: 2.1958 - classification_loss: 0.6965
 828/1000 [=======================>......] - ETA: 1:17 - loss: 2.8913 - regression_loss: 2.1953 - classification_loss: 0.6960
 829/1000 [=======================>......] - ETA: 1:17 - loss: 2.8878 - regression_loss: 2.1926 - classification_loss: 0.6952
 830/1000 [=======================>......] - ETA: 1:16 - loss: 2.8890 - regression_loss: 2.1937 - classification_loss: 0.6953
 831/1000 [=======================>......] - ETA: 1:16 - loss: 2.8855 - regression_loss: 2.1911 - classification_loss: 0.6944
 832/1000 [=======================>......] - ETA: 1:15 - loss: 2.8844 - regression_loss: 2.1904 - classification_loss: 0.6940
 833/1000 [=======================>......] - ETA: 1:15 - loss: 2.8850 - regression_loss: 2.1912 - classification_loss: 0.6938
 834/1000 [========================>.....] - ETA: 1:15 - loss: 2.8866 - regression_loss: 2.1928 - classification_loss: 0.6938
 835/1000 [========================>.....] - ETA: 1:14 - loss: 2.8864 - regression_loss: 2.1921 - classification_loss: 0.6943
 836/1000 [========================>.....] - ETA: 1:14 - loss: 2.8865 - regression_loss: 2.1925 - classification_loss: 0.6940
 837/1000 [========================>.....] - ETA: 1:13 - loss: 2.8894 - regression_loss: 2.1941 - classification_loss: 0.6952
 838/1000 [========================>.....] - ETA: 1:13 - loss: 2.8859 - regression_loss: 2.1915 - classification_loss: 0.6944
 839/1000 [========================>.....] - ETA: 1:12 - loss: 2.8897 - regression_loss: 2.1942 - classification_loss: 0.6956
 840/1000 [========================>.....] - ETA: 1:12 - loss: 2.8863 - regression_loss: 2.1915 - classification_loss: 0.6948
 841/1000 [========================>.....] - ETA: 1:11 - loss: 2.8877 - regression_loss: 2.1926 - classification_loss: 0.6951
 842/1000 [========================>.....] - ETA: 1:11 - loss: 2.8910 - regression_loss: 2.1949 - classification_loss: 0.6961
 843/1000 [========================>.....] - ETA: 1:11 - loss: 2.8938 - regression_loss: 2.1961 - classification_loss: 0.6977
 844/1000 [========================>.....] - ETA: 1:10 - loss: 2.8903 - regression_loss: 2.1935 - classification_loss: 0.6968
 845/1000 [========================>.....] - ETA: 1:10 - loss: 2.8914 - regression_loss: 2.1947 - classification_loss: 0.6967
 846/1000 [========================>.....] - ETA: 1:09 - loss: 2.8879 - regression_loss: 2.1921 - classification_loss: 0.6959
 847/1000 [========================>.....] - ETA: 1:09 - loss: 2.8845 - regression_loss: 2.1895 - classification_loss: 0.6950
 848/1000 [========================>.....] - ETA: 1:08 - loss: 2.8811 - regression_loss: 2.1869 - classification_loss: 0.6942
 849/1000 [========================>.....] - ETA: 1:08 - loss: 2.8826 - regression_loss: 2.1869 - classification_loss: 0.6956
 850/1000 [========================>.....] - ETA: 1:07 - loss: 2.8820 - regression_loss: 2.1871 - classification_loss: 0.6950
 851/1000 [========================>.....] - ETA: 1:07 - loss: 2.8807 - regression_loss: 2.1859 - classification_loss: 0.6949
 852/1000 [========================>.....] - ETA: 1:06 - loss: 2.8825 - regression_loss: 2.1878 - classification_loss: 0.6947
 853/1000 [========================>.....] - ETA: 1:06 - loss: 2.8845 - regression_loss: 2.1879 - classification_loss: 0.6966
 854/1000 [========================>.....] - ETA: 1:06 - loss: 2.8880 - regression_loss: 2.1900 - classification_loss: 0.6979
 855/1000 [========================>.....] - ETA: 1:05 - loss: 2.8902 - regression_loss: 2.1910 - classification_loss: 0.6992
 856/1000 [========================>.....] - ETA: 1:05 - loss: 2.8868 - regression_loss: 2.1884 - classification_loss: 0.6984
 857/1000 [========================>.....] - ETA: 1:04 - loss: 2.8866 - regression_loss: 2.1880 - classification_loss: 0.6986
 858/1000 [========================>.....] - ETA: 1:04 - loss: 2.8869 - regression_loss: 2.1873 - classification_loss: 0.6995
 859/1000 [========================>.....] - ETA: 1:03 - loss: 2.8856 - regression_loss: 2.1866 - classification_loss: 0.6990
 860/1000 [========================>.....] - ETA: 1:03 - loss: 2.8867 - regression_loss: 2.1874 - classification_loss: 0.6993
 861/1000 [========================>.....] - ETA: 1:02 - loss: 2.8834 - regression_loss: 2.1849 - classification_loss: 0.6985
 862/1000 [========================>.....] - ETA: 1:02 - loss: 2.8831 - regression_loss: 2.1845 - classification_loss: 0.6986
 863/1000 [========================>.....] - ETA: 1:01 - loss: 2.8828 - regression_loss: 2.1842 - classification_loss: 0.6986
 864/1000 [========================>.....] - ETA: 1:01 - loss: 2.8821 - regression_loss: 2.1839 - classification_loss: 0.6982
 865/1000 [========================>.....] - ETA: 1:01 - loss: 2.8851 - regression_loss: 2.1851 - classification_loss: 0.7000
 866/1000 [========================>.....] - ETA: 1:00 - loss: 2.8880 - regression_loss: 2.1872 - classification_loss: 0.7008
 867/1000 [=========================>....] - ETA: 1:00 - loss: 2.8848 - regression_loss: 2.1847 - classification_loss: 0.7001
 868/1000 [=========================>....] - ETA: 59s - loss: 2.8872 - regression_loss: 2.1860 - classification_loss: 0.7011 
 869/1000 [=========================>....] - ETA: 59s - loss: 2.8886 - regression_loss: 2.1872 - classification_loss: 0.7014
 870/1000 [=========================>....] - ETA: 58s - loss: 2.8897 - regression_loss: 2.1881 - classification_loss: 0.7016
 871/1000 [=========================>....] - ETA: 58s - loss: 2.8902 - regression_loss: 2.1889 - classification_loss: 0.7012
 872/1000 [=========================>....] - ETA: 57s - loss: 2.8869 - regression_loss: 2.1864 - classification_loss: 0.7004
 873/1000 [=========================>....] - ETA: 57s - loss: 2.8858 - regression_loss: 2.1859 - classification_loss: 0.6999
 874/1000 [=========================>....] - ETA: 56s - loss: 2.8825 - regression_loss: 2.1834 - classification_loss: 0.6991
 875/1000 [=========================>....] - ETA: 56s - loss: 2.8831 - regression_loss: 2.1843 - classification_loss: 0.6988
 876/1000 [=========================>....] - ETA: 56s - loss: 2.8798 - regression_loss: 2.1818 - classification_loss: 0.6980
 877/1000 [=========================>....] - ETA: 55s - loss: 2.8800 - regression_loss: 2.1819 - classification_loss: 0.6981
 878/1000 [=========================>....] - ETA: 55s - loss: 2.8817 - regression_loss: 2.1828 - classification_loss: 0.6989
 879/1000 [=========================>....] - ETA: 54s - loss: 2.8814 - regression_loss: 2.1828 - classification_loss: 0.6986
 880/1000 [=========================>....] - ETA: 54s - loss: 2.8823 - regression_loss: 2.1836 - classification_loss: 0.6988
 881/1000 [=========================>....] - ETA: 53s - loss: 2.8836 - regression_loss: 2.1850 - classification_loss: 0.6986
 882/1000 [=========================>....] - ETA: 53s - loss: 2.8838 - regression_loss: 2.1854 - classification_loss: 0.6984
 883/1000 [=========================>....] - ETA: 52s - loss: 2.8853 - regression_loss: 2.1866 - classification_loss: 0.6987
 884/1000 [=========================>....] - ETA: 52s - loss: 2.8821 - regression_loss: 2.1841 - classification_loss: 0.6980
 885/1000 [=========================>....] - ETA: 52s - loss: 2.8789 - regression_loss: 2.1817 - classification_loss: 0.6972
 886/1000 [=========================>....] - ETA: 51s - loss: 2.8803 - regression_loss: 2.1827 - classification_loss: 0.6975
 887/1000 [=========================>....] - ETA: 51s - loss: 2.8821 - regression_loss: 2.1845 - classification_loss: 0.6977
 888/1000 [=========================>....] - ETA: 50s - loss: 2.8789 - regression_loss: 2.1820 - classification_loss: 0.6969
 889/1000 [=========================>....] - ETA: 50s - loss: 2.8796 - regression_loss: 2.1831 - classification_loss: 0.6965
 890/1000 [=========================>....] - ETA: 49s - loss: 2.8801 - regression_loss: 2.1834 - classification_loss: 0.6967
 891/1000 [=========================>....] - ETA: 49s - loss: 2.8816 - regression_loss: 2.1851 - classification_loss: 0.6965
 892/1000 [=========================>....] - ETA: 48s - loss: 2.8836 - regression_loss: 2.1861 - classification_loss: 0.6975
 893/1000 [=========================>....] - ETA: 48s - loss: 2.8846 - regression_loss: 2.1869 - classification_loss: 0.6977
 894/1000 [=========================>....] - ETA: 47s - loss: 2.8840 - regression_loss: 2.1861 - classification_loss: 0.6978
 895/1000 [=========================>....] - ETA: 47s - loss: 2.8833 - regression_loss: 2.1858 - classification_loss: 0.6975
 896/1000 [=========================>....] - ETA: 47s - loss: 2.8801 - regression_loss: 2.1833 - classification_loss: 0.6967
 897/1000 [=========================>....] - ETA: 46s - loss: 2.8812 - regression_loss: 2.1840 - classification_loss: 0.6972
 898/1000 [=========================>....] - ETA: 46s - loss: 2.8819 - regression_loss: 2.1845 - classification_loss: 0.6974
 899/1000 [=========================>....] - ETA: 45s - loss: 2.8823 - regression_loss: 2.1850 - classification_loss: 0.6973
 900/1000 [==========================>...] - ETA: 45s - loss: 2.8827 - regression_loss: 2.1852 - classification_loss: 0.6975
 901/1000 [==========================>...] - ETA: 44s - loss: 2.8841 - regression_loss: 2.1856 - classification_loss: 0.6985
 902/1000 [==========================>...] - ETA: 44s - loss: 2.8873 - regression_loss: 2.1877 - classification_loss: 0.6996
 903/1000 [==========================>...] - ETA: 43s - loss: 2.8868 - regression_loss: 2.1876 - classification_loss: 0.6992
 904/1000 [==========================>...] - ETA: 43s - loss: 2.8874 - regression_loss: 2.1884 - classification_loss: 0.6990
 905/1000 [==========================>...] - ETA: 42s - loss: 2.8918 - regression_loss: 2.1919 - classification_loss: 0.6999
 906/1000 [==========================>...] - ETA: 42s - loss: 2.8886 - regression_loss: 2.1895 - classification_loss: 0.6991
 907/1000 [==========================>...] - ETA: 42s - loss: 2.8894 - regression_loss: 2.1899 - classification_loss: 0.6995
 908/1000 [==========================>...] - ETA: 41s - loss: 2.8896 - regression_loss: 2.1903 - classification_loss: 0.6993
 909/1000 [==========================>...] - ETA: 41s - loss: 2.8918 - regression_loss: 2.1916 - classification_loss: 0.7002
 910/1000 [==========================>...] - ETA: 40s - loss: 2.8933 - regression_loss: 2.1933 - classification_loss: 0.7001
 911/1000 [==========================>...] - ETA: 40s - loss: 2.8916 - regression_loss: 2.1921 - classification_loss: 0.6996
 912/1000 [==========================>...] - ETA: 39s - loss: 2.8935 - regression_loss: 2.1937 - classification_loss: 0.6998
 913/1000 [==========================>...] - ETA: 39s - loss: 2.8962 - regression_loss: 2.1964 - classification_loss: 0.6998
 914/1000 [==========================>...] - ETA: 38s - loss: 2.8931 - regression_loss: 2.1940 - classification_loss: 0.6990
 915/1000 [==========================>...] - ETA: 38s - loss: 2.8945 - regression_loss: 2.1945 - classification_loss: 0.7000
 916/1000 [==========================>...] - ETA: 37s - loss: 2.8950 - regression_loss: 2.1945 - classification_loss: 0.7005
 917/1000 [==========================>...] - ETA: 37s - loss: 2.8956 - regression_loss: 2.1952 - classification_loss: 0.7003
 918/1000 [==========================>...] - ETA: 37s - loss: 2.8924 - regression_loss: 2.1928 - classification_loss: 0.6996
 919/1000 [==========================>...] - ETA: 36s - loss: 2.8934 - regression_loss: 2.1935 - classification_loss: 0.6999
 920/1000 [==========================>...] - ETA: 36s - loss: 2.8932 - regression_loss: 2.1929 - classification_loss: 0.7003
 921/1000 [==========================>...] - ETA: 35s - loss: 2.8940 - regression_loss: 2.1938 - classification_loss: 0.7001
 922/1000 [==========================>...] - ETA: 35s - loss: 2.8928 - regression_loss: 2.1931 - classification_loss: 0.6997
 923/1000 [==========================>...] - ETA: 34s - loss: 2.8929 - regression_loss: 2.1934 - classification_loss: 0.6995
 924/1000 [==========================>...] - ETA: 34s - loss: 2.8897 - regression_loss: 2.1910 - classification_loss: 0.6987
 925/1000 [==========================>...] - ETA: 33s - loss: 2.8920 - regression_loss: 2.1921 - classification_loss: 0.6999
 926/1000 [==========================>...] - ETA: 33s - loss: 2.8888 - regression_loss: 2.1897 - classification_loss: 0.6991
 927/1000 [==========================>...] - ETA: 33s - loss: 2.8893 - regression_loss: 2.1906 - classification_loss: 0.6987
 928/1000 [==========================>...] - ETA: 32s - loss: 2.8898 - regression_loss: 2.1915 - classification_loss: 0.6983
 929/1000 [==========================>...] - ETA: 32s - loss: 2.8915 - regression_loss: 2.1925 - classification_loss: 0.6990
 930/1000 [==========================>...] - ETA: 31s - loss: 2.8884 - regression_loss: 2.1901 - classification_loss: 0.6983
 931/1000 [==========================>...] - ETA: 31s - loss: 2.8853 - regression_loss: 2.1877 - classification_loss: 0.6975
 932/1000 [==========================>...] - ETA: 30s - loss: 2.8864 - regression_loss: 2.1882 - classification_loss: 0.6982
 933/1000 [==========================>...] - ETA: 30s - loss: 2.8879 - regression_loss: 2.1898 - classification_loss: 0.6981
 934/1000 [===========================>..] - ETA: 29s - loss: 2.8848 - regression_loss: 2.1874 - classification_loss: 0.6974
 935/1000 [===========================>..] - ETA: 29s - loss: 2.8835 - regression_loss: 2.1864 - classification_loss: 0.6971
 936/1000 [===========================>..] - ETA: 28s - loss: 2.8849 - regression_loss: 2.1870 - classification_loss: 0.6979
 937/1000 [===========================>..] - ETA: 28s - loss: 2.8860 - regression_loss: 2.1883 - classification_loss: 0.6977
 938/1000 [===========================>..] - ETA: 28s - loss: 2.8858 - regression_loss: 2.1885 - classification_loss: 0.6974
 939/1000 [===========================>..] - ETA: 27s - loss: 2.8873 - regression_loss: 2.1896 - classification_loss: 0.6977
 940/1000 [===========================>..] - ETA: 27s - loss: 2.8870 - regression_loss: 2.1896 - classification_loss: 0.6974
 941/1000 [===========================>..] - ETA: 26s - loss: 2.8887 - regression_loss: 2.1913 - classification_loss: 0.6975
 942/1000 [===========================>..] - ETA: 26s - loss: 2.8889 - regression_loss: 2.1918 - classification_loss: 0.6972
 943/1000 [===========================>..] - ETA: 25s - loss: 2.8882 - regression_loss: 2.1914 - classification_loss: 0.6968
 944/1000 [===========================>..] - ETA: 25s - loss: 2.8852 - regression_loss: 2.1891 - classification_loss: 0.6961
 945/1000 [===========================>..] - ETA: 24s - loss: 2.8821 - regression_loss: 2.1868 - classification_loss: 0.6953
 946/1000 [===========================>..] - ETA: 24s - loss: 2.8831 - regression_loss: 2.1876 - classification_loss: 0.6956
 947/1000 [===========================>..] - ETA: 23s - loss: 2.8848 - regression_loss: 2.1884 - classification_loss: 0.6964
 948/1000 [===========================>..] - ETA: 23s - loss: 2.8818 - regression_loss: 2.1861 - classification_loss: 0.6957
 949/1000 [===========================>..] - ETA: 23s - loss: 2.8787 - regression_loss: 2.1838 - classification_loss: 0.6949
 950/1000 [===========================>..] - ETA: 22s - loss: 2.8805 - regression_loss: 2.1815 - classification_loss: 0.6991
 951/1000 [===========================>..] - ETA: 22s - loss: 2.8775 - regression_loss: 2.1792 - classification_loss: 0.6983
 952/1000 [===========================>..] - ETA: 21s - loss: 2.8800 - regression_loss: 2.1812 - classification_loss: 0.6988
 953/1000 [===========================>..] - ETA: 21s - loss: 2.8770 - regression_loss: 2.1789 - classification_loss: 0.6981
 954/1000 [===========================>..] - ETA: 20s - loss: 2.8798 - regression_loss: 2.1805 - classification_loss: 0.6994
 955/1000 [===========================>..] - ETA: 20s - loss: 2.8796 - regression_loss: 2.1807 - classification_loss: 0.6989
 956/1000 [===========================>..] - ETA: 19s - loss: 2.8801 - regression_loss: 2.1815 - classification_loss: 0.6986
 957/1000 [===========================>..] - ETA: 19s - loss: 2.8818 - regression_loss: 2.1822 - classification_loss: 0.6996
 958/1000 [===========================>..] - ETA: 18s - loss: 2.8835 - regression_loss: 2.1837 - classification_loss: 0.6998
 959/1000 [===========================>..] - ETA: 18s - loss: 2.8839 - regression_loss: 2.1839 - classification_loss: 0.7000
 960/1000 [===========================>..] - ETA: 18s - loss: 2.8865 - regression_loss: 2.1863 - classification_loss: 0.7002
 961/1000 [===========================>..] - ETA: 17s - loss: 2.8866 - regression_loss: 2.1868 - classification_loss: 0.6998
 962/1000 [===========================>..] - ETA: 17s - loss: 2.8876 - regression_loss: 2.1880 - classification_loss: 0.6996
 963/1000 [===========================>..] - ETA: 16s - loss: 2.8893 - regression_loss: 2.1885 - classification_loss: 0.7007
 964/1000 [===========================>..] - ETA: 16s - loss: 2.8917 - regression_loss: 2.1903 - classification_loss: 0.7014
 965/1000 [===========================>..] - ETA: 15s - loss: 2.8922 - regression_loss: 2.1909 - classification_loss: 0.7012
 966/1000 [===========================>..] - ETA: 15s - loss: 2.8923 - regression_loss: 2.1909 - classification_loss: 0.7014
 967/1000 [============================>.] - ETA: 14s - loss: 2.8923 - regression_loss: 2.1909 - classification_loss: 0.7014
 968/1000 [============================>.] - ETA: 14s - loss: 2.8928 - regression_loss: 2.1916 - classification_loss: 0.7012
 969/1000 [============================>.] - ETA: 14s - loss: 2.8898 - regression_loss: 2.1893 - classification_loss: 0.7005
 970/1000 [============================>.] - ETA: 13s - loss: 2.8923 - regression_loss: 2.1915 - classification_loss: 0.7009
 971/1000 [============================>.] - ETA: 13s - loss: 2.8926 - regression_loss: 2.1916 - classification_loss: 0.7009
 972/1000 [============================>.] - ETA: 12s - loss: 2.8933 - regression_loss: 2.1926 - classification_loss: 0.7007
 973/1000 [============================>.] - ETA: 12s - loss: 2.8949 - regression_loss: 2.1930 - classification_loss: 0.7019
 974/1000 [============================>.] - ETA: 11s - loss: 2.8919 - regression_loss: 2.1908 - classification_loss: 0.7011
 975/1000 [============================>.] - ETA: 11s - loss: 2.8928 - regression_loss: 2.1915 - classification_loss: 0.7013
 976/1000 [============================>.] - ETA: 10s - loss: 2.8898 - regression_loss: 2.1893 - classification_loss: 0.7006
 977/1000 [============================>.] - ETA: 10s - loss: 2.8870 - regression_loss: 2.1870 - classification_loss: 0.6999
 978/1000 [============================>.] - ETA: 9s - loss: 2.8840 - regression_loss: 2.1848 - classification_loss: 0.6992 
 979/1000 [============================>.] - ETA: 9s - loss: 2.8844 - regression_loss: 2.1853 - classification_loss: 0.6991
 980/1000 [============================>.] - ETA: 9s - loss: 2.8850 - regression_loss: 2.1861 - classification_loss: 0.6989
 981/1000 [============================>.] - ETA: 8s - loss: 2.8821 - regression_loss: 2.1839 - classification_loss: 0.6982
 982/1000 [============================>.] - ETA: 8s - loss: 2.8834 - regression_loss: 2.1843 - classification_loss: 0.6991
 983/1000 [============================>.] - ETA: 7s - loss: 2.8805 - regression_loss: 2.1820 - classification_loss: 0.6984
 984/1000 [============================>.] - ETA: 7s - loss: 2.8818 - regression_loss: 2.1832 - classification_loss: 0.6986
 985/1000 [============================>.] - ETA: 6s - loss: 2.8810 - regression_loss: 2.1829 - classification_loss: 0.6982
 986/1000 [============================>.] - ETA: 6s - loss: 2.8807 - regression_loss: 2.1830 - classification_loss: 0.6978
 987/1000 [============================>.] - ETA: 5s - loss: 2.8831 - regression_loss: 2.1844 - classification_loss: 0.6987
 988/1000 [============================>.] - ETA: 5s - loss: 2.8802 - regression_loss: 2.1822 - classification_loss: 0.6980
 989/1000 [============================>.] - ETA: 4s - loss: 2.8805 - regression_loss: 2.1822 - classification_loss: 0.6983
 990/1000 [============================>.] - ETA: 4s - loss: 2.8775 - regression_loss: 2.1800 - classification_loss: 0.6976
 991/1000 [============================>.] - ETA: 4s - loss: 2.8746 - regression_loss: 2.1778 - classification_loss: 0.6969
 992/1000 [============================>.] - ETA: 3s - loss: 2.8717 - regression_loss: 2.1756 - classification_loss: 0.6962
 993/1000 [============================>.] - ETA: 3s - loss: 2.8714 - regression_loss: 2.1755 - classification_loss: 0.6959
 994/1000 [============================>.] - ETA: 2s - loss: 2.8732 - regression_loss: 2.1770 - classification_loss: 0.6963
 995/1000 [============================>.] - ETA: 2s - loss: 2.8739 - regression_loss: 2.1768 - classification_loss: 0.6971
 996/1000 [============================>.] - ETA: 1s - loss: 2.8738 - regression_loss: 2.1770 - classification_loss: 0.6968
 997/1000 [============================>.] - ETA: 1s - loss: 2.8709 - regression_loss: 2.1748 - classification_loss: 0.6961
 998/1000 [============================>.] - ETA: 0s - loss: 2.8731 - regression_loss: 2.1769 - classification_loss: 0.6962
 999/1000 [============================>.] - ETA: 0s - loss: 2.8726 - regression_loss: 2.1766 - classification_loss: 0.6960
1000/1000 [==============================] - 452s 452ms/step - loss: 2.8753 - regression_loss: 2.1782 - classification_loss: 0.6971

Epoch 00017: saving model to ./snapshots/resnet50_csv_17.h5
0/2001/2002/2003/2004/2005/2006/2007/2008/2009/20010/20011/20012/20013/20014/20015/20016/20017/20018/20019/20020/20021/20022/20023/20024/20025/20026/20027/20028/20029/20030/20031/20032/20033/20034/20035/20036/20037/20038/20039/20040/20041/20042/20043/20044/20045/20046/20047/20048/20049/20050/20051/20052/20053/20054/20055/20056/20057/20058/20059/20060/20061/20062/20063/20064/20065/20066/20067/20068/20069/20070/20071/20072/20073/20074/20075/20076/20077/20078/20079/20080/20081/20082/20083/20084/20085/20086/20087/20088/20089/20090/20091/20092/20093/20094/20095/20096/20097/20098/20099/200100/200101/200102/200103/200104/200105/200106/200107/200108/200109/200110/200111/200112/200113/200114/200115/200116/200117/200118/200119/200120/200121/200122/200123/200124/200125/200126/200127/200128/200129/200130/200131/200132/200133/200134/200135/200136/200137/200138/200139/200140/200141/200142/200143/200144/200145/200146/200147/200148/200149/200150/200151/200152/200153/200154/200155/200156/200157/200158/200159/200160/200161/200162/200163/200164/200165/200166/200167/200168/200169/200170/200171/200172/200173/200174/200175/200176/200177/200178/200179/200180/200181/200182/200183/200184/200185/200186/200187/200188/200189/200190/200191/200192/200193/200194/200195/200196/200197/200198/200199/2000/2001/2002/2003/2004/2005/2006/2007/2008/2009/20010/20011/20012/20013/20014/20015/20016/20017/20018/20019/20020/20021/20022/20023/20024/20025/20026/20027/20028/20029/20030/20031/20032/20033/20034/20035/20036/20037/20038/20039/20040/20041/20042/20043/20044/20045/20046/20047/20048/20049/20050/20051/20052/20053/20054/20055/20056/20057/20058/20059/20060/20061/20062/20063/20064/20065/20066/20067/20068/20069/20070/20071/20072/20073/20074/20075/20076/20077/20078/20079/20080/20081/20082/20083/20084/20085/20086/20087/20088/20089/20090/20091/20092/20093/20094/20095/20096/20097/20098/20099/200100/200101/200102/200103/200104/200105/200106/200107/200108/200109/200110/200111/200112/200113/200114/200115/200116/200117/200118/200119/200120/200121/200122/200123/200124/200125/200126/200127/200128/200129/200130/200131/200132/200133/200134/200135/200136/200137/200138/200139/200140/200141/200142/200143/200144/200145/200146/200147/200148/200149/200150/200151/200152/200153/200154/200155/200156/200157/200158/200159/200160/200161/200162/200163/200164/200165/200166/200167/200168/200169/200170/200171/200172/200173/200174/200175/200176/200177/200178/200179/200180/200181/200182/200183/200184/200185/200186/200187/200188/200189/200190/200191/200192/200193/200194/200195/200196/200197/200198/200199/200

M 0.1842
N 0.0003
mAP: 0.0923
Epoch 18/30

   1/1000 [..............................] - ETA: 7:20 - loss: 2.5652 - regression_loss: 2.3863 - classification_loss: 0.1790
   2/1000 [..............................] - ETA: 7:27 - loss: 2.9583 - regression_loss: 2.5549 - classification_loss: 0.4035
   3/1000 [..............................] - ETA: 7:25 - loss: 3.1832 - regression_loss: 2.6531 - classification_loss: 0.5301
   4/1000 [..............................] - ETA: 7:25 - loss: 3.2964 - regression_loss: 2.4794 - classification_loss: 0.8170
   5/1000 [..............................] - ETA: 7:24 - loss: 3.2119 - regression_loss: 2.4888 - classification_loss: 0.7231
   6/1000 [..............................] - ETA: 7:25 - loss: 3.2564 - regression_loss: 2.4327 - classification_loss: 0.8236
   7/1000 [..............................] - ETA: 7:24 - loss: 2.7915 - regression_loss: 2.0852 - classification_loss: 0.7063
   8/1000 [..............................] - ETA: 7:26 - loss: 2.4425 - regression_loss: 1.8245 - classification_loss: 0.6180
   9/1000 [..............................] - ETA: 7:26 - loss: 2.7087 - regression_loss: 1.9239 - classification_loss: 0.7848
  10/1000 [..............................] - ETA: 7:25 - loss: 2.6450 - regression_loss: 1.8743 - classification_loss: 0.7707
  11/1000 [..............................] - ETA: 7:25 - loss: 2.5731 - regression_loss: 1.8488 - classification_loss: 0.7243
  12/1000 [..............................] - ETA: 7:25 - loss: 2.5576 - regression_loss: 1.8654 - classification_loss: 0.6922
  13/1000 [..............................] - ETA: 7:24 - loss: 2.5854 - regression_loss: 1.8561 - classification_loss: 0.7293
  14/1000 [..............................] - ETA: 7:23 - loss: 2.4007 - regression_loss: 1.7235 - classification_loss: 0.6772
  15/1000 [..............................] - ETA: 7:24 - loss: 2.2406 - regression_loss: 1.6086 - classification_loss: 0.6320
  16/1000 [..............................] - ETA: 7:21 - loss: 2.5065 - regression_loss: 1.5081 - classification_loss: 0.9985
  17/1000 [..............................] - ETA: 7:20 - loss: 2.5332 - regression_loss: 1.5674 - classification_loss: 0.9658
  18/1000 [..............................] - ETA: 7:20 - loss: 2.3924 - regression_loss: 1.4803 - classification_loss: 0.9121
  19/1000 [..............................] - ETA: 7:20 - loss: 2.4926 - regression_loss: 1.5678 - classification_loss: 0.9248
  20/1000 [..............................] - ETA: 7:19 - loss: 2.3783 - regression_loss: 1.4894 - classification_loss: 0.8889
  21/1000 [..............................] - ETA: 7:19 - loss: 2.3786 - regression_loss: 1.5061 - classification_loss: 0.8725
  22/1000 [..............................] - ETA: 7:19 - loss: 2.3797 - regression_loss: 1.5192 - classification_loss: 0.8605
  23/1000 [..............................] - ETA: 7:19 - loss: 2.4098 - regression_loss: 1.5599 - classification_loss: 0.8499
  24/1000 [..............................] - ETA: 7:18 - loss: 2.4156 - regression_loss: 1.5879 - classification_loss: 0.8277
  25/1000 [..............................] - ETA: 7:18 - loss: 2.3190 - regression_loss: 1.5244 - classification_loss: 0.7946
  26/1000 [..............................] - ETA: 7:17 - loss: 2.3503 - regression_loss: 1.5665 - classification_loss: 0.7837
  27/1000 [..............................] - ETA: 7:17 - loss: 2.3854 - regression_loss: 1.6143 - classification_loss: 0.7712
  28/1000 [..............................] - ETA: 7:16 - loss: 2.3962 - regression_loss: 1.6385 - classification_loss: 0.7577
  29/1000 [..............................] - ETA: 7:16 - loss: 2.3780 - regression_loss: 1.6387 - classification_loss: 0.7393
  30/1000 [..............................] - ETA: 7:16 - loss: 2.4080 - regression_loss: 1.6786 - classification_loss: 0.7294
  31/1000 [..............................] - ETA: 7:15 - loss: 2.4591 - regression_loss: 1.7246 - classification_loss: 0.7346
  32/1000 [..............................] - ETA: 7:15 - loss: 2.5053 - regression_loss: 1.7756 - classification_loss: 0.7297
  33/1000 [..............................] - ETA: 7:15 - loss: 2.4947 - regression_loss: 1.7721 - classification_loss: 0.7226
  34/1000 [>.............................] - ETA: 7:14 - loss: 2.4844 - regression_loss: 1.7678 - classification_loss: 0.7165
  35/1000 [>.............................] - ETA: 7:14 - loss: 2.4437 - regression_loss: 1.7173 - classification_loss: 0.7264
  36/1000 [>.............................] - ETA: 7:13 - loss: 2.3758 - regression_loss: 1.6696 - classification_loss: 0.7062
  37/1000 [>.............................] - ETA: 7:13 - loss: 2.4532 - regression_loss: 1.7478 - classification_loss: 0.7054
  38/1000 [>.............................] - ETA: 7:13 - loss: 2.5118 - regression_loss: 1.7751 - classification_loss: 0.7367
  39/1000 [>.............................] - ETA: 7:12 - loss: 2.5717 - regression_loss: 1.8075 - classification_loss: 0.7642
  40/1000 [>.............................] - ETA: 7:12 - loss: 2.5916 - regression_loss: 1.8304 - classification_loss: 0.7612
  41/1000 [>.............................] - ETA: 7:11 - loss: 2.5284 - regression_loss: 1.7858 - classification_loss: 0.7426
  42/1000 [>.............................] - ETA: 7:11 - loss: 2.5379 - regression_loss: 1.7999 - classification_loss: 0.7380
  43/1000 [>.............................] - ETA: 7:11 - loss: 2.5503 - regression_loss: 1.8211 - classification_loss: 0.7291
  44/1000 [>.............................] - ETA: 7:10 - loss: 2.5823 - regression_loss: 1.8375 - classification_loss: 0.7448
  45/1000 [>.............................] - ETA: 7:10 - loss: 2.6172 - regression_loss: 1.8547 - classification_loss: 0.7625
  46/1000 [>.............................] - ETA: 7:10 - loss: 2.5603 - regression_loss: 1.8144 - classification_loss: 0.7459
  47/1000 [>.............................] - ETA: 7:09 - loss: 2.5552 - regression_loss: 1.8156 - classification_loss: 0.7396
  48/1000 [>.............................] - ETA: 7:09 - loss: 2.5663 - regression_loss: 1.8170 - classification_loss: 0.7494
  49/1000 [>.............................] - ETA: 7:08 - loss: 2.5719 - regression_loss: 1.8278 - classification_loss: 0.7441
  50/1000 [>.............................] - ETA: 7:08 - loss: 2.5717 - regression_loss: 1.8353 - classification_loss: 0.7364
  51/1000 [>.............................] - ETA: 7:08 - loss: 2.5773 - regression_loss: 1.8501 - classification_loss: 0.7272
  52/1000 [>.............................] - ETA: 7:07 - loss: 2.5934 - regression_loss: 1.8579 - classification_loss: 0.7355
  53/1000 [>.............................] - ETA: 7:07 - loss: 2.5445 - regression_loss: 1.8229 - classification_loss: 0.7216
  54/1000 [>.............................] - ETA: 7:07 - loss: 2.5611 - regression_loss: 1.8356 - classification_loss: 0.7255
  55/1000 [>.............................] - ETA: 7:06 - loss: 2.5146 - regression_loss: 1.8022 - classification_loss: 0.7123
  56/1000 [>.............................] - ETA: 7:06 - loss: 2.5160 - regression_loss: 1.8036 - classification_loss: 0.7125
  57/1000 [>.............................] - ETA: 7:06 - loss: 2.5009 - regression_loss: 1.7963 - classification_loss: 0.7045
  58/1000 [>.............................] - ETA: 7:05 - loss: 2.4973 - regression_loss: 1.8009 - classification_loss: 0.6964
  59/1000 [>.............................] - ETA: 7:05 - loss: 2.4944 - regression_loss: 1.8049 - classification_loss: 0.6896
  60/1000 [>.............................] - ETA: 7:04 - loss: 2.4979 - regression_loss: 1.8098 - classification_loss: 0.6881
  61/1000 [>.............................] - ETA: 7:04 - loss: 2.5081 - regression_loss: 1.8270 - classification_loss: 0.6811
  62/1000 [>.............................] - ETA: 7:03 - loss: 2.4677 - regression_loss: 1.7975 - classification_loss: 0.6701
  63/1000 [>.............................] - ETA: 7:03 - loss: 2.4675 - regression_loss: 1.8001 - classification_loss: 0.6674
  64/1000 [>.............................] - ETA: 7:03 - loss: 2.4937 - regression_loss: 1.8195 - classification_loss: 0.6742
  65/1000 [>.............................] - ETA: 7:02 - loss: 2.5066 - regression_loss: 1.8327 - classification_loss: 0.6739
  66/1000 [>.............................] - ETA: 7:02 - loss: 2.5199 - regression_loss: 1.8412 - classification_loss: 0.6788
  67/1000 [=>............................] - ETA: 7:01 - loss: 2.5284 - regression_loss: 1.8468 - classification_loss: 0.6816
  68/1000 [=>............................] - ETA: 7:01 - loss: 2.5197 - regression_loss: 1.8404 - classification_loss: 0.6793
  69/1000 [=>............................] - ETA: 7:01 - loss: 2.4832 - regression_loss: 1.8137 - classification_loss: 0.6695
  70/1000 [=>............................] - ETA: 7:00 - loss: 2.4907 - regression_loss: 1.8236 - classification_loss: 0.6672
  71/1000 [=>............................] - ETA: 7:00 - loss: 2.4556 - regression_loss: 1.7979 - classification_loss: 0.6578
  72/1000 [=>............................] - ETA: 6:59 - loss: 2.4215 - regression_loss: 1.7729 - classification_loss: 0.6487
  73/1000 [=>............................] - ETA: 6:59 - loss: 2.4285 - regression_loss: 1.7857 - classification_loss: 0.6428
  74/1000 [=>............................] - ETA: 6:58 - loss: 2.4237 - regression_loss: 1.7852 - classification_loss: 0.6385
  75/1000 [=>............................] - ETA: 6:58 - loss: 2.3966 - regression_loss: 1.7614 - classification_loss: 0.6352
  76/1000 [=>............................] - ETA: 6:57 - loss: 2.3651 - regression_loss: 1.7382 - classification_loss: 0.6269
  77/1000 [=>............................] - ETA: 6:57 - loss: 2.3344 - regression_loss: 1.7156 - classification_loss: 0.6187
  78/1000 [=>............................] - ETA: 6:57 - loss: 2.3460 - regression_loss: 1.7318 - classification_loss: 0.6142
  79/1000 [=>............................] - ETA: 6:56 - loss: 2.3654 - regression_loss: 1.7517 - classification_loss: 0.6136
  80/1000 [=>............................] - ETA: 6:56 - loss: 2.3389 - regression_loss: 1.7298 - classification_loss: 0.6090
  81/1000 [=>............................] - ETA: 6:55 - loss: 2.3100 - regression_loss: 1.7085 - classification_loss: 0.6015
  82/1000 [=>............................] - ETA: 6:55 - loss: 2.3158 - regression_loss: 1.7160 - classification_loss: 0.5997
  83/1000 [=>............................] - ETA: 6:54 - loss: 2.3426 - regression_loss: 1.7394 - classification_loss: 0.6032
  84/1000 [=>............................] - ETA: 6:54 - loss: 2.3451 - regression_loss: 1.7450 - classification_loss: 0.6002
  85/1000 [=>............................] - ETA: 6:53 - loss: 2.3176 - regression_loss: 1.7244 - classification_loss: 0.5931
  86/1000 [=>............................] - ETA: 6:53 - loss: 2.3271 - regression_loss: 1.7371 - classification_loss: 0.5900
  87/1000 [=>............................] - ETA: 6:53 - loss: 2.3312 - regression_loss: 1.7418 - classification_loss: 0.5894
  88/1000 [=>............................] - ETA: 6:52 - loss: 2.3335 - regression_loss: 1.7458 - classification_loss: 0.5877
  89/1000 [=>............................] - ETA: 6:52 - loss: 2.3645 - regression_loss: 1.7553 - classification_loss: 0.6092
  90/1000 [=>............................] - ETA: 6:51 - loss: 2.3712 - regression_loss: 1.7657 - classification_loss: 0.6056
  91/1000 [=>............................] - ETA: 6:51 - loss: 2.4003 - regression_loss: 1.7796 - classification_loss: 0.6207
  92/1000 [=>............................] - ETA: 6:50 - loss: 2.4357 - regression_loss: 1.7962 - classification_loss: 0.6395
  93/1000 [=>............................] - ETA: 6:50 - loss: 2.4095 - regression_loss: 1.7769 - classification_loss: 0.6326
  94/1000 [=>............................] - ETA: 6:49 - loss: 2.4042 - regression_loss: 1.7746 - classification_loss: 0.6296
  95/1000 [=>............................] - ETA: 6:49 - loss: 2.3789 - regression_loss: 1.7559 - classification_loss: 0.6230
  96/1000 [=>............................] - ETA: 6:48 - loss: 2.4055 - regression_loss: 1.7713 - classification_loss: 0.6342
  97/1000 [=>............................] - ETA: 6:48 - loss: 2.4300 - regression_loss: 1.7837 - classification_loss: 0.6463
  98/1000 [=>............................] - ETA: 6:48 - loss: 2.4383 - regression_loss: 1.7929 - classification_loss: 0.6454
  99/1000 [=>............................] - ETA: 6:47 - loss: 2.4378 - regression_loss: 1.7954 - classification_loss: 0.6424
 100/1000 [==>...........................] - ETA: 6:47 - loss: 2.4412 - regression_loss: 1.7975 - classification_loss: 0.6437
 101/1000 [==>...........................] - ETA: 6:46 - loss: 2.4551 - regression_loss: 1.8088 - classification_loss: 0.6463
 102/1000 [==>...........................] - ETA: 6:46 - loss: 2.4602 - regression_loss: 1.8160 - classification_loss: 0.6442
 103/1000 [==>...........................] - ETA: 6:45 - loss: 2.4782 - regression_loss: 1.8244 - classification_loss: 0.6538
 104/1000 [==>...........................] - ETA: 6:45 - loss: 2.4825 - regression_loss: 1.8321 - classification_loss: 0.6503
 105/1000 [==>...........................] - ETA: 6:44 - loss: 2.4900 - regression_loss: 1.8425 - classification_loss: 0.6475
 106/1000 [==>...........................] - ETA: 6:44 - loss: 2.4914 - regression_loss: 1.8446 - classification_loss: 0.6468
 107/1000 [==>...........................] - ETA: 6:44 - loss: 2.5138 - regression_loss: 1.8698 - classification_loss: 0.6439
 108/1000 [==>...........................] - ETA: 6:43 - loss: 2.4905 - regression_loss: 1.8525 - classification_loss: 0.6380
 109/1000 [==>...........................] - ETA: 6:43 - loss: 2.4916 - regression_loss: 1.8537 - classification_loss: 0.6379
 110/1000 [==>...........................] - ETA: 6:42 - loss: 2.5059 - regression_loss: 1.8695 - classification_loss: 0.6365
 111/1000 [==>...........................] - ETA: 6:42 - loss: 2.5204 - regression_loss: 1.8773 - classification_loss: 0.6431
 112/1000 [==>...........................] - ETA: 6:41 - loss: 2.5318 - regression_loss: 1.8898 - classification_loss: 0.6419
 113/1000 [==>...........................] - ETA: 6:41 - loss: 2.5362 - regression_loss: 1.8972 - classification_loss: 0.6390
 114/1000 [==>...........................] - ETA: 6:40 - loss: 2.5140 - regression_loss: 1.8806 - classification_loss: 0.6334
 115/1000 [==>...........................] - ETA: 6:40 - loss: 2.5317 - regression_loss: 1.8956 - classification_loss: 0.6360
 116/1000 [==>...........................] - ETA: 6:39 - loss: 2.5099 - regression_loss: 1.8793 - classification_loss: 0.6306
 117/1000 [==>...........................] - ETA: 6:39 - loss: 2.5229 - regression_loss: 1.8878 - classification_loss: 0.6352
 118/1000 [==>...........................] - ETA: 6:39 - loss: 2.5374 - regression_loss: 1.8931 - classification_loss: 0.6443
 119/1000 [==>...........................] - ETA: 6:38 - loss: 2.5329 - regression_loss: 1.8905 - classification_loss: 0.6424
 120/1000 [==>...........................] - ETA: 6:38 - loss: 2.5266 - regression_loss: 1.8882 - classification_loss: 0.6384
 121/1000 [==>...........................] - ETA: 6:37 - loss: 2.5301 - regression_loss: 1.8945 - classification_loss: 0.6356
 122/1000 [==>...........................] - ETA: 6:37 - loss: 2.5093 - regression_loss: 1.8790 - classification_loss: 0.6304
 123/1000 [==>...........................] - ETA: 6:36 - loss: 2.5242 - regression_loss: 1.8912 - classification_loss: 0.6330
 124/1000 [==>...........................] - ETA: 6:36 - loss: 2.5282 - regression_loss: 1.8975 - classification_loss: 0.6307
 125/1000 [==>...........................] - ETA: 6:35 - loss: 2.5305 - regression_loss: 1.9018 - classification_loss: 0.6287
 126/1000 [==>...........................] - ETA: 6:35 - loss: 2.5459 - regression_loss: 1.9127 - classification_loss: 0.6332
 127/1000 [==>...........................] - ETA: 6:34 - loss: 2.5574 - regression_loss: 1.9214 - classification_loss: 0.6360
 128/1000 [==>...........................] - ETA: 6:34 - loss: 2.5375 - regression_loss: 1.9064 - classification_loss: 0.6310
 129/1000 [==>...........................] - ETA: 6:34 - loss: 2.5513 - regression_loss: 1.9116 - classification_loss: 0.6397
 130/1000 [==>...........................] - ETA: 6:33 - loss: 2.5543 - regression_loss: 1.9170 - classification_loss: 0.6374
 131/1000 [==>...........................] - ETA: 6:33 - loss: 2.5349 - regression_loss: 1.9023 - classification_loss: 0.6325
 132/1000 [==>...........................] - ETA: 6:32 - loss: 2.5355 - regression_loss: 1.9045 - classification_loss: 0.6309
 133/1000 [==>...........................] - ETA: 6:32 - loss: 2.5318 - regression_loss: 1.9038 - classification_loss: 0.6280
 134/1000 [===>..........................] - ETA: 6:31 - loss: 2.5369 - regression_loss: 1.9083 - classification_loss: 0.6287
 135/1000 [===>..........................] - ETA: 6:31 - loss: 2.5347 - regression_loss: 1.9080 - classification_loss: 0.6266
 136/1000 [===>..........................] - ETA: 6:30 - loss: 2.5160 - regression_loss: 1.8940 - classification_loss: 0.6220
 137/1000 [===>..........................] - ETA: 6:30 - loss: 2.5180 - regression_loss: 1.8972 - classification_loss: 0.6208
 138/1000 [===>..........................] - ETA: 6:30 - loss: 2.5368 - regression_loss: 1.9087 - classification_loss: 0.6280
 139/1000 [===>..........................] - ETA: 6:29 - loss: 2.5405 - regression_loss: 1.9116 - classification_loss: 0.6288
 140/1000 [===>..........................] - ETA: 6:29 - loss: 2.5534 - regression_loss: 1.9251 - classification_loss: 0.6283
 141/1000 [===>..........................] - ETA: 6:28 - loss: 2.5559 - regression_loss: 1.9266 - classification_loss: 0.6293
 142/1000 [===>..........................] - ETA: 6:28 - loss: 2.5379 - regression_loss: 1.9130 - classification_loss: 0.6249
 143/1000 [===>..........................] - ETA: 6:27 - loss: 2.5412 - regression_loss: 1.9161 - classification_loss: 0.6251
 144/1000 [===>..........................] - ETA: 6:27 - loss: 2.5492 - regression_loss: 1.9244 - classification_loss: 0.6248
 145/1000 [===>..........................] - ETA: 6:26 - loss: 2.5606 - regression_loss: 1.9303 - classification_loss: 0.6303
 146/1000 [===>..........................] - ETA: 6:26 - loss: 2.5431 - regression_loss: 1.9171 - classification_loss: 0.6260
 147/1000 [===>..........................] - ETA: 6:26 - loss: 2.5698 - regression_loss: 1.9380 - classification_loss: 0.6318
 148/1000 [===>..........................] - ETA: 6:25 - loss: 2.5765 - regression_loss: 1.9399 - classification_loss: 0.6365
 149/1000 [===>..........................] - ETA: 6:25 - loss: 2.5751 - regression_loss: 1.9407 - classification_loss: 0.6344
 150/1000 [===>..........................] - ETA: 6:24 - loss: 2.5762 - regression_loss: 1.9416 - classification_loss: 0.6346
 151/1000 [===>..........................] - ETA: 6:24 - loss: 2.5819 - regression_loss: 1.9462 - classification_loss: 0.6357
 152/1000 [===>..........................] - ETA: 6:23 - loss: 2.5992 - regression_loss: 1.9542 - classification_loss: 0.6450
 153/1000 [===>..........................] - ETA: 6:23 - loss: 2.5913 - regression_loss: 1.9414 - classification_loss: 0.6499
 154/1000 [===>..........................] - ETA: 6:23 - loss: 2.6113 - regression_loss: 1.9572 - classification_loss: 0.6541
 155/1000 [===>..........................] - ETA: 6:22 - loss: 2.6148 - regression_loss: 1.9620 - classification_loss: 0.6528
 156/1000 [===>..........................] - ETA: 6:22 - loss: 2.6244 - regression_loss: 1.9720 - classification_loss: 0.6524
 157/1000 [===>..........................] - ETA: 6:21 - loss: 2.6077 - regression_loss: 1.9594 - classification_loss: 0.6483
 158/1000 [===>..........................] - ETA: 6:21 - loss: 2.6123 - regression_loss: 1.9646 - classification_loss: 0.6476
 159/1000 [===>..........................] - ETA: 6:20 - loss: 2.6008 - regression_loss: 1.9523 - classification_loss: 0.6486
 160/1000 [===>..........................] - ETA: 6:20 - loss: 2.5993 - regression_loss: 1.9535 - classification_loss: 0.6459
 161/1000 [===>..........................] - ETA: 6:19 - loss: 2.5832 - regression_loss: 1.9413 - classification_loss: 0.6419
 162/1000 [===>..........................] - ETA: 6:19 - loss: 2.5853 - regression_loss: 1.9437 - classification_loss: 0.6416
 163/1000 [===>..........................] - ETA: 6:19 - loss: 2.5769 - regression_loss: 1.9317 - classification_loss: 0.6451
 164/1000 [===>..........................] - ETA: 6:18 - loss: 2.5974 - regression_loss: 1.9501 - classification_loss: 0.6473
 165/1000 [===>..........................] - ETA: 6:18 - loss: 2.5817 - regression_loss: 1.9383 - classification_loss: 0.6434
 166/1000 [===>..........................] - ETA: 6:17 - loss: 2.5661 - regression_loss: 1.9266 - classification_loss: 0.6395
 167/1000 [====>.........................] - ETA: 6:17 - loss: 2.5806 - regression_loss: 1.9346 - classification_loss: 0.6460
 168/1000 [====>.........................] - ETA: 6:16 - loss: 2.5792 - regression_loss: 1.9338 - classification_loss: 0.6454
 169/1000 [====>.........................] - ETA: 6:16 - loss: 2.5807 - regression_loss: 1.9355 - classification_loss: 0.6452
 170/1000 [====>.........................] - ETA: 6:15 - loss: 2.5889 - regression_loss: 1.9416 - classification_loss: 0.6473
 171/1000 [====>.........................] - ETA: 6:15 - loss: 2.5739 - regression_loss: 1.9302 - classification_loss: 0.6437
 172/1000 [====>.........................] - ETA: 6:14 - loss: 2.5724 - regression_loss: 1.9302 - classification_loss: 0.6422
 173/1000 [====>.........................] - ETA: 6:14 - loss: 2.5718 - regression_loss: 1.9296 - classification_loss: 0.6422
 174/1000 [====>.........................] - ETA: 6:13 - loss: 2.5570 - regression_loss: 1.9185 - classification_loss: 0.6385
 175/1000 [====>.........................] - ETA: 6:13 - loss: 2.5694 - regression_loss: 1.9309 - classification_loss: 0.6385
 176/1000 [====>.........................] - ETA: 6:12 - loss: 2.5758 - regression_loss: 1.9336 - classification_loss: 0.6422
 177/1000 [====>.........................] - ETA: 6:12 - loss: 2.5778 - regression_loss: 1.9371 - classification_loss: 0.6407
 178/1000 [====>.........................] - ETA: 6:11 - loss: 2.5879 - regression_loss: 1.9435 - classification_loss: 0.6444
 179/1000 [====>.........................] - ETA: 6:11 - loss: 2.6034 - regression_loss: 1.9572 - classification_loss: 0.6462
 180/1000 [====>.........................] - ETA: 6:11 - loss: 2.6048 - regression_loss: 1.9600 - classification_loss: 0.6448
 181/1000 [====>.........................] - ETA: 6:10 - loss: 2.6124 - regression_loss: 1.9683 - classification_loss: 0.6440
 182/1000 [====>.........................] - ETA: 6:10 - loss: 2.5984 - regression_loss: 1.9575 - classification_loss: 0.6409
 183/1000 [====>.........................] - ETA: 6:09 - loss: 2.6091 - regression_loss: 1.9630 - classification_loss: 0.6461
 184/1000 [====>.........................] - ETA: 6:09 - loss: 2.5949 - regression_loss: 1.9523 - classification_loss: 0.6426
 185/1000 [====>.........................] - ETA: 6:08 - loss: 2.6015 - regression_loss: 1.9587 - classification_loss: 0.6428
 186/1000 [====>.........................] - ETA: 6:08 - loss: 2.6030 - regression_loss: 1.9598 - classification_loss: 0.6432
 187/1000 [====>.........................] - ETA: 6:08 - loss: 2.5891 - regression_loss: 1.9494 - classification_loss: 0.6397
 188/1000 [====>.........................] - ETA: 6:07 - loss: 2.5753 - regression_loss: 1.9390 - classification_loss: 0.6363
 189/1000 [====>.........................] - ETA: 6:07 - loss: 2.5848 - regression_loss: 1.9449 - classification_loss: 0.6398
 190/1000 [====>.........................] - ETA: 6:06 - loss: 2.5967 - regression_loss: 1.9577 - classification_loss: 0.6390
 191/1000 [====>.........................] - ETA: 6:06 - loss: 2.6035 - regression_loss: 1.9644 - classification_loss: 0.6391
 192/1000 [====>.........................] - ETA: 6:05 - loss: 2.6084 - regression_loss: 1.9691 - classification_loss: 0.6393
 193/1000 [====>.........................] - ETA: 6:05 - loss: 2.6178 - regression_loss: 1.9775 - classification_loss: 0.6403
 194/1000 [====>.........................] - ETA: 6:04 - loss: 2.6280 - regression_loss: 1.9880 - classification_loss: 0.6400
 195/1000 [====>.........................] - ETA: 6:04 - loss: 2.6343 - regression_loss: 1.9936 - classification_loss: 0.6407
 196/1000 [====>.........................] - ETA: 6:04 - loss: 2.6285 - regression_loss: 1.9895 - classification_loss: 0.6390
 197/1000 [====>.........................] - ETA: 6:03 - loss: 2.6253 - regression_loss: 1.9886 - classification_loss: 0.6367
 198/1000 [====>.........................] - ETA: 6:03 - loss: 2.6321 - regression_loss: 1.9941 - classification_loss: 0.6380
 199/1000 [====>.........................] - ETA: 6:02 - loss: 2.6189 - regression_loss: 1.9841 - classification_loss: 0.6348
 200/1000 [=====>........................] - ETA: 6:02 - loss: 2.6064 - regression_loss: 1.9742 - classification_loss: 0.6322
 201/1000 [=====>........................] - ETA: 6:01 - loss: 2.5938 - regression_loss: 1.9644 - classification_loss: 0.6294
 202/1000 [=====>........................] - ETA: 6:01 - loss: 2.6118 - regression_loss: 1.9755 - classification_loss: 0.6363
 203/1000 [=====>........................] - ETA: 6:00 - loss: 2.6123 - regression_loss: 1.9777 - classification_loss: 0.6346
 204/1000 [=====>........................] - ETA: 6:00 - loss: 2.6154 - regression_loss: 1.9794 - classification_loss: 0.6360
 205/1000 [=====>........................] - ETA: 6:00 - loss: 2.6027 - regression_loss: 1.9697 - classification_loss: 0.6329
 206/1000 [=====>........................] - ETA: 5:59 - loss: 2.6025 - regression_loss: 1.9710 - classification_loss: 0.6315
 207/1000 [=====>........................] - ETA: 5:59 - loss: 2.6144 - regression_loss: 1.9784 - classification_loss: 0.6360
 208/1000 [=====>........................] - ETA: 5:58 - loss: 2.6137 - regression_loss: 1.9790 - classification_loss: 0.6348
 209/1000 [=====>........................] - ETA: 5:58 - loss: 2.6155 - regression_loss: 1.9823 - classification_loss: 0.6332
 210/1000 [=====>........................] - ETA: 5:57 - loss: 2.6288 - regression_loss: 1.9908 - classification_loss: 0.6380
 211/1000 [=====>........................] - ETA: 5:57 - loss: 2.6328 - regression_loss: 1.9946 - classification_loss: 0.6382
 212/1000 [=====>........................] - ETA: 5:56 - loss: 2.6204 - regression_loss: 1.9852 - classification_loss: 0.6352
 213/1000 [=====>........................] - ETA: 5:56 - loss: 2.6081 - regression_loss: 1.9758 - classification_loss: 0.6322
 214/1000 [=====>........................] - ETA: 5:56 - loss: 2.6141 - regression_loss: 1.9802 - classification_loss: 0.6339
 215/1000 [=====>........................] - ETA: 5:55 - loss: 2.6134 - regression_loss: 1.9810 - classification_loss: 0.6324
 216/1000 [=====>........................] - ETA: 5:55 - loss: 2.6151 - regression_loss: 1.9826 - classification_loss: 0.6325
 217/1000 [=====>........................] - ETA: 5:54 - loss: 2.6202 - regression_loss: 1.9846 - classification_loss: 0.6356
 218/1000 [=====>........................] - ETA: 5:54 - loss: 2.6331 - regression_loss: 1.9918 - classification_loss: 0.6414
 219/1000 [=====>........................] - ETA: 5:53 - loss: 2.6351 - regression_loss: 1.9941 - classification_loss: 0.6411
 220/1000 [=====>........................] - ETA: 5:53 - loss: 2.6331 - regression_loss: 1.9933 - classification_loss: 0.6397
 221/1000 [=====>........................] - ETA: 5:52 - loss: 2.6211 - regression_loss: 1.9843 - classification_loss: 0.6368
 222/1000 [=====>........................] - ETA: 5:52 - loss: 2.6232 - regression_loss: 1.9863 - classification_loss: 0.6369
 223/1000 [=====>........................] - ETA: 5:52 - loss: 2.6322 - regression_loss: 1.9928 - classification_loss: 0.6394
 224/1000 [=====>........................] - ETA: 5:51 - loss: 2.6296 - regression_loss: 1.9917 - classification_loss: 0.6379
 225/1000 [=====>........................] - ETA: 5:51 - loss: 2.6376 - regression_loss: 1.9944 - classification_loss: 0.6432
 226/1000 [=====>........................] - ETA: 5:50 - loss: 2.6401 - regression_loss: 1.9966 - classification_loss: 0.6435
 227/1000 [=====>........................] - ETA: 5:50 - loss: 2.6450 - regression_loss: 2.0016 - classification_loss: 0.6435
 228/1000 [=====>........................] - ETA: 5:49 - loss: 2.6443 - regression_loss: 2.0006 - classification_loss: 0.6437
 229/1000 [=====>........................] - ETA: 5:49 - loss: 2.6332 - regression_loss: 1.9918 - classification_loss: 0.6414
 230/1000 [=====>........................] - ETA: 5:48 - loss: 2.6387 - regression_loss: 1.9967 - classification_loss: 0.6419
 231/1000 [=====>........................] - ETA: 5:48 - loss: 2.6443 - regression_loss: 2.0002 - classification_loss: 0.6441
 232/1000 [=====>........................] - ETA: 5:47 - loss: 2.6470 - regression_loss: 2.0036 - classification_loss: 0.6434
 233/1000 [=====>........................] - ETA: 5:47 - loss: 2.6503 - regression_loss: 2.0073 - classification_loss: 0.6430
 234/1000 [======>.......................] - ETA: 5:46 - loss: 2.6616 - regression_loss: 2.0152 - classification_loss: 0.6464
 235/1000 [======>.......................] - ETA: 5:46 - loss: 2.6592 - regression_loss: 2.0145 - classification_loss: 0.6447
 236/1000 [======>.......................] - ETA: 5:45 - loss: 2.6596 - regression_loss: 2.0159 - classification_loss: 0.6437
 237/1000 [======>.......................] - ETA: 5:45 - loss: 2.6644 - regression_loss: 2.0197 - classification_loss: 0.6447
 238/1000 [======>.......................] - ETA: 5:45 - loss: 2.6532 - regression_loss: 2.0112 - classification_loss: 0.6420
 239/1000 [======>.......................] - ETA: 5:44 - loss: 2.6535 - regression_loss: 2.0120 - classification_loss: 0.6414
 240/1000 [======>.......................] - ETA: 5:44 - loss: 2.6603 - regression_loss: 2.0184 - classification_loss: 0.6418
 241/1000 [======>.......................] - ETA: 5:43 - loss: 2.6493 - regression_loss: 2.0101 - classification_loss: 0.6392
 242/1000 [======>.......................] - ETA: 5:43 - loss: 2.6560 - regression_loss: 2.0153 - classification_loss: 0.6407
 243/1000 [======>.......................] - ETA: 5:42 - loss: 2.6628 - regression_loss: 2.0203 - classification_loss: 0.6425
 244/1000 [======>.......................] - ETA: 5:42 - loss: 2.6519 - regression_loss: 2.0120 - classification_loss: 0.6399
 245/1000 [======>.......................] - ETA: 5:41 - loss: 2.6545 - regression_loss: 2.0143 - classification_loss: 0.6401
 246/1000 [======>.......................] - ETA: 5:41 - loss: 2.6451 - regression_loss: 2.0061 - classification_loss: 0.6390
 247/1000 [======>.......................] - ETA: 5:40 - loss: 2.6504 - regression_loss: 2.0113 - classification_loss: 0.6391
 248/1000 [======>.......................] - ETA: 5:40 - loss: 2.6498 - regression_loss: 2.0098 - classification_loss: 0.6400
 249/1000 [======>.......................] - ETA: 5:40 - loss: 2.6534 - regression_loss: 2.0139 - classification_loss: 0.6395
 250/1000 [======>.......................] - ETA: 5:39 - loss: 2.6857 - regression_loss: 2.0420 - classification_loss: 0.6437
 251/1000 [======>.......................] - ETA: 5:39 - loss: 2.6863 - regression_loss: 2.0435 - classification_loss: 0.6427
 252/1000 [======>.......................] - ETA: 5:38 - loss: 2.6905 - regression_loss: 2.0480 - classification_loss: 0.6425
 253/1000 [======>.......................] - ETA: 5:38 - loss: 2.6905 - regression_loss: 2.0487 - classification_loss: 0.6418
 254/1000 [======>.......................] - ETA: 5:37 - loss: 2.7002 - regression_loss: 2.0559 - classification_loss: 0.6443
 255/1000 [======>.......................] - ETA: 5:37 - loss: 2.7062 - regression_loss: 2.0615 - classification_loss: 0.6447
 256/1000 [======>.......................] - ETA: 5:36 - loss: 2.6956 - regression_loss: 2.0534 - classification_loss: 0.6422
 257/1000 [======>.......................] - ETA: 5:36 - loss: 2.7047 - regression_loss: 2.0601 - classification_loss: 0.6447
 258/1000 [======>.......................] - ETA: 5:35 - loss: 2.7035 - regression_loss: 2.0593 - classification_loss: 0.6442
 259/1000 [======>.......................] - ETA: 5:35 - loss: 2.7060 - regression_loss: 2.0614 - classification_loss: 0.6446
 260/1000 [======>.......................] - ETA: 5:35 - loss: 2.7135 - regression_loss: 2.0657 - classification_loss: 0.6478
 261/1000 [======>.......................] - ETA: 5:34 - loss: 2.7102 - regression_loss: 2.0635 - classification_loss: 0.6467
 262/1000 [======>.......................] - ETA: 5:34 - loss: 2.6998 - regression_loss: 2.0556 - classification_loss: 0.6442
 263/1000 [======>.......................] - ETA: 5:33 - loss: 2.7043 - regression_loss: 2.0583 - classification_loss: 0.6460
 264/1000 [======>.......................] - ETA: 5:33 - loss: 2.7077 - regression_loss: 2.0611 - classification_loss: 0.6466
 265/1000 [======>.......................] - ETA: 5:32 - loss: 2.7079 - regression_loss: 2.0629 - classification_loss: 0.6451
 266/1000 [======>.......................] - ETA: 5:32 - loss: 2.7044 - regression_loss: 2.0607 - classification_loss: 0.6437
 267/1000 [=======>......................] - ETA: 5:31 - loss: 2.7064 - regression_loss: 2.0632 - classification_loss: 0.6432
 268/1000 [=======>......................] - ETA: 5:31 - loss: 2.7065 - regression_loss: 2.0646 - classification_loss: 0.6419
 269/1000 [=======>......................] - ETA: 5:31 - loss: 2.6964 - regression_loss: 2.0569 - classification_loss: 0.6395
 270/1000 [=======>......................] - ETA: 5:30 - loss: 2.7052 - regression_loss: 2.0640 - classification_loss: 0.6412
 271/1000 [=======>......................] - ETA: 5:30 - loss: 2.7072 - regression_loss: 2.0670 - classification_loss: 0.6402
 272/1000 [=======>......................] - ETA: 5:29 - loss: 2.7082 - regression_loss: 2.0686 - classification_loss: 0.6395
 273/1000 [=======>......................] - ETA: 5:29 - loss: 2.7071 - regression_loss: 2.0690 - classification_loss: 0.6381
 274/1000 [=======>......................] - ETA: 5:28 - loss: 2.7064 - regression_loss: 2.0698 - classification_loss: 0.6365
 275/1000 [=======>......................] - ETA: 5:28 - loss: 2.7112 - regression_loss: 2.0750 - classification_loss: 0.6363
 276/1000 [=======>......................] - ETA: 5:27 - loss: 2.7080 - regression_loss: 2.0733 - classification_loss: 0.6347
 277/1000 [=======>......................] - ETA: 5:27 - loss: 2.7079 - regression_loss: 2.0736 - classification_loss: 0.6342
 278/1000 [=======>......................] - ETA: 5:26 - loss: 2.7107 - regression_loss: 2.0765 - classification_loss: 0.6343
 279/1000 [=======>......................] - ETA: 5:26 - loss: 2.7015 - regression_loss: 2.0690 - classification_loss: 0.6325
 280/1000 [=======>......................] - ETA: 5:26 - loss: 2.7003 - regression_loss: 2.0690 - classification_loss: 0.6313
 281/1000 [=======>......................] - ETA: 5:25 - loss: 2.7027 - regression_loss: 2.0702 - classification_loss: 0.6325
 282/1000 [=======>......................] - ETA: 5:25 - loss: 2.6931 - regression_loss: 2.0629 - classification_loss: 0.6303
 283/1000 [=======>......................] - ETA: 5:24 - loss: 2.6836 - regression_loss: 2.0556 - classification_loss: 0.6280
 284/1000 [=======>......................] - ETA: 5:24 - loss: 2.6871 - regression_loss: 2.0596 - classification_loss: 0.6275
 285/1000 [=======>......................] - ETA: 5:23 - loss: 2.6777 - regression_loss: 2.0524 - classification_loss: 0.6253
 286/1000 [=======>......................] - ETA: 5:23 - loss: 2.6890 - regression_loss: 2.0630 - classification_loss: 0.6261
 287/1000 [=======>......................] - ETA: 5:22 - loss: 2.6861 - regression_loss: 2.0616 - classification_loss: 0.6245
 288/1000 [=======>......................] - ETA: 5:22 - loss: 2.6843 - regression_loss: 2.0610 - classification_loss: 0.6233
 289/1000 [=======>......................] - ETA: 5:21 - loss: 2.6856 - regression_loss: 2.0629 - classification_loss: 0.6227
 290/1000 [=======>......................] - ETA: 5:21 - loss: 2.6822 - regression_loss: 2.0607 - classification_loss: 0.6215
 291/1000 [=======>......................] - ETA: 5:21 - loss: 2.6861 - regression_loss: 2.0639 - classification_loss: 0.6222
 292/1000 [=======>......................] - ETA: 5:20 - loss: 2.6891 - regression_loss: 2.0669 - classification_loss: 0.6223
 293/1000 [=======>......................] - ETA: 5:20 - loss: 2.6938 - regression_loss: 2.0702 - classification_loss: 0.6236
 294/1000 [=======>......................] - ETA: 5:19 - loss: 2.7007 - regression_loss: 2.0731 - classification_loss: 0.6276
 295/1000 [=======>......................] - ETA: 5:19 - loss: 2.7080 - regression_loss: 2.0764 - classification_loss: 0.6317
 296/1000 [=======>......................] - ETA: 5:18 - loss: 2.7136 - regression_loss: 2.0787 - classification_loss: 0.6349
 297/1000 [=======>......................] - ETA: 5:18 - loss: 2.7174 - regression_loss: 2.0830 - classification_loss: 0.6345
 298/1000 [=======>......................] - ETA: 5:17 - loss: 2.7083 - regression_loss: 2.0760 - classification_loss: 0.6324
 299/1000 [=======>......................] - ETA: 5:17 - loss: 2.6993 - regression_loss: 2.0690 - classification_loss: 0.6302
 300/1000 [========>.....................] - ETA: 5:16 - loss: 2.7010 - regression_loss: 2.0716 - classification_loss: 0.6294
 301/1000 [========>.....................] - ETA: 5:16 - loss: 2.6920 - regression_loss: 2.0647 - classification_loss: 0.6273
 302/1000 [========>.....................] - ETA: 5:15 - loss: 2.6939 - regression_loss: 2.0660 - classification_loss: 0.6279
 303/1000 [========>.....................] - ETA: 5:15 - loss: 2.6970 - regression_loss: 2.0701 - classification_loss: 0.6268
 304/1000 [========>.....................] - ETA: 5:15 - loss: 2.6904 - regression_loss: 2.0633 - classification_loss: 0.6271
 305/1000 [========>.....................] - ETA: 5:14 - loss: 2.6923 - regression_loss: 2.0649 - classification_loss: 0.6274
 306/1000 [========>.....................] - ETA: 5:14 - loss: 2.6943 - regression_loss: 2.0671 - classification_loss: 0.6271
 307/1000 [========>.....................] - ETA: 5:13 - loss: 2.6936 - regression_loss: 2.0674 - classification_loss: 0.6262
 308/1000 [========>.....................] - ETA: 5:13 - loss: 2.6957 - regression_loss: 2.0680 - classification_loss: 0.6276
 309/1000 [========>.....................] - ETA: 5:12 - loss: 2.6869 - regression_loss: 2.0613 - classification_loss: 0.6256
 310/1000 [========>.....................] - ETA: 5:12 - loss: 2.6865 - regression_loss: 2.0621 - classification_loss: 0.6244
 311/1000 [========>.....................] - ETA: 5:11 - loss: 2.6878 - regression_loss: 2.0641 - classification_loss: 0.6237
 312/1000 [========>.....................] - ETA: 5:11 - loss: 2.6880 - regression_loss: 2.0644 - classification_loss: 0.6236
 313/1000 [========>.....................] - ETA: 5:11 - loss: 2.6962 - regression_loss: 2.0688 - classification_loss: 0.6274
 314/1000 [========>.....................] - ETA: 5:10 - loss: 2.6983 - regression_loss: 2.0708 - classification_loss: 0.6275
 315/1000 [========>.....................] - ETA: 5:10 - loss: 2.6994 - regression_loss: 2.0727 - classification_loss: 0.6268
 316/1000 [========>.....................] - ETA: 5:09 - loss: 2.7023 - regression_loss: 2.0754 - classification_loss: 0.6269
 317/1000 [========>.....................] - ETA: 5:09 - loss: 2.6937 - regression_loss: 2.0688 - classification_loss: 0.6249
 318/1000 [========>.....................] - ETA: 5:08 - loss: 2.6904 - regression_loss: 2.0666 - classification_loss: 0.6238
 319/1000 [========>.....................] - ETA: 5:08 - loss: 2.6959 - regression_loss: 2.0710 - classification_loss: 0.6249
 320/1000 [========>.....................] - ETA: 5:07 - loss: 2.6955 - regression_loss: 2.0716 - classification_loss: 0.6239
 321/1000 [========>.....................] - ETA: 5:07 - loss: 2.6871 - regression_loss: 2.0651 - classification_loss: 0.6220
 322/1000 [========>.....................] - ETA: 5:06 - loss: 2.6916 - regression_loss: 2.0665 - classification_loss: 0.6251
 323/1000 [========>.....................] - ETA: 5:06 - loss: 2.6845 - regression_loss: 2.0601 - classification_loss: 0.6245
 324/1000 [========>.....................] - ETA: 5:06 - loss: 2.6763 - regression_loss: 2.0537 - classification_loss: 0.6226
 325/1000 [========>.....................] - ETA: 5:05 - loss: 2.6755 - regression_loss: 2.0524 - classification_loss: 0.6231
 326/1000 [========>.....................] - ETA: 5:05 - loss: 2.6758 - regression_loss: 2.0534 - classification_loss: 0.6224
 327/1000 [========>.....................] - ETA: 5:04 - loss: 2.6676 - regression_loss: 2.0472 - classification_loss: 0.6205
 328/1000 [========>.....................] - ETA: 5:04 - loss: 2.6730 - regression_loss: 2.0496 - classification_loss: 0.6233
 329/1000 [========>.....................] - ETA: 5:03 - loss: 2.6769 - regression_loss: 2.0535 - classification_loss: 0.6234
 330/1000 [========>.....................] - ETA: 5:03 - loss: 2.6690 - regression_loss: 2.0473 - classification_loss: 0.6217
 331/1000 [========>.....................] - ETA: 5:02 - loss: 2.6655 - regression_loss: 2.0449 - classification_loss: 0.6206
 332/1000 [========>.....................] - ETA: 5:02 - loss: 2.6693 - regression_loss: 2.0456 - classification_loss: 0.6238
 333/1000 [========>.....................] - ETA: 5:02 - loss: 2.6742 - regression_loss: 2.0496 - classification_loss: 0.6245
 334/1000 [=========>....................] - ETA: 5:01 - loss: 2.6725 - regression_loss: 2.0493 - classification_loss: 0.6232
 335/1000 [=========>....................] - ETA: 5:01 - loss: 2.6645 - regression_loss: 2.0432 - classification_loss: 0.6214
 336/1000 [=========>....................] - ETA: 5:00 - loss: 2.6566 - regression_loss: 2.0371 - classification_loss: 0.6195
 337/1000 [=========>....................] - ETA: 5:00 - loss: 2.6577 - regression_loss: 2.0372 - classification_loss: 0.6205
 338/1000 [=========>....................] - ETA: 4:59 - loss: 2.6498 - regression_loss: 2.0312 - classification_loss: 0.6186
 339/1000 [=========>....................] - ETA: 4:59 - loss: 2.6519 - regression_loss: 2.0333 - classification_loss: 0.6186
 340/1000 [=========>....................] - ETA: 4:58 - loss: 2.6507 - regression_loss: 2.0323 - classification_loss: 0.6184
 341/1000 [=========>....................] - ETA: 4:58 - loss: 2.6429 - regression_loss: 2.0263 - classification_loss: 0.6165
 342/1000 [=========>....................] - ETA: 4:57 - loss: 2.6352 - regression_loss: 2.0204 - classification_loss: 0.6147
 343/1000 [=========>....................] - ETA: 4:57 - loss: 2.6348 - regression_loss: 2.0207 - classification_loss: 0.6141
 344/1000 [=========>....................] - ETA: 4:57 - loss: 2.6458 - regression_loss: 2.0283 - classification_loss: 0.6175
 345/1000 [=========>....................] - ETA: 4:56 - loss: 2.6465 - regression_loss: 2.0285 - classification_loss: 0.6180
 346/1000 [=========>....................] - ETA: 4:56 - loss: 2.6429 - regression_loss: 2.0261 - classification_loss: 0.6168
 347/1000 [=========>....................] - ETA: 4:55 - loss: 2.6453 - regression_loss: 2.0273 - classification_loss: 0.6179
 348/1000 [=========>....................] - ETA: 4:55 - loss: 2.6377 - regression_loss: 2.0215 - classification_loss: 0.6162
 349/1000 [=========>....................] - ETA: 4:54 - loss: 2.6375 - regression_loss: 2.0221 - classification_loss: 0.6154
 350/1000 [=========>....................] - ETA: 4:54 - loss: 2.6404 - regression_loss: 2.0250 - classification_loss: 0.6153
 351/1000 [=========>....................] - ETA: 4:53 - loss: 2.6430 - regression_loss: 2.0279 - classification_loss: 0.6151
 352/1000 [=========>....................] - ETA: 4:53 - loss: 2.6514 - regression_loss: 2.0326 - classification_loss: 0.6187
 353/1000 [=========>....................] - ETA: 4:53 - loss: 2.6544 - regression_loss: 2.0349 - classification_loss: 0.6194
 354/1000 [=========>....................] - ETA: 4:52 - loss: 2.6646 - regression_loss: 2.0421 - classification_loss: 0.6225
 355/1000 [=========>....................] - ETA: 4:52 - loss: 2.6631 - regression_loss: 2.0417 - classification_loss: 0.6214
 356/1000 [=========>....................] - ETA: 4:51 - loss: 2.6627 - regression_loss: 2.0422 - classification_loss: 0.6205
 357/1000 [=========>....................] - ETA: 4:51 - loss: 2.6687 - regression_loss: 2.0459 - classification_loss: 0.6228
 358/1000 [=========>....................] - ETA: 4:50 - loss: 2.6689 - regression_loss: 2.0463 - classification_loss: 0.6226
 359/1000 [=========>....................] - ETA: 4:50 - loss: 2.6697 - regression_loss: 2.0482 - classification_loss: 0.6215
 360/1000 [=========>....................] - ETA: 4:49 - loss: 2.6702 - regression_loss: 2.0493 - classification_loss: 0.6209
 361/1000 [=========>....................] - ETA: 4:49 - loss: 2.6628 - regression_loss: 2.0436 - classification_loss: 0.6192
 362/1000 [=========>....................] - ETA: 4:48 - loss: 2.6555 - regression_loss: 2.0380 - classification_loss: 0.6175
 363/1000 [=========>....................] - ETA: 4:48 - loss: 2.6612 - regression_loss: 2.0415 - classification_loss: 0.6197
 364/1000 [=========>....................] - ETA: 4:48 - loss: 2.6539 - regression_loss: 2.0359 - classification_loss: 0.6180
 365/1000 [=========>....................] - ETA: 4:47 - loss: 2.6577 - regression_loss: 2.0391 - classification_loss: 0.6186
 366/1000 [=========>....................] - ETA: 4:47 - loss: 2.6507 - regression_loss: 2.0335 - classification_loss: 0.6172
 367/1000 [==========>...................] - ETA: 4:46 - loss: 2.6510 - regression_loss: 2.0349 - classification_loss: 0.6161
 368/1000 [==========>...................] - ETA: 4:46 - loss: 2.6526 - regression_loss: 2.0363 - classification_loss: 0.6163
 369/1000 [==========>...................] - ETA: 4:45 - loss: 2.6556 - regression_loss: 2.0393 - classification_loss: 0.6164
 370/1000 [==========>...................] - ETA: 4:45 - loss: 2.6547 - regression_loss: 2.0391 - classification_loss: 0.6156
 371/1000 [==========>...................] - ETA: 4:44 - loss: 2.6593 - regression_loss: 2.0420 - classification_loss: 0.6173
 372/1000 [==========>...................] - ETA: 4:44 - loss: 2.6655 - regression_loss: 2.0478 - classification_loss: 0.6177
 373/1000 [==========>...................] - ETA: 4:43 - loss: 2.6663 - regression_loss: 2.0481 - classification_loss: 0.6182
 374/1000 [==========>...................] - ETA: 4:43 - loss: 2.6653 - regression_loss: 2.0472 - classification_loss: 0.6181
 375/1000 [==========>...................] - ETA: 4:43 - loss: 2.6673 - regression_loss: 2.0497 - classification_loss: 0.6176
 376/1000 [==========>...................] - ETA: 4:42 - loss: 2.6717 - regression_loss: 2.0518 - classification_loss: 0.6198
 377/1000 [==========>...................] - ETA: 4:42 - loss: 2.6650 - regression_loss: 2.0464 - classification_loss: 0.6186
 378/1000 [==========>...................] - ETA: 4:41 - loss: 2.6674 - regression_loss: 2.0494 - classification_loss: 0.6180
 379/1000 [==========>...................] - ETA: 4:41 - loss: 2.6771 - regression_loss: 2.0573 - classification_loss: 0.6199
 380/1000 [==========>...................] - ETA: 4:40 - loss: 2.6773 - regression_loss: 2.0576 - classification_loss: 0.6198
 381/1000 [==========>...................] - ETA: 4:40 - loss: 2.6751 - regression_loss: 2.0562 - classification_loss: 0.6189
 382/1000 [==========>...................] - ETA: 4:39 - loss: 2.6762 - regression_loss: 2.0574 - classification_loss: 0.6187
 383/1000 [==========>...................] - ETA: 4:39 - loss: 2.6692 - regression_loss: 2.0521 - classification_loss: 0.6171
 384/1000 [==========>...................] - ETA: 4:38 - loss: 2.6697 - regression_loss: 2.0531 - classification_loss: 0.6166
 385/1000 [==========>...................] - ETA: 4:38 - loss: 2.6637 - regression_loss: 2.0478 - classification_loss: 0.6159
 386/1000 [==========>...................] - ETA: 4:38 - loss: 2.6705 - regression_loss: 2.0543 - classification_loss: 0.6162
 387/1000 [==========>...................] - ETA: 4:37 - loss: 2.6768 - regression_loss: 2.0571 - classification_loss: 0.6197
 388/1000 [==========>...................] - ETA: 4:37 - loss: 2.6809 - regression_loss: 2.0594 - classification_loss: 0.6215
 389/1000 [==========>...................] - ETA: 4:36 - loss: 2.6843 - regression_loss: 2.0626 - classification_loss: 0.6217
 390/1000 [==========>...................] - ETA: 4:36 - loss: 2.6835 - regression_loss: 2.0623 - classification_loss: 0.6212
 391/1000 [==========>...................] - ETA: 4:35 - loss: 2.6874 - regression_loss: 2.0660 - classification_loss: 0.6213
 392/1000 [==========>...................] - ETA: 4:35 - loss: 2.6933 - regression_loss: 2.0706 - classification_loss: 0.6227
 393/1000 [==========>...................] - ETA: 4:34 - loss: 2.6865 - regression_loss: 2.0654 - classification_loss: 0.6211
 394/1000 [==========>...................] - ETA: 4:34 - loss: 2.6937 - regression_loss: 2.0704 - classification_loss: 0.6233
 395/1000 [==========>...................] - ETA: 4:33 - loss: 2.6925 - regression_loss: 2.0699 - classification_loss: 0.6226
 396/1000 [==========>...................] - ETA: 4:33 - loss: 2.6911 - regression_loss: 2.0694 - classification_loss: 0.6216
 397/1000 [==========>...................] - ETA: 4:33 - loss: 2.6942 - regression_loss: 2.0724 - classification_loss: 0.6218
 398/1000 [==========>...................] - ETA: 4:32 - loss: 2.6951 - regression_loss: 2.0733 - classification_loss: 0.6218
 399/1000 [==========>...................] - ETA: 4:32 - loss: 2.6949 - regression_loss: 2.0732 - classification_loss: 0.6217
 400/1000 [===========>..................] - ETA: 4:31 - loss: 2.6944 - regression_loss: 2.0731 - classification_loss: 0.6213
 401/1000 [===========>..................] - ETA: 4:31 - loss: 2.7003 - regression_loss: 2.0778 - classification_loss: 0.6224
 402/1000 [===========>..................] - ETA: 4:30 - loss: 2.7034 - regression_loss: 2.0803 - classification_loss: 0.6231
 403/1000 [===========>..................] - ETA: 4:30 - loss: 2.7051 - regression_loss: 2.0810 - classification_loss: 0.6240
 404/1000 [===========>..................] - ETA: 4:29 - loss: 2.7102 - regression_loss: 2.0849 - classification_loss: 0.6253
 405/1000 [===========>..................] - ETA: 4:29 - loss: 2.7105 - regression_loss: 2.0851 - classification_loss: 0.6254
 406/1000 [===========>..................] - ETA: 4:29 - loss: 2.7090 - regression_loss: 2.0839 - classification_loss: 0.6251
 407/1000 [===========>..................] - ETA: 4:28 - loss: 2.7096 - regression_loss: 2.0847 - classification_loss: 0.6250
 408/1000 [===========>..................] - ETA: 4:28 - loss: 2.7134 - regression_loss: 2.0887 - classification_loss: 0.6247
 409/1000 [===========>..................] - ETA: 4:27 - loss: 2.7149 - regression_loss: 2.0900 - classification_loss: 0.6248
 410/1000 [===========>..................] - ETA: 4:27 - loss: 2.7148 - regression_loss: 2.0900 - classification_loss: 0.6249
 411/1000 [===========>..................] - ETA: 4:26 - loss: 2.7082 - regression_loss: 2.0849 - classification_loss: 0.6234
 412/1000 [===========>..................] - ETA: 4:26 - loss: 2.7017 - regression_loss: 2.0798 - classification_loss: 0.6219
 413/1000 [===========>..................] - ETA: 4:25 - loss: 2.7014 - regression_loss: 2.0800 - classification_loss: 0.6214
 414/1000 [===========>..................] - ETA: 4:25 - loss: 2.7049 - regression_loss: 2.0843 - classification_loss: 0.6206
 415/1000 [===========>..................] - ETA: 4:24 - loss: 2.7027 - regression_loss: 2.0828 - classification_loss: 0.6199
 416/1000 [===========>..................] - ETA: 4:24 - loss: 2.6962 - regression_loss: 2.0778 - classification_loss: 0.6184
 417/1000 [===========>..................] - ETA: 4:24 - loss: 2.6969 - regression_loss: 2.0784 - classification_loss: 0.6184
 418/1000 [===========>..................] - ETA: 4:23 - loss: 2.6984 - regression_loss: 2.0794 - classification_loss: 0.6189
 419/1000 [===========>..................] - ETA: 4:23 - loss: 2.7045 - regression_loss: 2.0842 - classification_loss: 0.6202
 420/1000 [===========>..................] - ETA: 4:22 - loss: 2.7081 - regression_loss: 2.0885 - classification_loss: 0.6196
 421/1000 [===========>..................] - ETA: 4:22 - loss: 2.7075 - regression_loss: 2.0887 - classification_loss: 0.6189
 422/1000 [===========>..................] - ETA: 4:21 - loss: 2.7011 - regression_loss: 2.0837 - classification_loss: 0.6174
 423/1000 [===========>..................] - ETA: 4:21 - loss: 2.7064 - regression_loss: 2.0874 - classification_loss: 0.6190
 424/1000 [===========>..................] - ETA: 4:20 - loss: 2.7043 - regression_loss: 2.0858 - classification_loss: 0.6186
 425/1000 [===========>..................] - ETA: 4:20 - loss: 2.7033 - regression_loss: 2.0853 - classification_loss: 0.6179
 426/1000 [===========>..................] - ETA: 4:19 - loss: 2.7080 - regression_loss: 2.0880 - classification_loss: 0.6199
 427/1000 [===========>..................] - ETA: 4:19 - loss: 2.7097 - regression_loss: 2.0899 - classification_loss: 0.6198
 428/1000 [===========>..................] - ETA: 4:19 - loss: 2.7103 - regression_loss: 2.0906 - classification_loss: 0.6196
 429/1000 [===========>..................] - ETA: 4:18 - loss: 2.7040 - regression_loss: 2.0858 - classification_loss: 0.6182
 430/1000 [===========>..................] - ETA: 4:18 - loss: 2.7062 - regression_loss: 2.0877 - classification_loss: 0.6185
 431/1000 [===========>..................] - ETA: 4:17 - loss: 2.7098 - regression_loss: 2.0892 - classification_loss: 0.6205
 432/1000 [===========>..................] - ETA: 4:17 - loss: 2.7104 - regression_loss: 2.0882 - classification_loss: 0.6222
 433/1000 [===========>..................] - ETA: 4:16 - loss: 2.7140 - regression_loss: 2.0913 - classification_loss: 0.6227
 434/1000 [============>.................] - ETA: 4:16 - loss: 2.7180 - regression_loss: 2.0945 - classification_loss: 0.6235
 435/1000 [============>.................] - ETA: 4:15 - loss: 2.7194 - regression_loss: 2.0954 - classification_loss: 0.6240
 436/1000 [============>.................] - ETA: 4:15 - loss: 2.7241 - regression_loss: 2.0984 - classification_loss: 0.6257
 437/1000 [============>.................] - ETA: 4:14 - loss: 2.7238 - regression_loss: 2.0985 - classification_loss: 0.6253
 438/1000 [============>.................] - ETA: 4:14 - loss: 2.7234 - regression_loss: 2.0978 - classification_loss: 0.6256
 439/1000 [============>.................] - ETA: 4:14 - loss: 2.7172 - regression_loss: 2.0930 - classification_loss: 0.6241
 440/1000 [============>.................] - ETA: 4:13 - loss: 2.7176 - regression_loss: 2.0942 - classification_loss: 0.6234
 441/1000 [============>.................] - ETA: 4:13 - loss: 2.7154 - regression_loss: 2.0929 - classification_loss: 0.6224
 442/1000 [============>.................] - ETA: 4:12 - loss: 2.7175 - regression_loss: 2.0941 - classification_loss: 0.6235
 443/1000 [============>.................] - ETA: 4:12 - loss: 2.7216 - regression_loss: 2.0968 - classification_loss: 0.6249
 444/1000 [============>.................] - ETA: 4:11 - loss: 2.7222 - regression_loss: 2.0982 - classification_loss: 0.6239
 445/1000 [============>.................] - ETA: 4:11 - loss: 2.7227 - regression_loss: 2.0990 - classification_loss: 0.6237
 446/1000 [============>.................] - ETA: 4:10 - loss: 2.7166 - regression_loss: 2.0943 - classification_loss: 0.6223
 447/1000 [============>.................] - ETA: 4:10 - loss: 2.7203 - regression_loss: 2.0971 - classification_loss: 0.6232
 448/1000 [============>.................] - ETA: 4:10 - loss: 2.7235 - regression_loss: 2.0999 - classification_loss: 0.6236
 449/1000 [============>.................] - ETA: 4:09 - loss: 2.7175 - regression_loss: 2.0952 - classification_loss: 0.6222
 450/1000 [============>.................] - ETA: 4:09 - loss: 2.7117 - regression_loss: 2.0906 - classification_loss: 0.6211
 451/1000 [============>.................] - ETA: 4:08 - loss: 2.7135 - regression_loss: 2.0928 - classification_loss: 0.6206
 452/1000 [============>.................] - ETA: 4:08 - loss: 2.7157 - regression_loss: 2.0939 - classification_loss: 0.6218
 453/1000 [============>.................] - ETA: 4:07 - loss: 2.7142 - regression_loss: 2.0928 - classification_loss: 0.6214
 454/1000 [============>.................] - ETA: 4:07 - loss: 2.7168 - regression_loss: 2.0949 - classification_loss: 0.6219
 455/1000 [============>.................] - ETA: 4:06 - loss: 2.7225 - regression_loss: 2.1007 - classification_loss: 0.6218
 456/1000 [============>.................] - ETA: 4:06 - loss: 2.7251 - regression_loss: 2.1029 - classification_loss: 0.6223
 457/1000 [============>.................] - ETA: 4:05 - loss: 2.7192 - regression_loss: 2.0983 - classification_loss: 0.6209
 458/1000 [============>.................] - ETA: 4:05 - loss: 2.7133 - regression_loss: 2.0937 - classification_loss: 0.6195
 459/1000 [============>.................] - ETA: 4:05 - loss: 2.7128 - regression_loss: 2.0932 - classification_loss: 0.6196
 460/1000 [============>.................] - ETA: 4:04 - loss: 2.7110 - regression_loss: 2.0922 - classification_loss: 0.6188
 461/1000 [============>.................] - ETA: 4:04 - loss: 2.7113 - regression_loss: 2.0929 - classification_loss: 0.6184
 462/1000 [============>.................] - ETA: 4:03 - loss: 2.7055 - regression_loss: 2.0884 - classification_loss: 0.6171
 463/1000 [============>.................] - ETA: 4:03 - loss: 2.7091 - regression_loss: 2.0904 - classification_loss: 0.6187
 464/1000 [============>.................] - ETA: 4:02 - loss: 2.7089 - regression_loss: 2.0911 - classification_loss: 0.6179
 465/1000 [============>.................] - ETA: 4:02 - loss: 2.7114 - regression_loss: 2.0910 - classification_loss: 0.6204
 466/1000 [============>.................] - ETA: 4:01 - loss: 2.7126 - regression_loss: 2.0914 - classification_loss: 0.6212
 467/1000 [=============>................] - ETA: 4:01 - loss: 2.7134 - regression_loss: 2.0912 - classification_loss: 0.6222
 468/1000 [=============>................] - ETA: 4:00 - loss: 2.7152 - regression_loss: 2.0926 - classification_loss: 0.6227
 469/1000 [=============>................] - ETA: 4:00 - loss: 2.7095 - regression_loss: 2.0881 - classification_loss: 0.6213
 470/1000 [=============>................] - ETA: 4:00 - loss: 2.7106 - regression_loss: 2.0895 - classification_loss: 0.6210
 471/1000 [=============>................] - ETA: 3:59 - loss: 2.7113 - regression_loss: 2.0905 - classification_loss: 0.6207
 472/1000 [=============>................] - ETA: 3:59 - loss: 2.7123 - regression_loss: 2.0916 - classification_loss: 0.6207
 473/1000 [=============>................] - ETA: 3:58 - loss: 2.7141 - regression_loss: 2.0920 - classification_loss: 0.6221
 474/1000 [=============>................] - ETA: 3:58 - loss: 2.7192 - regression_loss: 2.0956 - classification_loss: 0.6236
 475/1000 [=============>................] - ETA: 3:57 - loss: 2.7201 - regression_loss: 2.0952 - classification_loss: 0.6250
 476/1000 [=============>................] - ETA: 3:57 - loss: 2.7194 - regression_loss: 2.0946 - classification_loss: 0.6247
 477/1000 [=============>................] - ETA: 3:56 - loss: 2.7137 - regression_loss: 2.0902 - classification_loss: 0.6234
 478/1000 [=============>................] - ETA: 3:56 - loss: 2.7148 - regression_loss: 2.0910 - classification_loss: 0.6239
 479/1000 [=============>................] - ETA: 3:55 - loss: 2.7157 - regression_loss: 2.0912 - classification_loss: 0.6245
 480/1000 [=============>................] - ETA: 3:55 - loss: 2.7162 - regression_loss: 2.0903 - classification_loss: 0.6259
 481/1000 [=============>................] - ETA: 3:55 - loss: 2.7185 - regression_loss: 2.0923 - classification_loss: 0.6262
 482/1000 [=============>................] - ETA: 3:54 - loss: 2.7225 - regression_loss: 2.0952 - classification_loss: 0.6274
 483/1000 [=============>................] - ETA: 3:54 - loss: 2.7250 - regression_loss: 2.0978 - classification_loss: 0.6272
 484/1000 [=============>................] - ETA: 3:53 - loss: 2.7263 - regression_loss: 2.0996 - classification_loss: 0.6267
 485/1000 [=============>................] - ETA: 3:53 - loss: 2.7266 - regression_loss: 2.1002 - classification_loss: 0.6264
 486/1000 [=============>................] - ETA: 3:52 - loss: 2.7286 - regression_loss: 2.1020 - classification_loss: 0.6266
 487/1000 [=============>................] - ETA: 3:52 - loss: 2.7234 - regression_loss: 2.0977 - classification_loss: 0.6257
 488/1000 [=============>................] - ETA: 3:51 - loss: 2.7235 - regression_loss: 2.0984 - classification_loss: 0.6252
 489/1000 [=============>................] - ETA: 3:51 - loss: 2.7181 - regression_loss: 2.0941 - classification_loss: 0.6240
 490/1000 [=============>................] - ETA: 3:51 - loss: 2.7128 - regression_loss: 2.0898 - classification_loss: 0.6230
 491/1000 [=============>................] - ETA: 3:50 - loss: 2.7073 - regression_loss: 2.0855 - classification_loss: 0.6218
 492/1000 [=============>................] - ETA: 3:50 - loss: 2.7125 - regression_loss: 2.0877 - classification_loss: 0.6249
 493/1000 [=============>................] - ETA: 3:49 - loss: 2.7146 - regression_loss: 2.0898 - classification_loss: 0.6248
 494/1000 [=============>................] - ETA: 3:49 - loss: 2.7141 - regression_loss: 2.0899 - classification_loss: 0.6242
 495/1000 [=============>................] - ETA: 3:48 - loss: 2.7171 - regression_loss: 2.0925 - classification_loss: 0.6246
 496/1000 [=============>................] - ETA: 3:48 - loss: 2.7168 - regression_loss: 2.0925 - classification_loss: 0.6243
 497/1000 [=============>................] - ETA: 3:47 - loss: 2.7213 - regression_loss: 2.0968 - classification_loss: 0.6245
 498/1000 [=============>................] - ETA: 3:47 - loss: 2.7159 - regression_loss: 2.0926 - classification_loss: 0.6232
 499/1000 [=============>................] - ETA: 3:46 - loss: 2.7158 - regression_loss: 2.0931 - classification_loss: 0.6228
 500/1000 [==============>...............] - ETA: 3:46 - loss: 2.7160 - regression_loss: 2.0923 - classification_loss: 0.6238
 501/1000 [==============>...............] - ETA: 3:46 - loss: 2.7165 - regression_loss: 2.0924 - classification_loss: 0.6241
 502/1000 [==============>...............] - ETA: 3:45 - loss: 2.7155 - regression_loss: 2.0921 - classification_loss: 0.6234
 503/1000 [==============>...............] - ETA: 3:45 - loss: 2.7183 - regression_loss: 2.0943 - classification_loss: 0.6240
 504/1000 [==============>...............] - ETA: 3:44 - loss: 2.7153 - regression_loss: 2.0920 - classification_loss: 0.6233
 505/1000 [==============>...............] - ETA: 3:44 - loss: 2.7157 - regression_loss: 2.0927 - classification_loss: 0.6230
 506/1000 [==============>...............] - ETA: 3:43 - loss: 2.7203 - regression_loss: 2.0947 - classification_loss: 0.6255
 507/1000 [==============>...............] - ETA: 3:43 - loss: 2.7202 - regression_loss: 2.0943 - classification_loss: 0.6259
 508/1000 [==============>...............] - ETA: 3:42 - loss: 2.7189 - regression_loss: 2.0931 - classification_loss: 0.6258
 509/1000 [==============>...............] - ETA: 3:42 - loss: 2.7216 - regression_loss: 2.0945 - classification_loss: 0.6272
 510/1000 [==============>...............] - ETA: 3:41 - loss: 2.7248 - regression_loss: 2.0972 - classification_loss: 0.6276
 511/1000 [==============>...............] - ETA: 3:41 - loss: 2.7257 - regression_loss: 2.0981 - classification_loss: 0.6275
 512/1000 [==============>...............] - ETA: 3:41 - loss: 2.7301 - regression_loss: 2.1020 - classification_loss: 0.6281
 513/1000 [==============>...............] - ETA: 3:40 - loss: 2.7285 - regression_loss: 2.1010 - classification_loss: 0.6274
 514/1000 [==============>...............] - ETA: 3:40 - loss: 2.7268 - regression_loss: 2.1002 - classification_loss: 0.6266
 515/1000 [==============>...............] - ETA: 3:39 - loss: 2.7217 - regression_loss: 2.0961 - classification_loss: 0.6255
 516/1000 [==============>...............] - ETA: 3:39 - loss: 2.7218 - regression_loss: 2.0963 - classification_loss: 0.6255
 517/1000 [==============>...............] - ETA: 3:38 - loss: 2.7245 - regression_loss: 2.0986 - classification_loss: 0.6259
 518/1000 [==============>...............] - ETA: 3:38 - loss: 2.7236 - regression_loss: 2.0946 - classification_loss: 0.6291
 519/1000 [==============>...............] - ETA: 3:37 - loss: 2.7240 - regression_loss: 2.0950 - classification_loss: 0.6290
 520/1000 [==============>...............] - ETA: 3:37 - loss: 2.7266 - regression_loss: 2.0975 - classification_loss: 0.6291
 521/1000 [==============>...............] - ETA: 3:36 - loss: 2.7274 - regression_loss: 2.0983 - classification_loss: 0.6291
 522/1000 [==============>...............] - ETA: 3:36 - loss: 2.7316 - regression_loss: 2.1011 - classification_loss: 0.6306
 523/1000 [==============>...............] - ETA: 3:36 - loss: 2.7335 - regression_loss: 2.1022 - classification_loss: 0.6313
 524/1000 [==============>...............] - ETA: 3:35 - loss: 2.7282 - regression_loss: 2.0981 - classification_loss: 0.6301
 525/1000 [==============>...............] - ETA: 3:35 - loss: 2.7268 - regression_loss: 2.0972 - classification_loss: 0.6297
 526/1000 [==============>...............] - ETA: 3:34 - loss: 2.7279 - regression_loss: 2.0979 - classification_loss: 0.6301
 527/1000 [==============>...............] - ETA: 3:34 - loss: 2.7328 - regression_loss: 2.1011 - classification_loss: 0.6317
 528/1000 [==============>...............] - ETA: 3:33 - loss: 2.7364 - regression_loss: 2.1037 - classification_loss: 0.6327
 529/1000 [==============>...............] - ETA: 3:33 - loss: 2.7370 - regression_loss: 2.1044 - classification_loss: 0.6326
 530/1000 [==============>...............] - ETA: 3:32 - loss: 2.7384 - regression_loss: 2.1055 - classification_loss: 0.6329
 531/1000 [==============>...............] - ETA: 3:32 - loss: 2.7398 - regression_loss: 2.1065 - classification_loss: 0.6332
 532/1000 [==============>...............] - ETA: 3:31 - loss: 2.7433 - regression_loss: 2.1100 - classification_loss: 0.6333
 533/1000 [==============>...............] - ETA: 3:31 - loss: 2.7421 - regression_loss: 2.1090 - classification_loss: 0.6331
 534/1000 [===============>..............] - ETA: 3:31 - loss: 2.7435 - regression_loss: 2.1106 - classification_loss: 0.6329
 535/1000 [===============>..............] - ETA: 3:30 - loss: 2.7416 - regression_loss: 2.1066 - classification_loss: 0.6350
 536/1000 [===============>..............] - ETA: 3:30 - loss: 2.7446 - regression_loss: 2.1094 - classification_loss: 0.6352
 537/1000 [===============>..............] - ETA: 3:29 - loss: 2.7501 - regression_loss: 2.1149 - classification_loss: 0.6352
 538/1000 [===============>..............] - ETA: 3:29 - loss: 2.7535 - regression_loss: 2.1169 - classification_loss: 0.6365
 539/1000 [===============>..............] - ETA: 3:28 - loss: 2.7531 - regression_loss: 2.1167 - classification_loss: 0.6364
 540/1000 [===============>..............] - ETA: 3:28 - loss: 2.7481 - regression_loss: 2.1128 - classification_loss: 0.6354
 541/1000 [===============>..............] - ETA: 3:27 - loss: 2.7478 - regression_loss: 2.1126 - classification_loss: 0.6352
 542/1000 [===============>..............] - ETA: 3:27 - loss: 2.7471 - regression_loss: 2.1123 - classification_loss: 0.6348
 543/1000 [===============>..............] - ETA: 3:27 - loss: 2.7426 - regression_loss: 2.1084 - classification_loss: 0.6342
 544/1000 [===============>..............] - ETA: 3:26 - loss: 2.7444 - regression_loss: 2.1101 - classification_loss: 0.6343
 545/1000 [===============>..............] - ETA: 3:26 - loss: 2.7398 - regression_loss: 2.1062 - classification_loss: 0.6336
 546/1000 [===============>..............] - ETA: 3:25 - loss: 2.7427 - regression_loss: 2.1081 - classification_loss: 0.6346
 547/1000 [===============>..............] - ETA: 3:25 - loss: 2.7422 - regression_loss: 2.1080 - classification_loss: 0.6342
 548/1000 [===============>..............] - ETA: 3:24 - loss: 2.7447 - regression_loss: 2.1102 - classification_loss: 0.6344
 549/1000 [===============>..............] - ETA: 3:24 - loss: 2.7470 - regression_loss: 2.1119 - classification_loss: 0.6352
 550/1000 [===============>..............] - ETA: 3:23 - loss: 2.7420 - regression_loss: 2.1080 - classification_loss: 0.6340
 551/1000 [===============>..............] - ETA: 3:23 - loss: 2.7427 - regression_loss: 2.1088 - classification_loss: 0.6339
 552/1000 [===============>..............] - ETA: 3:22 - loss: 2.7450 - regression_loss: 2.1113 - classification_loss: 0.6337
 553/1000 [===============>..............] - ETA: 3:22 - loss: 2.7465 - regression_loss: 2.1121 - classification_loss: 0.6343
 554/1000 [===============>..............] - ETA: 3:21 - loss: 2.7509 - regression_loss: 2.1159 - classification_loss: 0.6350
 555/1000 [===============>..............] - ETA: 3:21 - loss: 2.7523 - regression_loss: 2.1173 - classification_loss: 0.6350
 556/1000 [===============>..............] - ETA: 3:21 - loss: 2.7509 - regression_loss: 2.1165 - classification_loss: 0.6343
 557/1000 [===============>..............] - ETA: 3:20 - loss: 2.7524 - regression_loss: 2.1177 - classification_loss: 0.6346
 558/1000 [===============>..............] - ETA: 3:20 - loss: 2.7513 - regression_loss: 2.1170 - classification_loss: 0.6343
 559/1000 [===============>..............] - ETA: 3:19 - loss: 2.7516 - regression_loss: 2.1174 - classification_loss: 0.6342
 560/1000 [===============>..............] - ETA: 3:19 - loss: 2.7559 - regression_loss: 2.1207 - classification_loss: 0.6352
 561/1000 [===============>..............] - ETA: 3:18 - loss: 2.7517 - regression_loss: 2.1170 - classification_loss: 0.6348
 562/1000 [===============>..............] - ETA: 3:18 - loss: 2.7468 - regression_loss: 2.1132 - classification_loss: 0.6337
 563/1000 [===============>..............] - ETA: 3:17 - loss: 2.7452 - regression_loss: 2.1119 - classification_loss: 0.6333
 564/1000 [===============>..............] - ETA: 3:17 - loss: 2.7464 - regression_loss: 2.1131 - classification_loss: 0.6333
 565/1000 [===============>..............] - ETA: 3:17 - loss: 2.7453 - regression_loss: 2.1122 - classification_loss: 0.6331
 566/1000 [===============>..............] - ETA: 3:16 - loss: 2.7405 - regression_loss: 2.1085 - classification_loss: 0.6320
 567/1000 [================>.............] - ETA: 3:16 - loss: 2.7418 - regression_loss: 2.1093 - classification_loss: 0.6325
 568/1000 [================>.............] - ETA: 3:15 - loss: 2.7457 - regression_loss: 2.1127 - classification_loss: 0.6330
 569/1000 [================>.............] - ETA: 3:15 - loss: 2.7499 - regression_loss: 2.1161 - classification_loss: 0.6338
 570/1000 [================>.............] - ETA: 3:14 - loss: 2.7451 - regression_loss: 2.1124 - classification_loss: 0.6327
 571/1000 [================>.............] - ETA: 3:14 - loss: 2.7466 - regression_loss: 2.1144 - classification_loss: 0.6323
 572/1000 [================>.............] - ETA: 3:13 - loss: 2.7476 - regression_loss: 2.1148 - classification_loss: 0.6328
 573/1000 [================>.............] - ETA: 3:13 - loss: 2.7508 - regression_loss: 2.1166 - classification_loss: 0.6342
 574/1000 [================>.............] - ETA: 3:12 - loss: 2.7543 - regression_loss: 2.1201 - classification_loss: 0.6342
 575/1000 [================>.............] - ETA: 3:12 - loss: 2.7546 - regression_loss: 2.1203 - classification_loss: 0.6343
 576/1000 [================>.............] - ETA: 3:12 - loss: 2.7535 - regression_loss: 2.1197 - classification_loss: 0.6338
 577/1000 [================>.............] - ETA: 3:11 - loss: 2.7533 - regression_loss: 2.1199 - classification_loss: 0.6334
 578/1000 [================>.............] - ETA: 3:11 - loss: 2.7545 - regression_loss: 2.1209 - classification_loss: 0.6335
 579/1000 [================>.............] - ETA: 3:10 - loss: 2.7568 - regression_loss: 2.1231 - classification_loss: 0.6337
 580/1000 [================>.............] - ETA: 3:10 - loss: 2.7606 - regression_loss: 2.1250 - classification_loss: 0.6356
 581/1000 [================>.............] - ETA: 3:09 - loss: 2.7636 - regression_loss: 2.1266 - classification_loss: 0.6370
 582/1000 [================>.............] - ETA: 3:09 - loss: 2.7681 - regression_loss: 2.1301 - classification_loss: 0.6381
 583/1000 [================>.............] - ETA: 3:08 - loss: 2.7634 - regression_loss: 2.1264 - classification_loss: 0.6370
 584/1000 [================>.............] - ETA: 3:08 - loss: 2.7643 - regression_loss: 2.1278 - classification_loss: 0.6365
 585/1000 [================>.............] - ETA: 3:07 - loss: 2.7686 - regression_loss: 2.1311 - classification_loss: 0.6375
 586/1000 [================>.............] - ETA: 3:07 - loss: 2.7717 - regression_loss: 2.1334 - classification_loss: 0.6383
 587/1000 [================>.............] - ETA: 3:06 - loss: 2.7734 - regression_loss: 2.1349 - classification_loss: 0.6386
 588/1000 [================>.............] - ETA: 3:06 - loss: 2.7687 - regression_loss: 2.1312 - classification_loss: 0.6375
 589/1000 [================>.............] - ETA: 3:06 - loss: 2.7691 - regression_loss: 2.1316 - classification_loss: 0.6375
 590/1000 [================>.............] - ETA: 3:05 - loss: 2.7706 - regression_loss: 2.1334 - classification_loss: 0.6372
 591/1000 [================>.............] - ETA: 3:05 - loss: 2.7659 - regression_loss: 2.1298 - classification_loss: 0.6361
 592/1000 [================>.............] - ETA: 3:04 - loss: 2.7673 - regression_loss: 2.1310 - classification_loss: 0.6363
 593/1000 [================>.............] - ETA: 3:04 - loss: 2.7674 - regression_loss: 2.1311 - classification_loss: 0.6363
 594/1000 [================>.............] - ETA: 3:03 - loss: 2.7667 - regression_loss: 2.1308 - classification_loss: 0.6359
 595/1000 [================>.............] - ETA: 3:03 - loss: 2.7654 - regression_loss: 2.1299 - classification_loss: 0.6355
 596/1000 [================>.............] - ETA: 3:02 - loss: 2.7655 - regression_loss: 2.1301 - classification_loss: 0.6354
 597/1000 [================>.............] - ETA: 3:02 - loss: 2.7677 - regression_loss: 2.1319 - classification_loss: 0.6359
 598/1000 [================>.............] - ETA: 3:02 - loss: 2.7682 - regression_loss: 2.1323 - classification_loss: 0.6359
 599/1000 [================>.............] - ETA: 3:01 - loss: 2.7704 - regression_loss: 2.1341 - classification_loss: 0.6364
 600/1000 [=================>............] - ETA: 3:01 - loss: 2.7691 - regression_loss: 2.1335 - classification_loss: 0.6357
 601/1000 [=================>............] - ETA: 3:00 - loss: 2.7646 - regression_loss: 2.1299 - classification_loss: 0.6346
 602/1000 [=================>............] - ETA: 3:00 - loss: 2.7673 - regression_loss: 2.1318 - classification_loss: 0.6355
 603/1000 [=================>............] - ETA: 2:59 - loss: 2.7653 - regression_loss: 2.1305 - classification_loss: 0.6348
 604/1000 [=================>............] - ETA: 2:59 - loss: 2.7682 - regression_loss: 2.1322 - classification_loss: 0.6360
 605/1000 [=================>............] - ETA: 2:58 - loss: 2.7636 - regression_loss: 2.1286 - classification_loss: 0.6350
 606/1000 [=================>............] - ETA: 2:58 - loss: 2.7591 - regression_loss: 2.1251 - classification_loss: 0.6339
 607/1000 [=================>............] - ETA: 2:57 - loss: 2.7584 - regression_loss: 2.1248 - classification_loss: 0.6336
 608/1000 [=================>............] - ETA: 2:57 - loss: 2.7589 - regression_loss: 2.1256 - classification_loss: 0.6334
 609/1000 [=================>............] - ETA: 2:57 - loss: 2.7595 - regression_loss: 2.1257 - classification_loss: 0.6338
 610/1000 [=================>............] - ETA: 2:56 - loss: 2.7612 - regression_loss: 2.1276 - classification_loss: 0.6335
 611/1000 [=================>............] - ETA: 2:56 - loss: 2.7638 - regression_loss: 2.1298 - classification_loss: 0.6341
 612/1000 [=================>............] - ETA: 2:55 - loss: 2.7593 - regression_loss: 2.1263 - classification_loss: 0.6330
 613/1000 [=================>............] - ETA: 2:55 - loss: 2.7629 - regression_loss: 2.1281 - classification_loss: 0.6348
 614/1000 [=================>............] - ETA: 2:54 - loss: 2.7614 - regression_loss: 2.1272 - classification_loss: 0.6342
 615/1000 [=================>............] - ETA: 2:54 - loss: 2.7625 - regression_loss: 2.1283 - classification_loss: 0.6343
 616/1000 [=================>............] - ETA: 2:53 - loss: 2.7588 - regression_loss: 2.1248 - classification_loss: 0.6340
 617/1000 [=================>............] - ETA: 2:53 - loss: 2.7632 - regression_loss: 2.1276 - classification_loss: 0.6356
 618/1000 [=================>............] - ETA: 2:52 - loss: 2.7660 - regression_loss: 2.1307 - classification_loss: 0.6353
 619/1000 [=================>............] - ETA: 2:52 - loss: 2.7662 - regression_loss: 2.1314 - classification_loss: 0.6348
 620/1000 [=================>............] - ETA: 2:52 - loss: 2.7689 - regression_loss: 2.1338 - classification_loss: 0.6351
 621/1000 [=================>............] - ETA: 2:51 - loss: 2.7724 - regression_loss: 2.1357 - classification_loss: 0.6367
 622/1000 [=================>............] - ETA: 2:51 - loss: 2.7772 - regression_loss: 2.1398 - classification_loss: 0.6374
 623/1000 [=================>............] - ETA: 2:50 - loss: 2.7800 - regression_loss: 2.1416 - classification_loss: 0.6384
 624/1000 [=================>............] - ETA: 2:50 - loss: 2.7801 - regression_loss: 2.1420 - classification_loss: 0.6381
 625/1000 [=================>............] - ETA: 2:49 - loss: 2.7757 - regression_loss: 2.1386 - classification_loss: 0.6371
 626/1000 [=================>............] - ETA: 2:49 - loss: 2.7756 - regression_loss: 2.1386 - classification_loss: 0.6371
 627/1000 [=================>............] - ETA: 2:48 - loss: 2.7776 - regression_loss: 2.1406 - classification_loss: 0.6370
 628/1000 [=================>............] - ETA: 2:48 - loss: 2.7828 - regression_loss: 2.1456 - classification_loss: 0.6372
 629/1000 [=================>............] - ETA: 2:48 - loss: 2.7868 - regression_loss: 2.1471 - classification_loss: 0.6396
 630/1000 [=================>............] - ETA: 2:47 - loss: 2.7823 - regression_loss: 2.1437 - classification_loss: 0.6386
 631/1000 [=================>............] - ETA: 2:47 - loss: 2.7861 - regression_loss: 2.1460 - classification_loss: 0.6401
 632/1000 [=================>............] - ETA: 2:46 - loss: 2.7881 - regression_loss: 2.1477 - classification_loss: 0.6404
 633/1000 [=================>............] - ETA: 2:46 - loss: 2.7885 - regression_loss: 2.1478 - classification_loss: 0.6407
 634/1000 [==================>...........] - ETA: 2:45 - loss: 2.7884 - regression_loss: 2.1479 - classification_loss: 0.6405
 635/1000 [==================>...........] - ETA: 2:45 - loss: 2.7841 - regression_loss: 2.1445 - classification_loss: 0.6396
 636/1000 [==================>...........] - ETA: 2:44 - loss: 2.7866 - regression_loss: 2.1460 - classification_loss: 0.6406
 637/1000 [==================>...........] - ETA: 2:44 - loss: 2.7894 - regression_loss: 2.1482 - classification_loss: 0.6412
 638/1000 [==================>...........] - ETA: 2:43 - loss: 2.7892 - regression_loss: 2.1482 - classification_loss: 0.6410
 639/1000 [==================>...........] - ETA: 2:43 - loss: 2.7910 - regression_loss: 2.1500 - classification_loss: 0.6411
 640/1000 [==================>...........] - ETA: 2:43 - loss: 2.7952 - regression_loss: 2.1518 - classification_loss: 0.6434
 641/1000 [==================>...........] - ETA: 2:42 - loss: 2.7909 - regression_loss: 2.1485 - classification_loss: 0.6424
 642/1000 [==================>...........] - ETA: 2:42 - loss: 2.7865 - regression_loss: 2.1451 - classification_loss: 0.6414
 643/1000 [==================>...........] - ETA: 2:41 - loss: 2.7889 - regression_loss: 2.1472 - classification_loss: 0.6417
 644/1000 [==================>...........] - ETA: 2:41 - loss: 2.7885 - regression_loss: 2.1469 - classification_loss: 0.6416
 645/1000 [==================>...........] - ETA: 2:40 - loss: 2.7900 - regression_loss: 2.1486 - classification_loss: 0.6414
 646/1000 [==================>...........] - ETA: 2:40 - loss: 2.7857 - regression_loss: 2.1453 - classification_loss: 0.6404
 647/1000 [==================>...........] - ETA: 2:39 - loss: 2.7814 - regression_loss: 2.1420 - classification_loss: 0.6394
 648/1000 [==================>...........] - ETA: 2:39 - loss: 2.7802 - regression_loss: 2.1412 - classification_loss: 0.6390
 649/1000 [==================>...........] - ETA: 2:38 - loss: 2.7811 - regression_loss: 2.1422 - classification_loss: 0.6389
 650/1000 [==================>...........] - ETA: 2:38 - loss: 2.7776 - regression_loss: 2.1389 - classification_loss: 0.6386
 651/1000 [==================>...........] - ETA: 2:38 - loss: 2.7733 - regression_loss: 2.1356 - classification_loss: 0.6376
 652/1000 [==================>...........] - ETA: 2:37 - loss: 2.7776 - regression_loss: 2.1372 - classification_loss: 0.6404
 653/1000 [==================>...........] - ETA: 2:37 - loss: 2.7808 - regression_loss: 2.1398 - classification_loss: 0.6409
 654/1000 [==================>...........] - ETA: 2:36 - loss: 2.7770 - regression_loss: 2.1366 - classification_loss: 0.6404
 655/1000 [==================>...........] - ETA: 2:36 - loss: 2.7782 - regression_loss: 2.1372 - classification_loss: 0.6410
 656/1000 [==================>...........] - ETA: 2:35 - loss: 2.7790 - regression_loss: 2.1370 - classification_loss: 0.6421
 657/1000 [==================>...........] - ETA: 2:35 - loss: 2.7748 - regression_loss: 2.1337 - classification_loss: 0.6411
 658/1000 [==================>...........] - ETA: 2:34 - loss: 2.7755 - regression_loss: 2.1335 - classification_loss: 0.6420
 659/1000 [==================>...........] - ETA: 2:34 - loss: 2.7794 - regression_loss: 2.1365 - classification_loss: 0.6428
 660/1000 [==================>...........] - ETA: 2:33 - loss: 2.7783 - regression_loss: 2.1357 - classification_loss: 0.6426
 661/1000 [==================>...........] - ETA: 2:33 - loss: 2.7742 - regression_loss: 2.1324 - classification_loss: 0.6418
 662/1000 [==================>...........] - ETA: 2:33 - loss: 2.7738 - regression_loss: 2.1319 - classification_loss: 0.6420
 663/1000 [==================>...........] - ETA: 2:32 - loss: 2.7743 - regression_loss: 2.1322 - classification_loss: 0.6421
 664/1000 [==================>...........] - ETA: 2:32 - loss: 2.7744 - regression_loss: 2.1326 - classification_loss: 0.6417
 665/1000 [==================>...........] - ETA: 2:31 - loss: 2.7702 - regression_loss: 2.1294 - classification_loss: 0.6408
 666/1000 [==================>...........] - ETA: 2:31 - loss: 2.7703 - regression_loss: 2.1295 - classification_loss: 0.6408
 667/1000 [===================>..........] - ETA: 2:30 - loss: 2.7662 - regression_loss: 2.1263 - classification_loss: 0.6398
 668/1000 [===================>..........] - ETA: 2:30 - loss: 2.7667 - regression_loss: 2.1265 - classification_loss: 0.6403
 669/1000 [===================>..........] - ETA: 2:29 - loss: 2.7683 - regression_loss: 2.1275 - classification_loss: 0.6408
 670/1000 [===================>..........] - ETA: 2:29 - loss: 2.7641 - regression_loss: 2.1243 - classification_loss: 0.6398
 671/1000 [===================>..........] - ETA: 2:28 - loss: 2.7640 - regression_loss: 2.1242 - classification_loss: 0.6398
 672/1000 [===================>..........] - ETA: 2:28 - loss: 2.7647 - regression_loss: 2.1247 - classification_loss: 0.6399
 673/1000 [===================>..........] - ETA: 2:28 - loss: 2.7663 - regression_loss: 2.1261 - classification_loss: 0.6401
 674/1000 [===================>..........] - ETA: 2:27 - loss: 2.7664 - regression_loss: 2.1255 - classification_loss: 0.6409
 675/1000 [===================>..........] - ETA: 2:27 - loss: 2.7690 - regression_loss: 2.1264 - classification_loss: 0.6426
 676/1000 [===================>..........] - ETA: 2:26 - loss: 2.7705 - regression_loss: 2.1263 - classification_loss: 0.6442
 677/1000 [===================>..........] - ETA: 2:26 - loss: 2.7749 - regression_loss: 2.1287 - classification_loss: 0.6461
 678/1000 [===================>..........] - ETA: 2:25 - loss: 2.7708 - regression_loss: 2.1256 - classification_loss: 0.6452
 679/1000 [===================>..........] - ETA: 2:25 - loss: 2.7717 - regression_loss: 2.1265 - classification_loss: 0.6452
 680/1000 [===================>..........] - ETA: 2:24 - loss: 2.7722 - regression_loss: 2.1273 - classification_loss: 0.6449
 681/1000 [===================>..........] - ETA: 2:24 - loss: 2.7717 - regression_loss: 2.1270 - classification_loss: 0.6447
 682/1000 [===================>..........] - ETA: 2:23 - loss: 2.7736 - regression_loss: 2.1283 - classification_loss: 0.6454
 683/1000 [===================>..........] - ETA: 2:23 - loss: 2.7725 - regression_loss: 2.1277 - classification_loss: 0.6448
 684/1000 [===================>..........] - ETA: 2:23 - loss: 2.7730 - regression_loss: 2.1281 - classification_loss: 0.6449
 685/1000 [===================>..........] - ETA: 2:22 - loss: 2.7728 - regression_loss: 2.1282 - classification_loss: 0.6446
 686/1000 [===================>..........] - ETA: 2:22 - loss: 2.7728 - regression_loss: 2.1285 - classification_loss: 0.6443
 687/1000 [===================>..........] - ETA: 2:21 - loss: 2.7731 - regression_loss: 2.1293 - classification_loss: 0.6439
 688/1000 [===================>..........] - ETA: 2:21 - loss: 2.7694 - regression_loss: 2.1262 - classification_loss: 0.6432
 689/1000 [===================>..........] - ETA: 2:20 - loss: 2.7692 - regression_loss: 2.1259 - classification_loss: 0.6433
 690/1000 [===================>..........] - ETA: 2:20 - loss: 2.7701 - regression_loss: 2.1265 - classification_loss: 0.6436
 691/1000 [===================>..........] - ETA: 2:19 - loss: 2.7711 - regression_loss: 2.1273 - classification_loss: 0.6438
 692/1000 [===================>..........] - ETA: 2:19 - loss: 2.7721 - regression_loss: 2.1285 - classification_loss: 0.6436
 693/1000 [===================>..........] - ETA: 2:19 - loss: 2.7800 - regression_loss: 2.1341 - classification_loss: 0.6459
 694/1000 [===================>..........] - ETA: 2:18 - loss: 2.7760 - regression_loss: 2.1310 - classification_loss: 0.6450
 695/1000 [===================>..........] - ETA: 2:18 - loss: 2.7720 - regression_loss: 2.1279 - classification_loss: 0.6441
 696/1000 [===================>..........] - ETA: 2:17 - loss: 2.7742 - regression_loss: 2.1293 - classification_loss: 0.6449
 697/1000 [===================>..........] - ETA: 2:17 - loss: 2.7772 - regression_loss: 2.1312 - classification_loss: 0.6460
 698/1000 [===================>..........] - ETA: 2:16 - loss: 2.7733 - regression_loss: 2.1282 - classification_loss: 0.6451
 699/1000 [===================>..........] - ETA: 2:16 - loss: 2.7693 - regression_loss: 2.1251 - classification_loss: 0.6442
 700/1000 [====================>.........] - ETA: 2:15 - loss: 2.7687 - regression_loss: 2.1249 - classification_loss: 0.6438
 701/1000 [====================>.........] - ETA: 2:15 - loss: 2.7759 - regression_loss: 2.1301 - classification_loss: 0.6458
 702/1000 [====================>.........] - ETA: 2:14 - loss: 2.7788 - regression_loss: 2.1325 - classification_loss: 0.6462
 703/1000 [====================>.........] - ETA: 2:14 - loss: 2.7837 - regression_loss: 2.1376 - classification_loss: 0.6461
 704/1000 [====================>.........] - ETA: 2:14 - loss: 2.7866 - regression_loss: 2.1390 - classification_loss: 0.6476
 705/1000 [====================>.........] - ETA: 2:13 - loss: 2.7862 - regression_loss: 2.1393 - classification_loss: 0.6470
 706/1000 [====================>.........] - ETA: 2:13 - loss: 2.7855 - regression_loss: 2.1389 - classification_loss: 0.6466
 707/1000 [====================>.........] - ETA: 2:12 - loss: 2.7848 - regression_loss: 2.1386 - classification_loss: 0.6462
 708/1000 [====================>.........] - ETA: 2:12 - loss: 2.7887 - regression_loss: 2.1424 - classification_loss: 0.6462
 709/1000 [====================>.........] - ETA: 2:11 - loss: 2.7878 - regression_loss: 2.1418 - classification_loss: 0.6461
 710/1000 [====================>.........] - ETA: 2:11 - loss: 2.7839 - regression_loss: 2.1387 - classification_loss: 0.6452
 711/1000 [====================>.........] - ETA: 2:10 - loss: 2.7871 - regression_loss: 2.1417 - classification_loss: 0.6455
 712/1000 [====================>.........] - ETA: 2:10 - loss: 2.7864 - regression_loss: 2.1414 - classification_loss: 0.6451
 713/1000 [====================>.........] - ETA: 2:09 - loss: 2.7872 - regression_loss: 2.1421 - classification_loss: 0.6451
 714/1000 [====================>.........] - ETA: 2:09 - loss: 2.7836 - regression_loss: 2.1391 - classification_loss: 0.6445
 715/1000 [====================>.........] - ETA: 2:09 - loss: 2.7797 - regression_loss: 2.1361 - classification_loss: 0.6436
 716/1000 [====================>.........] - ETA: 2:08 - loss: 2.7807 - regression_loss: 2.1371 - classification_loss: 0.6436
 717/1000 [====================>.........] - ETA: 2:08 - loss: 2.7809 - regression_loss: 2.1374 - classification_loss: 0.6434
 718/1000 [====================>.........] - ETA: 2:07 - loss: 2.7827 - regression_loss: 2.1387 - classification_loss: 0.6440
 719/1000 [====================>.........] - ETA: 2:07 - loss: 2.7845 - regression_loss: 2.1401 - classification_loss: 0.6443
 720/1000 [====================>.........] - ETA: 2:06 - loss: 2.7886 - regression_loss: 2.1435 - classification_loss: 0.6452
 721/1000 [====================>.........] - ETA: 2:06 - loss: 2.7882 - regression_loss: 2.1433 - classification_loss: 0.6449
 722/1000 [====================>.........] - ETA: 2:05 - loss: 2.7913 - regression_loss: 2.1446 - classification_loss: 0.6467
 723/1000 [====================>.........] - ETA: 2:05 - loss: 2.7902 - regression_loss: 2.1440 - classification_loss: 0.6462
 724/1000 [====================>.........] - ETA: 2:04 - loss: 2.7864 - regression_loss: 2.1411 - classification_loss: 0.6453
 725/1000 [====================>.........] - ETA: 2:04 - loss: 2.7887 - regression_loss: 2.1429 - classification_loss: 0.6458
 726/1000 [====================>.........] - ETA: 2:04 - loss: 2.7849 - regression_loss: 2.1400 - classification_loss: 0.6449
 727/1000 [====================>.........] - ETA: 2:03 - loss: 2.7863 - regression_loss: 2.1411 - classification_loss: 0.6452
 728/1000 [====================>.........] - ETA: 2:03 - loss: 2.7825 - regression_loss: 2.1382 - classification_loss: 0.6443
 729/1000 [====================>.........] - ETA: 2:02 - loss: 2.7843 - regression_loss: 2.1394 - classification_loss: 0.6449
 730/1000 [====================>.........] - ETA: 2:02 - loss: 2.7863 - regression_loss: 2.1401 - classification_loss: 0.6463
 731/1000 [====================>.........] - ETA: 2:01 - loss: 2.7880 - regression_loss: 2.1404 - classification_loss: 0.6477
 732/1000 [====================>.........] - ETA: 2:01 - loss: 2.7905 - regression_loss: 2.1427 - classification_loss: 0.6478
 733/1000 [====================>.........] - ETA: 2:00 - loss: 2.7930 - regression_loss: 2.1443 - classification_loss: 0.6487
 734/1000 [=====================>........] - ETA: 2:00 - loss: 2.7942 - regression_loss: 2.1447 - classification_loss: 0.6495
 735/1000 [=====================>........] - ETA: 1:59 - loss: 2.7944 - regression_loss: 2.1449 - classification_loss: 0.6495
 736/1000 [=====================>........] - ETA: 1:59 - loss: 2.7941 - regression_loss: 2.1450 - classification_loss: 0.6491
 737/1000 [=====================>........] - ETA: 1:59 - loss: 2.7932 - regression_loss: 2.1446 - classification_loss: 0.6486
 738/1000 [=====================>........] - ETA: 1:58 - loss: 2.7928 - regression_loss: 2.1447 - classification_loss: 0.6481
 739/1000 [=====================>........] - ETA: 1:58 - loss: 2.7924 - regression_loss: 2.1443 - classification_loss: 0.6481
 740/1000 [=====================>........] - ETA: 1:57 - loss: 2.7886 - regression_loss: 2.1414 - classification_loss: 0.6472
 741/1000 [=====================>........] - ETA: 1:57 - loss: 2.7911 - regression_loss: 2.1426 - classification_loss: 0.6485
 742/1000 [=====================>........] - ETA: 1:56 - loss: 2.7916 - regression_loss: 2.1432 - classification_loss: 0.6485
 743/1000 [=====================>........] - ETA: 1:56 - loss: 2.7946 - regression_loss: 2.1455 - classification_loss: 0.6491
 744/1000 [=====================>........] - ETA: 1:55 - loss: 2.7970 - regression_loss: 2.1480 - classification_loss: 0.6489
 745/1000 [=====================>........] - ETA: 1:55 - loss: 2.7979 - regression_loss: 2.1482 - classification_loss: 0.6497
 746/1000 [=====================>........] - ETA: 1:54 - loss: 2.7962 - regression_loss: 2.1471 - classification_loss: 0.6491
 747/1000 [=====================>........] - ETA: 1:54 - loss: 2.7980 - regression_loss: 2.1481 - classification_loss: 0.6498
 748/1000 [=====================>........] - ETA: 1:54 - loss: 2.7972 - regression_loss: 2.1477 - classification_loss: 0.6496
 749/1000 [=====================>........] - ETA: 1:53 - loss: 2.7976 - regression_loss: 2.1485 - classification_loss: 0.6491
 750/1000 [=====================>........] - ETA: 1:53 - loss: 2.7972 - regression_loss: 2.1481 - classification_loss: 0.6491
 751/1000 [=====================>........] - ETA: 1:52 - loss: 2.7978 - regression_loss: 2.1487 - classification_loss: 0.6490
 752/1000 [=====================>........] - ETA: 1:52 - loss: 2.7965 - regression_loss: 2.1481 - classification_loss: 0.6485
 753/1000 [=====================>........] - ETA: 1:51 - loss: 2.7928 - regression_loss: 2.1452 - classification_loss: 0.6476
 754/1000 [=====================>........] - ETA: 1:51 - loss: 2.7929 - regression_loss: 2.1455 - classification_loss: 0.6474
 755/1000 [=====================>........] - ETA: 1:50 - loss: 2.7923 - regression_loss: 2.1452 - classification_loss: 0.6471
 756/1000 [=====================>........] - ETA: 1:50 - loss: 2.7937 - regression_loss: 2.1467 - classification_loss: 0.6470
 757/1000 [=====================>........] - ETA: 1:50 - loss: 2.7946 - regression_loss: 2.1479 - classification_loss: 0.6466
 758/1000 [=====================>........] - ETA: 1:49 - loss: 2.7975 - regression_loss: 2.1495 - classification_loss: 0.6480
 759/1000 [=====================>........] - ETA: 1:49 - loss: 2.7979 - regression_loss: 2.1501 - classification_loss: 0.6478
 760/1000 [=====================>........] - ETA: 1:48 - loss: 2.7989 - regression_loss: 2.1511 - classification_loss: 0.6478
 761/1000 [=====================>........] - ETA: 1:48 - loss: 2.7982 - regression_loss: 2.1510 - classification_loss: 0.6472
 762/1000 [=====================>........] - ETA: 1:47 - loss: 2.7945 - regression_loss: 2.1481 - classification_loss: 0.6464
 763/1000 [=====================>........] - ETA: 1:47 - loss: 2.7973 - regression_loss: 2.1496 - classification_loss: 0.6477
 764/1000 [=====================>........] - ETA: 1:46 - loss: 2.7975 - regression_loss: 2.1495 - classification_loss: 0.6480
 765/1000 [=====================>........] - ETA: 1:46 - loss: 2.7970 - regression_loss: 2.1494 - classification_loss: 0.6476
 766/1000 [=====================>........] - ETA: 1:45 - loss: 2.7934 - regression_loss: 2.1466 - classification_loss: 0.6468
 767/1000 [======================>.......] - ETA: 1:45 - loss: 2.7925 - regression_loss: 2.1438 - classification_loss: 0.6487
 768/1000 [======================>.......] - ETA: 1:45 - loss: 2.7937 - regression_loss: 2.1451 - classification_loss: 0.6485
 769/1000 [======================>.......] - ETA: 1:44 - loss: 2.7901 - regression_loss: 2.1423 - classification_loss: 0.6478
 770/1000 [======================>.......] - ETA: 1:44 - loss: 2.7893 - regression_loss: 2.1420 - classification_loss: 0.6473
 771/1000 [======================>.......] - ETA: 1:43 - loss: 2.7857 - regression_loss: 2.1392 - classification_loss: 0.6465
 772/1000 [======================>.......] - ETA: 1:43 - loss: 2.7821 - regression_loss: 2.1365 - classification_loss: 0.6457
 773/1000 [======================>.......] - ETA: 1:42 - loss: 2.7856 - regression_loss: 2.1385 - classification_loss: 0.6471
 774/1000 [======================>.......] - ETA: 1:42 - loss: 2.7861 - regression_loss: 2.1387 - classification_loss: 0.6474
 775/1000 [======================>.......] - ETA: 1:41 - loss: 2.7825 - regression_loss: 2.1359 - classification_loss: 0.6466
 776/1000 [======================>.......] - ETA: 1:41 - loss: 2.7789 - regression_loss: 2.1332 - classification_loss: 0.6458
 777/1000 [======================>.......] - ETA: 1:40 - loss: 2.7822 - regression_loss: 2.1353 - classification_loss: 0.6469
 778/1000 [======================>.......] - ETA: 1:40 - loss: 2.7837 - regression_loss: 2.1358 - classification_loss: 0.6479
 779/1000 [======================>.......] - ETA: 1:40 - loss: 2.7854 - regression_loss: 2.1371 - classification_loss: 0.6483
 780/1000 [======================>.......] - ETA: 1:39 - loss: 2.7863 - regression_loss: 2.1375 - classification_loss: 0.6488
 781/1000 [======================>.......] - ETA: 1:39 - loss: 2.7866 - regression_loss: 2.1380 - classification_loss: 0.6486
 782/1000 [======================>.......] - ETA: 1:38 - loss: 2.7881 - regression_loss: 2.1388 - classification_loss: 0.6493
 783/1000 [======================>.......] - ETA: 1:38 - loss: 2.7896 - regression_loss: 2.1406 - classification_loss: 0.6490
 784/1000 [======================>.......] - ETA: 1:37 - loss: 2.7901 - regression_loss: 2.1415 - classification_loss: 0.6486
 785/1000 [======================>.......] - ETA: 1:37 - loss: 2.7926 - regression_loss: 2.1429 - classification_loss: 0.6497
 786/1000 [======================>.......] - ETA: 1:36 - loss: 2.7891 - regression_loss: 2.1401 - classification_loss: 0.6489
 787/1000 [======================>.......] - ETA: 1:36 - loss: 2.7858 - regression_loss: 2.1374 - classification_loss: 0.6484
 788/1000 [======================>.......] - ETA: 1:35 - loss: 2.7880 - regression_loss: 2.1394 - classification_loss: 0.6486
 789/1000 [======================>.......] - ETA: 1:35 - loss: 2.7868 - regression_loss: 2.1383 - classification_loss: 0.6485
 790/1000 [======================>.......] - ETA: 1:35 - loss: 2.7875 - regression_loss: 2.1386 - classification_loss: 0.6488
 791/1000 [======================>.......] - ETA: 1:34 - loss: 2.7839 - regression_loss: 2.1359 - classification_loss: 0.6480
 792/1000 [======================>.......] - ETA: 1:34 - loss: 2.7842 - regression_loss: 2.1361 - classification_loss: 0.6480
 793/1000 [======================>.......] - ETA: 1:33 - loss: 2.7840 - regression_loss: 2.1355 - classification_loss: 0.6484
 794/1000 [======================>.......] - ETA: 1:33 - loss: 2.7805 - regression_loss: 2.1328 - classification_loss: 0.6476
 795/1000 [======================>.......] - ETA: 1:32 - loss: 2.7770 - regression_loss: 2.1302 - classification_loss: 0.6468
 796/1000 [======================>.......] - ETA: 1:32 - loss: 2.7776 - regression_loss: 2.1304 - classification_loss: 0.6471
 797/1000 [======================>.......] - ETA: 1:31 - loss: 2.7771 - regression_loss: 2.1300 - classification_loss: 0.6471
 798/1000 [======================>.......] - ETA: 1:31 - loss: 2.7780 - regression_loss: 2.1308 - classification_loss: 0.6471
 799/1000 [======================>.......] - ETA: 1:30 - loss: 2.7786 - regression_loss: 2.1311 - classification_loss: 0.6475
 800/1000 [=======================>......] - ETA: 1:30 - loss: 2.7783 - regression_loss: 2.1309 - classification_loss: 0.6474
 801/1000 [=======================>......] - ETA: 1:30 - loss: 2.7787 - regression_loss: 2.1312 - classification_loss: 0.6475
 802/1000 [=======================>......] - ETA: 1:29 - loss: 2.7753 - regression_loss: 2.1286 - classification_loss: 0.6467
 803/1000 [=======================>......] - ETA: 1:29 - loss: 2.7766 - regression_loss: 2.1296 - classification_loss: 0.6470
 804/1000 [=======================>......] - ETA: 1:28 - loss: 2.7782 - regression_loss: 2.1310 - classification_loss: 0.6472
 805/1000 [=======================>......] - ETA: 1:28 - loss: 2.7795 - regression_loss: 2.1313 - classification_loss: 0.6482
 806/1000 [=======================>......] - ETA: 1:27 - loss: 2.7801 - regression_loss: 2.1306 - classification_loss: 0.6494
 807/1000 [=======================>......] - ETA: 1:27 - loss: 2.7843 - regression_loss: 2.1338 - classification_loss: 0.6505
 808/1000 [=======================>......] - ETA: 1:26 - loss: 2.7808 - regression_loss: 2.1312 - classification_loss: 0.6497
 809/1000 [=======================>......] - ETA: 1:26 - loss: 2.7823 - regression_loss: 2.1322 - classification_loss: 0.6501
 810/1000 [=======================>......] - ETA: 1:26 - loss: 2.7832 - regression_loss: 2.1330 - classification_loss: 0.6502
 811/1000 [=======================>......] - ETA: 1:25 - loss: 2.7847 - regression_loss: 2.1344 - classification_loss: 0.6503
 812/1000 [=======================>......] - ETA: 1:25 - loss: 2.7855 - regression_loss: 2.1351 - classification_loss: 0.6505
 813/1000 [=======================>......] - ETA: 1:24 - loss: 2.7878 - regression_loss: 2.1370 - classification_loss: 0.6508
 814/1000 [=======================>......] - ETA: 1:24 - loss: 2.7846 - regression_loss: 2.1344 - classification_loss: 0.6502
 815/1000 [=======================>......] - ETA: 1:23 - loss: 2.7868 - regression_loss: 2.1359 - classification_loss: 0.6508
 816/1000 [=======================>......] - ETA: 1:23 - loss: 2.7834 - regression_loss: 2.1333 - classification_loss: 0.6501
 817/1000 [=======================>......] - ETA: 1:22 - loss: 2.7862 - regression_loss: 2.1358 - classification_loss: 0.6504
 818/1000 [=======================>......] - ETA: 1:22 - loss: 2.7875 - regression_loss: 2.1368 - classification_loss: 0.6507
 819/1000 [=======================>......] - ETA: 1:21 - loss: 2.7862 - regression_loss: 2.1357 - classification_loss: 0.6505
 820/1000 [=======================>......] - ETA: 1:21 - loss: 2.7882 - regression_loss: 2.1375 - classification_loss: 0.6508
 821/1000 [=======================>......] - ETA: 1:21 - loss: 2.7895 - regression_loss: 2.1382 - classification_loss: 0.6513
 822/1000 [=======================>......] - ETA: 1:20 - loss: 2.7894 - regression_loss: 2.1383 - classification_loss: 0.6512
 823/1000 [=======================>......] - ETA: 1:20 - loss: 2.7861 - regression_loss: 2.1357 - classification_loss: 0.6504
 824/1000 [=======================>......] - ETA: 1:19 - loss: 2.7891 - regression_loss: 2.1378 - classification_loss: 0.6513
 825/1000 [=======================>......] - ETA: 1:19 - loss: 2.7927 - regression_loss: 2.1399 - classification_loss: 0.6528
 826/1000 [=======================>......] - ETA: 1:18 - loss: 2.7927 - regression_loss: 2.1398 - classification_loss: 0.6529
 827/1000 [=======================>......] - ETA: 1:18 - loss: 2.7936 - regression_loss: 2.1406 - classification_loss: 0.6530
 828/1000 [=======================>......] - ETA: 1:17 - loss: 2.7927 - regression_loss: 2.1400 - classification_loss: 0.6527
 829/1000 [=======================>......] - ETA: 1:17 - loss: 2.7943 - regression_loss: 2.1415 - classification_loss: 0.6528
 830/1000 [=======================>......] - ETA: 1:16 - loss: 2.7932 - regression_loss: 2.1405 - classification_loss: 0.6527
 831/1000 [=======================>......] - ETA: 1:16 - loss: 2.7899 - regression_loss: 2.1380 - classification_loss: 0.6519
 832/1000 [=======================>......] - ETA: 1:16 - loss: 2.7905 - regression_loss: 2.1387 - classification_loss: 0.6519
 833/1000 [=======================>......] - ETA: 1:15 - loss: 2.7905 - regression_loss: 2.1383 - classification_loss: 0.6522
 834/1000 [========================>.....] - ETA: 1:15 - loss: 2.7912 - regression_loss: 2.1385 - classification_loss: 0.6527
 835/1000 [========================>.....] - ETA: 1:14 - loss: 2.7907 - regression_loss: 2.1381 - classification_loss: 0.6526
 836/1000 [========================>.....] - ETA: 1:14 - loss: 2.7874 - regression_loss: 2.1355 - classification_loss: 0.6518
 837/1000 [========================>.....] - ETA: 1:13 - loss: 2.7904 - regression_loss: 2.1374 - classification_loss: 0.6530
 838/1000 [========================>.....] - ETA: 1:13 - loss: 2.7909 - regression_loss: 2.1379 - classification_loss: 0.6530
 839/1000 [========================>.....] - ETA: 1:12 - loss: 2.7876 - regression_loss: 2.1353 - classification_loss: 0.6523
 840/1000 [========================>.....] - ETA: 1:12 - loss: 2.7888 - regression_loss: 2.1352 - classification_loss: 0.6536
 841/1000 [========================>.....] - ETA: 1:11 - loss: 2.7856 - regression_loss: 2.1327 - classification_loss: 0.6529
 842/1000 [========================>.....] - ETA: 1:11 - loss: 2.7867 - regression_loss: 2.1336 - classification_loss: 0.6531
 843/1000 [========================>.....] - ETA: 1:11 - loss: 2.7875 - regression_loss: 2.1343 - classification_loss: 0.6532
 844/1000 [========================>.....] - ETA: 1:10 - loss: 2.7861 - regression_loss: 2.1333 - classification_loss: 0.6528
 845/1000 [========================>.....] - ETA: 1:10 - loss: 2.7828 - regression_loss: 2.1308 - classification_loss: 0.6521
 846/1000 [========================>.....] - ETA: 1:09 - loss: 2.7850 - regression_loss: 2.1318 - classification_loss: 0.6532
 847/1000 [========================>.....] - ETA: 1:09 - loss: 2.7843 - regression_loss: 2.1311 - classification_loss: 0.6532
 848/1000 [========================>.....] - ETA: 1:08 - loss: 2.7810 - regression_loss: 2.1286 - classification_loss: 0.6524
 849/1000 [========================>.....] - ETA: 1:08 - loss: 2.7820 - regression_loss: 2.1296 - classification_loss: 0.6524
 850/1000 [========================>.....] - ETA: 1:07 - loss: 2.7849 - regression_loss: 2.1313 - classification_loss: 0.6536
 851/1000 [========================>.....] - ETA: 1:07 - loss: 2.7851 - regression_loss: 2.1316 - classification_loss: 0.6535
 852/1000 [========================>.....] - ETA: 1:07 - loss: 2.7818 - regression_loss: 2.1291 - classification_loss: 0.6527
 853/1000 [========================>.....] - ETA: 1:06 - loss: 2.7785 - regression_loss: 2.1266 - classification_loss: 0.6520
 854/1000 [========================>.....] - ETA: 1:06 - loss: 2.7753 - regression_loss: 2.1241 - classification_loss: 0.6512
 855/1000 [========================>.....] - ETA: 1:05 - loss: 2.7751 - regression_loss: 2.1243 - classification_loss: 0.6509
 856/1000 [========================>.....] - ETA: 1:05 - loss: 2.7784 - regression_loss: 2.1264 - classification_loss: 0.6520
 857/1000 [========================>.....] - ETA: 1:04 - loss: 2.7752 - regression_loss: 2.1239 - classification_loss: 0.6513
 858/1000 [========================>.....] - ETA: 1:04 - loss: 2.7720 - regression_loss: 2.1215 - classification_loss: 0.6505
 859/1000 [========================>.....] - ETA: 1:03 - loss: 2.7751 - regression_loss: 2.1234 - classification_loss: 0.6518
 860/1000 [========================>.....] - ETA: 1:03 - loss: 2.7772 - regression_loss: 2.1249 - classification_loss: 0.6523
 861/1000 [========================>.....] - ETA: 1:02 - loss: 2.7789 - regression_loss: 2.1258 - classification_loss: 0.6532
 862/1000 [========================>.....] - ETA: 1:02 - loss: 2.7757 - regression_loss: 2.1233 - classification_loss: 0.6524
 863/1000 [========================>.....] - ETA: 1:02 - loss: 2.7783 - regression_loss: 2.1245 - classification_loss: 0.6538
 864/1000 [========================>.....] - ETA: 1:01 - loss: 2.7804 - regression_loss: 2.1255 - classification_loss: 0.6549
 865/1000 [========================>.....] - ETA: 1:01 - loss: 2.7779 - regression_loss: 2.1231 - classification_loss: 0.6548
 866/1000 [========================>.....] - ETA: 1:00 - loss: 2.7771 - regression_loss: 2.1225 - classification_loss: 0.6546
 867/1000 [=========================>....] - ETA: 1:00 - loss: 2.7744 - regression_loss: 2.1201 - classification_loss: 0.6543
 868/1000 [=========================>....] - ETA: 59s - loss: 2.7748 - regression_loss: 2.1205 - classification_loss: 0.6543 
 869/1000 [=========================>....] - ETA: 59s - loss: 2.7716 - regression_loss: 2.1181 - classification_loss: 0.6535
 870/1000 [=========================>....] - ETA: 58s - loss: 2.7711 - regression_loss: 2.1176 - classification_loss: 0.6535
 871/1000 [=========================>....] - ETA: 58s - loss: 2.7680 - regression_loss: 2.1152 - classification_loss: 0.6528
 872/1000 [=========================>....] - ETA: 57s - loss: 2.7688 - regression_loss: 2.1159 - classification_loss: 0.6529
 873/1000 [=========================>....] - ETA: 57s - loss: 2.7657 - regression_loss: 2.1135 - classification_loss: 0.6521
 874/1000 [=========================>....] - ETA: 57s - loss: 2.7625 - regression_loss: 2.1111 - classification_loss: 0.6514
 875/1000 [=========================>....] - ETA: 56s - loss: 2.7633 - regression_loss: 2.1120 - classification_loss: 0.6513
 876/1000 [=========================>....] - ETA: 56s - loss: 2.7645 - regression_loss: 2.1128 - classification_loss: 0.6517
 877/1000 [=========================>....] - ETA: 55s - loss: 2.7667 - regression_loss: 2.1141 - classification_loss: 0.6526
 878/1000 [=========================>....] - ETA: 55s - loss: 2.7693 - regression_loss: 2.1156 - classification_loss: 0.6537
 879/1000 [=========================>....] - ETA: 54s - loss: 2.7723 - regression_loss: 2.1174 - classification_loss: 0.6549
 880/1000 [=========================>....] - ETA: 54s - loss: 2.7746 - regression_loss: 2.1187 - classification_loss: 0.6559
 881/1000 [=========================>....] - ETA: 53s - loss: 2.7747 - regression_loss: 2.1186 - classification_loss: 0.6560
 882/1000 [=========================>....] - ETA: 53s - loss: 2.7770 - regression_loss: 2.1199 - classification_loss: 0.6571
 883/1000 [=========================>....] - ETA: 52s - loss: 2.7738 - regression_loss: 2.1175 - classification_loss: 0.6564
 884/1000 [=========================>....] - ETA: 52s - loss: 2.7731 - regression_loss: 2.1168 - classification_loss: 0.6563
 885/1000 [=========================>....] - ETA: 52s - loss: 2.7700 - regression_loss: 2.1144 - classification_loss: 0.6556
 886/1000 [=========================>....] - ETA: 51s - loss: 2.7714 - regression_loss: 2.1151 - classification_loss: 0.6563
 887/1000 [=========================>....] - ETA: 51s - loss: 2.7728 - regression_loss: 2.1166 - classification_loss: 0.6563
 888/1000 [=========================>....] - ETA: 50s - loss: 2.7741 - regression_loss: 2.1176 - classification_loss: 0.6565
 889/1000 [=========================>....] - ETA: 50s - loss: 2.7763 - regression_loss: 2.1188 - classification_loss: 0.6575
 890/1000 [=========================>....] - ETA: 49s - loss: 2.7774 - regression_loss: 2.1198 - classification_loss: 0.6576
 891/1000 [=========================>....] - ETA: 49s - loss: 2.7784 - regression_loss: 2.1198 - classification_loss: 0.6586
 892/1000 [=========================>....] - ETA: 48s - loss: 2.7753 - regression_loss: 2.1174 - classification_loss: 0.6579
 893/1000 [=========================>....] - ETA: 48s - loss: 2.7744 - regression_loss: 2.1168 - classification_loss: 0.6576
 894/1000 [=========================>....] - ETA: 47s - loss: 2.7713 - regression_loss: 2.1144 - classification_loss: 0.6569
 895/1000 [=========================>....] - ETA: 47s - loss: 2.7682 - regression_loss: 2.1121 - classification_loss: 0.6562
 896/1000 [=========================>....] - ETA: 47s - loss: 2.7651 - regression_loss: 2.1097 - classification_loss: 0.6554
 897/1000 [=========================>....] - ETA: 46s - loss: 2.7651 - regression_loss: 2.1096 - classification_loss: 0.6555
 898/1000 [=========================>....] - ETA: 46s - loss: 2.7650 - regression_loss: 2.1098 - classification_loss: 0.6552
 899/1000 [=========================>....] - ETA: 45s - loss: 2.7671 - regression_loss: 2.1108 - classification_loss: 0.6563
 900/1000 [==========================>...] - ETA: 45s - loss: 2.7690 - regression_loss: 2.1122 - classification_loss: 0.6568
 901/1000 [==========================>...] - ETA: 44s - loss: 2.7690 - regression_loss: 2.1122 - classification_loss: 0.6568
 902/1000 [==========================>...] - ETA: 44s - loss: 2.7702 - regression_loss: 2.1124 - classification_loss: 0.6578
 903/1000 [==========================>...] - ETA: 43s - loss: 2.7713 - regression_loss: 2.1123 - classification_loss: 0.6590
 904/1000 [==========================>...] - ETA: 43s - loss: 2.7715 - regression_loss: 2.1125 - classification_loss: 0.6589
 905/1000 [==========================>...] - ETA: 43s - loss: 2.7719 - regression_loss: 2.1127 - classification_loss: 0.6592
 906/1000 [==========================>...] - ETA: 42s - loss: 2.7688 - regression_loss: 2.1103 - classification_loss: 0.6585
 907/1000 [==========================>...] - ETA: 42s - loss: 2.7705 - regression_loss: 2.1118 - classification_loss: 0.6587
 908/1000 [==========================>...] - ETA: 41s - loss: 2.7704 - regression_loss: 2.1114 - classification_loss: 0.6590
 909/1000 [==========================>...] - ETA: 41s - loss: 2.7673 - regression_loss: 2.1090 - classification_loss: 0.6583
 910/1000 [==========================>...] - ETA: 40s - loss: 2.7683 - regression_loss: 2.1090 - classification_loss: 0.6592
 911/1000 [==========================>...] - ETA: 40s - loss: 2.7677 - regression_loss: 2.1086 - classification_loss: 0.6591
 912/1000 [==========================>...] - ETA: 39s - loss: 2.7685 - regression_loss: 2.1088 - classification_loss: 0.6596
 913/1000 [==========================>...] - ETA: 39s - loss: 2.7687 - regression_loss: 2.1093 - classification_loss: 0.6594
 914/1000 [==========================>...] - ETA: 38s - loss: 2.7706 - regression_loss: 2.1110 - classification_loss: 0.6596
 915/1000 [==========================>...] - ETA: 38s - loss: 2.7712 - regression_loss: 2.1116 - classification_loss: 0.6596
 916/1000 [==========================>...] - ETA: 38s - loss: 2.7717 - regression_loss: 2.1117 - classification_loss: 0.6600
 917/1000 [==========================>...] - ETA: 37s - loss: 2.7722 - regression_loss: 2.1120 - classification_loss: 0.6602
 918/1000 [==========================>...] - ETA: 37s - loss: 2.7725 - regression_loss: 2.1120 - classification_loss: 0.6606
 919/1000 [==========================>...] - ETA: 36s - loss: 2.7695 - regression_loss: 2.1097 - classification_loss: 0.6599
 920/1000 [==========================>...] - ETA: 36s - loss: 2.7670 - regression_loss: 2.1074 - classification_loss: 0.6596
 921/1000 [==========================>...] - ETA: 35s - loss: 2.7676 - regression_loss: 2.1082 - classification_loss: 0.6594
 922/1000 [==========================>...] - ETA: 35s - loss: 2.7664 - regression_loss: 2.1075 - classification_loss: 0.6589
 923/1000 [==========================>...] - ETA: 34s - loss: 2.7654 - regression_loss: 2.1069 - classification_loss: 0.6585
 924/1000 [==========================>...] - ETA: 34s - loss: 2.7677 - regression_loss: 2.1083 - classification_loss: 0.6595
 925/1000 [==========================>...] - ETA: 33s - loss: 2.7691 - regression_loss: 2.1094 - classification_loss: 0.6597
 926/1000 [==========================>...] - ETA: 33s - loss: 2.7694 - regression_loss: 2.1094 - classification_loss: 0.6600
 927/1000 [==========================>...] - ETA: 33s - loss: 2.7692 - regression_loss: 2.1094 - classification_loss: 0.6598
 928/1000 [==========================>...] - ETA: 32s - loss: 2.7663 - regression_loss: 2.1071 - classification_loss: 0.6592
 929/1000 [==========================>...] - ETA: 32s - loss: 2.7668 - regression_loss: 2.1078 - classification_loss: 0.6590
 930/1000 [==========================>...] - ETA: 31s - loss: 2.7686 - regression_loss: 2.1096 - classification_loss: 0.6590
 931/1000 [==========================>...] - ETA: 31s - loss: 2.7697 - regression_loss: 2.1103 - classification_loss: 0.6594
 932/1000 [==========================>...] - ETA: 30s - loss: 2.7720 - regression_loss: 2.1116 - classification_loss: 0.6603
 933/1000 [==========================>...] - ETA: 30s - loss: 2.7711 - regression_loss: 2.1110 - classification_loss: 0.6601
 934/1000 [===========================>..] - ETA: 29s - loss: 2.7733 - regression_loss: 2.1128 - classification_loss: 0.6605
 935/1000 [===========================>..] - ETA: 29s - loss: 2.7704 - regression_loss: 2.1105 - classification_loss: 0.6599
 936/1000 [===========================>..] - ETA: 28s - loss: 2.7714 - regression_loss: 2.1112 - classification_loss: 0.6602
 937/1000 [===========================>..] - ETA: 28s - loss: 2.7713 - regression_loss: 2.1114 - classification_loss: 0.6599
 938/1000 [===========================>..] - ETA: 28s - loss: 2.7717 - regression_loss: 2.1119 - classification_loss: 0.6597
 939/1000 [===========================>..] - ETA: 27s - loss: 2.7687 - regression_loss: 2.1097 - classification_loss: 0.6590
 940/1000 [===========================>..] - ETA: 27s - loss: 2.7694 - regression_loss: 2.1102 - classification_loss: 0.6592
 941/1000 [===========================>..] - ETA: 26s - loss: 2.7729 - regression_loss: 2.1133 - classification_loss: 0.6596
 942/1000 [===========================>..] - ETA: 26s - loss: 2.7733 - regression_loss: 2.1137 - classification_loss: 0.6595
 943/1000 [===========================>..] - ETA: 25s - loss: 2.7749 - regression_loss: 2.1147 - classification_loss: 0.6603
 944/1000 [===========================>..] - ETA: 25s - loss: 2.7748 - regression_loss: 2.1147 - classification_loss: 0.6602
 945/1000 [===========================>..] - ETA: 24s - loss: 2.7755 - regression_loss: 2.1147 - classification_loss: 0.6607
 946/1000 [===========================>..] - ETA: 24s - loss: 2.7775 - regression_loss: 2.1158 - classification_loss: 0.6617
 947/1000 [===========================>..] - ETA: 23s - loss: 2.7779 - regression_loss: 2.1162 - classification_loss: 0.6617
 948/1000 [===========================>..] - ETA: 23s - loss: 2.7750 - regression_loss: 2.1140 - classification_loss: 0.6610
 949/1000 [===========================>..] - ETA: 23s - loss: 2.7757 - regression_loss: 2.1149 - classification_loss: 0.6608
 950/1000 [===========================>..] - ETA: 22s - loss: 2.7762 - regression_loss: 2.1155 - classification_loss: 0.6607
 951/1000 [===========================>..] - ETA: 22s - loss: 2.7769 - regression_loss: 2.1163 - classification_loss: 0.6606
 952/1000 [===========================>..] - ETA: 21s - loss: 2.7784 - regression_loss: 2.1176 - classification_loss: 0.6608
 953/1000 [===========================>..] - ETA: 21s - loss: 2.7786 - regression_loss: 2.1181 - classification_loss: 0.6605
 954/1000 [===========================>..] - ETA: 20s - loss: 2.7788 - regression_loss: 2.1184 - classification_loss: 0.6604
 955/1000 [===========================>..] - ETA: 20s - loss: 2.7810 - regression_loss: 2.1200 - classification_loss: 0.6610
 956/1000 [===========================>..] - ETA: 19s - loss: 2.7821 - regression_loss: 2.1213 - classification_loss: 0.6608
 957/1000 [===========================>..] - ETA: 19s - loss: 2.7792 - regression_loss: 2.1190 - classification_loss: 0.6602
 958/1000 [===========================>..] - ETA: 19s - loss: 2.7763 - regression_loss: 2.1168 - classification_loss: 0.6595
 959/1000 [===========================>..] - ETA: 18s - loss: 2.7783 - regression_loss: 2.1181 - classification_loss: 0.6601
 960/1000 [===========================>..] - ETA: 18s - loss: 2.7787 - regression_loss: 2.1188 - classification_loss: 0.6600
 961/1000 [===========================>..] - ETA: 17s - loss: 2.7759 - regression_loss: 2.1166 - classification_loss: 0.6593
 962/1000 [===========================>..] - ETA: 17s - loss: 2.7778 - regression_loss: 2.1185 - classification_loss: 0.6593
 963/1000 [===========================>..] - ETA: 16s - loss: 2.7782 - regression_loss: 2.1191 - classification_loss: 0.6591
 964/1000 [===========================>..] - ETA: 16s - loss: 2.7796 - regression_loss: 2.1203 - classification_loss: 0.6594
 965/1000 [===========================>..] - ETA: 15s - loss: 2.7802 - regression_loss: 2.1205 - classification_loss: 0.6597
 966/1000 [===========================>..] - ETA: 15s - loss: 2.7817 - regression_loss: 2.1217 - classification_loss: 0.6599
 967/1000 [============================>.] - ETA: 14s - loss: 2.7823 - regression_loss: 2.1225 - classification_loss: 0.6598
 968/1000 [============================>.] - ETA: 14s - loss: 2.7794 - regression_loss: 2.1203 - classification_loss: 0.6591
 969/1000 [============================>.] - ETA: 14s - loss: 2.7767 - regression_loss: 2.1181 - classification_loss: 0.6586
 970/1000 [============================>.] - ETA: 13s - loss: 2.7759 - regression_loss: 2.1177 - classification_loss: 0.6582
 971/1000 [============================>.] - ETA: 13s - loss: 2.7754 - regression_loss: 2.1174 - classification_loss: 0.6580
 972/1000 [============================>.] - ETA: 12s - loss: 2.7756 - regression_loss: 2.1177 - classification_loss: 0.6579
 973/1000 [============================>.] - ETA: 12s - loss: 2.7727 - regression_loss: 2.1155 - classification_loss: 0.6572
 974/1000 [============================>.] - ETA: 11s - loss: 2.7737 - regression_loss: 2.1164 - classification_loss: 0.6572
 975/1000 [============================>.] - ETA: 11s - loss: 2.7761 - regression_loss: 2.1182 - classification_loss: 0.6579
 976/1000 [============================>.] - ETA: 10s - loss: 2.7756 - regression_loss: 2.1180 - classification_loss: 0.6576
 977/1000 [============================>.] - ETA: 10s - loss: 2.7777 - regression_loss: 2.1191 - classification_loss: 0.6585
 978/1000 [============================>.] - ETA: 9s - loss: 2.7792 - regression_loss: 2.1203 - classification_loss: 0.6588 
 979/1000 [============================>.] - ETA: 9s - loss: 2.7783 - regression_loss: 2.1198 - classification_loss: 0.6585
 980/1000 [============================>.] - ETA: 9s - loss: 2.7755 - regression_loss: 2.1177 - classification_loss: 0.6578
 981/1000 [============================>.] - ETA: 8s - loss: 2.7727 - regression_loss: 2.1155 - classification_loss: 0.6572
 982/1000 [============================>.] - ETA: 8s - loss: 2.7698 - regression_loss: 2.1133 - classification_loss: 0.6565
 983/1000 [============================>.] - ETA: 7s - loss: 2.7702 - regression_loss: 2.1138 - classification_loss: 0.6564
 984/1000 [============================>.] - ETA: 7s - loss: 2.7674 - regression_loss: 2.1116 - classification_loss: 0.6557
 985/1000 [============================>.] - ETA: 6s - loss: 2.7697 - regression_loss: 2.1133 - classification_loss: 0.6565
 986/1000 [============================>.] - ETA: 6s - loss: 2.7703 - regression_loss: 2.1138 - classification_loss: 0.6565
 987/1000 [============================>.] - ETA: 5s - loss: 2.7720 - regression_loss: 2.1152 - classification_loss: 0.6567
 988/1000 [============================>.] - ETA: 5s - loss: 2.7692 - regression_loss: 2.1131 - classification_loss: 0.6561
 989/1000 [============================>.] - ETA: 4s - loss: 2.7707 - regression_loss: 2.1140 - classification_loss: 0.6567
 990/1000 [============================>.] - ETA: 4s - loss: 2.7705 - regression_loss: 2.1140 - classification_loss: 0.6565
 991/1000 [============================>.] - ETA: 4s - loss: 2.7708 - regression_loss: 2.1144 - classification_loss: 0.6565
 992/1000 [============================>.] - ETA: 3s - loss: 2.7747 - regression_loss: 2.1164 - classification_loss: 0.6583
 993/1000 [============================>.] - ETA: 3s - loss: 2.7749 - regression_loss: 2.1165 - classification_loss: 0.6584
 994/1000 [============================>.] - ETA: 2s - loss: 2.7735 - regression_loss: 2.1156 - classification_loss: 0.6579
 995/1000 [============================>.] - ETA: 2s - loss: 2.7742 - regression_loss: 2.1161 - classification_loss: 0.6581
 996/1000 [============================>.] - ETA: 1s - loss: 2.7765 - regression_loss: 2.1173 - classification_loss: 0.6592
 997/1000 [============================>.] - ETA: 1s - loss: 2.7798 - regression_loss: 2.1192 - classification_loss: 0.6606
 998/1000 [============================>.] - ETA: 0s - loss: 2.7770 - regression_loss: 2.1170 - classification_loss: 0.6599
 999/1000 [============================>.] - ETA: 0s - loss: 2.7801 - regression_loss: 2.1187 - classification_loss: 0.6614
1000/1000 [==============================] - 453s 453ms/step - loss: 2.7806 - regression_loss: 2.1184 - classification_loss: 0.6622

Epoch 00018: saving model to ./snapshots/resnet50_csv_18.h5
0/2001/2002/2003/2004/2005/2006/2007/2008/2009/20010/20011/20012/20013/20014/20015/20016/20017/20018/20019/20020/20021/20022/20023/20024/20025/20026/20027/20028/20029/20030/20031/20032/20033/20034/20035/20036/20037/20038/20039/20040/20041/20042/20043/20044/20045/20046/20047/20048/20049/20050/20051/20052/20053/20054/20055/20056/20057/20058/20059/20060/20061/20062/20063/20064/20065/20066/20067/20068/20069/20070/20071/20072/20073/20074/20075/20076/20077/20078/20079/20080/20081/20082/20083/20084/20085/20086/20087/20088/20089/20090/20091/20092/20093/20094/20095/20096/20097/20098/20099/200100/200101/200102/200103/200104/200105/200106/200107/200108/200109/200110/200111/200112/200113/200114/200115/200116/200117/200118/200119/200120/200121/200122/200123/200124/200125/200126/200127/200128/200129/200130/200131/200132/200133/200134/200135/200136/200137/200138/200139/200140/200141/200142/200143/200144/200145/200146/200147/200148/200149/200150/200151/200152/200153/200154/200155/200156/200157/200158/200159/200160/200161/200162/200163/200164/200165/200166/200167/200168/200169/200170/200171/200172/200173/200174/200175/200176/200177/200178/200179/200180/200181/200182/200183/200184/200185/200186/200187/200188/200189/200190/200191/200192/200193/200194/200195/200196/200197/200198/200199/2000/2001/2002/2003/2004/2005/2006/2007/2008/2009/20010/20011/20012/20013/20014/20015/20016/20017/20018/20019/20020/20021/20022/20023/20024/20025/20026/20027/20028/20029/20030/20031/20032/20033/20034/20035/20036/20037/20038/20039/20040/20041/20042/20043/20044/20045/20046/20047/20048/20049/20050/20051/20052/20053/20054/20055/20056/20057/20058/20059/20060/20061/20062/20063/20064/20065/20066/20067/20068/20069/20070/20071/20072/20073/20074/20075/20076/20077/20078/20079/20080/20081/20082/20083/20084/20085/20086/20087/20088/20089/20090/20091/20092/20093/20094/20095/20096/20097/20098/20099/200100/200101/200102/200103/200104/200105/200106/200107/200108/200109/200110/200111/200112/200113/200114/200115/200116/200117/200118/200119/200120/200121/200122/200123/200124/200125/200126/200127/200128/200129/200130/200131/200132/200133/200134/200135/200136/200137/200138/200139/200140/200141/200142/200143/200144/200145/200146/200147/200148/200149/200150/200151/200152/200153/200154/200155/200156/200157/200158/200159/200160/200161/200162/200163/200164/200165/200166/200167/200168/200169/200170/200171/200172/200173/200174/200175/200176/200177/200178/200179/200180/200181/200182/200183/200184/200185/200186/200187/200188/200189/200190/200191/200192/200193/200194/200195/200196/200197/200198/200199/200

M 0.1963
N 0.0009
mAP: 0.0986
Epoch 19/30

   1/1000 [..............................] - ETA: 7:20 - loss: 7.0425e-04 - regression_loss: 0.0000e+00 - classification_loss: 7.0425e-04
   2/1000 [..............................] - ETA: 7:26 - loss: 1.3515 - regression_loss: 1.1460 - classification_loss: 0.2055            
   3/1000 [..............................] - ETA: 7:27 - loss: 2.3364 - regression_loss: 1.8795 - classification_loss: 0.4569
   4/1000 [..............................] - ETA: 7:26 - loss: 2.4762 - regression_loss: 1.9216 - classification_loss: 0.5547
   5/1000 [..............................] - ETA: 7:25 - loss: 3.0011 - regression_loss: 2.1973 - classification_loss: 0.8038
   6/1000 [..............................] - ETA: 7:25 - loss: 3.0506 - regression_loss: 2.2568 - classification_loss: 0.7938
   7/1000 [..............................] - ETA: 7:25 - loss: 2.6188 - regression_loss: 1.9344 - classification_loss: 0.6844
   8/1000 [..............................] - ETA: 7:25 - loss: 2.7212 - regression_loss: 2.0372 - classification_loss: 0.6840
   9/1000 [..............................] - ETA: 7:25 - loss: 2.8137 - regression_loss: 2.1132 - classification_loss: 0.7005
  10/1000 [..............................] - ETA: 7:26 - loss: 2.5323 - regression_loss: 1.9019 - classification_loss: 0.6304
  11/1000 [..............................] - ETA: 7:26 - loss: 2.5448 - regression_loss: 1.9374 - classification_loss: 0.6074
  12/1000 [..............................] - ETA: 7:27 - loss: 2.5040 - regression_loss: 1.9270 - classification_loss: 0.5769
  13/1000 [..............................] - ETA: 7:26 - loss: 2.7797 - regression_loss: 2.1355 - classification_loss: 0.6442
  14/1000 [..............................] - ETA: 7:25 - loss: 2.7626 - regression_loss: 2.1358 - classification_loss: 0.6268
  15/1000 [..............................] - ETA: 7:25 - loss: 2.7515 - regression_loss: 2.1232 - classification_loss: 0.6283
  16/1000 [..............................] - ETA: 7:24 - loss: 2.8250 - regression_loss: 2.1704 - classification_loss: 0.6547
  17/1000 [..............................] - ETA: 7:23 - loss: 2.6589 - regression_loss: 2.0427 - classification_loss: 0.6161
  18/1000 [..............................] - ETA: 7:23 - loss: 2.7279 - regression_loss: 2.0877 - classification_loss: 0.6402
  19/1000 [..............................] - ETA: 7:23 - loss: 2.5843 - regression_loss: 1.9778 - classification_loss: 0.6065
  20/1000 [..............................] - ETA: 7:22 - loss: 2.4551 - regression_loss: 1.8789 - classification_loss: 0.5762
  21/1000 [..............................] - ETA: 7:22 - loss: 2.4596 - regression_loss: 1.8815 - classification_loss: 0.5781
  22/1000 [..............................] - ETA: 7:22 - loss: 2.5428 - regression_loss: 1.9252 - classification_loss: 0.6176
  23/1000 [..............................] - ETA: 7:21 - loss: 2.4323 - regression_loss: 1.8415 - classification_loss: 0.5908
  24/1000 [..............................] - ETA: 7:20 - loss: 2.3309 - regression_loss: 1.7647 - classification_loss: 0.5662
  25/1000 [..............................] - ETA: 7:20 - loss: 2.2377 - regression_loss: 1.6942 - classification_loss: 0.5435
  26/1000 [..............................] - ETA: 7:19 - loss: 2.2864 - regression_loss: 1.7391 - classification_loss: 0.5473
  27/1000 [..............................] - ETA: 7:19 - loss: 2.2017 - regression_loss: 1.6747 - classification_loss: 0.5270
  28/1000 [..............................] - ETA: 7:18 - loss: 2.1939 - regression_loss: 1.6763 - classification_loss: 0.5176
  29/1000 [..............................] - ETA: 7:18 - loss: 2.2103 - regression_loss: 1.7029 - classification_loss: 0.5074
  30/1000 [..............................] - ETA: 7:17 - loss: 2.2090 - regression_loss: 1.7104 - classification_loss: 0.4985
  31/1000 [..............................] - ETA: 7:17 - loss: 2.3042 - regression_loss: 1.7650 - classification_loss: 0.5391
  32/1000 [..............................] - ETA: 7:16 - loss: 2.2322 - regression_loss: 1.7099 - classification_loss: 0.5223
  33/1000 [..............................] - ETA: 7:16 - loss: 2.2782 - regression_loss: 1.7570 - classification_loss: 0.5212
  34/1000 [>.............................] - ETA: 7:16 - loss: 2.2112 - regression_loss: 1.7054 - classification_loss: 0.5059
  35/1000 [>.............................] - ETA: 7:15 - loss: 2.2165 - regression_loss: 1.7196 - classification_loss: 0.4969
  36/1000 [>.............................] - ETA: 7:15 - loss: 2.2280 - regression_loss: 1.7310 - classification_loss: 0.4970
  37/1000 [>.............................] - ETA: 7:15 - loss: 2.2717 - regression_loss: 1.7458 - classification_loss: 0.5260
  38/1000 [>.............................] - ETA: 7:14 - loss: 2.3353 - regression_loss: 1.7810 - classification_loss: 0.5544
  39/1000 [>.............................] - ETA: 7:14 - loss: 2.4111 - regression_loss: 1.8176 - classification_loss: 0.5935
  40/1000 [>.............................] - ETA: 7:14 - loss: 2.4063 - regression_loss: 1.8215 - classification_loss: 0.5848
  41/1000 [>.............................] - ETA: 7:13 - loss: 2.5011 - regression_loss: 1.8732 - classification_loss: 0.6279
  42/1000 [>.............................] - ETA: 7:13 - loss: 2.5743 - regression_loss: 1.9192 - classification_loss: 0.6551
  43/1000 [>.............................] - ETA: 7:12 - loss: 2.5629 - regression_loss: 1.9134 - classification_loss: 0.6496
  44/1000 [>.............................] - ETA: 7:12 - loss: 2.6065 - regression_loss: 1.9472 - classification_loss: 0.6592
  45/1000 [>.............................] - ETA: 7:11 - loss: 2.5947 - regression_loss: 1.9427 - classification_loss: 0.6520
  46/1000 [>.............................] - ETA: 7:11 - loss: 2.6124 - regression_loss: 1.9689 - classification_loss: 0.6435
  47/1000 [>.............................] - ETA: 7:10 - loss: 2.6478 - regression_loss: 1.9918 - classification_loss: 0.6560
  48/1000 [>.............................] - ETA: 7:10 - loss: 2.5927 - regression_loss: 1.9503 - classification_loss: 0.6423
  49/1000 [>.............................] - ETA: 7:09 - loss: 2.5785 - regression_loss: 1.9441 - classification_loss: 0.6343
  50/1000 [>.............................] - ETA: 7:09 - loss: 2.5269 - regression_loss: 1.9053 - classification_loss: 0.6216
  51/1000 [>.............................] - ETA: 7:08 - loss: 2.5140 - regression_loss: 1.8963 - classification_loss: 0.6177
  52/1000 [>.............................] - ETA: 7:08 - loss: 2.4657 - regression_loss: 1.8598 - classification_loss: 0.6058
  53/1000 [>.............................] - ETA: 7:08 - loss: 2.5211 - regression_loss: 1.8935 - classification_loss: 0.6276
  54/1000 [>.............................] - ETA: 7:06 - loss: 2.5242 - regression_loss: 1.8939 - classification_loss: 0.6303
  55/1000 [>.............................] - ETA: 7:06 - loss: 2.5414 - regression_loss: 1.9186 - classification_loss: 0.6229
  56/1000 [>.............................] - ETA: 7:05 - loss: 2.5647 - regression_loss: 1.9225 - classification_loss: 0.6422
  57/1000 [>.............................] - ETA: 7:05 - loss: 2.5632 - regression_loss: 1.9255 - classification_loss: 0.6377
  58/1000 [>.............................] - ETA: 7:05 - loss: 2.6067 - regression_loss: 1.9509 - classification_loss: 0.6558
  59/1000 [>.............................] - ETA: 7:04 - loss: 2.6095 - regression_loss: 1.9534 - classification_loss: 0.6561
  60/1000 [>.............................] - ETA: 7:04 - loss: 2.6280 - regression_loss: 1.9716 - classification_loss: 0.6564
  61/1000 [>.............................] - ETA: 7:03 - loss: 2.6231 - regression_loss: 1.9734 - classification_loss: 0.6497
  62/1000 [>.............................] - ETA: 7:03 - loss: 2.6130 - regression_loss: 1.9687 - classification_loss: 0.6442
  63/1000 [>.............................] - ETA: 7:02 - loss: 2.5717 - regression_loss: 1.9375 - classification_loss: 0.6342
  64/1000 [>.............................] - ETA: 7:02 - loss: 2.5882 - regression_loss: 1.9551 - classification_loss: 0.6330
  65/1000 [>.............................] - ETA: 7:02 - loss: 2.5816 - regression_loss: 1.9548 - classification_loss: 0.6269
  66/1000 [>.............................] - ETA: 7:01 - loss: 2.5425 - regression_loss: 1.9252 - classification_loss: 0.6174
  67/1000 [=>............................] - ETA: 7:01 - loss: 2.5334 - regression_loss: 1.9199 - classification_loss: 0.6135
  68/1000 [=>............................] - ETA: 7:00 - loss: 2.5355 - regression_loss: 1.9226 - classification_loss: 0.6129
  69/1000 [=>............................] - ETA: 7:00 - loss: 2.5058 - regression_loss: 1.8947 - classification_loss: 0.6110
  70/1000 [=>............................] - ETA: 6:59 - loss: 2.5051 - regression_loss: 1.9000 - classification_loss: 0.6051
  71/1000 [=>............................] - ETA: 6:59 - loss: 2.5076 - regression_loss: 1.9052 - classification_loss: 0.6024
  72/1000 [=>............................] - ETA: 6:58 - loss: 2.4922 - regression_loss: 1.8961 - classification_loss: 0.5961
  73/1000 [=>............................] - ETA: 6:58 - loss: 2.5019 - regression_loss: 1.9038 - classification_loss: 0.5981
  74/1000 [=>............................] - ETA: 6:57 - loss: 2.5135 - regression_loss: 1.9031 - classification_loss: 0.6103
  75/1000 [=>............................] - ETA: 6:57 - loss: 2.5057 - regression_loss: 1.9002 - classification_loss: 0.6055
  76/1000 [=>............................] - ETA: 6:56 - loss: 2.5010 - regression_loss: 1.9001 - classification_loss: 0.6009
  77/1000 [=>............................] - ETA: 6:56 - loss: 2.5047 - regression_loss: 1.9054 - classification_loss: 0.5993
  78/1000 [=>............................] - ETA: 6:56 - loss: 2.5280 - regression_loss: 1.9193 - classification_loss: 0.6087
  79/1000 [=>............................] - ETA: 6:55 - loss: 2.5479 - regression_loss: 1.9335 - classification_loss: 0.6145
  80/1000 [=>............................] - ETA: 6:55 - loss: 2.5719 - regression_loss: 1.9506 - classification_loss: 0.6213
  81/1000 [=>............................] - ETA: 6:54 - loss: 2.5401 - regression_loss: 1.9265 - classification_loss: 0.6136
  82/1000 [=>............................] - ETA: 6:54 - loss: 2.5287 - regression_loss: 1.9194 - classification_loss: 0.6092
  83/1000 [=>............................] - ETA: 6:53 - loss: 2.4982 - regression_loss: 1.8963 - classification_loss: 0.6019
  84/1000 [=>............................] - ETA: 6:53 - loss: 2.4684 - regression_loss: 1.8737 - classification_loss: 0.5947
  85/1000 [=>............................] - ETA: 6:53 - loss: 2.4873 - regression_loss: 1.8921 - classification_loss: 0.5952
  86/1000 [=>............................] - ETA: 6:52 - loss: 2.4895 - regression_loss: 1.8956 - classification_loss: 0.5939
  87/1000 [=>............................] - ETA: 6:52 - loss: 2.4609 - regression_loss: 1.8738 - classification_loss: 0.5871
  88/1000 [=>............................] - ETA: 6:51 - loss: 2.4601 - regression_loss: 1.8764 - classification_loss: 0.5836
  89/1000 [=>............................] - ETA: 6:51 - loss: 2.4324 - regression_loss: 1.8553 - classification_loss: 0.5771
  90/1000 [=>............................] - ETA: 6:50 - loss: 2.4324 - regression_loss: 1.8572 - classification_loss: 0.5752
  91/1000 [=>............................] - ETA: 6:50 - loss: 2.4631 - regression_loss: 1.8781 - classification_loss: 0.5850
  92/1000 [=>............................] - ETA: 6:49 - loss: 2.4847 - regression_loss: 1.8954 - classification_loss: 0.5893
  93/1000 [=>............................] - ETA: 6:49 - loss: 2.4971 - regression_loss: 1.8985 - classification_loss: 0.5987
  94/1000 [=>............................] - ETA: 6:48 - loss: 2.5165 - regression_loss: 1.9176 - classification_loss: 0.5989
  95/1000 [=>............................] - ETA: 6:48 - loss: 2.5269 - regression_loss: 1.9260 - classification_loss: 0.6009
  96/1000 [=>............................] - ETA: 6:48 - loss: 2.5198 - regression_loss: 1.9209 - classification_loss: 0.5989
  97/1000 [=>............................] - ETA: 6:47 - loss: 2.4938 - regression_loss: 1.9011 - classification_loss: 0.5928
  98/1000 [=>............................] - ETA: 6:47 - loss: 2.5028 - regression_loss: 1.9107 - classification_loss: 0.5921
  99/1000 [=>............................] - ETA: 6:46 - loss: 2.5175 - regression_loss: 1.9229 - classification_loss: 0.5946
 100/1000 [==>...........................] - ETA: 6:46 - loss: 2.5334 - regression_loss: 1.9378 - classification_loss: 0.5956
 101/1000 [==>...........................] - ETA: 6:45 - loss: 2.5087 - regression_loss: 1.9186 - classification_loss: 0.5901
 102/1000 [==>...........................] - ETA: 6:45 - loss: 2.5032 - regression_loss: 1.9135 - classification_loss: 0.5896
 103/1000 [==>...........................] - ETA: 6:45 - loss: 2.5078 - regression_loss: 1.9202 - classification_loss: 0.5876
 104/1000 [==>...........................] - ETA: 6:44 - loss: 2.4837 - regression_loss: 1.9017 - classification_loss: 0.5820
 105/1000 [==>...........................] - ETA: 6:44 - loss: 2.4836 - regression_loss: 1.9044 - classification_loss: 0.5792
 106/1000 [==>...........................] - ETA: 6:43 - loss: 2.4988 - regression_loss: 1.9172 - classification_loss: 0.5816
 107/1000 [==>...........................] - ETA: 6:43 - loss: 2.5275 - regression_loss: 1.9339 - classification_loss: 0.5936
 108/1000 [==>...........................] - ETA: 6:42 - loss: 2.5366 - regression_loss: 1.9427 - classification_loss: 0.5940
 109/1000 [==>...........................] - ETA: 6:42 - loss: 2.5134 - regression_loss: 1.9248 - classification_loss: 0.5885
 110/1000 [==>...........................] - ETA: 6:41 - loss: 2.5319 - regression_loss: 1.9319 - classification_loss: 0.6001
 111/1000 [==>...........................] - ETA: 6:41 - loss: 2.5104 - regression_loss: 1.9145 - classification_loss: 0.5960
 112/1000 [==>...........................] - ETA: 6:41 - loss: 2.5086 - regression_loss: 1.9150 - classification_loss: 0.5935
 113/1000 [==>...........................] - ETA: 6:40 - loss: 2.5140 - regression_loss: 1.9209 - classification_loss: 0.5931
 114/1000 [==>...........................] - ETA: 6:40 - loss: 2.5181 - regression_loss: 1.9265 - classification_loss: 0.5916
 115/1000 [==>...........................] - ETA: 6:39 - loss: 2.5308 - regression_loss: 1.9386 - classification_loss: 0.5922
 116/1000 [==>...........................] - ETA: 6:39 - loss: 2.5451 - regression_loss: 1.9519 - classification_loss: 0.5932
 117/1000 [==>...........................] - ETA: 6:39 - loss: 2.5382 - regression_loss: 1.9480 - classification_loss: 0.5902
 118/1000 [==>...........................] - ETA: 6:38 - loss: 2.5349 - regression_loss: 1.9451 - classification_loss: 0.5898
 119/1000 [==>...........................] - ETA: 6:38 - loss: 2.5370 - regression_loss: 1.9479 - classification_loss: 0.5891
 120/1000 [==>...........................] - ETA: 6:37 - loss: 2.5375 - regression_loss: 1.9506 - classification_loss: 0.5868
 121/1000 [==>...........................] - ETA: 6:37 - loss: 2.5567 - regression_loss: 1.9685 - classification_loss: 0.5882
 122/1000 [==>...........................] - ETA: 6:36 - loss: 2.5720 - regression_loss: 1.9831 - classification_loss: 0.5890
 123/1000 [==>...........................] - ETA: 6:36 - loss: 2.5781 - regression_loss: 1.9889 - classification_loss: 0.5891
 124/1000 [==>...........................] - ETA: 6:35 - loss: 2.6045 - regression_loss: 2.0065 - classification_loss: 0.5980
 125/1000 [==>...........................] - ETA: 6:35 - loss: 2.5983 - regression_loss: 2.0026 - classification_loss: 0.5957
 126/1000 [==>...........................] - ETA: 6:35 - loss: 2.6021 - regression_loss: 2.0064 - classification_loss: 0.5958
 127/1000 [==>...........................] - ETA: 6:34 - loss: 2.6066 - regression_loss: 2.0114 - classification_loss: 0.5953
 128/1000 [==>...........................] - ETA: 6:34 - loss: 2.5863 - regression_loss: 1.9957 - classification_loss: 0.5906
 129/1000 [==>...........................] - ETA: 6:33 - loss: 2.5884 - regression_loss: 1.9982 - classification_loss: 0.5901
 130/1000 [==>...........................] - ETA: 6:33 - loss: 2.5931 - regression_loss: 2.0023 - classification_loss: 0.5908
 131/1000 [==>...........................] - ETA: 6:32 - loss: 2.5733 - regression_loss: 1.9870 - classification_loss: 0.5863
 132/1000 [==>...........................] - ETA: 6:32 - loss: 2.5721 - regression_loss: 1.9884 - classification_loss: 0.5837
 133/1000 [==>...........................] - ETA: 6:31 - loss: 2.5528 - regression_loss: 1.9735 - classification_loss: 0.5794
 134/1000 [===>..........................] - ETA: 6:31 - loss: 2.5495 - regression_loss: 1.9726 - classification_loss: 0.5769
 135/1000 [===>..........................] - ETA: 6:31 - loss: 2.5561 - regression_loss: 1.9761 - classification_loss: 0.5800
 136/1000 [===>..........................] - ETA: 6:30 - loss: 2.5533 - regression_loss: 1.9748 - classification_loss: 0.5784
 137/1000 [===>..........................] - ETA: 6:30 - loss: 2.5513 - regression_loss: 1.9735 - classification_loss: 0.5778
 138/1000 [===>..........................] - ETA: 6:29 - loss: 2.5511 - regression_loss: 1.9737 - classification_loss: 0.5774
 139/1000 [===>..........................] - ETA: 6:29 - loss: 2.5441 - regression_loss: 1.9692 - classification_loss: 0.5749
 140/1000 [===>..........................] - ETA: 6:28 - loss: 2.5432 - regression_loss: 1.9677 - classification_loss: 0.5755
 141/1000 [===>..........................] - ETA: 6:28 - loss: 2.5251 - regression_loss: 1.9537 - classification_loss: 0.5714
 142/1000 [===>..........................] - ETA: 6:28 - loss: 2.5311 - regression_loss: 1.9592 - classification_loss: 0.5719
 143/1000 [===>..........................] - ETA: 6:27 - loss: 2.5425 - regression_loss: 1.9703 - classification_loss: 0.5722
 144/1000 [===>..........................] - ETA: 6:27 - loss: 2.5589 - regression_loss: 1.9790 - classification_loss: 0.5799
 145/1000 [===>..........................] - ETA: 6:26 - loss: 2.5598 - regression_loss: 1.9822 - classification_loss: 0.5776
 146/1000 [===>..........................] - ETA: 6:26 - loss: 2.5687 - regression_loss: 1.9868 - classification_loss: 0.5819
 147/1000 [===>..........................] - ETA: 6:25 - loss: 2.5680 - regression_loss: 1.9870 - classification_loss: 0.5809
 148/1000 [===>..........................] - ETA: 6:25 - loss: 2.5745 - regression_loss: 1.9952 - classification_loss: 0.5793
 149/1000 [===>..........................] - ETA: 6:24 - loss: 2.5767 - regression_loss: 1.9986 - classification_loss: 0.5781
 150/1000 [===>..........................] - ETA: 6:24 - loss: 2.5611 - regression_loss: 1.9853 - classification_loss: 0.5758
 151/1000 [===>..........................] - ETA: 6:23 - loss: 2.5609 - regression_loss: 1.9863 - classification_loss: 0.5747
 152/1000 [===>..........................] - ETA: 6:23 - loss: 2.5563 - regression_loss: 1.9823 - classification_loss: 0.5740
 153/1000 [===>..........................] - ETA: 6:22 - loss: 2.5495 - regression_loss: 1.9775 - classification_loss: 0.5719
 154/1000 [===>..........................] - ETA: 6:22 - loss: 2.5676 - regression_loss: 1.9908 - classification_loss: 0.5768
 155/1000 [===>..........................] - ETA: 6:22 - loss: 2.5510 - regression_loss: 1.9779 - classification_loss: 0.5731
 156/1000 [===>..........................] - ETA: 6:21 - loss: 2.5453 - regression_loss: 1.9738 - classification_loss: 0.5715
 157/1000 [===>..........................] - ETA: 6:21 - loss: 2.5557 - regression_loss: 1.9811 - classification_loss: 0.5745
 158/1000 [===>..........................] - ETA: 6:20 - loss: 2.5544 - regression_loss: 1.9817 - classification_loss: 0.5727
 159/1000 [===>..........................] - ETA: 6:20 - loss: 2.5619 - regression_loss: 1.9886 - classification_loss: 0.5733
 160/1000 [===>..........................] - ETA: 6:19 - loss: 2.5792 - regression_loss: 2.0028 - classification_loss: 0.5763
 161/1000 [===>..........................] - ETA: 6:19 - loss: 2.5802 - regression_loss: 2.0053 - classification_loss: 0.5750
 162/1000 [===>..........................] - ETA: 6:19 - loss: 2.5960 - regression_loss: 2.0175 - classification_loss: 0.5784
 163/1000 [===>..........................] - ETA: 6:18 - loss: 2.5900 - regression_loss: 2.0142 - classification_loss: 0.5758
 164/1000 [===>..........................] - ETA: 6:18 - loss: 2.6135 - regression_loss: 2.0284 - classification_loss: 0.5851
 165/1000 [===>..........................] - ETA: 6:17 - loss: 2.5982 - regression_loss: 2.0161 - classification_loss: 0.5821
 166/1000 [===>..........................] - ETA: 6:17 - loss: 2.6036 - regression_loss: 2.0214 - classification_loss: 0.5821
 167/1000 [====>.........................] - ETA: 6:16 - loss: 2.5882 - regression_loss: 2.0093 - classification_loss: 0.5788
 168/1000 [====>.........................] - ETA: 6:16 - loss: 2.5728 - regression_loss: 1.9974 - classification_loss: 0.5754
 169/1000 [====>.........................] - ETA: 6:15 - loss: 2.5756 - regression_loss: 1.9991 - classification_loss: 0.5765
 170/1000 [====>.........................] - ETA: 6:15 - loss: 2.5933 - regression_loss: 2.0142 - classification_loss: 0.5792
 171/1000 [====>.........................] - ETA: 6:14 - loss: 2.6067 - regression_loss: 2.0236 - classification_loss: 0.5830
 172/1000 [====>.........................] - ETA: 6:14 - loss: 2.5927 - regression_loss: 2.0119 - classification_loss: 0.5808
 173/1000 [====>.........................] - ETA: 6:14 - loss: 2.5779 - regression_loss: 2.0003 - classification_loss: 0.5776
 174/1000 [====>.........................] - ETA: 6:13 - loss: 2.5845 - regression_loss: 2.0034 - classification_loss: 0.5812
 175/1000 [====>.........................] - ETA: 6:13 - loss: 2.5698 - regression_loss: 1.9919 - classification_loss: 0.5779
 176/1000 [====>.........................] - ETA: 6:12 - loss: 2.5853 - regression_loss: 2.0017 - classification_loss: 0.5835
 177/1000 [====>.........................] - ETA: 6:12 - loss: 2.5862 - regression_loss: 2.0026 - classification_loss: 0.5836
 178/1000 [====>.........................] - ETA: 6:11 - loss: 2.5835 - regression_loss: 1.9996 - classification_loss: 0.5839
 179/1000 [====>.........................] - ETA: 6:11 - loss: 2.5866 - regression_loss: 2.0022 - classification_loss: 0.5844
 180/1000 [====>.........................] - ETA: 6:10 - loss: 2.5946 - regression_loss: 2.0075 - classification_loss: 0.5871
 181/1000 [====>.........................] - ETA: 6:10 - loss: 2.6012 - regression_loss: 2.0075 - classification_loss: 0.5937
 182/1000 [====>.........................] - ETA: 6:10 - loss: 2.6093 - regression_loss: 2.0100 - classification_loss: 0.5993
 183/1000 [====>.........................] - ETA: 6:09 - loss: 2.6154 - regression_loss: 2.0145 - classification_loss: 0.6009
 184/1000 [====>.........................] - ETA: 6:09 - loss: 2.6205 - regression_loss: 2.0188 - classification_loss: 0.6017
 185/1000 [====>.........................] - ETA: 6:08 - loss: 2.6206 - regression_loss: 2.0196 - classification_loss: 0.6010
 186/1000 [====>.........................] - ETA: 6:08 - loss: 2.6250 - regression_loss: 2.0235 - classification_loss: 0.6015
 187/1000 [====>.........................] - ETA: 6:07 - loss: 2.6109 - regression_loss: 2.0126 - classification_loss: 0.5983
 188/1000 [====>.........................] - ETA: 6:07 - loss: 2.6188 - regression_loss: 2.0206 - classification_loss: 0.5982
 189/1000 [====>.........................] - ETA: 6:06 - loss: 2.6234 - regression_loss: 2.0256 - classification_loss: 0.5978
 190/1000 [====>.........................] - ETA: 6:06 - loss: 2.6200 - regression_loss: 2.0213 - classification_loss: 0.5988
 191/1000 [====>.........................] - ETA: 6:05 - loss: 2.6240 - regression_loss: 2.0237 - classification_loss: 0.6002
 192/1000 [====>.........................] - ETA: 6:05 - loss: 2.6309 - regression_loss: 2.0309 - classification_loss: 0.6000
 193/1000 [====>.........................] - ETA: 6:05 - loss: 2.6182 - regression_loss: 2.0203 - classification_loss: 0.5979
 194/1000 [====>.........................] - ETA: 6:04 - loss: 2.6047 - regression_loss: 2.0099 - classification_loss: 0.5948
 195/1000 [====>.........................] - ETA: 6:04 - loss: 2.6149 - regression_loss: 2.0194 - classification_loss: 0.5955
 196/1000 [====>.........................] - ETA: 6:03 - loss: 2.6182 - regression_loss: 2.0227 - classification_loss: 0.5955
 197/1000 [====>.........................] - ETA: 6:03 - loss: 2.6295 - regression_loss: 2.0285 - classification_loss: 0.6010
 198/1000 [====>.........................] - ETA: 6:02 - loss: 2.6351 - regression_loss: 2.0324 - classification_loss: 0.6028
 199/1000 [====>.........................] - ETA: 6:02 - loss: 2.6354 - regression_loss: 2.0339 - classification_loss: 0.6016
 200/1000 [=====>........................] - ETA: 6:01 - loss: 2.6366 - regression_loss: 2.0350 - classification_loss: 0.6016
 201/1000 [=====>........................] - ETA: 6:01 - loss: 2.6441 - regression_loss: 2.0428 - classification_loss: 0.6013
 202/1000 [=====>........................] - ETA: 6:01 - loss: 2.6491 - regression_loss: 2.0474 - classification_loss: 0.6016
 203/1000 [=====>........................] - ETA: 6:00 - loss: 2.6538 - regression_loss: 2.0506 - classification_loss: 0.6031
 204/1000 [=====>........................] - ETA: 6:00 - loss: 2.6535 - regression_loss: 2.0508 - classification_loss: 0.6027
 205/1000 [=====>........................] - ETA: 5:59 - loss: 2.6539 - regression_loss: 2.0524 - classification_loss: 0.6015
 206/1000 [=====>........................] - ETA: 5:59 - loss: 2.6410 - regression_loss: 2.0424 - classification_loss: 0.5986
 207/1000 [=====>........................] - ETA: 5:58 - loss: 2.6441 - regression_loss: 2.0437 - classification_loss: 0.6004
 208/1000 [=====>........................] - ETA: 5:58 - loss: 2.6444 - regression_loss: 2.0412 - classification_loss: 0.6032
 209/1000 [=====>........................] - ETA: 5:57 - loss: 2.6454 - regression_loss: 2.0429 - classification_loss: 0.6025
 210/1000 [=====>........................] - ETA: 5:57 - loss: 2.6423 - regression_loss: 2.0415 - classification_loss: 0.6008
 211/1000 [=====>........................] - ETA: 5:56 - loss: 2.6509 - regression_loss: 2.0488 - classification_loss: 0.6020
 212/1000 [=====>........................] - ETA: 5:56 - loss: 2.6516 - regression_loss: 2.0486 - classification_loss: 0.6030
 213/1000 [=====>........................] - ETA: 5:56 - loss: 2.6521 - regression_loss: 2.0501 - classification_loss: 0.6020
 214/1000 [=====>........................] - ETA: 5:55 - loss: 2.6564 - regression_loss: 2.0537 - classification_loss: 0.6027
 215/1000 [=====>........................] - ETA: 5:55 - loss: 2.6521 - regression_loss: 2.0508 - classification_loss: 0.6013
 216/1000 [=====>........................] - ETA: 5:54 - loss: 2.6529 - regression_loss: 2.0526 - classification_loss: 0.6003
 217/1000 [=====>........................] - ETA: 5:54 - loss: 2.6656 - regression_loss: 2.0595 - classification_loss: 0.6061
 218/1000 [=====>........................] - ETA: 5:53 - loss: 2.6720 - regression_loss: 2.0661 - classification_loss: 0.6059
 219/1000 [=====>........................] - ETA: 5:53 - loss: 2.6629 - regression_loss: 2.0567 - classification_loss: 0.6063
 220/1000 [=====>........................] - ETA: 5:52 - loss: 2.6620 - regression_loss: 2.0572 - classification_loss: 0.6048
 221/1000 [=====>........................] - ETA: 5:52 - loss: 2.6500 - regression_loss: 2.0479 - classification_loss: 0.6020
 222/1000 [=====>........................] - ETA: 5:52 - loss: 2.6380 - regression_loss: 2.0387 - classification_loss: 0.5993
 223/1000 [=====>........................] - ETA: 5:51 - loss: 2.6414 - regression_loss: 2.0419 - classification_loss: 0.5995
 224/1000 [=====>........................] - ETA: 5:51 - loss: 2.6302 - regression_loss: 2.0328 - classification_loss: 0.5975
 225/1000 [=====>........................] - ETA: 5:50 - loss: 2.6295 - regression_loss: 2.0329 - classification_loss: 0.5966
 226/1000 [=====>........................] - ETA: 5:50 - loss: 2.6299 - regression_loss: 2.0332 - classification_loss: 0.5967
 227/1000 [=====>........................] - ETA: 5:49 - loss: 2.6308 - regression_loss: 2.0342 - classification_loss: 0.5966
 228/1000 [=====>........................] - ETA: 5:49 - loss: 2.6424 - regression_loss: 2.0418 - classification_loss: 0.6006
 229/1000 [=====>........................] - ETA: 5:48 - loss: 2.6445 - regression_loss: 2.0431 - classification_loss: 0.6015
 230/1000 [=====>........................] - ETA: 5:48 - loss: 2.6583 - regression_loss: 2.0514 - classification_loss: 0.6069
 231/1000 [=====>........................] - ETA: 5:47 - loss: 2.6611 - regression_loss: 2.0546 - classification_loss: 0.6065
 232/1000 [=====>........................] - ETA: 5:47 - loss: 2.6562 - regression_loss: 2.0507 - classification_loss: 0.6055
 233/1000 [=====>........................] - ETA: 5:47 - loss: 2.6570 - regression_loss: 2.0518 - classification_loss: 0.6051
 234/1000 [======>.......................] - ETA: 5:46 - loss: 2.6572 - regression_loss: 2.0517 - classification_loss: 0.6055
 235/1000 [======>.......................] - ETA: 5:46 - loss: 2.6602 - regression_loss: 2.0555 - classification_loss: 0.6046
 236/1000 [======>.......................] - ETA: 5:45 - loss: 2.6597 - regression_loss: 2.0556 - classification_loss: 0.6040
 237/1000 [======>.......................] - ETA: 5:45 - loss: 2.6619 - regression_loss: 2.0572 - classification_loss: 0.6047
 238/1000 [======>.......................] - ETA: 5:44 - loss: 2.6507 - regression_loss: 2.0485 - classification_loss: 0.6022
 239/1000 [======>.......................] - ETA: 5:44 - loss: 2.6489 - regression_loss: 2.0469 - classification_loss: 0.6020
 240/1000 [======>.......................] - ETA: 5:43 - loss: 2.6491 - regression_loss: 2.0484 - classification_loss: 0.6007
 241/1000 [======>.......................] - ETA: 5:43 - loss: 2.6381 - regression_loss: 2.0399 - classification_loss: 0.5982
 242/1000 [======>.......................] - ETA: 5:42 - loss: 2.6389 - regression_loss: 2.0401 - classification_loss: 0.5988
 243/1000 [======>.......................] - ETA: 5:42 - loss: 2.6428 - regression_loss: 2.0442 - classification_loss: 0.5986
 244/1000 [======>.......................] - ETA: 5:42 - loss: 2.6483 - regression_loss: 2.0499 - classification_loss: 0.5983
 245/1000 [======>.......................] - ETA: 5:41 - loss: 2.6499 - regression_loss: 2.0519 - classification_loss: 0.5980
 246/1000 [======>.......................] - ETA: 5:41 - loss: 2.6587 - regression_loss: 2.0605 - classification_loss: 0.5982
 247/1000 [======>.......................] - ETA: 5:40 - loss: 2.6603 - regression_loss: 2.0624 - classification_loss: 0.5979
 248/1000 [======>.......................] - ETA: 5:40 - loss: 2.6498 - regression_loss: 2.0540 - classification_loss: 0.5957
 249/1000 [======>.......................] - ETA: 5:39 - loss: 2.6391 - regression_loss: 2.0458 - classification_loss: 0.5933
 250/1000 [======>.......................] - ETA: 5:39 - loss: 2.6408 - regression_loss: 2.0457 - classification_loss: 0.5951
 251/1000 [======>.......................] - ETA: 5:38 - loss: 2.6445 - regression_loss: 2.0494 - classification_loss: 0.5951
 252/1000 [======>.......................] - ETA: 5:38 - loss: 2.6533 - regression_loss: 2.0569 - classification_loss: 0.5964
 253/1000 [======>.......................] - ETA: 5:38 - loss: 2.6509 - regression_loss: 2.0555 - classification_loss: 0.5954
 254/1000 [======>.......................] - ETA: 5:37 - loss: 2.6518 - regression_loss: 2.0553 - classification_loss: 0.5966
 255/1000 [======>.......................] - ETA: 5:37 - loss: 2.6522 - regression_loss: 2.0567 - classification_loss: 0.5954
 256/1000 [======>.......................] - ETA: 5:36 - loss: 2.6588 - regression_loss: 2.0631 - classification_loss: 0.5957
 257/1000 [======>.......................] - ETA: 5:36 - loss: 2.6620 - regression_loss: 2.0662 - classification_loss: 0.5958
 258/1000 [======>.......................] - ETA: 5:35 - loss: 2.6627 - regression_loss: 2.0679 - classification_loss: 0.5947
 259/1000 [======>.......................] - ETA: 5:35 - loss: 2.6627 - regression_loss: 2.0685 - classification_loss: 0.5942
 260/1000 [======>.......................] - ETA: 5:34 - loss: 2.6652 - regression_loss: 2.0718 - classification_loss: 0.5935
 261/1000 [======>.......................] - ETA: 5:34 - loss: 2.6625 - regression_loss: 2.0701 - classification_loss: 0.5924
 262/1000 [======>.......................] - ETA: 5:33 - loss: 2.6670 - regression_loss: 2.0710 - classification_loss: 0.5960
 263/1000 [======>.......................] - ETA: 5:33 - loss: 2.6707 - regression_loss: 2.0739 - classification_loss: 0.5968
 264/1000 [======>.......................] - ETA: 5:33 - loss: 2.6751 - regression_loss: 2.0790 - classification_loss: 0.5961
 265/1000 [======>.......................] - ETA: 5:32 - loss: 2.6743 - regression_loss: 2.0788 - classification_loss: 0.5955
 266/1000 [======>.......................] - ETA: 5:32 - loss: 2.6643 - regression_loss: 2.0710 - classification_loss: 0.5933
 267/1000 [=======>......................] - ETA: 5:31 - loss: 2.6543 - regression_loss: 2.0633 - classification_loss: 0.5911
 268/1000 [=======>......................] - ETA: 5:31 - loss: 2.6527 - regression_loss: 2.0625 - classification_loss: 0.5902
 269/1000 [=======>......................] - ETA: 5:30 - loss: 2.6617 - regression_loss: 2.0711 - classification_loss: 0.5906
 270/1000 [=======>......................] - ETA: 5:30 - loss: 2.6519 - regression_loss: 2.0634 - classification_loss: 0.5884
 271/1000 [=======>......................] - ETA: 5:29 - loss: 2.6526 - regression_loss: 2.0615 - classification_loss: 0.5911
 272/1000 [=======>......................] - ETA: 5:29 - loss: 2.6645 - regression_loss: 2.0708 - classification_loss: 0.5937
 273/1000 [=======>......................] - ETA: 5:28 - loss: 2.6681 - regression_loss: 2.0745 - classification_loss: 0.5936
 274/1000 [=======>......................] - ETA: 5:28 - loss: 2.6584 - regression_loss: 2.0670 - classification_loss: 0.5914
 275/1000 [=======>......................] - ETA: 5:28 - loss: 2.6490 - regression_loss: 2.0594 - classification_loss: 0.5896
 276/1000 [=======>......................] - ETA: 5:27 - loss: 2.6483 - regression_loss: 2.0595 - classification_loss: 0.5888
 277/1000 [=======>......................] - ETA: 5:27 - loss: 2.6527 - regression_loss: 2.0634 - classification_loss: 0.5893
 278/1000 [=======>......................] - ETA: 5:26 - loss: 2.6513 - regression_loss: 2.0622 - classification_loss: 0.5891
 279/1000 [=======>......................] - ETA: 5:26 - loss: 2.6419 - regression_loss: 2.0548 - classification_loss: 0.5870
 280/1000 [=======>......................] - ETA: 5:25 - loss: 2.6428 - regression_loss: 2.0558 - classification_loss: 0.5870
 281/1000 [=======>......................] - ETA: 5:25 - loss: 2.6479 - regression_loss: 2.0595 - classification_loss: 0.5884
 282/1000 [=======>......................] - ETA: 5:24 - loss: 2.6499 - regression_loss: 2.0599 - classification_loss: 0.5900
 283/1000 [=======>......................] - ETA: 5:24 - loss: 2.6521 - regression_loss: 2.0620 - classification_loss: 0.5902
 284/1000 [=======>......................] - ETA: 5:24 - loss: 2.6509 - regression_loss: 2.0615 - classification_loss: 0.5894
 285/1000 [=======>......................] - ETA: 5:23 - loss: 2.6580 - regression_loss: 2.0639 - classification_loss: 0.5941
 286/1000 [=======>......................] - ETA: 5:23 - loss: 2.6636 - regression_loss: 2.0684 - classification_loss: 0.5952
 287/1000 [=======>......................] - ETA: 5:22 - loss: 2.6712 - regression_loss: 2.0740 - classification_loss: 0.5972
 288/1000 [=======>......................] - ETA: 5:22 - loss: 2.6755 - regression_loss: 2.0740 - classification_loss: 0.6015
 289/1000 [=======>......................] - ETA: 5:21 - loss: 2.6817 - regression_loss: 2.0785 - classification_loss: 0.6033
 290/1000 [=======>......................] - ETA: 5:21 - loss: 2.6838 - regression_loss: 2.0798 - classification_loss: 0.6041
 291/1000 [=======>......................] - ETA: 5:20 - loss: 2.6923 - regression_loss: 2.0849 - classification_loss: 0.6074
 292/1000 [=======>......................] - ETA: 5:20 - loss: 2.6846 - regression_loss: 2.0778 - classification_loss: 0.6068
 293/1000 [=======>......................] - ETA: 5:19 - loss: 2.6754 - regression_loss: 2.0707 - classification_loss: 0.6047
 294/1000 [=======>......................] - ETA: 5:19 - loss: 2.6779 - regression_loss: 2.0728 - classification_loss: 0.6052
 295/1000 [=======>......................] - ETA: 5:19 - loss: 2.6733 - regression_loss: 2.0686 - classification_loss: 0.6047
 296/1000 [=======>......................] - ETA: 5:18 - loss: 2.6643 - regression_loss: 2.0616 - classification_loss: 0.6027
 297/1000 [=======>......................] - ETA: 5:18 - loss: 2.6651 - regression_loss: 2.0611 - classification_loss: 0.6040
 298/1000 [=======>......................] - ETA: 5:17 - loss: 2.6643 - regression_loss: 2.0612 - classification_loss: 0.6031
 299/1000 [=======>......................] - ETA: 5:17 - loss: 2.6656 - regression_loss: 2.0631 - classification_loss: 0.6026
 300/1000 [========>.....................] - ETA: 5:16 - loss: 2.6664 - regression_loss: 2.0637 - classification_loss: 0.6027
 301/1000 [========>.....................] - ETA: 5:16 - loss: 2.6662 - regression_loss: 2.0643 - classification_loss: 0.6019
 302/1000 [========>.....................] - ETA: 5:15 - loss: 2.6695 - regression_loss: 2.0675 - classification_loss: 0.6019
 303/1000 [========>.....................] - ETA: 5:15 - loss: 2.6746 - regression_loss: 2.0723 - classification_loss: 0.6023
 304/1000 [========>.....................] - ETA: 5:14 - loss: 2.6827 - regression_loss: 2.0764 - classification_loss: 0.6063
 305/1000 [========>.....................] - ETA: 5:14 - loss: 2.6883 - regression_loss: 2.0793 - classification_loss: 0.6089
 306/1000 [========>.....................] - ETA: 5:14 - loss: 2.6891 - regression_loss: 2.0791 - classification_loss: 0.6101
 307/1000 [========>.....................] - ETA: 5:13 - loss: 2.6931 - regression_loss: 2.0831 - classification_loss: 0.6100
 308/1000 [========>.....................] - ETA: 5:13 - loss: 2.6875 - regression_loss: 2.0763 - classification_loss: 0.6112
 309/1000 [========>.....................] - ETA: 5:12 - loss: 2.6791 - regression_loss: 2.0696 - classification_loss: 0.6095
 310/1000 [========>.....................] - ETA: 5:12 - loss: 2.6793 - regression_loss: 2.0702 - classification_loss: 0.6091
 311/1000 [========>.....................] - ETA: 5:11 - loss: 2.6796 - regression_loss: 2.0709 - classification_loss: 0.6087
 312/1000 [========>.....................] - ETA: 5:11 - loss: 2.6714 - regression_loss: 2.0643 - classification_loss: 0.6072
 313/1000 [========>.....................] - ETA: 5:10 - loss: 2.6722 - regression_loss: 2.0652 - classification_loss: 0.6071
 314/1000 [========>.....................] - ETA: 5:10 - loss: 2.6786 - regression_loss: 2.0703 - classification_loss: 0.6083
 315/1000 [========>.....................] - ETA: 5:10 - loss: 2.6788 - regression_loss: 2.0713 - classification_loss: 0.6075
 316/1000 [========>.....................] - ETA: 5:09 - loss: 2.6769 - regression_loss: 2.0703 - classification_loss: 0.6066
 317/1000 [========>.....................] - ETA: 5:09 - loss: 2.6920 - regression_loss: 2.0820 - classification_loss: 0.6100
 318/1000 [========>.....................] - ETA: 5:08 - loss: 2.6836 - regression_loss: 2.0755 - classification_loss: 0.6081
 319/1000 [========>.....................] - ETA: 5:08 - loss: 2.6751 - regression_loss: 2.0690 - classification_loss: 0.6062
 320/1000 [========>.....................] - ETA: 5:07 - loss: 2.6765 - regression_loss: 2.0697 - classification_loss: 0.6068
 321/1000 [========>.....................] - ETA: 5:07 - loss: 2.6685 - regression_loss: 2.0633 - classification_loss: 0.6052
 322/1000 [========>.....................] - ETA: 5:06 - loss: 2.6724 - regression_loss: 2.0670 - classification_loss: 0.6055
 323/1000 [========>.....................] - ETA: 5:06 - loss: 2.6771 - regression_loss: 2.0704 - classification_loss: 0.6068
 324/1000 [========>.....................] - ETA: 5:05 - loss: 2.6689 - regression_loss: 2.0640 - classification_loss: 0.6049
 325/1000 [========>.....................] - ETA: 5:05 - loss: 2.6740 - regression_loss: 2.0692 - classification_loss: 0.6047
 326/1000 [========>.....................] - ETA: 5:05 - loss: 2.6757 - regression_loss: 2.0709 - classification_loss: 0.6048
 327/1000 [========>.....................] - ETA: 5:04 - loss: 2.6770 - regression_loss: 2.0721 - classification_loss: 0.6049
 328/1000 [========>.....................] - ETA: 5:04 - loss: 2.6828 - regression_loss: 2.0744 - classification_loss: 0.6085
 329/1000 [========>.....................] - ETA: 5:03 - loss: 2.6834 - regression_loss: 2.0753 - classification_loss: 0.6081
 330/1000 [========>.....................] - ETA: 5:03 - loss: 2.6876 - regression_loss: 2.0783 - classification_loss: 0.6093
 331/1000 [========>.....................] - ETA: 5:02 - loss: 2.6889 - regression_loss: 2.0805 - classification_loss: 0.6084
 332/1000 [========>.....................] - ETA: 5:02 - loss: 2.6810 - regression_loss: 2.0742 - classification_loss: 0.6068
 333/1000 [========>.....................] - ETA: 5:01 - loss: 2.6729 - regression_loss: 2.0680 - classification_loss: 0.6050
 334/1000 [=========>....................] - ETA: 5:01 - loss: 2.6783 - regression_loss: 2.0732 - classification_loss: 0.6052
 335/1000 [=========>....................] - ETA: 5:00 - loss: 2.6795 - regression_loss: 2.0748 - classification_loss: 0.6046
 336/1000 [=========>....................] - ETA: 5:00 - loss: 2.6796 - regression_loss: 2.0755 - classification_loss: 0.6041
 337/1000 [=========>....................] - ETA: 5:00 - loss: 2.6717 - regression_loss: 2.0693 - classification_loss: 0.6023
 338/1000 [=========>....................] - ETA: 4:59 - loss: 2.6755 - regression_loss: 2.0732 - classification_loss: 0.6023
 339/1000 [=========>....................] - ETA: 4:59 - loss: 2.6797 - regression_loss: 2.0743 - classification_loss: 0.6055
 340/1000 [=========>....................] - ETA: 4:58 - loss: 2.6817 - regression_loss: 2.0766 - classification_loss: 0.6051
 341/1000 [=========>....................] - ETA: 4:58 - loss: 2.6891 - regression_loss: 2.0810 - classification_loss: 0.6081
 342/1000 [=========>....................] - ETA: 4:57 - loss: 2.6903 - regression_loss: 2.0818 - classification_loss: 0.6085
 343/1000 [=========>....................] - ETA: 4:57 - loss: 2.6825 - regression_loss: 2.0757 - classification_loss: 0.6067
 344/1000 [=========>....................] - ETA: 4:56 - loss: 2.6858 - regression_loss: 2.0774 - classification_loss: 0.6084
 345/1000 [=========>....................] - ETA: 4:56 - loss: 2.6862 - regression_loss: 2.0777 - classification_loss: 0.6085
 346/1000 [=========>....................] - ETA: 4:56 - loss: 2.6787 - regression_loss: 2.0717 - classification_loss: 0.6070
 347/1000 [=========>....................] - ETA: 4:55 - loss: 2.6805 - regression_loss: 2.0724 - classification_loss: 0.6081
 348/1000 [=========>....................] - ETA: 4:55 - loss: 2.6827 - regression_loss: 2.0719 - classification_loss: 0.6108
 349/1000 [=========>....................] - ETA: 4:54 - loss: 2.6831 - regression_loss: 2.0734 - classification_loss: 0.6097
 350/1000 [=========>....................] - ETA: 4:54 - loss: 2.6754 - regression_loss: 2.0675 - classification_loss: 0.6079
 351/1000 [=========>....................] - ETA: 4:53 - loss: 2.6679 - regression_loss: 2.0616 - classification_loss: 0.6063
 352/1000 [=========>....................] - ETA: 4:53 - loss: 2.6772 - regression_loss: 2.0668 - classification_loss: 0.6105
 353/1000 [=========>....................] - ETA: 4:52 - loss: 2.6782 - regression_loss: 2.0678 - classification_loss: 0.6104
 354/1000 [=========>....................] - ETA: 4:52 - loss: 2.6787 - regression_loss: 2.0689 - classification_loss: 0.6098
 355/1000 [=========>....................] - ETA: 4:51 - loss: 2.6820 - regression_loss: 2.0713 - classification_loss: 0.6107
 356/1000 [=========>....................] - ETA: 4:51 - loss: 2.6873 - regression_loss: 2.0740 - classification_loss: 0.6134
 357/1000 [=========>....................] - ETA: 4:51 - loss: 2.6926 - regression_loss: 2.0786 - classification_loss: 0.6140
 358/1000 [=========>....................] - ETA: 4:50 - loss: 2.6938 - regression_loss: 2.0785 - classification_loss: 0.6153
 359/1000 [=========>....................] - ETA: 4:50 - loss: 2.6863 - regression_loss: 2.0727 - classification_loss: 0.6136
 360/1000 [=========>....................] - ETA: 4:49 - loss: 2.6863 - regression_loss: 2.0726 - classification_loss: 0.6136
 361/1000 [=========>....................] - ETA: 4:49 - loss: 2.6898 - regression_loss: 2.0754 - classification_loss: 0.6144
 362/1000 [=========>....................] - ETA: 4:48 - loss: 2.6824 - regression_loss: 2.0697 - classification_loss: 0.6128
 363/1000 [=========>....................] - ETA: 4:48 - loss: 2.6750 - regression_loss: 2.0640 - classification_loss: 0.6111
 364/1000 [=========>....................] - ETA: 4:47 - loss: 2.6791 - regression_loss: 2.0659 - classification_loss: 0.6132
 365/1000 [=========>....................] - ETA: 4:47 - loss: 2.6845 - regression_loss: 2.0705 - classification_loss: 0.6140
 366/1000 [=========>....................] - ETA: 4:46 - loss: 2.6835 - regression_loss: 2.0689 - classification_loss: 0.6146
 367/1000 [==========>...................] - ETA: 4:46 - loss: 2.6905 - regression_loss: 2.0755 - classification_loss: 0.6150
 368/1000 [==========>...................] - ETA: 4:46 - loss: 2.6962 - regression_loss: 2.0786 - classification_loss: 0.6176
 369/1000 [==========>...................] - ETA: 4:45 - loss: 2.6946 - regression_loss: 2.0773 - classification_loss: 0.6172
 370/1000 [==========>...................] - ETA: 4:45 - loss: 2.6949 - regression_loss: 2.0777 - classification_loss: 0.6171
 371/1000 [==========>...................] - ETA: 4:44 - loss: 2.6968 - regression_loss: 2.0803 - classification_loss: 0.6165
 372/1000 [==========>...................] - ETA: 4:44 - loss: 2.6896 - regression_loss: 2.0747 - classification_loss: 0.6149
 373/1000 [==========>...................] - ETA: 4:43 - loss: 2.6952 - regression_loss: 2.0776 - classification_loss: 0.6176
 374/1000 [==========>...................] - ETA: 4:43 - loss: 2.6967 - regression_loss: 2.0784 - classification_loss: 0.6183
 375/1000 [==========>...................] - ETA: 4:42 - loss: 2.6989 - regression_loss: 2.0778 - classification_loss: 0.6210
 376/1000 [==========>...................] - ETA: 4:42 - loss: 2.6966 - regression_loss: 2.0765 - classification_loss: 0.6201
 377/1000 [==========>...................] - ETA: 4:41 - loss: 2.6895 - regression_loss: 2.0710 - classification_loss: 0.6184
 378/1000 [==========>...................] - ETA: 4:41 - loss: 2.6902 - regression_loss: 2.0724 - classification_loss: 0.6178
 379/1000 [==========>...................] - ETA: 4:40 - loss: 2.6913 - regression_loss: 2.0733 - classification_loss: 0.6180
 380/1000 [==========>...................] - ETA: 4:40 - loss: 2.6842 - regression_loss: 2.0678 - classification_loss: 0.6164
 381/1000 [==========>...................] - ETA: 4:40 - loss: 2.6772 - regression_loss: 2.0624 - classification_loss: 0.6148
 382/1000 [==========>...................] - ETA: 4:39 - loss: 2.6702 - regression_loss: 2.0570 - classification_loss: 0.6132
 383/1000 [==========>...................] - ETA: 4:39 - loss: 2.6632 - regression_loss: 2.0516 - classification_loss: 0.6116
 384/1000 [==========>...................] - ETA: 4:38 - loss: 2.6563 - regression_loss: 2.0463 - classification_loss: 0.6100
 385/1000 [==========>...................] - ETA: 4:38 - loss: 2.6643 - regression_loss: 2.0539 - classification_loss: 0.6104
 386/1000 [==========>...................] - ETA: 4:37 - loss: 2.6709 - regression_loss: 2.0603 - classification_loss: 0.6106
 387/1000 [==========>...................] - ETA: 4:37 - loss: 2.6681 - regression_loss: 2.0587 - classification_loss: 0.6094
 388/1000 [==========>...................] - ETA: 4:36 - loss: 2.6663 - regression_loss: 2.0571 - classification_loss: 0.6092
 389/1000 [==========>...................] - ETA: 4:36 - loss: 2.6595 - regression_loss: 2.0518 - classification_loss: 0.6077
 390/1000 [==========>...................] - ETA: 4:36 - loss: 2.6602 - regression_loss: 2.0525 - classification_loss: 0.6077
 391/1000 [==========>...................] - ETA: 4:35 - loss: 2.6626 - regression_loss: 2.0555 - classification_loss: 0.6071
 392/1000 [==========>...................] - ETA: 4:35 - loss: 2.6678 - regression_loss: 2.0599 - classification_loss: 0.6079
 393/1000 [==========>...................] - ETA: 4:34 - loss: 2.6662 - regression_loss: 2.0588 - classification_loss: 0.6074
 394/1000 [==========>...................] - ETA: 4:34 - loss: 2.6706 - regression_loss: 2.0611 - classification_loss: 0.6096
 395/1000 [==========>...................] - ETA: 4:33 - loss: 2.6767 - regression_loss: 2.0655 - classification_loss: 0.6113
 396/1000 [==========>...................] - ETA: 4:33 - loss: 2.6702 - regression_loss: 2.0602 - classification_loss: 0.6099
 397/1000 [==========>...................] - ETA: 4:32 - loss: 2.6738 - regression_loss: 2.0630 - classification_loss: 0.6108
 398/1000 [==========>...................] - ETA: 4:32 - loss: 2.6671 - regression_loss: 2.0579 - classification_loss: 0.6092
 399/1000 [==========>...................] - ETA: 4:31 - loss: 2.6674 - regression_loss: 2.0586 - classification_loss: 0.6089
 400/1000 [===========>..................] - ETA: 4:31 - loss: 2.6608 - regression_loss: 2.0534 - classification_loss: 0.6074
 401/1000 [===========>..................] - ETA: 4:30 - loss: 2.6653 - regression_loss: 2.0563 - classification_loss: 0.6090
 402/1000 [===========>..................] - ETA: 4:30 - loss: 2.6774 - regression_loss: 2.0651 - classification_loss: 0.6123
 403/1000 [===========>..................] - ETA: 4:30 - loss: 2.6765 - regression_loss: 2.0644 - classification_loss: 0.6121
 404/1000 [===========>..................] - ETA: 4:29 - loss: 2.6781 - regression_loss: 2.0665 - classification_loss: 0.6116
 405/1000 [===========>..................] - ETA: 4:29 - loss: 2.6782 - regression_loss: 2.0669 - classification_loss: 0.6114
 406/1000 [===========>..................] - ETA: 4:28 - loss: 2.6716 - regression_loss: 2.0618 - classification_loss: 0.6099
 407/1000 [===========>..................] - ETA: 4:28 - loss: 2.6753 - regression_loss: 2.0655 - classification_loss: 0.6098
 408/1000 [===========>..................] - ETA: 4:27 - loss: 2.6791 - regression_loss: 2.0667 - classification_loss: 0.6124
 409/1000 [===========>..................] - ETA: 4:27 - loss: 2.6808 - regression_loss: 2.0678 - classification_loss: 0.6130
 410/1000 [===========>..................] - ETA: 4:26 - loss: 2.6778 - regression_loss: 2.0658 - classification_loss: 0.6120
 411/1000 [===========>..................] - ETA: 4:26 - loss: 2.6785 - regression_loss: 2.0650 - classification_loss: 0.6135
 412/1000 [===========>..................] - ETA: 4:26 - loss: 2.6816 - regression_loss: 2.0672 - classification_loss: 0.6144
 413/1000 [===========>..................] - ETA: 4:25 - loss: 2.6807 - regression_loss: 2.0663 - classification_loss: 0.6145
 414/1000 [===========>..................] - ETA: 4:25 - loss: 2.6814 - regression_loss: 2.0652 - classification_loss: 0.6162
 415/1000 [===========>..................] - ETA: 4:24 - loss: 2.6749 - regression_loss: 2.0602 - classification_loss: 0.6147
 416/1000 [===========>..................] - ETA: 4:24 - loss: 2.6685 - regression_loss: 2.0552 - classification_loss: 0.6132
 417/1000 [===========>..................] - ETA: 4:23 - loss: 2.6746 - regression_loss: 2.0599 - classification_loss: 0.6148
 418/1000 [===========>..................] - ETA: 4:23 - loss: 2.6682 - regression_loss: 2.0550 - classification_loss: 0.6133
 419/1000 [===========>..................] - ETA: 4:22 - loss: 2.6619 - regression_loss: 2.0500 - classification_loss: 0.6118
 420/1000 [===========>..................] - ETA: 4:22 - loss: 2.6627 - regression_loss: 2.0517 - classification_loss: 0.6110
 421/1000 [===========>..................] - ETA: 4:21 - loss: 2.6651 - regression_loss: 2.0531 - classification_loss: 0.6120
 422/1000 [===========>..................] - ETA: 4:21 - loss: 2.6670 - regression_loss: 2.0537 - classification_loss: 0.6133
 423/1000 [===========>..................] - ETA: 4:21 - loss: 2.6698 - regression_loss: 2.0569 - classification_loss: 0.6129
 424/1000 [===========>..................] - ETA: 4:20 - loss: 2.6635 - regression_loss: 2.0521 - classification_loss: 0.6114
 425/1000 [===========>..................] - ETA: 4:20 - loss: 2.6651 - regression_loss: 2.0527 - classification_loss: 0.6124
 426/1000 [===========>..................] - ETA: 4:19 - loss: 2.6673 - regression_loss: 2.0535 - classification_loss: 0.6137
 427/1000 [===========>..................] - ETA: 4:19 - loss: 2.6690 - regression_loss: 2.0550 - classification_loss: 0.6140
 428/1000 [===========>..................] - ETA: 4:18 - loss: 2.6711 - regression_loss: 2.0577 - classification_loss: 0.6134
 429/1000 [===========>..................] - ETA: 4:18 - loss: 2.6706 - regression_loss: 2.0583 - classification_loss: 0.6124
 430/1000 [===========>..................] - ETA: 4:17 - loss: 2.6732 - regression_loss: 2.0611 - classification_loss: 0.6121
 431/1000 [===========>..................] - ETA: 4:17 - loss: 2.6792 - regression_loss: 2.0638 - classification_loss: 0.6154
 432/1000 [===========>..................] - ETA: 4:17 - loss: 2.6812 - regression_loss: 2.0651 - classification_loss: 0.6161
 433/1000 [===========>..................] - ETA: 4:16 - loss: 2.6822 - regression_loss: 2.0662 - classification_loss: 0.6160
 434/1000 [============>.................] - ETA: 4:16 - loss: 2.6876 - regression_loss: 2.0708 - classification_loss: 0.6168
 435/1000 [============>.................] - ETA: 4:15 - loss: 2.6859 - regression_loss: 2.0699 - classification_loss: 0.6160
 436/1000 [============>.................] - ETA: 4:15 - loss: 2.6851 - regression_loss: 2.0700 - classification_loss: 0.6151
 437/1000 [============>.................] - ETA: 4:14 - loss: 2.6855 - regression_loss: 2.0713 - classification_loss: 0.6142
 438/1000 [============>.................] - ETA: 4:14 - loss: 2.6804 - regression_loss: 2.0665 - classification_loss: 0.6138
 439/1000 [============>.................] - ETA: 4:13 - loss: 2.6794 - regression_loss: 2.0664 - classification_loss: 0.6130
 440/1000 [============>.................] - ETA: 4:13 - loss: 2.6810 - regression_loss: 2.0676 - classification_loss: 0.6134
 441/1000 [============>.................] - ETA: 4:12 - loss: 2.6851 - regression_loss: 2.0695 - classification_loss: 0.6156
 442/1000 [============>.................] - ETA: 4:12 - loss: 2.6790 - regression_loss: 2.0649 - classification_loss: 0.6142
 443/1000 [============>.................] - ETA: 4:12 - loss: 2.6730 - regression_loss: 2.0602 - classification_loss: 0.6128
 444/1000 [============>.................] - ETA: 4:11 - loss: 2.6787 - regression_loss: 2.0636 - classification_loss: 0.6152
 445/1000 [============>.................] - ETA: 4:11 - loss: 2.6727 - regression_loss: 2.0589 - classification_loss: 0.6138
 446/1000 [============>.................] - ETA: 4:10 - loss: 2.6733 - regression_loss: 2.0601 - classification_loss: 0.6132
 447/1000 [============>.................] - ETA: 4:10 - loss: 2.6721 - regression_loss: 2.0586 - classification_loss: 0.6135
 448/1000 [============>.................] - ETA: 4:09 - loss: 2.6733 - regression_loss: 2.0598 - classification_loss: 0.6135
 449/1000 [============>.................] - ETA: 4:09 - loss: 2.6742 - regression_loss: 2.0605 - classification_loss: 0.6137
 450/1000 [============>.................] - ETA: 4:08 - loss: 2.6726 - regression_loss: 2.0600 - classification_loss: 0.6126
 451/1000 [============>.................] - ETA: 4:08 - loss: 2.6717 - regression_loss: 2.0596 - classification_loss: 0.6121
 452/1000 [============>.................] - ETA: 4:07 - loss: 2.6690 - regression_loss: 2.0551 - classification_loss: 0.6140
 453/1000 [============>.................] - ETA: 4:07 - loss: 2.6692 - regression_loss: 2.0559 - classification_loss: 0.6133
 454/1000 [============>.................] - ETA: 4:07 - loss: 2.6712 - regression_loss: 2.0574 - classification_loss: 0.6139
 455/1000 [============>.................] - ETA: 4:06 - loss: 2.6716 - regression_loss: 2.0578 - classification_loss: 0.6138
 456/1000 [============>.................] - ETA: 4:06 - loss: 2.6745 - regression_loss: 2.0598 - classification_loss: 0.6147
 457/1000 [============>.................] - ETA: 4:05 - loss: 2.6729 - regression_loss: 2.0585 - classification_loss: 0.6144
 458/1000 [============>.................] - ETA: 4:05 - loss: 2.6737 - regression_loss: 2.0598 - classification_loss: 0.6140
 459/1000 [============>.................] - ETA: 4:04 - loss: 2.6787 - regression_loss: 2.0625 - classification_loss: 0.6162
 460/1000 [============>.................] - ETA: 4:04 - loss: 2.6871 - regression_loss: 2.0689 - classification_loss: 0.6181
 461/1000 [============>.................] - ETA: 4:03 - loss: 2.6909 - regression_loss: 2.0700 - classification_loss: 0.6209
 462/1000 [============>.................] - ETA: 4:03 - loss: 2.6949 - regression_loss: 2.0720 - classification_loss: 0.6229
 463/1000 [============>.................] - ETA: 4:02 - loss: 2.6933 - regression_loss: 2.0706 - classification_loss: 0.6228
 464/1000 [============>.................] - ETA: 4:02 - loss: 2.6931 - regression_loss: 2.0708 - classification_loss: 0.6223
 465/1000 [============>.................] - ETA: 4:02 - loss: 2.6873 - regression_loss: 2.0663 - classification_loss: 0.6210
 466/1000 [============>.................] - ETA: 4:01 - loss: 2.6899 - regression_loss: 2.0688 - classification_loss: 0.6211
 467/1000 [=============>................] - ETA: 4:01 - loss: 2.6841 - regression_loss: 2.0644 - classification_loss: 0.6198
 468/1000 [=============>................] - ETA: 4:00 - loss: 2.6860 - regression_loss: 2.0656 - classification_loss: 0.6203
 469/1000 [=============>................] - ETA: 4:00 - loss: 2.6803 - regression_loss: 2.0612 - classification_loss: 0.6190
 470/1000 [=============>................] - ETA: 3:59 - loss: 2.6750 - regression_loss: 2.0569 - classification_loss: 0.6182
 471/1000 [=============>................] - ETA: 3:59 - loss: 2.6777 - regression_loss: 2.0598 - classification_loss: 0.6179
 472/1000 [=============>................] - ETA: 3:58 - loss: 2.6794 - regression_loss: 2.0615 - classification_loss: 0.6178
 473/1000 [=============>................] - ETA: 3:58 - loss: 2.6821 - regression_loss: 2.0641 - classification_loss: 0.6180
 474/1000 [=============>................] - ETA: 3:57 - loss: 2.6828 - regression_loss: 2.0642 - classification_loss: 0.6186
 475/1000 [=============>................] - ETA: 3:57 - loss: 2.6791 - regression_loss: 2.0598 - classification_loss: 0.6193
 476/1000 [=============>................] - ETA: 3:57 - loss: 2.6795 - regression_loss: 2.0607 - classification_loss: 0.6188
 477/1000 [=============>................] - ETA: 3:56 - loss: 2.6783 - regression_loss: 2.0597 - classification_loss: 0.6186
 478/1000 [=============>................] - ETA: 3:56 - loss: 2.6830 - regression_loss: 2.0626 - classification_loss: 0.6204
 479/1000 [=============>................] - ETA: 3:55 - loss: 2.6828 - regression_loss: 2.0630 - classification_loss: 0.6198
 480/1000 [=============>................] - ETA: 3:55 - loss: 2.6844 - regression_loss: 2.0642 - classification_loss: 0.6202
 481/1000 [=============>................] - ETA: 3:54 - loss: 2.6906 - regression_loss: 2.0681 - classification_loss: 0.6224
 482/1000 [=============>................] - ETA: 3:54 - loss: 2.6908 - regression_loss: 2.0685 - classification_loss: 0.6222
 483/1000 [=============>................] - ETA: 3:53 - loss: 2.6902 - regression_loss: 2.0684 - classification_loss: 0.6218
 484/1000 [=============>................] - ETA: 3:53 - loss: 2.6846 - regression_loss: 2.0641 - classification_loss: 0.6205
 485/1000 [=============>................] - ETA: 3:53 - loss: 2.6870 - regression_loss: 2.0650 - classification_loss: 0.6220
 486/1000 [=============>................] - ETA: 3:52 - loss: 2.6890 - regression_loss: 2.0657 - classification_loss: 0.6233
 487/1000 [=============>................] - ETA: 3:52 - loss: 2.6835 - regression_loss: 2.0614 - classification_loss: 0.6220
 488/1000 [=============>................] - ETA: 3:51 - loss: 2.6881 - regression_loss: 2.0634 - classification_loss: 0.6247
 489/1000 [=============>................] - ETA: 3:51 - loss: 2.6889 - regression_loss: 2.0634 - classification_loss: 0.6255
 490/1000 [=============>................] - ETA: 3:50 - loss: 2.6882 - regression_loss: 2.0631 - classification_loss: 0.6251
 491/1000 [=============>................] - ETA: 3:50 - loss: 2.6827 - regression_loss: 2.0589 - classification_loss: 0.6238
 492/1000 [=============>................] - ETA: 3:49 - loss: 2.6828 - regression_loss: 2.0596 - classification_loss: 0.6232
 493/1000 [=============>................] - ETA: 3:49 - loss: 2.6836 - regression_loss: 2.0609 - classification_loss: 0.6227
 494/1000 [=============>................] - ETA: 3:48 - loss: 2.6828 - regression_loss: 2.0604 - classification_loss: 0.6225
 495/1000 [=============>................] - ETA: 3:48 - loss: 2.6810 - regression_loss: 2.0594 - classification_loss: 0.6216
 496/1000 [=============>................] - ETA: 3:48 - loss: 2.6819 - regression_loss: 2.0603 - classification_loss: 0.6216
 497/1000 [=============>................] - ETA: 3:47 - loss: 2.6839 - regression_loss: 2.0628 - classification_loss: 0.6211
 498/1000 [=============>................] - ETA: 3:47 - loss: 2.6856 - regression_loss: 2.0651 - classification_loss: 0.6204
 499/1000 [=============>................] - ETA: 3:46 - loss: 2.6847 - regression_loss: 2.0647 - classification_loss: 0.6201
 500/1000 [==============>...............] - ETA: 3:46 - loss: 2.6858 - regression_loss: 2.0660 - classification_loss: 0.6199
 501/1000 [==============>...............] - ETA: 3:45 - loss: 2.6901 - regression_loss: 2.0678 - classification_loss: 0.6223
 502/1000 [==============>...............] - ETA: 3:45 - loss: 2.6884 - regression_loss: 2.0667 - classification_loss: 0.6217
 503/1000 [==============>...............] - ETA: 3:44 - loss: 2.6830 - regression_loss: 2.0626 - classification_loss: 0.6204
 504/1000 [==============>...............] - ETA: 3:44 - loss: 2.6865 - regression_loss: 2.0653 - classification_loss: 0.6212
 505/1000 [==============>...............] - ETA: 3:43 - loss: 2.6878 - regression_loss: 2.0672 - classification_loss: 0.6206
 506/1000 [==============>...............] - ETA: 3:43 - loss: 2.6892 - regression_loss: 2.0677 - classification_loss: 0.6215
 507/1000 [==============>...............] - ETA: 3:43 - loss: 2.6850 - regression_loss: 2.0637 - classification_loss: 0.6213
 508/1000 [==============>...............] - ETA: 3:42 - loss: 2.6813 - regression_loss: 2.0596 - classification_loss: 0.6217
 509/1000 [==============>...............] - ETA: 3:42 - loss: 2.6840 - regression_loss: 2.0610 - classification_loss: 0.6230
 510/1000 [==============>...............] - ETA: 3:41 - loss: 2.6840 - regression_loss: 2.0569 - classification_loss: 0.6270
 511/1000 [==============>...............] - ETA: 3:41 - loss: 2.6878 - regression_loss: 2.0589 - classification_loss: 0.6289
 512/1000 [==============>...............] - ETA: 3:40 - loss: 2.6902 - regression_loss: 2.0615 - classification_loss: 0.6287
 513/1000 [==============>...............] - ETA: 3:40 - loss: 2.6904 - regression_loss: 2.0622 - classification_loss: 0.6282
 514/1000 [==============>...............] - ETA: 3:39 - loss: 2.6943 - regression_loss: 2.0651 - classification_loss: 0.6292
 515/1000 [==============>...............] - ETA: 3:39 - loss: 2.6973 - regression_loss: 2.0674 - classification_loss: 0.6300
 516/1000 [==============>...............] - ETA: 3:38 - loss: 2.6979 - regression_loss: 2.0679 - classification_loss: 0.6300
 517/1000 [==============>...............] - ETA: 3:38 - loss: 2.7025 - regression_loss: 2.0720 - classification_loss: 0.6305
 518/1000 [==============>...............] - ETA: 3:38 - loss: 2.6973 - regression_loss: 2.0680 - classification_loss: 0.6292
 519/1000 [==============>...............] - ETA: 3:37 - loss: 2.6999 - regression_loss: 2.0705 - classification_loss: 0.6293
 520/1000 [==============>...............] - ETA: 3:37 - loss: 2.6947 - regression_loss: 2.0665 - classification_loss: 0.6282
 521/1000 [==============>...............] - ETA: 3:36 - loss: 2.6980 - regression_loss: 2.0697 - classification_loss: 0.6283
 522/1000 [==============>...............] - ETA: 3:36 - loss: 2.7010 - regression_loss: 2.0717 - classification_loss: 0.6293
 523/1000 [==============>...............] - ETA: 3:35 - loss: 2.7036 - regression_loss: 2.0741 - classification_loss: 0.6295
 524/1000 [==============>...............] - ETA: 3:35 - loss: 2.7054 - regression_loss: 2.0755 - classification_loss: 0.6299
 525/1000 [==============>...............] - ETA: 3:34 - loss: 2.7003 - regression_loss: 2.0716 - classification_loss: 0.6288
 526/1000 [==============>...............] - ETA: 3:34 - loss: 2.7018 - regression_loss: 2.0729 - classification_loss: 0.6289
 527/1000 [==============>...............] - ETA: 3:33 - loss: 2.7043 - regression_loss: 2.0754 - classification_loss: 0.6288
 528/1000 [==============>...............] - ETA: 3:33 - loss: 2.7052 - regression_loss: 2.0764 - classification_loss: 0.6288
 529/1000 [==============>...............] - ETA: 3:33 - loss: 2.7081 - regression_loss: 2.0793 - classification_loss: 0.6288
 530/1000 [==============>...............] - ETA: 3:32 - loss: 2.7123 - regression_loss: 2.0833 - classification_loss: 0.6290
 531/1000 [==============>...............] - ETA: 3:32 - loss: 2.7130 - regression_loss: 2.0840 - classification_loss: 0.6290
 532/1000 [==============>...............] - ETA: 3:31 - loss: 2.7148 - regression_loss: 2.0856 - classification_loss: 0.6292
 533/1000 [==============>...............] - ETA: 3:31 - loss: 2.7135 - regression_loss: 2.0845 - classification_loss: 0.6290
 534/1000 [===============>..............] - ETA: 3:30 - loss: 2.7158 - regression_loss: 2.0862 - classification_loss: 0.6296
 535/1000 [===============>..............] - ETA: 3:30 - loss: 2.7176 - regression_loss: 2.0879 - classification_loss: 0.6297
 536/1000 [===============>..............] - ETA: 3:29 - loss: 2.7174 - regression_loss: 2.0877 - classification_loss: 0.6296
 537/1000 [===============>..............] - ETA: 3:29 - loss: 2.7123 - regression_loss: 2.0838 - classification_loss: 0.6285
 538/1000 [===============>..............] - ETA: 3:28 - loss: 2.7121 - regression_loss: 2.0837 - classification_loss: 0.6284
 539/1000 [===============>..............] - ETA: 3:28 - loss: 2.7071 - regression_loss: 2.0799 - classification_loss: 0.6272
 540/1000 [===============>..............] - ETA: 3:28 - loss: 2.7062 - regression_loss: 2.0794 - classification_loss: 0.6268
 541/1000 [===============>..............] - ETA: 3:27 - loss: 2.7053 - regression_loss: 2.0789 - classification_loss: 0.6264
 542/1000 [===============>..............] - ETA: 3:27 - loss: 2.7003 - regression_loss: 2.0751 - classification_loss: 0.6252
 543/1000 [===============>..............] - ETA: 3:26 - loss: 2.7011 - regression_loss: 2.0762 - classification_loss: 0.6249
 544/1000 [===============>..............] - ETA: 3:26 - loss: 2.7029 - regression_loss: 2.0778 - classification_loss: 0.6251
 545/1000 [===============>..............] - ETA: 3:25 - loss: 2.7029 - regression_loss: 2.0780 - classification_loss: 0.6249
 546/1000 [===============>..............] - ETA: 3:25 - loss: 2.7031 - regression_loss: 2.0787 - classification_loss: 0.6244
 547/1000 [===============>..............] - ETA: 3:24 - loss: 2.7050 - regression_loss: 2.0805 - classification_loss: 0.6244
 548/1000 [===============>..............] - ETA: 3:24 - loss: 2.7049 - regression_loss: 2.0808 - classification_loss: 0.6241
 549/1000 [===============>..............] - ETA: 3:24 - loss: 2.7000 - regression_loss: 2.0770 - classification_loss: 0.6230
 550/1000 [===============>..............] - ETA: 3:23 - loss: 2.6982 - regression_loss: 2.0759 - classification_loss: 0.6223
 551/1000 [===============>..............] - ETA: 3:23 - loss: 2.7023 - regression_loss: 2.0798 - classification_loss: 0.6225
 552/1000 [===============>..............] - ETA: 3:22 - loss: 2.7055 - regression_loss: 2.0823 - classification_loss: 0.6232
 553/1000 [===============>..............] - ETA: 3:22 - loss: 2.7076 - regression_loss: 2.0847 - classification_loss: 0.6229
 554/1000 [===============>..............] - ETA: 3:21 - loss: 2.7027 - regression_loss: 2.0810 - classification_loss: 0.6217
 555/1000 [===============>..............] - ETA: 3:21 - loss: 2.6978 - regression_loss: 2.0772 - classification_loss: 0.6206
 556/1000 [===============>..............] - ETA: 3:20 - loss: 2.6994 - regression_loss: 2.0791 - classification_loss: 0.6203
 557/1000 [===============>..............] - ETA: 3:20 - loss: 2.7001 - regression_loss: 2.0800 - classification_loss: 0.6201
 558/1000 [===============>..............] - ETA: 3:19 - loss: 2.6983 - regression_loss: 2.0789 - classification_loss: 0.6194
 559/1000 [===============>..............] - ETA: 3:19 - loss: 2.7035 - regression_loss: 2.0816 - classification_loss: 0.6220
 560/1000 [===============>..............] - ETA: 3:19 - loss: 2.7032 - regression_loss: 2.0816 - classification_loss: 0.6215
 561/1000 [===============>..............] - ETA: 3:18 - loss: 2.7026 - regression_loss: 2.0816 - classification_loss: 0.6211
 562/1000 [===============>..............] - ETA: 3:18 - loss: 2.7029 - regression_loss: 2.0823 - classification_loss: 0.6206
 563/1000 [===============>..............] - ETA: 3:17 - loss: 2.6981 - regression_loss: 2.0786 - classification_loss: 0.6195
 564/1000 [===============>..............] - ETA: 3:17 - loss: 2.6933 - regression_loss: 2.0750 - classification_loss: 0.6184
 565/1000 [===============>..............] - ETA: 3:16 - loss: 2.6886 - regression_loss: 2.0713 - classification_loss: 0.6173
 566/1000 [===============>..............] - ETA: 3:16 - loss: 2.6910 - regression_loss: 2.0738 - classification_loss: 0.6172
 567/1000 [================>.............] - ETA: 3:15 - loss: 2.6945 - regression_loss: 2.0768 - classification_loss: 0.6177
 568/1000 [================>.............] - ETA: 3:15 - loss: 2.6965 - regression_loss: 2.0779 - classification_loss: 0.6186
 569/1000 [================>.............] - ETA: 3:15 - loss: 2.6991 - regression_loss: 2.0777 - classification_loss: 0.6214
 570/1000 [================>.............] - ETA: 3:14 - loss: 2.7028 - regression_loss: 2.0790 - classification_loss: 0.6238
 571/1000 [================>.............] - ETA: 3:14 - loss: 2.7052 - regression_loss: 2.0818 - classification_loss: 0.6234
 572/1000 [================>.............] - ETA: 3:13 - loss: 2.7086 - regression_loss: 2.0843 - classification_loss: 0.6242
 573/1000 [================>.............] - ETA: 3:13 - loss: 2.7143 - regression_loss: 2.0874 - classification_loss: 0.6270
 574/1000 [================>.............] - ETA: 3:12 - loss: 2.7156 - regression_loss: 2.0883 - classification_loss: 0.6273
 575/1000 [================>.............] - ETA: 3:12 - loss: 2.7190 - regression_loss: 2.0901 - classification_loss: 0.6288
 576/1000 [================>.............] - ETA: 3:11 - loss: 2.7179 - regression_loss: 2.0897 - classification_loss: 0.6282
 577/1000 [================>.............] - ETA: 3:11 - loss: 2.7131 - regression_loss: 2.0860 - classification_loss: 0.6271
 578/1000 [================>.............] - ETA: 3:10 - loss: 2.7085 - regression_loss: 2.0824 - classification_loss: 0.6260
 579/1000 [================>.............] - ETA: 3:10 - loss: 2.7092 - regression_loss: 2.0836 - classification_loss: 0.6256
 580/1000 [================>.............] - ETA: 3:10 - loss: 2.7104 - regression_loss: 2.0847 - classification_loss: 0.6256
 581/1000 [================>.............] - ETA: 3:09 - loss: 2.7134 - regression_loss: 2.0867 - classification_loss: 0.6267
 582/1000 [================>.............] - ETA: 3:09 - loss: 2.7173 - regression_loss: 2.0883 - classification_loss: 0.6290
 583/1000 [================>.............] - ETA: 3:08 - loss: 2.7206 - regression_loss: 2.0916 - classification_loss: 0.6290
 584/1000 [================>.............] - ETA: 3:08 - loss: 2.7202 - regression_loss: 2.0913 - classification_loss: 0.6288
 585/1000 [================>.............] - ETA: 3:07 - loss: 2.7202 - regression_loss: 2.0916 - classification_loss: 0.6287
 586/1000 [================>.............] - ETA: 3:07 - loss: 2.7156 - regression_loss: 2.0880 - classification_loss: 0.6276
 587/1000 [================>.............] - ETA: 3:06 - loss: 2.7188 - regression_loss: 2.0904 - classification_loss: 0.6283
 588/1000 [================>.............] - ETA: 3:06 - loss: 2.7205 - regression_loss: 2.0918 - classification_loss: 0.6287
 589/1000 [================>.............] - ETA: 3:05 - loss: 2.7229 - regression_loss: 2.0936 - classification_loss: 0.6293
 590/1000 [================>.............] - ETA: 3:05 - loss: 2.7253 - regression_loss: 2.0961 - classification_loss: 0.6292
 591/1000 [================>.............] - ETA: 3:05 - loss: 2.7267 - regression_loss: 2.0979 - classification_loss: 0.6289
 592/1000 [================>.............] - ETA: 3:04 - loss: 2.7270 - regression_loss: 2.0981 - classification_loss: 0.6289
 593/1000 [================>.............] - ETA: 3:04 - loss: 2.7224 - regression_loss: 2.0946 - classification_loss: 0.6278
 594/1000 [================>.............] - ETA: 3:03 - loss: 2.7215 - regression_loss: 2.0942 - classification_loss: 0.6273
 595/1000 [================>.............] - ETA: 3:03 - loss: 2.7169 - regression_loss: 2.0907 - classification_loss: 0.6263
 596/1000 [================>.............] - ETA: 3:02 - loss: 2.7124 - regression_loss: 2.0872 - classification_loss: 0.6252
 597/1000 [================>.............] - ETA: 3:02 - loss: 2.7152 - regression_loss: 2.0903 - classification_loss: 0.6249
 598/1000 [================>.............] - ETA: 3:01 - loss: 2.7169 - regression_loss: 2.0923 - classification_loss: 0.6246
 599/1000 [================>.............] - ETA: 3:01 - loss: 2.7169 - regression_loss: 2.0925 - classification_loss: 0.6243
 600/1000 [=================>............] - ETA: 3:00 - loss: 2.7204 - regression_loss: 2.0955 - classification_loss: 0.6249
 601/1000 [=================>............] - ETA: 3:00 - loss: 2.7198 - regression_loss: 2.0953 - classification_loss: 0.6244
 602/1000 [=================>............] - ETA: 3:00 - loss: 2.7215 - regression_loss: 2.0972 - classification_loss: 0.6243
 603/1000 [=================>............] - ETA: 2:59 - loss: 2.7222 - regression_loss: 2.0980 - classification_loss: 0.6242
 604/1000 [=================>............] - ETA: 2:59 - loss: 2.7246 - regression_loss: 2.1000 - classification_loss: 0.6246
 605/1000 [=================>............] - ETA: 2:58 - loss: 2.7237 - regression_loss: 2.0994 - classification_loss: 0.6243
 606/1000 [=================>............] - ETA: 2:58 - loss: 2.7245 - regression_loss: 2.1008 - classification_loss: 0.6237
 607/1000 [=================>............] - ETA: 2:57 - loss: 2.7257 - regression_loss: 2.1021 - classification_loss: 0.6236
 608/1000 [=================>............] - ETA: 2:57 - loss: 2.7274 - regression_loss: 2.1037 - classification_loss: 0.6236
 609/1000 [=================>............] - ETA: 2:56 - loss: 2.7273 - regression_loss: 2.1041 - classification_loss: 0.6231
 610/1000 [=================>............] - ETA: 2:56 - loss: 2.7269 - regression_loss: 2.1043 - classification_loss: 0.6226
 611/1000 [=================>............] - ETA: 2:56 - loss: 2.7307 - regression_loss: 2.1065 - classification_loss: 0.6242
 612/1000 [=================>............] - ETA: 2:55 - loss: 2.7320 - regression_loss: 2.1077 - classification_loss: 0.6244
 613/1000 [=================>............] - ETA: 2:55 - loss: 2.7346 - regression_loss: 2.1098 - classification_loss: 0.6249
 614/1000 [=================>............] - ETA: 2:54 - loss: 2.7350 - regression_loss: 2.1108 - classification_loss: 0.6242
 615/1000 [=================>............] - ETA: 2:54 - loss: 2.7363 - regression_loss: 2.1123 - classification_loss: 0.6240
 616/1000 [=================>............] - ETA: 2:53 - loss: 2.7353 - regression_loss: 2.1119 - classification_loss: 0.6234
 617/1000 [=================>............] - ETA: 2:53 - loss: 2.7309 - regression_loss: 2.1085 - classification_loss: 0.6224
 618/1000 [=================>............] - ETA: 2:52 - loss: 2.7356 - regression_loss: 2.1109 - classification_loss: 0.6247
 619/1000 [=================>............] - ETA: 2:52 - loss: 2.7350 - regression_loss: 2.1107 - classification_loss: 0.6243
 620/1000 [=================>............] - ETA: 2:51 - loss: 2.7356 - regression_loss: 2.1114 - classification_loss: 0.6242
 621/1000 [=================>............] - ETA: 2:51 - loss: 2.7343 - regression_loss: 2.1105 - classification_loss: 0.6237
 622/1000 [=================>............] - ETA: 2:51 - loss: 2.7355 - regression_loss: 2.1115 - classification_loss: 0.6240
 623/1000 [=================>............] - ETA: 2:50 - loss: 2.7311 - regression_loss: 2.1081 - classification_loss: 0.6230
 624/1000 [=================>............] - ETA: 2:50 - loss: 2.7320 - regression_loss: 2.1088 - classification_loss: 0.6231
 625/1000 [=================>............] - ETA: 2:49 - loss: 2.7325 - regression_loss: 2.1097 - classification_loss: 0.6229
 626/1000 [=================>............] - ETA: 2:49 - loss: 2.7328 - regression_loss: 2.1105 - classification_loss: 0.6222
 627/1000 [=================>............] - ETA: 2:48 - loss: 2.7328 - regression_loss: 2.1108 - classification_loss: 0.6220
 628/1000 [=================>............] - ETA: 2:48 - loss: 2.7324 - regression_loss: 2.1106 - classification_loss: 0.6218
 629/1000 [=================>............] - ETA: 2:47 - loss: 2.7332 - regression_loss: 2.1117 - classification_loss: 0.6216
 630/1000 [=================>............] - ETA: 2:47 - loss: 2.7337 - regression_loss: 2.1119 - classification_loss: 0.6217
 631/1000 [=================>............] - ETA: 2:47 - loss: 2.7294 - regression_loss: 2.1086 - classification_loss: 0.6208
 632/1000 [=================>............] - ETA: 2:46 - loss: 2.7301 - regression_loss: 2.1097 - classification_loss: 0.6205
 633/1000 [=================>............] - ETA: 2:46 - loss: 2.7307 - regression_loss: 2.1107 - classification_loss: 0.6200
 634/1000 [==================>...........] - ETA: 2:45 - loss: 2.7311 - regression_loss: 2.1113 - classification_loss: 0.6197
 635/1000 [==================>...........] - ETA: 2:45 - loss: 2.7306 - regression_loss: 2.1110 - classification_loss: 0.6197
 636/1000 [==================>...........] - ETA: 2:44 - loss: 2.7306 - regression_loss: 2.1111 - classification_loss: 0.6196
 637/1000 [==================>...........] - ETA: 2:44 - loss: 2.7293 - regression_loss: 2.1102 - classification_loss: 0.6191
 638/1000 [==================>...........] - ETA: 2:43 - loss: 2.7251 - regression_loss: 2.1069 - classification_loss: 0.6182
 639/1000 [==================>...........] - ETA: 2:43 - loss: 2.7243 - regression_loss: 2.1064 - classification_loss: 0.6179
 640/1000 [==================>...........] - ETA: 2:42 - loss: 2.7246 - regression_loss: 2.1070 - classification_loss: 0.6176
 641/1000 [==================>...........] - ETA: 2:42 - loss: 2.7262 - regression_loss: 2.1089 - classification_loss: 0.6173
 642/1000 [==================>...........] - ETA: 2:42 - loss: 2.7256 - regression_loss: 2.1089 - classification_loss: 0.6166
 643/1000 [==================>...........] - ETA: 2:41 - loss: 2.7252 - regression_loss: 2.1089 - classification_loss: 0.6163
 644/1000 [==================>...........] - ETA: 2:41 - loss: 2.7242 - regression_loss: 2.1085 - classification_loss: 0.6157
 645/1000 [==================>...........] - ETA: 2:40 - loss: 2.7200 - regression_loss: 2.1053 - classification_loss: 0.6148
 646/1000 [==================>...........] - ETA: 2:40 - loss: 2.7221 - regression_loss: 2.1073 - classification_loss: 0.6149
 647/1000 [==================>...........] - ETA: 2:39 - loss: 2.7232 - regression_loss: 2.1079 - classification_loss: 0.6153
 648/1000 [==================>...........] - ETA: 2:39 - loss: 2.7190 - regression_loss: 2.1047 - classification_loss: 0.6144
 649/1000 [==================>...........] - ETA: 2:38 - loss: 2.7189 - regression_loss: 2.1048 - classification_loss: 0.6141
 650/1000 [==================>...........] - ETA: 2:38 - loss: 2.7197 - regression_loss: 2.1058 - classification_loss: 0.6139
 651/1000 [==================>...........] - ETA: 2:37 - loss: 2.7155 - regression_loss: 2.1026 - classification_loss: 0.6129
 652/1000 [==================>...........] - ETA: 2:37 - loss: 2.7115 - regression_loss: 2.0993 - classification_loss: 0.6122
 653/1000 [==================>...........] - ETA: 2:37 - loss: 2.7125 - regression_loss: 2.1004 - classification_loss: 0.6121
 654/1000 [==================>...........] - ETA: 2:36 - loss: 2.7152 - regression_loss: 2.1028 - classification_loss: 0.6125
 655/1000 [==================>...........] - ETA: 2:36 - loss: 2.7148 - regression_loss: 2.1028 - classification_loss: 0.6121
 656/1000 [==================>...........] - ETA: 2:35 - loss: 2.7169 - regression_loss: 2.1043 - classification_loss: 0.6125
 657/1000 [==================>...........] - ETA: 2:35 - loss: 2.7190 - regression_loss: 2.1054 - classification_loss: 0.6136
 658/1000 [==================>...........] - ETA: 2:34 - loss: 2.7149 - regression_loss: 2.1022 - classification_loss: 0.6127
 659/1000 [==================>...........] - ETA: 2:34 - loss: 2.7176 - regression_loss: 2.1030 - classification_loss: 0.6145
 660/1000 [==================>...........] - ETA: 2:33 - loss: 2.7196 - regression_loss: 2.1044 - classification_loss: 0.6152
 661/1000 [==================>...........] - ETA: 2:33 - loss: 2.7196 - regression_loss: 2.1046 - classification_loss: 0.6150
 662/1000 [==================>...........] - ETA: 2:32 - loss: 2.7204 - regression_loss: 2.1056 - classification_loss: 0.6148
 663/1000 [==================>...........] - ETA: 2:32 - loss: 2.7241 - regression_loss: 2.1072 - classification_loss: 0.6169
 664/1000 [==================>...........] - ETA: 2:32 - loss: 2.7258 - regression_loss: 2.1088 - classification_loss: 0.6170
 665/1000 [==================>...........] - ETA: 2:31 - loss: 2.7255 - regression_loss: 2.1087 - classification_loss: 0.6168
 666/1000 [==================>...........] - ETA: 2:31 - loss: 2.7288 - regression_loss: 2.1107 - classification_loss: 0.6181
 667/1000 [===================>..........] - ETA: 2:30 - loss: 2.7247 - regression_loss: 2.1075 - classification_loss: 0.6172
 668/1000 [===================>..........] - ETA: 2:30 - loss: 2.7207 - regression_loss: 2.1044 - classification_loss: 0.6163
 669/1000 [===================>..........] - ETA: 2:29 - loss: 2.7166 - regression_loss: 2.1012 - classification_loss: 0.6154
 670/1000 [===================>..........] - ETA: 2:29 - loss: 2.7177 - regression_loss: 2.1018 - classification_loss: 0.6159
 671/1000 [===================>..........] - ETA: 2:28 - loss: 2.7136 - regression_loss: 2.0987 - classification_loss: 0.6149
 672/1000 [===================>..........] - ETA: 2:28 - loss: 2.7152 - regression_loss: 2.1000 - classification_loss: 0.6152
 673/1000 [===================>..........] - ETA: 2:27 - loss: 2.7178 - regression_loss: 2.1025 - classification_loss: 0.6153
 674/1000 [===================>..........] - ETA: 2:27 - loss: 2.7218 - regression_loss: 2.1052 - classification_loss: 0.6166
 675/1000 [===================>..........] - ETA: 2:27 - loss: 2.7232 - regression_loss: 2.1063 - classification_loss: 0.6168
 676/1000 [===================>..........] - ETA: 2:26 - loss: 2.7271 - regression_loss: 2.1084 - classification_loss: 0.6186
 677/1000 [===================>..........] - ETA: 2:26 - loss: 2.7230 - regression_loss: 2.1053 - classification_loss: 0.6177
 678/1000 [===================>..........] - ETA: 2:25 - loss: 2.7253 - regression_loss: 2.1072 - classification_loss: 0.6181
 679/1000 [===================>..........] - ETA: 2:25 - loss: 2.7258 - regression_loss: 2.1073 - classification_loss: 0.6185
 680/1000 [===================>..........] - ETA: 2:24 - loss: 2.7223 - regression_loss: 2.1042 - classification_loss: 0.6181
 681/1000 [===================>..........] - ETA: 2:24 - loss: 2.7213 - regression_loss: 2.1037 - classification_loss: 0.6176
 682/1000 [===================>..........] - ETA: 2:23 - loss: 2.7173 - regression_loss: 2.1006 - classification_loss: 0.6167
 683/1000 [===================>..........] - ETA: 2:23 - loss: 2.7161 - regression_loss: 2.0997 - classification_loss: 0.6164
 684/1000 [===================>..........] - ETA: 2:23 - loss: 2.7121 - regression_loss: 2.0966 - classification_loss: 0.6155
 685/1000 [===================>..........] - ETA: 2:22 - loss: 2.7082 - regression_loss: 2.0936 - classification_loss: 0.6146
 686/1000 [===================>..........] - ETA: 2:22 - loss: 2.7090 - regression_loss: 2.0943 - classification_loss: 0.6147
 687/1000 [===================>..........] - ETA: 2:21 - loss: 2.7098 - regression_loss: 2.0950 - classification_loss: 0.6147
 688/1000 [===================>..........] - ETA: 2:21 - loss: 2.7058 - regression_loss: 2.0920 - classification_loss: 0.6138
 689/1000 [===================>..........] - ETA: 2:20 - loss: 2.7019 - regression_loss: 2.0890 - classification_loss: 0.6129
 690/1000 [===================>..........] - ETA: 2:20 - loss: 2.7052 - regression_loss: 2.0909 - classification_loss: 0.6143
 691/1000 [===================>..........] - ETA: 2:19 - loss: 2.7061 - regression_loss: 2.0902 - classification_loss: 0.6159
 692/1000 [===================>..........] - ETA: 2:19 - loss: 2.7022 - regression_loss: 2.0872 - classification_loss: 0.6150
 693/1000 [===================>..........] - ETA: 2:18 - loss: 2.7041 - regression_loss: 2.0887 - classification_loss: 0.6154
 694/1000 [===================>..........] - ETA: 2:18 - loss: 2.7056 - regression_loss: 2.0901 - classification_loss: 0.6155
 695/1000 [===================>..........] - ETA: 2:18 - loss: 2.7018 - regression_loss: 2.0871 - classification_loss: 0.6147
 696/1000 [===================>..........] - ETA: 2:17 - loss: 2.6979 - regression_loss: 2.0841 - classification_loss: 0.6138
 697/1000 [===================>..........] - ETA: 2:17 - loss: 2.7003 - regression_loss: 2.0865 - classification_loss: 0.6138
 698/1000 [===================>..........] - ETA: 2:16 - loss: 2.7020 - regression_loss: 2.0883 - classification_loss: 0.6137
 699/1000 [===================>..........] - ETA: 2:16 - loss: 2.6981 - regression_loss: 2.0853 - classification_loss: 0.6128
 700/1000 [====================>.........] - ETA: 2:15 - loss: 2.7000 - regression_loss: 2.0869 - classification_loss: 0.6131
 701/1000 [====================>.........] - ETA: 2:15 - loss: 2.7018 - regression_loss: 2.0885 - classification_loss: 0.6133
 702/1000 [====================>.........] - ETA: 2:14 - loss: 2.7046 - regression_loss: 2.0907 - classification_loss: 0.6139
 703/1000 [====================>.........] - ETA: 2:14 - loss: 2.7093 - regression_loss: 2.0946 - classification_loss: 0.6147
 704/1000 [====================>.........] - ETA: 2:13 - loss: 2.7131 - regression_loss: 2.0968 - classification_loss: 0.6163
 705/1000 [====================>.........] - ETA: 2:13 - loss: 2.7145 - regression_loss: 2.0977 - classification_loss: 0.6168
 706/1000 [====================>.........] - ETA: 2:13 - loss: 2.7150 - regression_loss: 2.0983 - classification_loss: 0.6167
 707/1000 [====================>.........] - ETA: 2:12 - loss: 2.7156 - regression_loss: 2.0985 - classification_loss: 0.6171
 708/1000 [====================>.........] - ETA: 2:12 - loss: 2.7118 - regression_loss: 2.0956 - classification_loss: 0.6162
 709/1000 [====================>.........] - ETA: 2:11 - loss: 2.7120 - regression_loss: 2.0954 - classification_loss: 0.6166
 710/1000 [====================>.........] - ETA: 2:11 - loss: 2.7110 - regression_loss: 2.0948 - classification_loss: 0.6162
 711/1000 [====================>.........] - ETA: 2:10 - loss: 2.7072 - regression_loss: 2.0918 - classification_loss: 0.6153
 712/1000 [====================>.........] - ETA: 2:10 - loss: 2.7109 - regression_loss: 2.0942 - classification_loss: 0.6167
 713/1000 [====================>.........] - ETA: 2:09 - loss: 2.7146 - regression_loss: 2.0962 - classification_loss: 0.6184
 714/1000 [====================>.........] - ETA: 2:09 - loss: 2.7176 - regression_loss: 2.0978 - classification_loss: 0.6198
 715/1000 [====================>.........] - ETA: 2:08 - loss: 2.7210 - regression_loss: 2.1007 - classification_loss: 0.6203
 716/1000 [====================>.........] - ETA: 2:08 - loss: 2.7230 - regression_loss: 2.1021 - classification_loss: 0.6209
 717/1000 [====================>.........] - ETA: 2:08 - loss: 2.7246 - regression_loss: 2.1024 - classification_loss: 0.6222
 718/1000 [====================>.........] - ETA: 2:07 - loss: 2.7271 - regression_loss: 2.1041 - classification_loss: 0.6230
 719/1000 [====================>.........] - ETA: 2:07 - loss: 2.7272 - regression_loss: 2.1047 - classification_loss: 0.6225
 720/1000 [====================>.........] - ETA: 2:06 - loss: 2.7279 - regression_loss: 2.1057 - classification_loss: 0.6222
 721/1000 [====================>.........] - ETA: 2:06 - loss: 2.7287 - regression_loss: 2.1063 - classification_loss: 0.6225
 722/1000 [====================>.........] - ETA: 2:05 - loss: 2.7287 - regression_loss: 2.1063 - classification_loss: 0.6224
 723/1000 [====================>.........] - ETA: 2:05 - loss: 2.7268 - regression_loss: 2.1050 - classification_loss: 0.6218
 724/1000 [====================>.........] - ETA: 2:04 - loss: 2.7230 - regression_loss: 2.1021 - classification_loss: 0.6209
 725/1000 [====================>.........] - ETA: 2:04 - loss: 2.7226 - regression_loss: 2.1020 - classification_loss: 0.6206
 726/1000 [====================>.........] - ETA: 2:04 - loss: 2.7242 - regression_loss: 2.1026 - classification_loss: 0.6216
 727/1000 [====================>.........] - ETA: 2:03 - loss: 2.7204 - regression_loss: 2.0997 - classification_loss: 0.6208
 728/1000 [====================>.........] - ETA: 2:03 - loss: 2.7217 - regression_loss: 2.1009 - classification_loss: 0.6208
 729/1000 [====================>.........] - ETA: 2:02 - loss: 2.7228 - regression_loss: 2.1019 - classification_loss: 0.6209
 730/1000 [====================>.........] - ETA: 2:02 - loss: 2.7243 - regression_loss: 2.1034 - classification_loss: 0.6210
 731/1000 [====================>.........] - ETA: 2:01 - loss: 2.7243 - regression_loss: 2.1037 - classification_loss: 0.6206
 732/1000 [====================>.........] - ETA: 2:01 - loss: 2.7248 - regression_loss: 2.1038 - classification_loss: 0.6209
 733/1000 [====================>.........] - ETA: 2:00 - loss: 2.7274 - regression_loss: 2.1061 - classification_loss: 0.6212
 734/1000 [=====================>........] - ETA: 2:00 - loss: 2.7236 - regression_loss: 2.1033 - classification_loss: 0.6204
 735/1000 [=====================>........] - ETA: 1:59 - loss: 2.7260 - regression_loss: 2.1050 - classification_loss: 0.6211
 736/1000 [=====================>........] - ETA: 1:59 - loss: 2.7271 - regression_loss: 2.1058 - classification_loss: 0.6213
 737/1000 [=====================>........] - ETA: 1:59 - loss: 2.7234 - regression_loss: 2.1030 - classification_loss: 0.6205
 738/1000 [=====================>........] - ETA: 1:58 - loss: 2.7197 - regression_loss: 2.1001 - classification_loss: 0.6196
 739/1000 [=====================>........] - ETA: 1:58 - loss: 2.7199 - regression_loss: 2.1005 - classification_loss: 0.6194
 740/1000 [=====================>........] - ETA: 1:57 - loss: 2.7187 - regression_loss: 2.0995 - classification_loss: 0.6192
 741/1000 [=====================>........] - ETA: 1:57 - loss: 2.7221 - regression_loss: 2.1022 - classification_loss: 0.6198
 742/1000 [=====================>........] - ETA: 1:56 - loss: 2.7184 - regression_loss: 2.0994 - classification_loss: 0.6190
 743/1000 [=====================>........] - ETA: 1:56 - loss: 2.7211 - regression_loss: 2.1012 - classification_loss: 0.6199
 744/1000 [=====================>........] - ETA: 1:55 - loss: 2.7228 - regression_loss: 2.1030 - classification_loss: 0.6197
 745/1000 [=====================>........] - ETA: 1:55 - loss: 2.7222 - regression_loss: 2.1027 - classification_loss: 0.6194
 746/1000 [=====================>........] - ETA: 1:54 - loss: 2.7185 - regression_loss: 2.0999 - classification_loss: 0.6186
 747/1000 [=====================>........] - ETA: 1:54 - loss: 2.7166 - regression_loss: 2.0971 - classification_loss: 0.6195
 748/1000 [=====================>........] - ETA: 1:54 - loss: 2.7169 - regression_loss: 2.0977 - classification_loss: 0.6192
 749/1000 [=====================>........] - ETA: 1:53 - loss: 2.7133 - regression_loss: 2.0949 - classification_loss: 0.6184
 750/1000 [=====================>........] - ETA: 1:53 - loss: 2.7136 - regression_loss: 2.0947 - classification_loss: 0.6189
 751/1000 [=====================>........] - ETA: 1:52 - loss: 2.7100 - regression_loss: 2.0919 - classification_loss: 0.6181
 752/1000 [=====================>........] - ETA: 1:52 - loss: 2.7064 - regression_loss: 2.0892 - classification_loss: 0.6173
 753/1000 [=====================>........] - ETA: 1:51 - loss: 2.7053 - regression_loss: 2.0885 - classification_loss: 0.6168
 754/1000 [=====================>........] - ETA: 1:51 - loss: 2.7057 - regression_loss: 2.0887 - classification_loss: 0.6170
 755/1000 [=====================>........] - ETA: 1:50 - loss: 2.7021 - regression_loss: 2.0859 - classification_loss: 0.6161
 756/1000 [=====================>........] - ETA: 1:50 - loss: 2.7061 - regression_loss: 2.0884 - classification_loss: 0.6178
 757/1000 [=====================>........] - ETA: 1:49 - loss: 2.7052 - regression_loss: 2.0877 - classification_loss: 0.6176
 758/1000 [=====================>........] - ETA: 1:49 - loss: 2.7089 - regression_loss: 2.0902 - classification_loss: 0.6187
 759/1000 [=====================>........] - ETA: 1:49 - loss: 2.7056 - regression_loss: 2.0875 - classification_loss: 0.6181
 760/1000 [=====================>........] - ETA: 1:48 - loss: 2.7020 - regression_loss: 2.0847 - classification_loss: 0.6172
 761/1000 [=====================>........] - ETA: 1:48 - loss: 2.7026 - regression_loss: 2.0852 - classification_loss: 0.6174
 762/1000 [=====================>........] - ETA: 1:47 - loss: 2.7053 - regression_loss: 2.0873 - classification_loss: 0.6180
 763/1000 [=====================>........] - ETA: 1:47 - loss: 2.7071 - regression_loss: 2.0888 - classification_loss: 0.6183
 764/1000 [=====================>........] - ETA: 1:46 - loss: 2.7087 - regression_loss: 2.0896 - classification_loss: 0.6190
 765/1000 [=====================>........] - ETA: 1:46 - loss: 2.7117 - regression_loss: 2.0911 - classification_loss: 0.6205
 766/1000 [=====================>........] - ETA: 1:45 - loss: 2.7082 - regression_loss: 2.0884 - classification_loss: 0.6197
 767/1000 [======================>.......] - ETA: 1:45 - loss: 2.7046 - regression_loss: 2.0857 - classification_loss: 0.6189
 768/1000 [======================>.......] - ETA: 1:45 - loss: 2.7049 - regression_loss: 2.0856 - classification_loss: 0.6193
 769/1000 [======================>.......] - ETA: 1:44 - loss: 2.7013 - regression_loss: 2.0828 - classification_loss: 0.6185
 770/1000 [======================>.......] - ETA: 1:44 - loss: 2.6978 - regression_loss: 2.0801 - classification_loss: 0.6177
 771/1000 [======================>.......] - ETA: 1:43 - loss: 2.6943 - regression_loss: 2.0774 - classification_loss: 0.6169
 772/1000 [======================>.......] - ETA: 1:43 - loss: 2.6997 - regression_loss: 2.0798 - classification_loss: 0.6199
 773/1000 [======================>.......] - ETA: 1:42 - loss: 2.7012 - regression_loss: 2.0810 - classification_loss: 0.6201
 774/1000 [======================>.......] - ETA: 1:42 - loss: 2.7037 - regression_loss: 2.0822 - classification_loss: 0.6215
 775/1000 [======================>.......] - ETA: 1:41 - loss: 2.7033 - regression_loss: 2.0819 - classification_loss: 0.6214
 776/1000 [======================>.......] - ETA: 1:41 - loss: 2.7046 - regression_loss: 2.0818 - classification_loss: 0.6228
 777/1000 [======================>.......] - ETA: 1:40 - loss: 2.7034 - regression_loss: 2.0809 - classification_loss: 0.6224
 778/1000 [======================>.......] - ETA: 1:40 - loss: 2.7063 - regression_loss: 2.0825 - classification_loss: 0.6238
 779/1000 [======================>.......] - ETA: 1:40 - loss: 2.7062 - regression_loss: 2.0822 - classification_loss: 0.6240
 780/1000 [======================>.......] - ETA: 1:39 - loss: 2.7085 - regression_loss: 2.0827 - classification_loss: 0.6257
 781/1000 [======================>.......] - ETA: 1:39 - loss: 2.7098 - regression_loss: 2.0829 - classification_loss: 0.6269
 782/1000 [======================>.......] - ETA: 1:38 - loss: 2.7127 - regression_loss: 2.0844 - classification_loss: 0.6283
 783/1000 [======================>.......] - ETA: 1:38 - loss: 2.7128 - regression_loss: 2.0839 - classification_loss: 0.6289
 784/1000 [======================>.......] - ETA: 1:37 - loss: 2.7127 - regression_loss: 2.0839 - classification_loss: 0.6287
 785/1000 [======================>.......] - ETA: 1:37 - loss: 2.7150 - regression_loss: 2.0849 - classification_loss: 0.6301
 786/1000 [======================>.......] - ETA: 1:36 - loss: 2.7148 - regression_loss: 2.0848 - classification_loss: 0.6300
 787/1000 [======================>.......] - ETA: 1:36 - loss: 2.7160 - regression_loss: 2.0858 - classification_loss: 0.6302
 788/1000 [======================>.......] - ETA: 1:35 - loss: 2.7167 - regression_loss: 2.0857 - classification_loss: 0.6310
 789/1000 [======================>.......] - ETA: 1:35 - loss: 2.7133 - regression_loss: 2.0831 - classification_loss: 0.6302
 790/1000 [======================>.......] - ETA: 1:35 - loss: 2.7137 - regression_loss: 2.0835 - classification_loss: 0.6301
 791/1000 [======================>.......] - ETA: 1:34 - loss: 2.7147 - regression_loss: 2.0841 - classification_loss: 0.6306
 792/1000 [======================>.......] - ETA: 1:34 - loss: 2.7113 - regression_loss: 2.0814 - classification_loss: 0.6298
 793/1000 [======================>.......] - ETA: 1:33 - loss: 2.7119 - regression_loss: 2.0818 - classification_loss: 0.6301
 794/1000 [======================>.......] - ETA: 1:33 - loss: 2.7122 - regression_loss: 2.0820 - classification_loss: 0.6302
 795/1000 [======================>.......] - ETA: 1:32 - loss: 2.7129 - regression_loss: 2.0830 - classification_loss: 0.6299
 796/1000 [======================>.......] - ETA: 1:32 - loss: 2.7126 - regression_loss: 2.0827 - classification_loss: 0.6299
 797/1000 [======================>.......] - ETA: 1:31 - loss: 2.7093 - regression_loss: 2.0801 - classification_loss: 0.6292
 798/1000 [======================>.......] - ETA: 1:31 - loss: 2.7127 - regression_loss: 2.0821 - classification_loss: 0.6306
 799/1000 [======================>.......] - ETA: 1:30 - loss: 2.7120 - regression_loss: 2.0794 - classification_loss: 0.6326
 800/1000 [=======================>......] - ETA: 1:30 - loss: 2.7122 - regression_loss: 2.0799 - classification_loss: 0.6323
 801/1000 [=======================>......] - ETA: 1:30 - loss: 2.7150 - regression_loss: 2.0820 - classification_loss: 0.6329
 802/1000 [=======================>......] - ETA: 1:29 - loss: 2.7116 - regression_loss: 2.0794 - classification_loss: 0.6322
 803/1000 [=======================>......] - ETA: 1:29 - loss: 2.7117 - regression_loss: 2.0794 - classification_loss: 0.6323
 804/1000 [=======================>......] - ETA: 1:28 - loss: 2.7122 - regression_loss: 2.0797 - classification_loss: 0.6325
 805/1000 [=======================>......] - ETA: 1:28 - loss: 2.7136 - regression_loss: 2.0806 - classification_loss: 0.6329
 806/1000 [=======================>......] - ETA: 1:27 - loss: 2.7140 - regression_loss: 2.0810 - classification_loss: 0.6330
 807/1000 [=======================>......] - ETA: 1:27 - loss: 2.7137 - regression_loss: 2.0807 - classification_loss: 0.6330
 808/1000 [=======================>......] - ETA: 1:26 - loss: 2.7165 - regression_loss: 2.0822 - classification_loss: 0.6342
 809/1000 [=======================>......] - ETA: 1:26 - loss: 2.7152 - regression_loss: 2.0815 - classification_loss: 0.6337
 810/1000 [=======================>......] - ETA: 1:25 - loss: 2.7158 - regression_loss: 2.0820 - classification_loss: 0.6338
 811/1000 [=======================>......] - ETA: 1:25 - loss: 2.7166 - regression_loss: 2.0828 - classification_loss: 0.6338
 812/1000 [=======================>......] - ETA: 1:25 - loss: 2.7179 - regression_loss: 2.0839 - classification_loss: 0.6340
 813/1000 [=======================>......] - ETA: 1:24 - loss: 2.7180 - regression_loss: 2.0846 - classification_loss: 0.6334
 814/1000 [=======================>......] - ETA: 1:24 - loss: 2.7181 - regression_loss: 2.0849 - classification_loss: 0.6332
 815/1000 [=======================>......] - ETA: 1:23 - loss: 2.7180 - regression_loss: 2.0848 - classification_loss: 0.6331
 816/1000 [=======================>......] - ETA: 1:23 - loss: 2.7179 - regression_loss: 2.0849 - classification_loss: 0.6330
 817/1000 [=======================>......] - ETA: 1:22 - loss: 2.7147 - regression_loss: 2.0824 - classification_loss: 0.6323
 818/1000 [=======================>......] - ETA: 1:22 - loss: 2.7183 - regression_loss: 2.0835 - classification_loss: 0.6348
 819/1000 [=======================>......] - ETA: 1:21 - loss: 2.7192 - regression_loss: 2.0846 - classification_loss: 0.6347
 820/1000 [=======================>......] - ETA: 1:21 - loss: 2.7175 - regression_loss: 2.0833 - classification_loss: 0.6342
 821/1000 [=======================>......] - ETA: 1:21 - loss: 2.7169 - regression_loss: 2.0830 - classification_loss: 0.6338
 822/1000 [=======================>......] - ETA: 1:20 - loss: 2.7163 - regression_loss: 2.0827 - classification_loss: 0.6336
 823/1000 [=======================>......] - ETA: 1:20 - loss: 2.7158 - regression_loss: 2.0826 - classification_loss: 0.6332
 824/1000 [=======================>......] - ETA: 1:19 - loss: 2.7157 - regression_loss: 2.0829 - classification_loss: 0.6329
 825/1000 [=======================>......] - ETA: 1:19 - loss: 2.7169 - regression_loss: 2.0837 - classification_loss: 0.6332
 826/1000 [=======================>......] - ETA: 1:18 - loss: 2.7136 - regression_loss: 2.0811 - classification_loss: 0.6325
 827/1000 [=======================>......] - ETA: 1:18 - loss: 2.7107 - regression_loss: 2.0786 - classification_loss: 0.6321
 828/1000 [=======================>......] - ETA: 1:17 - loss: 2.7136 - regression_loss: 2.0811 - classification_loss: 0.6324
 829/1000 [=======================>......] - ETA: 1:17 - loss: 2.7106 - regression_loss: 2.0786 - classification_loss: 0.6320
 830/1000 [=======================>......] - ETA: 1:16 - loss: 2.7073 - regression_loss: 2.0761 - classification_loss: 0.6312
 831/1000 [=======================>......] - ETA: 1:16 - loss: 2.7085 - regression_loss: 2.0771 - classification_loss: 0.6314
 832/1000 [=======================>......] - ETA: 1:16 - loss: 2.7081 - regression_loss: 2.0770 - classification_loss: 0.6311
 833/1000 [=======================>......] - ETA: 1:15 - loss: 2.7080 - regression_loss: 2.0770 - classification_loss: 0.6311
 834/1000 [========================>.....] - ETA: 1:15 - loss: 2.7112 - regression_loss: 2.0794 - classification_loss: 0.6318
 835/1000 [========================>.....] - ETA: 1:14 - loss: 2.7111 - regression_loss: 2.0796 - classification_loss: 0.6314
 836/1000 [========================>.....] - ETA: 1:14 - loss: 2.7108 - regression_loss: 2.0795 - classification_loss: 0.6314
 837/1000 [========================>.....] - ETA: 1:13 - loss: 2.7110 - regression_loss: 2.0798 - classification_loss: 0.6311
 838/1000 [========================>.....] - ETA: 1:13 - loss: 2.7126 - regression_loss: 2.0809 - classification_loss: 0.6317
 839/1000 [========================>.....] - ETA: 1:12 - loss: 2.7095 - regression_loss: 2.0784 - classification_loss: 0.6310
 840/1000 [========================>.....] - ETA: 1:12 - loss: 2.7088 - regression_loss: 2.0779 - classification_loss: 0.6310
 841/1000 [========================>.....] - ETA: 1:11 - loss: 2.7095 - regression_loss: 2.0786 - classification_loss: 0.6309
 842/1000 [========================>.....] - ETA: 1:11 - loss: 2.7109 - regression_loss: 2.0797 - classification_loss: 0.6312
 843/1000 [========================>.....] - ETA: 1:11 - loss: 2.7080 - regression_loss: 2.0772 - classification_loss: 0.6308
 844/1000 [========================>.....] - ETA: 1:10 - loss: 2.7049 - regression_loss: 2.0747 - classification_loss: 0.6302
 845/1000 [========================>.....] - ETA: 1:10 - loss: 2.7061 - regression_loss: 2.0753 - classification_loss: 0.6308
 846/1000 [========================>.....] - ETA: 1:09 - loss: 2.7064 - regression_loss: 2.0751 - classification_loss: 0.6313
 847/1000 [========================>.....] - ETA: 1:09 - loss: 2.7083 - regression_loss: 2.0764 - classification_loss: 0.6319
 848/1000 [========================>.....] - ETA: 1:08 - loss: 2.7052 - regression_loss: 2.0740 - classification_loss: 0.6312
 849/1000 [========================>.....] - ETA: 1:08 - loss: 2.7075 - regression_loss: 2.0753 - classification_loss: 0.6322
 850/1000 [========================>.....] - ETA: 1:07 - loss: 2.7043 - regression_loss: 2.0728 - classification_loss: 0.6315
 851/1000 [========================>.....] - ETA: 1:07 - loss: 2.7042 - regression_loss: 2.0730 - classification_loss: 0.6312
 852/1000 [========================>.....] - ETA: 1:06 - loss: 2.7057 - regression_loss: 2.0742 - classification_loss: 0.6315
 853/1000 [========================>.....] - ETA: 1:06 - loss: 2.7050 - regression_loss: 2.0738 - classification_loss: 0.6313
 854/1000 [========================>.....] - ETA: 1:06 - loss: 2.7054 - regression_loss: 2.0741 - classification_loss: 0.6313
 855/1000 [========================>.....] - ETA: 1:05 - loss: 2.7022 - regression_loss: 2.0717 - classification_loss: 0.6305
 856/1000 [========================>.....] - ETA: 1:05 - loss: 2.7035 - regression_loss: 2.0727 - classification_loss: 0.6309
 857/1000 [========================>.....] - ETA: 1:04 - loss: 2.7064 - regression_loss: 2.0745 - classification_loss: 0.6319
 858/1000 [========================>.....] - ETA: 1:04 - loss: 2.7033 - regression_loss: 2.0721 - classification_loss: 0.6311
 859/1000 [========================>.....] - ETA: 1:03 - loss: 2.7001 - regression_loss: 2.0697 - classification_loss: 0.6304
 860/1000 [========================>.....] - ETA: 1:03 - loss: 2.6988 - regression_loss: 2.0686 - classification_loss: 0.6302
 861/1000 [========================>.....] - ETA: 1:02 - loss: 2.6993 - regression_loss: 2.0691 - classification_loss: 0.6301
 862/1000 [========================>.....] - ETA: 1:02 - loss: 2.6962 - regression_loss: 2.0667 - classification_loss: 0.6294
 863/1000 [========================>.....] - ETA: 1:02 - loss: 2.6978 - regression_loss: 2.0677 - classification_loss: 0.6301
 864/1000 [========================>.....] - ETA: 1:01 - loss: 2.6947 - regression_loss: 2.0653 - classification_loss: 0.6294
 865/1000 [========================>.....] - ETA: 1:01 - loss: 2.6950 - regression_loss: 2.0657 - classification_loss: 0.6293
 866/1000 [========================>.....] - ETA: 1:00 - loss: 2.6918 - regression_loss: 2.0633 - classification_loss: 0.6286
 867/1000 [=========================>....] - ETA: 1:00 - loss: 2.6908 - regression_loss: 2.0625 - classification_loss: 0.6282
 868/1000 [=========================>....] - ETA: 59s - loss: 2.6877 - regression_loss: 2.0602 - classification_loss: 0.6275 
 869/1000 [=========================>....] - ETA: 59s - loss: 2.6866 - regression_loss: 2.0593 - classification_loss: 0.6273
 870/1000 [=========================>....] - ETA: 58s - loss: 2.6882 - regression_loss: 2.0605 - classification_loss: 0.6276
 871/1000 [=========================>....] - ETA: 58s - loss: 2.6851 - regression_loss: 2.0582 - classification_loss: 0.6269
 872/1000 [=========================>....] - ETA: 57s - loss: 2.6820 - regression_loss: 2.0558 - classification_loss: 0.6262
 873/1000 [=========================>....] - ETA: 57s - loss: 2.6844 - regression_loss: 2.0581 - classification_loss: 0.6263
 874/1000 [=========================>....] - ETA: 57s - loss: 2.6859 - regression_loss: 2.0595 - classification_loss: 0.6264
 875/1000 [=========================>....] - ETA: 56s - loss: 2.6861 - regression_loss: 2.0598 - classification_loss: 0.6263
 876/1000 [=========================>....] - ETA: 56s - loss: 2.6886 - regression_loss: 2.0618 - classification_loss: 0.6268
 877/1000 [=========================>....] - ETA: 55s - loss: 2.6905 - regression_loss: 2.0622 - classification_loss: 0.6283
 878/1000 [=========================>....] - ETA: 55s - loss: 2.6875 - regression_loss: 2.0599 - classification_loss: 0.6276
 879/1000 [=========================>....] - ETA: 54s - loss: 2.6907 - regression_loss: 2.0617 - classification_loss: 0.6290
 880/1000 [=========================>....] - ETA: 54s - loss: 2.6936 - regression_loss: 2.0630 - classification_loss: 0.6306
 881/1000 [=========================>....] - ETA: 53s - loss: 2.6955 - regression_loss: 2.0649 - classification_loss: 0.6305
 882/1000 [=========================>....] - ETA: 53s - loss: 2.6993 - regression_loss: 2.0671 - classification_loss: 0.6322
 883/1000 [=========================>....] - ETA: 52s - loss: 2.7019 - regression_loss: 2.0681 - classification_loss: 0.6338
 884/1000 [=========================>....] - ETA: 52s - loss: 2.6988 - regression_loss: 2.0657 - classification_loss: 0.6331
 885/1000 [=========================>....] - ETA: 52s - loss: 2.7001 - regression_loss: 2.0666 - classification_loss: 0.6334
 886/1000 [=========================>....] - ETA: 51s - loss: 2.7004 - regression_loss: 2.0663 - classification_loss: 0.6340
 887/1000 [=========================>....] - ETA: 51s - loss: 2.7020 - regression_loss: 2.0681 - classification_loss: 0.6340
 888/1000 [=========================>....] - ETA: 50s - loss: 2.7026 - regression_loss: 2.0684 - classification_loss: 0.6341
 889/1000 [=========================>....] - ETA: 50s - loss: 2.7038 - regression_loss: 2.0689 - classification_loss: 0.6349
 890/1000 [=========================>....] - ETA: 49s - loss: 2.7008 - regression_loss: 2.0666 - classification_loss: 0.6342
 891/1000 [=========================>....] - ETA: 49s - loss: 2.6978 - regression_loss: 2.0642 - classification_loss: 0.6335
 892/1000 [=========================>....] - ETA: 48s - loss: 2.6947 - regression_loss: 2.0619 - classification_loss: 0.6328
 893/1000 [=========================>....] - ETA: 48s - loss: 2.6974 - regression_loss: 2.0631 - classification_loss: 0.6343
 894/1000 [=========================>....] - ETA: 47s - loss: 2.7023 - regression_loss: 2.0664 - classification_loss: 0.6359
 895/1000 [=========================>....] - ETA: 47s - loss: 2.7035 - regression_loss: 2.0678 - classification_loss: 0.6357
 896/1000 [=========================>....] - ETA: 47s - loss: 2.7049 - regression_loss: 2.0691 - classification_loss: 0.6358
 897/1000 [=========================>....] - ETA: 46s - loss: 2.7047 - regression_loss: 2.0688 - classification_loss: 0.6359
 898/1000 [=========================>....] - ETA: 46s - loss: 2.7035 - regression_loss: 2.0678 - classification_loss: 0.6357
 899/1000 [=========================>....] - ETA: 45s - loss: 2.7026 - regression_loss: 2.0673 - classification_loss: 0.6353
 900/1000 [==========================>...] - ETA: 45s - loss: 2.7024 - regression_loss: 2.0675 - classification_loss: 0.6350
 901/1000 [==========================>...] - ETA: 44s - loss: 2.7039 - regression_loss: 2.0692 - classification_loss: 0.6347
 902/1000 [==========================>...] - ETA: 44s - loss: 2.7062 - regression_loss: 2.0699 - classification_loss: 0.6363
 903/1000 [==========================>...] - ETA: 43s - loss: 2.7032 - regression_loss: 2.0676 - classification_loss: 0.6356
 904/1000 [==========================>...] - ETA: 43s - loss: 2.7037 - regression_loss: 2.0679 - classification_loss: 0.6358
 905/1000 [==========================>...] - ETA: 42s - loss: 2.7036 - regression_loss: 2.0681 - classification_loss: 0.6355
 906/1000 [==========================>...] - ETA: 42s - loss: 2.7006 - regression_loss: 2.0658 - classification_loss: 0.6348
 907/1000 [==========================>...] - ETA: 42s - loss: 2.7035 - regression_loss: 2.0676 - classification_loss: 0.6359
 908/1000 [==========================>...] - ETA: 41s - loss: 2.7024 - regression_loss: 2.0669 - classification_loss: 0.6355
 909/1000 [==========================>...] - ETA: 41s - loss: 2.7056 - regression_loss: 2.0682 - classification_loss: 0.6374
 910/1000 [==========================>...] - ETA: 40s - loss: 2.7071 - regression_loss: 2.0697 - classification_loss: 0.6375
 911/1000 [==========================>...] - ETA: 40s - loss: 2.7084 - regression_loss: 2.0711 - classification_loss: 0.6374
 912/1000 [==========================>...] - ETA: 39s - loss: 2.7104 - regression_loss: 2.0725 - classification_loss: 0.6378
 913/1000 [==========================>...] - ETA: 39s - loss: 2.7112 - regression_loss: 2.0730 - classification_loss: 0.6381
 914/1000 [==========================>...] - ETA: 38s - loss: 2.7109 - regression_loss: 2.0733 - classification_loss: 0.6376
 915/1000 [==========================>...] - ETA: 38s - loss: 2.7107 - regression_loss: 2.0732 - classification_loss: 0.6375
 916/1000 [==========================>...] - ETA: 38s - loss: 2.7079 - regression_loss: 2.0709 - classification_loss: 0.6370
 917/1000 [==========================>...] - ETA: 37s - loss: 2.7073 - regression_loss: 2.0707 - classification_loss: 0.6366
 918/1000 [==========================>...] - ETA: 37s - loss: 2.7043 - regression_loss: 2.0684 - classification_loss: 0.6359
 919/1000 [==========================>...] - ETA: 36s - loss: 2.7033 - regression_loss: 2.0678 - classification_loss: 0.6355
 920/1000 [==========================>...] - ETA: 36s - loss: 2.7038 - regression_loss: 2.0687 - classification_loss: 0.6352
 921/1000 [==========================>...] - ETA: 35s - loss: 2.7060 - regression_loss: 2.0702 - classification_loss: 0.6358
 922/1000 [==========================>...] - ETA: 35s - loss: 2.7057 - regression_loss: 2.0704 - classification_loss: 0.6354
 923/1000 [==========================>...] - ETA: 34s - loss: 2.7079 - regression_loss: 2.0724 - classification_loss: 0.6355
 924/1000 [==========================>...] - ETA: 34s - loss: 2.7077 - regression_loss: 2.0725 - classification_loss: 0.6352
 925/1000 [==========================>...] - ETA: 33s - loss: 2.7063 - regression_loss: 2.0715 - classification_loss: 0.6349
 926/1000 [==========================>...] - ETA: 33s - loss: 2.7054 - regression_loss: 2.0708 - classification_loss: 0.6346
 927/1000 [==========================>...] - ETA: 33s - loss: 2.7053 - regression_loss: 2.0708 - classification_loss: 0.6345
 928/1000 [==========================>...] - ETA: 32s - loss: 2.7073 - regression_loss: 2.0722 - classification_loss: 0.6350
 929/1000 [==========================>...] - ETA: 32s - loss: 2.7043 - regression_loss: 2.0700 - classification_loss: 0.6343
 930/1000 [==========================>...] - ETA: 31s - loss: 2.7049 - regression_loss: 2.0707 - classification_loss: 0.6342
 931/1000 [==========================>...] - ETA: 31s - loss: 2.7051 - regression_loss: 2.0708 - classification_loss: 0.6343
 932/1000 [==========================>...] - ETA: 30s - loss: 2.7022 - regression_loss: 2.0686 - classification_loss: 0.6336
 933/1000 [==========================>...] - ETA: 30s - loss: 2.7028 - regression_loss: 2.0695 - classification_loss: 0.6333
 934/1000 [===========================>..] - ETA: 29s - loss: 2.7039 - regression_loss: 2.0701 - classification_loss: 0.6338
 935/1000 [===========================>..] - ETA: 29s - loss: 2.7010 - regression_loss: 2.0679 - classification_loss: 0.6331
 936/1000 [===========================>..] - ETA: 28s - loss: 2.7008 - regression_loss: 2.0678 - classification_loss: 0.6329
 937/1000 [===========================>..] - ETA: 28s - loss: 2.7027 - regression_loss: 2.0694 - classification_loss: 0.6332
 938/1000 [===========================>..] - ETA: 28s - loss: 2.7025 - regression_loss: 2.0694 - classification_loss: 0.6331
 939/1000 [===========================>..] - ETA: 27s - loss: 2.7049 - regression_loss: 2.0710 - classification_loss: 0.6338
 940/1000 [===========================>..] - ETA: 27s - loss: 2.7042 - regression_loss: 2.0707 - classification_loss: 0.6334
 941/1000 [===========================>..] - ETA: 26s - loss: 2.7013 - regression_loss: 2.0685 - classification_loss: 0.6327
 942/1000 [===========================>..] - ETA: 26s - loss: 2.7017 - regression_loss: 2.0691 - classification_loss: 0.6326
 943/1000 [===========================>..] - ETA: 25s - loss: 2.7019 - regression_loss: 2.0697 - classification_loss: 0.6323
 944/1000 [===========================>..] - ETA: 25s - loss: 2.7028 - regression_loss: 2.0702 - classification_loss: 0.6326
 945/1000 [===========================>..] - ETA: 24s - loss: 2.6999 - regression_loss: 2.0680 - classification_loss: 0.6319
 946/1000 [===========================>..] - ETA: 24s - loss: 2.7031 - regression_loss: 2.0700 - classification_loss: 0.6331
 947/1000 [===========================>..] - ETA: 23s - loss: 2.7019 - regression_loss: 2.0692 - classification_loss: 0.6327
 948/1000 [===========================>..] - ETA: 23s - loss: 2.7047 - regression_loss: 2.0710 - classification_loss: 0.6337
 949/1000 [===========================>..] - ETA: 23s - loss: 2.7043 - regression_loss: 2.0709 - classification_loss: 0.6334
 950/1000 [===========================>..] - ETA: 22s - loss: 2.7051 - regression_loss: 2.0716 - classification_loss: 0.6335
 951/1000 [===========================>..] - ETA: 22s - loss: 2.7060 - regression_loss: 2.0723 - classification_loss: 0.6337
 952/1000 [===========================>..] - ETA: 21s - loss: 2.7088 - regression_loss: 2.0749 - classification_loss: 0.6340
 953/1000 [===========================>..] - ETA: 21s - loss: 2.7110 - regression_loss: 2.0768 - classification_loss: 0.6342
 954/1000 [===========================>..] - ETA: 20s - loss: 2.7124 - regression_loss: 2.0781 - classification_loss: 0.6343
 955/1000 [===========================>..] - ETA: 20s - loss: 2.7133 - regression_loss: 2.0787 - classification_loss: 0.6347
 956/1000 [===========================>..] - ETA: 19s - loss: 2.7138 - regression_loss: 2.0782 - classification_loss: 0.6356
 957/1000 [===========================>..] - ETA: 19s - loss: 2.7110 - regression_loss: 2.0760 - classification_loss: 0.6350
 958/1000 [===========================>..] - ETA: 19s - loss: 2.7127 - regression_loss: 2.0775 - classification_loss: 0.6352
 959/1000 [===========================>..] - ETA: 18s - loss: 2.7145 - regression_loss: 2.0791 - classification_loss: 0.6354
 960/1000 [===========================>..] - ETA: 18s - loss: 2.7148 - regression_loss: 2.0798 - classification_loss: 0.6350
 961/1000 [===========================>..] - ETA: 17s - loss: 2.7160 - regression_loss: 2.0810 - classification_loss: 0.6349
 962/1000 [===========================>..] - ETA: 17s - loss: 2.7167 - regression_loss: 2.0821 - classification_loss: 0.6346
 963/1000 [===========================>..] - ETA: 16s - loss: 2.7139 - regression_loss: 2.0799 - classification_loss: 0.6339
 964/1000 [===========================>..] - ETA: 16s - loss: 2.7142 - regression_loss: 2.0805 - classification_loss: 0.6337
 965/1000 [===========================>..] - ETA: 15s - loss: 2.7147 - regression_loss: 2.0808 - classification_loss: 0.6339
 966/1000 [===========================>..] - ETA: 15s - loss: 2.7186 - regression_loss: 2.0845 - classification_loss: 0.6340
 967/1000 [============================>.] - ETA: 14s - loss: 2.7185 - regression_loss: 2.0842 - classification_loss: 0.6342
 968/1000 [============================>.] - ETA: 14s - loss: 2.7157 - regression_loss: 2.0821 - classification_loss: 0.6336
 969/1000 [============================>.] - ETA: 14s - loss: 2.7129 - regression_loss: 2.0799 - classification_loss: 0.6329
 970/1000 [============================>.] - ETA: 13s - loss: 2.7142 - regression_loss: 2.0811 - classification_loss: 0.6331
 971/1000 [============================>.] - ETA: 13s - loss: 2.7147 - regression_loss: 2.0818 - classification_loss: 0.6330
 972/1000 [============================>.] - ETA: 12s - loss: 2.7152 - regression_loss: 2.0823 - classification_loss: 0.6330
 973/1000 [============================>.] - ETA: 12s - loss: 2.7159 - regression_loss: 2.0830 - classification_loss: 0.6329
 974/1000 [============================>.] - ETA: 11s - loss: 2.7153 - regression_loss: 2.0829 - classification_loss: 0.6324
 975/1000 [============================>.] - ETA: 11s - loss: 2.7171 - regression_loss: 2.0836 - classification_loss: 0.6335
 976/1000 [============================>.] - ETA: 10s - loss: 2.7153 - regression_loss: 2.0815 - classification_loss: 0.6338
 977/1000 [============================>.] - ETA: 10s - loss: 2.7127 - regression_loss: 2.0794 - classification_loss: 0.6333
 978/1000 [============================>.] - ETA: 9s - loss: 2.7140 - regression_loss: 2.0804 - classification_loss: 0.6336 
 979/1000 [============================>.] - ETA: 9s - loss: 2.7156 - regression_loss: 2.0814 - classification_loss: 0.6342
 980/1000 [============================>.] - ETA: 9s - loss: 2.7166 - regression_loss: 2.0822 - classification_loss: 0.6343
 981/1000 [============================>.] - ETA: 8s - loss: 2.7172 - regression_loss: 2.0824 - classification_loss: 0.6348
 982/1000 [============================>.] - ETA: 8s - loss: 2.7144 - regression_loss: 2.0803 - classification_loss: 0.6342
 983/1000 [============================>.] - ETA: 7s - loss: 2.7161 - regression_loss: 2.0810 - classification_loss: 0.6351
 984/1000 [============================>.] - ETA: 7s - loss: 2.7189 - regression_loss: 2.0827 - classification_loss: 0.6361
 985/1000 [============================>.] - ETA: 6s - loss: 2.7162 - regression_loss: 2.0806 - classification_loss: 0.6356
 986/1000 [============================>.] - ETA: 6s - loss: 2.7169 - regression_loss: 2.0815 - classification_loss: 0.6354
 987/1000 [============================>.] - ETA: 5s - loss: 2.7176 - regression_loss: 2.0821 - classification_loss: 0.6355
 988/1000 [============================>.] - ETA: 5s - loss: 2.7190 - regression_loss: 2.0826 - classification_loss: 0.6364
 989/1000 [============================>.] - ETA: 4s - loss: 2.7163 - regression_loss: 2.0805 - classification_loss: 0.6358
 990/1000 [============================>.] - ETA: 4s - loss: 2.7162 - regression_loss: 2.0805 - classification_loss: 0.6358
 991/1000 [============================>.] - ETA: 4s - loss: 2.7135 - regression_loss: 2.0784 - classification_loss: 0.6351
 992/1000 [============================>.] - ETA: 3s - loss: 2.7158 - regression_loss: 2.0799 - classification_loss: 0.6359
 993/1000 [============================>.] - ETA: 3s - loss: 2.7161 - regression_loss: 2.0801 - classification_loss: 0.6360
 994/1000 [============================>.] - ETA: 2s - loss: 2.7160 - regression_loss: 2.0799 - classification_loss: 0.6361
 995/1000 [============================>.] - ETA: 2s - loss: 2.7178 - regression_loss: 2.0815 - classification_loss: 0.6363
 996/1000 [============================>.] - ETA: 1s - loss: 2.7188 - regression_loss: 2.0824 - classification_loss: 0.6364
 997/1000 [============================>.] - ETA: 1s - loss: 2.7212 - regression_loss: 2.0837 - classification_loss: 0.6375
 998/1000 [============================>.] - ETA: 0s - loss: 2.7185 - regression_loss: 2.0816 - classification_loss: 0.6368
 999/1000 [============================>.] - ETA: 0s - loss: 2.7200 - regression_loss: 2.0829 - classification_loss: 0.6371
1000/1000 [==============================] - 452s 452ms/step - loss: 2.7205 - regression_loss: 2.0830 - classification_loss: 0.6376

Epoch 00019: saving model to ./snapshots/resnet50_csv_19.h5
0/2001/2002/2003/2004/2005/2006/2007/2008/2009/20010/20011/20012/20013/20014/20015/20016/20017/20018/20019/20020/20021/20022/20023/20024/20025/20026/20027/20028/20029/20030/20031/20032/20033/20034/20035/20036/20037/20038/20039/20040/20041/20042/20043/20044/20045/20046/20047/20048/20049/20050/20051/20052/20053/20054/20055/20056/20057/20058/20059/20060/20061/20062/20063/20064/20065/20066/20067/20068/20069/20070/20071/20072/20073/20074/20075/20076/20077/20078/20079/20080/20081/20082/20083/20084/20085/20086/20087/20088/20089/20090/20091/20092/20093/20094/20095/20096/20097/20098/20099/200100/200101/200102/200103/200104/200105/200106/200107/200108/200109/200110/200111/200112/200113/200114/200115/200116/200117/200118/200119/200120/200121/200122/200123/200124/200125/200126/200127/200128/200129/200130/200131/200132/200133/200134/200135/200136/200137/200138/200139/200140/200141/200142/200143/200144/200145/200146/200147/200148/200149/200150/200151/200152/200153/200154/200155/200156/200157/200158/200159/200160/200161/200162/200163/200164/200165/200166/200167/200168/200169/200170/200171/200172/200173/200174/200175/200176/200177/200178/200179/200180/200181/200182/200183/200184/200185/200186/200187/200188/200189/200190/200191/200192/200193/200194/200195/200196/200197/200198/200199/2000/2001/2002/2003/2004/2005/2006/2007/2008/2009/20010/20011/20012/20013/20014/20015/20016/20017/20018/20019/20020/20021/20022/20023/20024/20025/20026/20027/20028/20029/20030/20031/20032/20033/20034/20035/20036/20037/20038/20039/20040/20041/20042/20043/20044/20045/20046/20047/20048/20049/20050/20051/20052/20053/20054/20055/20056/20057/20058/20059/20060/20061/20062/20063/20064/20065/20066/20067/20068/20069/20070/20071/20072/20073/20074/20075/20076/20077/20078/20079/20080/20081/20082/20083/20084/20085/20086/20087/20088/20089/20090/20091/20092/20093/20094/20095/20096/20097/20098/20099/200100/200101/200102/200103/200104/200105/200106/200107/200108/200109/200110/200111/200112/200113/200114/200115/200116/200117/200118/200119/200120/200121/200122/200123/200124/200125/200126/200127/200128/200129/200130/200131/200132/200133/200134/200135/200136/200137/200138/200139/200140/200141/200142/200143/200144/200145/200146/200147/200148/200149/200150/200151/200152/200153/200154/200155/200156/200157/200158/200159/200160/200161/200162/200163/200164/200165/200166/200167/200168/200169/200170/200171/200172/200173/200174/200175/200176/200177/200178/200179/200180/200181/200182/200183/200184/200185/200186/200187/200188/200189/200190/200191/200192/200193/200194/200195/200196/200197/200198/200199/200

M 0.1645
N 0.0116
mAP: 0.0881
Epoch 20/30

   1/1000 [..............................] - ETA: 7:23 - loss: 2.4764 - regression_loss: 1.7489 - classification_loss: 0.7275
   2/1000 [..............................] - ETA: 7:27 - loss: 2.8921 - regression_loss: 2.0551 - classification_loss: 0.8370
   3/1000 [..............................] - ETA: 7:28 - loss: 2.8390 - regression_loss: 2.0156 - classification_loss: 0.8234
   4/1000 [..............................] - ETA: 7:27 - loss: 3.1979 - regression_loss: 2.3153 - classification_loss: 0.8826
   5/1000 [..............................] - ETA: 7:28 - loss: 2.5583 - regression_loss: 1.8522 - classification_loss: 0.7061
   6/1000 [..............................] - ETA: 7:28 - loss: 2.1319 - regression_loss: 1.5435 - classification_loss: 0.5884
   7/1000 [..............................] - ETA: 7:27 - loss: 2.0593 - regression_loss: 1.5057 - classification_loss: 0.5536
   8/1000 [..............................] - ETA: 7:21 - loss: 2.2813 - regression_loss: 1.7013 - classification_loss: 0.5800
   9/1000 [..............................] - ETA: 7:21 - loss: 2.3110 - regression_loss: 1.7403 - classification_loss: 0.5707
  10/1000 [..............................] - ETA: 7:21 - loss: 2.3617 - regression_loss: 1.7850 - classification_loss: 0.5767
  11/1000 [..............................] - ETA: 7:21 - loss: 2.5486 - regression_loss: 1.9415 - classification_loss: 0.6071
  12/1000 [..............................] - ETA: 7:21 - loss: 2.5370 - regression_loss: 1.9489 - classification_loss: 0.5882
  13/1000 [..............................] - ETA: 7:21 - loss: 2.5754 - regression_loss: 1.9710 - classification_loss: 0.6044
  14/1000 [..............................] - ETA: 7:21 - loss: 2.5450 - regression_loss: 1.9610 - classification_loss: 0.5840
  15/1000 [..............................] - ETA: 7:21 - loss: 2.5903 - regression_loss: 1.9974 - classification_loss: 0.5929
  16/1000 [..............................] - ETA: 7:21 - loss: 2.7482 - regression_loss: 2.1257 - classification_loss: 0.6225
  17/1000 [..............................] - ETA: 7:21 - loss: 2.6975 - regression_loss: 2.0760 - classification_loss: 0.6215
  18/1000 [..............................] - ETA: 7:20 - loss: 2.7880 - regression_loss: 2.1543 - classification_loss: 0.6338
  19/1000 [..............................] - ETA: 7:20 - loss: 2.9543 - regression_loss: 2.2467 - classification_loss: 0.7076
  20/1000 [..............................] - ETA: 7:20 - loss: 2.8137 - regression_loss: 2.1344 - classification_loss: 0.6794
  21/1000 [..............................] - ETA: 7:20 - loss: 2.7844 - regression_loss: 2.1196 - classification_loss: 0.6649
  22/1000 [..............................] - ETA: 7:20 - loss: 2.7736 - regression_loss: 2.1161 - classification_loss: 0.6575
  23/1000 [..............................] - ETA: 7:20 - loss: 2.6531 - regression_loss: 2.0241 - classification_loss: 0.6290
  24/1000 [..............................] - ETA: 7:19 - loss: 2.7142 - regression_loss: 2.0832 - classification_loss: 0.6310
  25/1000 [..............................] - ETA: 7:19 - loss: 2.7539 - regression_loss: 2.1137 - classification_loss: 0.6402
  26/1000 [..............................] - ETA: 7:18 - loss: 2.7650 - regression_loss: 2.1231 - classification_loss: 0.6419
  27/1000 [..............................] - ETA: 7:18 - loss: 2.7587 - regression_loss: 2.1240 - classification_loss: 0.6347
  28/1000 [..............................] - ETA: 7:17 - loss: 2.7361 - regression_loss: 2.1082 - classification_loss: 0.6279
  29/1000 [..............................] - ETA: 7:17 - loss: 2.7356 - regression_loss: 2.1088 - classification_loss: 0.6268
  30/1000 [..............................] - ETA: 7:16 - loss: 2.7704 - regression_loss: 2.1382 - classification_loss: 0.6322
  31/1000 [..............................] - ETA: 7:16 - loss: 2.7996 - regression_loss: 2.1668 - classification_loss: 0.6328
  32/1000 [..............................] - ETA: 7:15 - loss: 2.7189 - regression_loss: 2.0991 - classification_loss: 0.6199
  33/1000 [..............................] - ETA: 7:15 - loss: 2.6365 - regression_loss: 2.0354 - classification_loss: 0.6011
  34/1000 [>.............................] - ETA: 7:14 - loss: 2.6531 - regression_loss: 2.0495 - classification_loss: 0.6036
  35/1000 [>.............................] - ETA: 7:14 - loss: 2.7223 - regression_loss: 2.0996 - classification_loss: 0.6228
  36/1000 [>.............................] - ETA: 7:14 - loss: 2.7464 - regression_loss: 2.1314 - classification_loss: 0.6150
  37/1000 [>.............................] - ETA: 7:13 - loss: 2.7445 - regression_loss: 2.1365 - classification_loss: 0.6080
  38/1000 [>.............................] - ETA: 7:13 - loss: 2.7468 - regression_loss: 2.1390 - classification_loss: 0.6077
  39/1000 [>.............................] - ETA: 7:12 - loss: 2.7619 - regression_loss: 2.1626 - classification_loss: 0.5993
  40/1000 [>.............................] - ETA: 7:12 - loss: 2.7687 - regression_loss: 2.1800 - classification_loss: 0.5887
  41/1000 [>.............................] - ETA: 7:11 - loss: 2.7511 - regression_loss: 2.1688 - classification_loss: 0.5823
  42/1000 [>.............................] - ETA: 7:11 - loss: 2.7705 - regression_loss: 2.1847 - classification_loss: 0.5858
  43/1000 [>.............................] - ETA: 7:11 - loss: 2.7061 - regression_loss: 2.1339 - classification_loss: 0.5722
  44/1000 [>.............................] - ETA: 7:10 - loss: 2.7229 - regression_loss: 2.1455 - classification_loss: 0.5774
  45/1000 [>.............................] - ETA: 7:09 - loss: 2.7294 - regression_loss: 2.1547 - classification_loss: 0.5747
  46/1000 [>.............................] - ETA: 7:09 - loss: 2.7340 - regression_loss: 2.1633 - classification_loss: 0.5708
  47/1000 [>.............................] - ETA: 7:08 - loss: 2.7346 - regression_loss: 2.1698 - classification_loss: 0.5648
  48/1000 [>.............................] - ETA: 7:08 - loss: 2.7316 - regression_loss: 2.1650 - classification_loss: 0.5666
  49/1000 [>.............................] - ETA: 7:07 - loss: 2.7556 - regression_loss: 2.1860 - classification_loss: 0.5696
  50/1000 [>.............................] - ETA: 7:07 - loss: 2.8250 - regression_loss: 2.2329 - classification_loss: 0.5921
  51/1000 [>.............................] - ETA: 7:07 - loss: 2.8021 - regression_loss: 2.2158 - classification_loss: 0.5862
  52/1000 [>.............................] - ETA: 7:06 - loss: 2.8071 - regression_loss: 2.2221 - classification_loss: 0.5850
  53/1000 [>.............................] - ETA: 7:06 - loss: 2.8411 - regression_loss: 2.2490 - classification_loss: 0.5922
  54/1000 [>.............................] - ETA: 7:05 - loss: 2.8502 - regression_loss: 2.2575 - classification_loss: 0.5927
  55/1000 [>.............................] - ETA: 7:05 - loss: 2.8273 - regression_loss: 2.2410 - classification_loss: 0.5863
  56/1000 [>.............................] - ETA: 7:05 - loss: 2.7779 - regression_loss: 2.2010 - classification_loss: 0.5770
  57/1000 [>.............................] - ETA: 7:04 - loss: 2.7653 - regression_loss: 2.1934 - classification_loss: 0.5719
  58/1000 [>.............................] - ETA: 7:04 - loss: 2.7643 - regression_loss: 2.1944 - classification_loss: 0.5699
  59/1000 [>.............................] - ETA: 7:03 - loss: 2.7174 - regression_loss: 2.1572 - classification_loss: 0.5602
  60/1000 [>.............................] - ETA: 7:03 - loss: 2.6722 - regression_loss: 2.1213 - classification_loss: 0.5509
  61/1000 [>.............................] - ETA: 7:03 - loss: 2.6586 - regression_loss: 2.1128 - classification_loss: 0.5458
  62/1000 [>.............................] - ETA: 7:02 - loss: 2.6524 - regression_loss: 2.1115 - classification_loss: 0.5409
  63/1000 [>.............................] - ETA: 7:02 - loss: 2.6804 - regression_loss: 2.1376 - classification_loss: 0.5428
  64/1000 [>.............................] - ETA: 7:01 - loss: 2.6797 - regression_loss: 2.1384 - classification_loss: 0.5413
  65/1000 [>.............................] - ETA: 7:01 - loss: 2.6384 - regression_loss: 2.1055 - classification_loss: 0.5329
  66/1000 [>.............................] - ETA: 7:01 - loss: 2.6566 - regression_loss: 2.1024 - classification_loss: 0.5542
  67/1000 [=>............................] - ETA: 7:00 - loss: 2.6799 - regression_loss: 2.1083 - classification_loss: 0.5717
  68/1000 [=>............................] - ETA: 7:00 - loss: 2.6704 - regression_loss: 2.1000 - classification_loss: 0.5704
  69/1000 [=>............................] - ETA: 6:59 - loss: 2.7254 - regression_loss: 2.1395 - classification_loss: 0.5858
  70/1000 [=>............................] - ETA: 6:59 - loss: 2.7519 - regression_loss: 2.1566 - classification_loss: 0.5953
  71/1000 [=>............................] - ETA: 6:59 - loss: 2.7718 - regression_loss: 2.1719 - classification_loss: 0.5999
  72/1000 [=>............................] - ETA: 6:58 - loss: 2.7813 - regression_loss: 2.1822 - classification_loss: 0.5992
  73/1000 [=>............................] - ETA: 6:57 - loss: 2.7432 - regression_loss: 2.1523 - classification_loss: 0.5910
  74/1000 [=>............................] - ETA: 6:57 - loss: 2.7595 - regression_loss: 2.1653 - classification_loss: 0.5942
  75/1000 [=>............................] - ETA: 6:57 - loss: 2.7751 - regression_loss: 2.1645 - classification_loss: 0.6105
  76/1000 [=>............................] - ETA: 6:56 - loss: 2.7628 - regression_loss: 2.1570 - classification_loss: 0.6058
  77/1000 [=>............................] - ETA: 6:56 - loss: 2.7854 - regression_loss: 2.1763 - classification_loss: 0.6092
  78/1000 [=>............................] - ETA: 6:55 - loss: 2.7815 - regression_loss: 2.1763 - classification_loss: 0.6052
  79/1000 [=>............................] - ETA: 6:55 - loss: 2.7750 - regression_loss: 2.1712 - classification_loss: 0.6038
  80/1000 [=>............................] - ETA: 6:54 - loss: 2.7791 - regression_loss: 2.1782 - classification_loss: 0.6009
  81/1000 [=>............................] - ETA: 6:54 - loss: 2.7835 - regression_loss: 2.1847 - classification_loss: 0.5988
  82/1000 [=>............................] - ETA: 6:54 - loss: 2.8060 - regression_loss: 2.1998 - classification_loss: 0.6062
  83/1000 [=>............................] - ETA: 6:53 - loss: 2.8094 - regression_loss: 2.1960 - classification_loss: 0.6134
  84/1000 [=>............................] - ETA: 6:53 - loss: 2.8299 - regression_loss: 2.2164 - classification_loss: 0.6135
  85/1000 [=>............................] - ETA: 6:52 - loss: 2.8326 - regression_loss: 2.2210 - classification_loss: 0.6116
  86/1000 [=>............................] - ETA: 6:52 - loss: 2.8398 - regression_loss: 2.2296 - classification_loss: 0.6101
  87/1000 [=>............................] - ETA: 6:52 - loss: 2.8444 - regression_loss: 2.2357 - classification_loss: 0.6087
  88/1000 [=>............................] - ETA: 6:51 - loss: 2.8737 - regression_loss: 2.2529 - classification_loss: 0.6208
  89/1000 [=>............................] - ETA: 6:51 - loss: 2.8415 - regression_loss: 2.2276 - classification_loss: 0.6139
  90/1000 [=>............................] - ETA: 6:51 - loss: 2.8488 - regression_loss: 2.2325 - classification_loss: 0.6163
  91/1000 [=>............................] - ETA: 6:50 - loss: 2.8175 - regression_loss: 2.2080 - classification_loss: 0.6095
  92/1000 [=>............................] - ETA: 6:50 - loss: 2.8228 - regression_loss: 2.2109 - classification_loss: 0.6119
  93/1000 [=>............................] - ETA: 6:49 - loss: 2.8207 - regression_loss: 2.2103 - classification_loss: 0.6104
  94/1000 [=>............................] - ETA: 6:49 - loss: 2.8054 - regression_loss: 2.1992 - classification_loss: 0.6061
  95/1000 [=>............................] - ETA: 6:48 - loss: 2.8419 - regression_loss: 2.2298 - classification_loss: 0.6120
  96/1000 [=>............................] - ETA: 6:48 - loss: 2.8390 - regression_loss: 2.2309 - classification_loss: 0.6081
  97/1000 [=>............................] - ETA: 6:47 - loss: 2.8409 - regression_loss: 2.2302 - classification_loss: 0.6108
  98/1000 [=>............................] - ETA: 6:47 - loss: 2.8123 - regression_loss: 2.2074 - classification_loss: 0.6049
  99/1000 [=>............................] - ETA: 6:47 - loss: 2.8011 - regression_loss: 2.2003 - classification_loss: 0.6008
 100/1000 [==>...........................] - ETA: 6:46 - loss: 2.7731 - regression_loss: 2.1783 - classification_loss: 0.5948
 101/1000 [==>...........................] - ETA: 6:46 - loss: 2.7855 - regression_loss: 2.1890 - classification_loss: 0.5965
 102/1000 [==>...........................] - ETA: 6:45 - loss: 2.7991 - regression_loss: 2.2018 - classification_loss: 0.5973
 103/1000 [==>...........................] - ETA: 6:45 - loss: 2.7719 - regression_loss: 2.1804 - classification_loss: 0.5915
 104/1000 [==>...........................] - ETA: 6:44 - loss: 2.7674 - regression_loss: 2.1786 - classification_loss: 0.5888
 105/1000 [==>...........................] - ETA: 6:44 - loss: 2.7955 - regression_loss: 2.1968 - classification_loss: 0.5987
 106/1000 [==>...........................] - ETA: 6:44 - loss: 2.7870 - regression_loss: 2.1761 - classification_loss: 0.6109
 107/1000 [==>...........................] - ETA: 6:43 - loss: 2.7811 - regression_loss: 2.1709 - classification_loss: 0.6101
 108/1000 [==>...........................] - ETA: 6:43 - loss: 2.7553 - regression_loss: 2.1508 - classification_loss: 0.6045
 109/1000 [==>...........................] - ETA: 6:42 - loss: 2.7300 - regression_loss: 2.1311 - classification_loss: 0.5989
 110/1000 [==>...........................] - ETA: 6:42 - loss: 2.7233 - regression_loss: 2.1269 - classification_loss: 0.5964
 111/1000 [==>...........................] - ETA: 6:41 - loss: 2.7225 - regression_loss: 2.1275 - classification_loss: 0.5949
 112/1000 [==>...........................] - ETA: 6:41 - loss: 2.7279 - regression_loss: 2.1340 - classification_loss: 0.5939
 113/1000 [==>...........................] - ETA: 6:40 - loss: 2.7257 - regression_loss: 2.1335 - classification_loss: 0.5922
 114/1000 [==>...........................] - ETA: 6:40 - loss: 2.7342 - regression_loss: 2.1412 - classification_loss: 0.5930
 115/1000 [==>...........................] - ETA: 6:39 - loss: 2.7396 - regression_loss: 2.1481 - classification_loss: 0.5915
 116/1000 [==>...........................] - ETA: 6:39 - loss: 2.7619 - regression_loss: 2.1588 - classification_loss: 0.6031
 117/1000 [==>...........................] - ETA: 6:38 - loss: 2.7383 - regression_loss: 2.1403 - classification_loss: 0.5980
 118/1000 [==>...........................] - ETA: 6:38 - loss: 2.7391 - regression_loss: 2.1411 - classification_loss: 0.5980
 119/1000 [==>...........................] - ETA: 6:37 - loss: 2.7425 - regression_loss: 2.1419 - classification_loss: 0.6006
 120/1000 [==>...........................] - ETA: 6:37 - loss: 2.7197 - regression_loss: 2.1240 - classification_loss: 0.5956
 121/1000 [==>...........................] - ETA: 6:36 - loss: 2.7129 - regression_loss: 2.1189 - classification_loss: 0.5940
 122/1000 [==>...........................] - ETA: 6:36 - loss: 2.7109 - regression_loss: 2.1176 - classification_loss: 0.5933
 123/1000 [==>...........................] - ETA: 6:36 - loss: 2.7064 - regression_loss: 2.1160 - classification_loss: 0.5904
 124/1000 [==>...........................] - ETA: 6:35 - loss: 2.7105 - regression_loss: 2.1210 - classification_loss: 0.5895
 125/1000 [==>...........................] - ETA: 6:34 - loss: 2.7062 - regression_loss: 2.1171 - classification_loss: 0.5891
 126/1000 [==>...........................] - ETA: 6:34 - loss: 2.7106 - regression_loss: 2.1155 - classification_loss: 0.5951
 127/1000 [==>...........................] - ETA: 6:34 - loss: 2.7276 - regression_loss: 2.1303 - classification_loss: 0.5973
 128/1000 [==>...........................] - ETA: 6:33 - loss: 2.7230 - regression_loss: 2.1288 - classification_loss: 0.5942
 129/1000 [==>...........................] - ETA: 6:33 - loss: 2.7031 - regression_loss: 2.1123 - classification_loss: 0.5909
 130/1000 [==>...........................] - ETA: 6:32 - loss: 2.6970 - regression_loss: 2.0960 - classification_loss: 0.6009
 131/1000 [==>...........................] - ETA: 6:32 - loss: 2.7028 - regression_loss: 2.1031 - classification_loss: 0.5997
 132/1000 [==>...........................] - ETA: 6:31 - loss: 2.7173 - regression_loss: 2.1132 - classification_loss: 0.6041
 133/1000 [==>...........................] - ETA: 6:31 - loss: 2.7210 - regression_loss: 2.1151 - classification_loss: 0.6059
 134/1000 [===>..........................] - ETA: 6:30 - loss: 2.7225 - regression_loss: 2.1162 - classification_loss: 0.6062
 135/1000 [===>..........................] - ETA: 6:30 - loss: 2.7216 - regression_loss: 2.1138 - classification_loss: 0.6078
 136/1000 [===>..........................] - ETA: 6:29 - loss: 2.7170 - regression_loss: 2.1104 - classification_loss: 0.6066
 137/1000 [===>..........................] - ETA: 6:29 - loss: 2.7273 - regression_loss: 2.1141 - classification_loss: 0.6132
 138/1000 [===>..........................] - ETA: 6:29 - loss: 2.7338 - regression_loss: 2.1186 - classification_loss: 0.6152
 139/1000 [===>..........................] - ETA: 6:28 - loss: 2.7249 - regression_loss: 2.1132 - classification_loss: 0.6117
 140/1000 [===>..........................] - ETA: 6:28 - loss: 2.7246 - regression_loss: 2.1144 - classification_loss: 0.6101
 141/1000 [===>..........................] - ETA: 6:27 - loss: 2.7198 - regression_loss: 2.1119 - classification_loss: 0.6080
 142/1000 [===>..........................] - ETA: 6:27 - loss: 2.7178 - regression_loss: 2.1116 - classification_loss: 0.6062
 143/1000 [===>..........................] - ETA: 6:26 - loss: 2.7162 - regression_loss: 2.1115 - classification_loss: 0.6047
 144/1000 [===>..........................] - ETA: 6:26 - loss: 2.7000 - regression_loss: 2.0968 - classification_loss: 0.6032
 145/1000 [===>..........................] - ETA: 6:25 - loss: 2.6955 - regression_loss: 2.0952 - classification_loss: 0.6004
 146/1000 [===>..........................] - ETA: 6:25 - loss: 2.7066 - regression_loss: 2.1062 - classification_loss: 0.6004
 147/1000 [===>..........................] - ETA: 6:25 - loss: 2.7012 - regression_loss: 2.1010 - classification_loss: 0.6002
 148/1000 [===>..........................] - ETA: 6:24 - loss: 2.6956 - regression_loss: 2.0868 - classification_loss: 0.6088
 149/1000 [===>..........................] - ETA: 6:24 - loss: 2.7063 - regression_loss: 2.0939 - classification_loss: 0.6125
 150/1000 [===>..........................] - ETA: 6:23 - loss: 2.7035 - regression_loss: 2.0935 - classification_loss: 0.6100
 151/1000 [===>..........................] - ETA: 6:23 - loss: 2.6982 - regression_loss: 2.0903 - classification_loss: 0.6079
 152/1000 [===>..........................] - ETA: 6:22 - loss: 2.7000 - regression_loss: 2.0945 - classification_loss: 0.6055
 153/1000 [===>..........................] - ETA: 6:22 - loss: 2.7066 - regression_loss: 2.1010 - classification_loss: 0.6056
 154/1000 [===>..........................] - ETA: 6:21 - loss: 2.6893 - regression_loss: 2.0874 - classification_loss: 0.6019
 155/1000 [===>..........................] - ETA: 6:21 - loss: 2.6723 - regression_loss: 2.0739 - classification_loss: 0.5984
 156/1000 [===>..........................] - ETA: 6:21 - loss: 2.6899 - regression_loss: 2.0843 - classification_loss: 0.6056
 157/1000 [===>..........................] - ETA: 6:20 - loss: 2.6887 - regression_loss: 2.0853 - classification_loss: 0.6034
 158/1000 [===>..........................] - ETA: 6:20 - loss: 2.6920 - regression_loss: 2.0893 - classification_loss: 0.6027
 159/1000 [===>..........................] - ETA: 6:19 - loss: 2.6924 - regression_loss: 2.0917 - classification_loss: 0.6008
 160/1000 [===>..........................] - ETA: 6:19 - loss: 2.6756 - regression_loss: 2.0786 - classification_loss: 0.5970
 161/1000 [===>..........................] - ETA: 6:18 - loss: 2.6590 - regression_loss: 2.0657 - classification_loss: 0.5933
 162/1000 [===>..........................] - ETA: 6:18 - loss: 2.6653 - regression_loss: 2.0713 - classification_loss: 0.5940
 163/1000 [===>..........................] - ETA: 6:18 - loss: 2.6651 - regression_loss: 2.0717 - classification_loss: 0.5934
 164/1000 [===>..........................] - ETA: 6:17 - loss: 2.6686 - regression_loss: 2.0750 - classification_loss: 0.5936
 165/1000 [===>..........................] - ETA: 6:17 - loss: 2.6770 - regression_loss: 2.0837 - classification_loss: 0.5933
 166/1000 [===>..........................] - ETA: 6:16 - loss: 2.6766 - regression_loss: 2.0833 - classification_loss: 0.5933
 167/1000 [====>.........................] - ETA: 6:16 - loss: 2.6747 - regression_loss: 2.0831 - classification_loss: 0.5916
 168/1000 [====>.........................] - ETA: 6:15 - loss: 2.6832 - regression_loss: 2.0899 - classification_loss: 0.5933
 169/1000 [====>.........................] - ETA: 6:15 - loss: 2.6860 - regression_loss: 2.0949 - classification_loss: 0.5911
 170/1000 [====>.........................] - ETA: 6:14 - loss: 2.6889 - regression_loss: 2.0969 - classification_loss: 0.5920
 171/1000 [====>.........................] - ETA: 6:14 - loss: 2.6886 - regression_loss: 2.0980 - classification_loss: 0.5906
 172/1000 [====>.........................] - ETA: 6:13 - loss: 2.6731 - regression_loss: 2.0858 - classification_loss: 0.5873
 173/1000 [====>.........................] - ETA: 6:13 - loss: 2.6817 - regression_loss: 2.0936 - classification_loss: 0.5881
 174/1000 [====>.........................] - ETA: 6:12 - loss: 2.6849 - regression_loss: 2.0965 - classification_loss: 0.5884
 175/1000 [====>.........................] - ETA: 6:12 - loss: 2.6939 - regression_loss: 2.1003 - classification_loss: 0.5936
 176/1000 [====>.........................] - ETA: 6:12 - loss: 2.6917 - regression_loss: 2.0970 - classification_loss: 0.5947
 177/1000 [====>.........................] - ETA: 6:11 - loss: 2.7026 - regression_loss: 2.1076 - classification_loss: 0.5950
 178/1000 [====>.........................] - ETA: 6:11 - loss: 2.7078 - regression_loss: 2.1126 - classification_loss: 0.5952
 179/1000 [====>.........................] - ETA: 6:10 - loss: 2.7114 - regression_loss: 2.1152 - classification_loss: 0.5962
 180/1000 [====>.........................] - ETA: 6:10 - loss: 2.7131 - regression_loss: 2.1147 - classification_loss: 0.5985
 181/1000 [====>.........................] - ETA: 6:09 - loss: 2.6982 - regression_loss: 2.1030 - classification_loss: 0.5952
 182/1000 [====>.........................] - ETA: 6:09 - loss: 2.6980 - regression_loss: 2.1041 - classification_loss: 0.5939
 183/1000 [====>.........................] - ETA: 6:08 - loss: 2.7090 - regression_loss: 2.1120 - classification_loss: 0.5969
 184/1000 [====>.........................] - ETA: 6:08 - loss: 2.7150 - regression_loss: 2.1167 - classification_loss: 0.5983
 185/1000 [====>.........................] - ETA: 6:08 - loss: 2.7087 - regression_loss: 2.1124 - classification_loss: 0.5963
 186/1000 [====>.........................] - ETA: 6:07 - loss: 2.7168 - regression_loss: 2.1205 - classification_loss: 0.5963
 187/1000 [====>.........................] - ETA: 6:07 - loss: 2.7023 - regression_loss: 2.1092 - classification_loss: 0.5931
 188/1000 [====>.........................] - ETA: 6:06 - loss: 2.7102 - regression_loss: 2.1154 - classification_loss: 0.5948
 189/1000 [====>.........................] - ETA: 6:06 - loss: 2.7189 - regression_loss: 2.1218 - classification_loss: 0.5971
 190/1000 [====>.........................] - ETA: 6:05 - loss: 2.7322 - regression_loss: 2.1298 - classification_loss: 0.6023
 191/1000 [====>.........................] - ETA: 6:05 - loss: 2.7316 - regression_loss: 2.1287 - classification_loss: 0.6029
 192/1000 [====>.........................] - ETA: 6:04 - loss: 2.7266 - regression_loss: 2.1256 - classification_loss: 0.6010
 193/1000 [====>.........................] - ETA: 6:04 - loss: 2.7349 - regression_loss: 2.1324 - classification_loss: 0.6024
 194/1000 [====>.........................] - ETA: 6:03 - loss: 2.7209 - regression_loss: 2.1215 - classification_loss: 0.5994
 195/1000 [====>.........................] - ETA: 6:03 - loss: 2.7227 - regression_loss: 2.1228 - classification_loss: 0.5999
 196/1000 [====>.........................] - ETA: 6:02 - loss: 2.7468 - regression_loss: 2.1384 - classification_loss: 0.6084
 197/1000 [====>.........................] - ETA: 6:02 - loss: 2.7467 - regression_loss: 2.1400 - classification_loss: 0.6068
 198/1000 [====>.........................] - ETA: 6:01 - loss: 2.7480 - regression_loss: 2.1427 - classification_loss: 0.6053
 199/1000 [====>.........................] - ETA: 6:01 - loss: 2.7533 - regression_loss: 2.1492 - classification_loss: 0.6042
 200/1000 [=====>........................] - ETA: 6:00 - loss: 2.7540 - regression_loss: 2.1515 - classification_loss: 0.6025
 201/1000 [=====>........................] - ETA: 6:00 - loss: 2.7480 - regression_loss: 2.1474 - classification_loss: 0.6007
 202/1000 [=====>........................] - ETA: 6:00 - loss: 2.7344 - regression_loss: 2.1368 - classification_loss: 0.5977
 203/1000 [=====>........................] - ETA: 5:59 - loss: 2.7398 - regression_loss: 2.1420 - classification_loss: 0.5978
 204/1000 [=====>........................] - ETA: 5:59 - loss: 2.7296 - regression_loss: 2.1315 - classification_loss: 0.5981
 205/1000 [=====>........................] - ETA: 5:58 - loss: 2.7346 - regression_loss: 2.1368 - classification_loss: 0.5979
 206/1000 [=====>........................] - ETA: 5:58 - loss: 2.7368 - regression_loss: 2.1397 - classification_loss: 0.5971
 207/1000 [=====>........................] - ETA: 5:57 - loss: 2.7417 - regression_loss: 2.1415 - classification_loss: 0.6002
 208/1000 [=====>........................] - ETA: 5:57 - loss: 2.7423 - regression_loss: 2.1428 - classification_loss: 0.5995
 209/1000 [=====>........................] - ETA: 5:56 - loss: 2.7438 - regression_loss: 2.1435 - classification_loss: 0.6004
 210/1000 [=====>........................] - ETA: 5:56 - loss: 2.7313 - regression_loss: 2.1333 - classification_loss: 0.5980
 211/1000 [=====>........................] - ETA: 5:56 - loss: 2.7293 - regression_loss: 2.1328 - classification_loss: 0.5965
 212/1000 [=====>........................] - ETA: 5:55 - loss: 2.7286 - regression_loss: 2.1331 - classification_loss: 0.5955
 213/1000 [=====>........................] - ETA: 5:55 - loss: 2.7423 - regression_loss: 2.1419 - classification_loss: 0.6005
 214/1000 [=====>........................] - ETA: 5:54 - loss: 2.7505 - regression_loss: 2.1483 - classification_loss: 0.6022
 215/1000 [=====>........................] - ETA: 5:54 - loss: 2.7527 - regression_loss: 2.1508 - classification_loss: 0.6019
 216/1000 [=====>........................] - ETA: 5:53 - loss: 2.7594 - regression_loss: 2.1569 - classification_loss: 0.6025
 217/1000 [=====>........................] - ETA: 5:53 - loss: 2.7467 - regression_loss: 2.1470 - classification_loss: 0.5997
 218/1000 [=====>........................] - ETA: 5:52 - loss: 2.7521 - regression_loss: 2.1519 - classification_loss: 0.6002
 219/1000 [=====>........................] - ETA: 5:52 - loss: 2.7552 - regression_loss: 2.1555 - classification_loss: 0.5998
 220/1000 [=====>........................] - ETA: 5:52 - loss: 2.7591 - regression_loss: 2.1597 - classification_loss: 0.5994
 221/1000 [=====>........................] - ETA: 5:51 - loss: 2.7466 - regression_loss: 2.1499 - classification_loss: 0.5967
 222/1000 [=====>........................] - ETA: 5:51 - loss: 2.7604 - regression_loss: 2.1586 - classification_loss: 0.6017
 223/1000 [=====>........................] - ETA: 5:50 - loss: 2.7659 - regression_loss: 2.1620 - classification_loss: 0.6039
 224/1000 [=====>........................] - ETA: 5:50 - loss: 2.7536 - regression_loss: 2.1524 - classification_loss: 0.6012
 225/1000 [=====>........................] - ETA: 5:49 - loss: 2.7620 - regression_loss: 2.1580 - classification_loss: 0.6040
 226/1000 [=====>........................] - ETA: 5:49 - loss: 2.7602 - regression_loss: 2.1568 - classification_loss: 0.6034
 227/1000 [=====>........................] - ETA: 5:49 - loss: 2.7600 - regression_loss: 2.1565 - classification_loss: 0.6034
 228/1000 [=====>........................] - ETA: 5:48 - loss: 2.7616 - regression_loss: 2.1584 - classification_loss: 0.6033
 229/1000 [=====>........................] - ETA: 5:48 - loss: 2.7600 - regression_loss: 2.1574 - classification_loss: 0.6026
 230/1000 [=====>........................] - ETA: 5:47 - loss: 2.7569 - regression_loss: 2.1552 - classification_loss: 0.6017
 231/1000 [=====>........................] - ETA: 5:47 - loss: 2.7615 - regression_loss: 2.1578 - classification_loss: 0.6037
 232/1000 [=====>........................] - ETA: 5:46 - loss: 2.7615 - regression_loss: 2.1588 - classification_loss: 0.6027
 233/1000 [=====>........................] - ETA: 5:46 - loss: 2.7645 - regression_loss: 2.1602 - classification_loss: 0.6042
 234/1000 [======>.......................] - ETA: 5:45 - loss: 2.7665 - regression_loss: 2.1510 - classification_loss: 0.6156
 235/1000 [======>.......................] - ETA: 5:45 - loss: 2.7737 - regression_loss: 2.1574 - classification_loss: 0.6163
 236/1000 [======>.......................] - ETA: 5:44 - loss: 2.7805 - regression_loss: 2.1635 - classification_loss: 0.6170
 237/1000 [======>.......................] - ETA: 5:44 - loss: 2.7792 - regression_loss: 2.1629 - classification_loss: 0.6164
 238/1000 [======>.......................] - ETA: 5:43 - loss: 2.7810 - regression_loss: 2.1659 - classification_loss: 0.6151
 239/1000 [======>.......................] - ETA: 5:43 - loss: 2.7694 - regression_loss: 2.1568 - classification_loss: 0.6125
 240/1000 [======>.......................] - ETA: 5:43 - loss: 2.7580 - regression_loss: 2.1478 - classification_loss: 0.6102
 241/1000 [======>.......................] - ETA: 5:42 - loss: 2.7562 - regression_loss: 2.1460 - classification_loss: 0.6102
 242/1000 [======>.......................] - ETA: 5:42 - loss: 2.7628 - regression_loss: 2.1526 - classification_loss: 0.6102
 243/1000 [======>.......................] - ETA: 5:41 - loss: 2.7667 - regression_loss: 2.1567 - classification_loss: 0.6100
 244/1000 [======>.......................] - ETA: 5:41 - loss: 2.7711 - regression_loss: 2.1606 - classification_loss: 0.6106
 245/1000 [======>.......................] - ETA: 5:40 - loss: 2.7746 - regression_loss: 2.1640 - classification_loss: 0.6105
 246/1000 [======>.......................] - ETA: 5:40 - loss: 2.7633 - regression_loss: 2.1553 - classification_loss: 0.6080
 247/1000 [======>.......................] - ETA: 5:39 - loss: 2.7667 - regression_loss: 2.1587 - classification_loss: 0.6080
 248/1000 [======>.......................] - ETA: 5:39 - loss: 2.7693 - regression_loss: 2.1617 - classification_loss: 0.6076
 249/1000 [======>.......................] - ETA: 5:39 - loss: 2.7706 - regression_loss: 2.1631 - classification_loss: 0.6075
 250/1000 [======>.......................] - ETA: 5:38 - loss: 2.7762 - regression_loss: 2.1646 - classification_loss: 0.6117
 251/1000 [======>.......................] - ETA: 5:38 - loss: 2.7745 - regression_loss: 2.1646 - classification_loss: 0.6099
 252/1000 [======>.......................] - ETA: 5:37 - loss: 2.7635 - regression_loss: 2.1560 - classification_loss: 0.6075
 253/1000 [======>.......................] - ETA: 5:37 - loss: 2.7653 - regression_loss: 2.1574 - classification_loss: 0.6080
 254/1000 [======>.......................] - ETA: 5:36 - loss: 2.7653 - regression_loss: 2.1552 - classification_loss: 0.6102
 255/1000 [======>.......................] - ETA: 5:36 - loss: 2.7724 - regression_loss: 2.1606 - classification_loss: 0.6117
 256/1000 [======>.......................] - ETA: 5:35 - loss: 2.7806 - regression_loss: 2.1645 - classification_loss: 0.6161
 257/1000 [======>.......................] - ETA: 5:35 - loss: 2.7896 - regression_loss: 2.1697 - classification_loss: 0.6199
 258/1000 [======>.......................] - ETA: 5:35 - loss: 2.7925 - regression_loss: 2.1734 - classification_loss: 0.6191
 259/1000 [======>.......................] - ETA: 5:34 - loss: 2.7925 - regression_loss: 2.1724 - classification_loss: 0.6201
 260/1000 [======>.......................] - ETA: 5:34 - loss: 2.7818 - regression_loss: 2.1641 - classification_loss: 0.6177
 261/1000 [======>.......................] - ETA: 5:33 - loss: 2.7870 - regression_loss: 2.1662 - classification_loss: 0.6208
 262/1000 [======>.......................] - ETA: 5:33 - loss: 2.7764 - regression_loss: 2.1579 - classification_loss: 0.6184
 263/1000 [======>.......................] - ETA: 5:32 - loss: 2.7775 - regression_loss: 2.1592 - classification_loss: 0.6183
 264/1000 [======>.......................] - ETA: 5:32 - loss: 2.7825 - regression_loss: 2.1633 - classification_loss: 0.6192
 265/1000 [======>.......................] - ETA: 5:31 - loss: 2.7888 - regression_loss: 2.1662 - classification_loss: 0.6225
 266/1000 [======>.......................] - ETA: 5:31 - loss: 2.7972 - regression_loss: 2.1717 - classification_loss: 0.6255
 267/1000 [=======>......................] - ETA: 5:31 - loss: 2.8003 - regression_loss: 2.1741 - classification_loss: 0.6262
 268/1000 [=======>......................] - ETA: 5:30 - loss: 2.8009 - regression_loss: 2.1754 - classification_loss: 0.6256
 269/1000 [=======>......................] - ETA: 5:30 - loss: 2.8007 - regression_loss: 2.1756 - classification_loss: 0.6251
 270/1000 [=======>......................] - ETA: 5:29 - loss: 2.7903 - regression_loss: 2.1676 - classification_loss: 0.6227
 271/1000 [=======>......................] - ETA: 5:29 - loss: 2.7945 - regression_loss: 2.1709 - classification_loss: 0.6237
 272/1000 [=======>......................] - ETA: 5:28 - loss: 2.7996 - regression_loss: 2.1769 - classification_loss: 0.6227
 273/1000 [=======>......................] - ETA: 5:28 - loss: 2.7895 - regression_loss: 2.1690 - classification_loss: 0.6205
 274/1000 [=======>......................] - ETA: 5:27 - loss: 2.7796 - regression_loss: 2.1610 - classification_loss: 0.6185
 275/1000 [=======>......................] - ETA: 5:27 - loss: 2.7827 - regression_loss: 2.1617 - classification_loss: 0.6211
 276/1000 [=======>......................] - ETA: 5:27 - loss: 2.7877 - regression_loss: 2.1667 - classification_loss: 0.6210
 277/1000 [=======>......................] - ETA: 5:26 - loss: 2.7890 - regression_loss: 2.1686 - classification_loss: 0.6204
 278/1000 [=======>......................] - ETA: 5:26 - loss: 2.7951 - regression_loss: 2.1754 - classification_loss: 0.6197
 279/1000 [=======>......................] - ETA: 5:25 - loss: 2.7941 - regression_loss: 2.1742 - classification_loss: 0.6199
 280/1000 [=======>......................] - ETA: 5:25 - loss: 2.7953 - regression_loss: 2.1763 - classification_loss: 0.6190
 281/1000 [=======>......................] - ETA: 5:24 - loss: 2.7975 - regression_loss: 2.1776 - classification_loss: 0.6199
 282/1000 [=======>......................] - ETA: 5:24 - loss: 2.7983 - regression_loss: 2.1793 - classification_loss: 0.6190
 283/1000 [=======>......................] - ETA: 5:23 - loss: 2.7979 - regression_loss: 2.1783 - classification_loss: 0.6196
 284/1000 [=======>......................] - ETA: 5:23 - loss: 2.8019 - regression_loss: 2.1802 - classification_loss: 0.6217
 285/1000 [=======>......................] - ETA: 5:22 - loss: 2.8075 - regression_loss: 2.1855 - classification_loss: 0.6221
 286/1000 [=======>......................] - ETA: 5:22 - loss: 2.8163 - regression_loss: 2.1899 - classification_loss: 0.6264
 287/1000 [=======>......................] - ETA: 5:22 - loss: 2.8065 - regression_loss: 2.1823 - classification_loss: 0.6242
 288/1000 [=======>......................] - ETA: 5:21 - loss: 2.8067 - regression_loss: 2.1826 - classification_loss: 0.6241
 289/1000 [=======>......................] - ETA: 5:21 - loss: 2.8047 - regression_loss: 2.1816 - classification_loss: 0.6232
 290/1000 [=======>......................] - ETA: 5:20 - loss: 2.8064 - regression_loss: 2.1840 - classification_loss: 0.6224
 291/1000 [=======>......................] - ETA: 5:20 - loss: 2.8068 - regression_loss: 2.1842 - classification_loss: 0.6226
 292/1000 [=======>......................] - ETA: 5:19 - loss: 2.8054 - regression_loss: 2.1825 - classification_loss: 0.6229
 293/1000 [=======>......................] - ETA: 5:19 - loss: 2.8064 - regression_loss: 2.1838 - classification_loss: 0.6227
 294/1000 [=======>......................] - ETA: 5:18 - loss: 2.8038 - regression_loss: 2.1820 - classification_loss: 0.6218
 295/1000 [=======>......................] - ETA: 5:18 - loss: 2.8018 - regression_loss: 2.1803 - classification_loss: 0.6214
 296/1000 [=======>......................] - ETA: 5:18 - loss: 2.8098 - regression_loss: 2.1877 - classification_loss: 0.6220
 297/1000 [=======>......................] - ETA: 5:17 - loss: 2.8142 - regression_loss: 2.1914 - classification_loss: 0.6228
 298/1000 [=======>......................] - ETA: 5:17 - loss: 2.8147 - regression_loss: 2.1916 - classification_loss: 0.6230
 299/1000 [=======>......................] - ETA: 5:16 - loss: 2.8052 - regression_loss: 2.1843 - classification_loss: 0.6210
 300/1000 [========>.....................] - ETA: 5:16 - loss: 2.8118 - regression_loss: 2.1893 - classification_loss: 0.6224
 301/1000 [========>.....................] - ETA: 5:15 - loss: 2.8024 - regression_loss: 2.1820 - classification_loss: 0.6204
 302/1000 [========>.....................] - ETA: 5:15 - loss: 2.7931 - regression_loss: 2.1748 - classification_loss: 0.6183
 303/1000 [========>.....................] - ETA: 5:15 - loss: 2.7943 - regression_loss: 2.1759 - classification_loss: 0.6184
 304/1000 [========>.....................] - ETA: 5:14 - loss: 2.7949 - regression_loss: 2.1763 - classification_loss: 0.6186
 305/1000 [========>.....................] - ETA: 5:14 - loss: 2.7950 - regression_loss: 2.1765 - classification_loss: 0.6184
 306/1000 [========>.....................] - ETA: 5:13 - loss: 2.7991 - regression_loss: 2.1804 - classification_loss: 0.6188
 307/1000 [========>.....................] - ETA: 5:13 - loss: 2.8001 - regression_loss: 2.1815 - classification_loss: 0.6185
 308/1000 [========>.....................] - ETA: 5:12 - loss: 2.7910 - regression_loss: 2.1745 - classification_loss: 0.6166
 309/1000 [========>.....................] - ETA: 5:12 - loss: 2.7887 - regression_loss: 2.1731 - classification_loss: 0.6156
 310/1000 [========>.....................] - ETA: 5:11 - loss: 2.7798 - regression_loss: 2.1661 - classification_loss: 0.6137
 311/1000 [========>.....................] - ETA: 5:11 - loss: 2.7855 - regression_loss: 2.1699 - classification_loss: 0.6156
 312/1000 [========>.....................] - ETA: 5:11 - loss: 2.7830 - regression_loss: 2.1684 - classification_loss: 0.6146
 313/1000 [========>.....................] - ETA: 5:10 - loss: 2.7856 - regression_loss: 2.1716 - classification_loss: 0.6141
 314/1000 [========>.....................] - ETA: 5:10 - loss: 2.7768 - regression_loss: 2.1647 - classification_loss: 0.6121
 315/1000 [========>.....................] - ETA: 5:09 - loss: 2.7829 - regression_loss: 2.1700 - classification_loss: 0.6130
 316/1000 [========>.....................] - ETA: 5:09 - loss: 2.7831 - regression_loss: 2.1704 - classification_loss: 0.6127
 317/1000 [========>.....................] - ETA: 5:08 - loss: 2.7743 - regression_loss: 2.1635 - classification_loss: 0.6108
 318/1000 [========>.....................] - ETA: 5:08 - loss: 2.7741 - regression_loss: 2.1642 - classification_loss: 0.6099
 319/1000 [========>.....................] - ETA: 5:07 - loss: 2.7654 - regression_loss: 2.1574 - classification_loss: 0.6080
 320/1000 [========>.....................] - ETA: 5:07 - loss: 2.7722 - regression_loss: 2.1635 - classification_loss: 0.6087
 321/1000 [========>.....................] - ETA: 5:06 - loss: 2.7765 - regression_loss: 2.1669 - classification_loss: 0.6096
 322/1000 [========>.....................] - ETA: 5:06 - loss: 2.7679 - regression_loss: 2.1602 - classification_loss: 0.6077
 323/1000 [========>.....................] - ETA: 5:06 - loss: 2.7716 - regression_loss: 2.1623 - classification_loss: 0.6093
 324/1000 [========>.....................] - ETA: 5:05 - loss: 2.7753 - regression_loss: 2.1642 - classification_loss: 0.6111
 325/1000 [========>.....................] - ETA: 5:05 - loss: 2.7769 - regression_loss: 2.1661 - classification_loss: 0.6108
 326/1000 [========>.....................] - ETA: 5:04 - loss: 2.7784 - regression_loss: 2.1678 - classification_loss: 0.6106
 327/1000 [========>.....................] - ETA: 5:04 - loss: 2.7795 - regression_loss: 2.1683 - classification_loss: 0.6112
 328/1000 [========>.....................] - ETA: 5:03 - loss: 2.7769 - regression_loss: 2.1667 - classification_loss: 0.6102
 329/1000 [========>.....................] - ETA: 5:03 - loss: 2.7684 - regression_loss: 2.1601 - classification_loss: 0.6083
 330/1000 [========>.....................] - ETA: 5:02 - loss: 2.7677 - regression_loss: 2.1599 - classification_loss: 0.6078
 331/1000 [========>.....................] - ETA: 5:02 - loss: 2.7667 - regression_loss: 2.1596 - classification_loss: 0.6071
 332/1000 [========>.....................] - ETA: 5:01 - loss: 2.7586 - regression_loss: 2.1531 - classification_loss: 0.6055
 333/1000 [========>.....................] - ETA: 5:01 - loss: 2.7569 - regression_loss: 2.1520 - classification_loss: 0.6048
 334/1000 [=========>....................] - ETA: 5:01 - loss: 2.7565 - regression_loss: 2.1525 - classification_loss: 0.6040
 335/1000 [=========>....................] - ETA: 5:00 - loss: 2.7604 - regression_loss: 2.1559 - classification_loss: 0.6045
 336/1000 [=========>....................] - ETA: 5:00 - loss: 2.7657 - regression_loss: 2.1607 - classification_loss: 0.6050
 337/1000 [=========>....................] - ETA: 4:59 - loss: 2.7678 - regression_loss: 2.1628 - classification_loss: 0.6050
 338/1000 [=========>....................] - ETA: 4:59 - loss: 2.7665 - regression_loss: 2.1622 - classification_loss: 0.6043
 339/1000 [=========>....................] - ETA: 4:58 - loss: 2.7583 - regression_loss: 2.1558 - classification_loss: 0.6025
 340/1000 [=========>....................] - ETA: 4:58 - loss: 2.7502 - regression_loss: 2.1495 - classification_loss: 0.6008
 341/1000 [=========>....................] - ETA: 4:57 - loss: 2.7491 - regression_loss: 2.1486 - classification_loss: 0.6005
 342/1000 [=========>....................] - ETA: 4:57 - loss: 2.7411 - regression_loss: 2.1423 - classification_loss: 0.5988
 343/1000 [=========>....................] - ETA: 4:56 - loss: 2.7331 - regression_loss: 2.1361 - classification_loss: 0.5971
 344/1000 [=========>....................] - ETA: 4:56 - loss: 2.7252 - regression_loss: 2.1299 - classification_loss: 0.5953
 345/1000 [=========>....................] - ETA: 4:56 - loss: 2.7274 - regression_loss: 2.1315 - classification_loss: 0.5959
 346/1000 [=========>....................] - ETA: 4:55 - loss: 2.7263 - regression_loss: 2.1309 - classification_loss: 0.5954
 347/1000 [=========>....................] - ETA: 4:55 - loss: 2.7184 - regression_loss: 2.1248 - classification_loss: 0.5937
 348/1000 [=========>....................] - ETA: 4:54 - loss: 2.7170 - regression_loss: 2.1235 - classification_loss: 0.5935
 349/1000 [=========>....................] - ETA: 4:54 - loss: 2.7212 - regression_loss: 2.1278 - classification_loss: 0.5935
 350/1000 [=========>....................] - ETA: 4:53 - loss: 2.7135 - regression_loss: 2.1217 - classification_loss: 0.5918
 351/1000 [=========>....................] - ETA: 4:53 - loss: 2.7137 - regression_loss: 2.1221 - classification_loss: 0.5916
 352/1000 [=========>....................] - ETA: 4:52 - loss: 2.7196 - regression_loss: 2.1235 - classification_loss: 0.5961
 353/1000 [=========>....................] - ETA: 4:52 - loss: 2.7183 - regression_loss: 2.1227 - classification_loss: 0.5957
 354/1000 [=========>....................] - ETA: 4:51 - loss: 2.7200 - regression_loss: 2.1226 - classification_loss: 0.5974
 355/1000 [=========>....................] - ETA: 4:51 - loss: 2.7220 - regression_loss: 2.1254 - classification_loss: 0.5966
 356/1000 [=========>....................] - ETA: 4:51 - loss: 2.7157 - regression_loss: 2.1194 - classification_loss: 0.5963
 357/1000 [=========>....................] - ETA: 4:50 - loss: 2.7198 - regression_loss: 2.1224 - classification_loss: 0.5975
 358/1000 [=========>....................] - ETA: 4:50 - loss: 2.7122 - regression_loss: 2.1164 - classification_loss: 0.5958
 359/1000 [=========>....................] - ETA: 4:49 - loss: 2.7051 - regression_loss: 2.1105 - classification_loss: 0.5946
 360/1000 [=========>....................] - ETA: 4:49 - loss: 2.6976 - regression_loss: 2.1047 - classification_loss: 0.5930
 361/1000 [=========>....................] - ETA: 4:48 - loss: 2.6902 - regression_loss: 2.0989 - classification_loss: 0.5913
 362/1000 [=========>....................] - ETA: 4:48 - loss: 2.6923 - regression_loss: 2.1011 - classification_loss: 0.5912
 363/1000 [=========>....................] - ETA: 4:47 - loss: 2.6950 - regression_loss: 2.1029 - classification_loss: 0.5921
 364/1000 [=========>....................] - ETA: 4:47 - loss: 2.6975 - regression_loss: 2.1042 - classification_loss: 0.5933
 365/1000 [=========>....................] - ETA: 4:47 - loss: 2.6901 - regression_loss: 2.0985 - classification_loss: 0.5917
 366/1000 [=========>....................] - ETA: 4:46 - loss: 2.6926 - regression_loss: 2.1007 - classification_loss: 0.5919
 367/1000 [==========>...................] - ETA: 4:46 - loss: 2.6974 - regression_loss: 2.1029 - classification_loss: 0.5945
 368/1000 [==========>...................] - ETA: 4:45 - loss: 2.6901 - regression_loss: 2.0972 - classification_loss: 0.5929
 369/1000 [==========>...................] - ETA: 4:45 - loss: 2.6956 - regression_loss: 2.0992 - classification_loss: 0.5964
 370/1000 [==========>...................] - ETA: 4:44 - loss: 2.7027 - regression_loss: 2.1012 - classification_loss: 0.6015
 371/1000 [==========>...................] - ETA: 4:44 - loss: 2.7033 - regression_loss: 2.1023 - classification_loss: 0.6010
 372/1000 [==========>...................] - ETA: 4:43 - loss: 2.6963 - regression_loss: 2.0966 - classification_loss: 0.5996
 373/1000 [==========>...................] - ETA: 4:43 - loss: 2.6890 - regression_loss: 2.0910 - classification_loss: 0.5980
 374/1000 [==========>...................] - ETA: 4:42 - loss: 2.6818 - regression_loss: 2.0854 - classification_loss: 0.5964
 375/1000 [==========>...................] - ETA: 4:42 - loss: 2.6747 - regression_loss: 2.0798 - classification_loss: 0.5948
 376/1000 [==========>...................] - ETA: 4:42 - loss: 2.6760 - regression_loss: 2.0810 - classification_loss: 0.5950
 377/1000 [==========>...................] - ETA: 4:41 - loss: 2.6782 - regression_loss: 2.0826 - classification_loss: 0.5956
 378/1000 [==========>...................] - ETA: 4:41 - loss: 2.6711 - regression_loss: 2.0771 - classification_loss: 0.5940
 379/1000 [==========>...................] - ETA: 4:40 - loss: 2.6769 - regression_loss: 2.0802 - classification_loss: 0.5968
 380/1000 [==========>...................] - ETA: 4:40 - loss: 2.6776 - regression_loss: 2.0814 - classification_loss: 0.5962
 381/1000 [==========>...................] - ETA: 4:39 - loss: 2.6706 - regression_loss: 2.0759 - classification_loss: 0.5947
 382/1000 [==========>...................] - ETA: 4:39 - loss: 2.6636 - regression_loss: 2.0705 - classification_loss: 0.5931
 383/1000 [==========>...................] - ETA: 4:38 - loss: 2.6650 - regression_loss: 2.0708 - classification_loss: 0.5942
 384/1000 [==========>...................] - ETA: 4:38 - loss: 2.6627 - regression_loss: 2.0695 - classification_loss: 0.5932
 385/1000 [==========>...................] - ETA: 4:37 - loss: 2.6616 - regression_loss: 2.0689 - classification_loss: 0.5927
 386/1000 [==========>...................] - ETA: 4:37 - loss: 2.6648 - regression_loss: 2.0712 - classification_loss: 0.5936
 387/1000 [==========>...................] - ETA: 4:37 - loss: 2.6579 - regression_loss: 2.0659 - classification_loss: 0.5921
 388/1000 [==========>...................] - ETA: 4:36 - loss: 2.6511 - regression_loss: 2.0605 - classification_loss: 0.5905
 389/1000 [==========>...................] - ETA: 4:36 - loss: 2.6445 - regression_loss: 2.0552 - classification_loss: 0.5892
 390/1000 [==========>...................] - ETA: 4:35 - loss: 2.6469 - regression_loss: 2.0575 - classification_loss: 0.5895
 391/1000 [==========>...................] - ETA: 4:35 - loss: 2.6480 - regression_loss: 2.0582 - classification_loss: 0.5899
 392/1000 [==========>...................] - ETA: 4:34 - loss: 2.6499 - regression_loss: 2.0605 - classification_loss: 0.5893
 393/1000 [==========>...................] - ETA: 4:34 - loss: 2.6512 - regression_loss: 2.0618 - classification_loss: 0.5894
 394/1000 [==========>...................] - ETA: 4:33 - loss: 2.6530 - regression_loss: 2.0638 - classification_loss: 0.5892
 395/1000 [==========>...................] - ETA: 4:33 - loss: 2.6594 - regression_loss: 2.0655 - classification_loss: 0.5939
 396/1000 [==========>...................] - ETA: 4:33 - loss: 2.6584 - regression_loss: 2.0652 - classification_loss: 0.5932
 397/1000 [==========>...................] - ETA: 4:32 - loss: 2.6652 - regression_loss: 2.0693 - classification_loss: 0.5958
 398/1000 [==========>...................] - ETA: 4:32 - loss: 2.6735 - regression_loss: 2.0742 - classification_loss: 0.5993
 399/1000 [==========>...................] - ETA: 4:31 - loss: 2.6757 - regression_loss: 2.0762 - classification_loss: 0.5996
 400/1000 [===========>..................] - ETA: 4:31 - loss: 2.6773 - regression_loss: 2.0776 - classification_loss: 0.5997
 401/1000 [===========>..................] - ETA: 4:30 - loss: 2.6787 - regression_loss: 2.0793 - classification_loss: 0.5993
 402/1000 [===========>..................] - ETA: 4:30 - loss: 2.6720 - regression_loss: 2.0742 - classification_loss: 0.5978
 403/1000 [===========>..................] - ETA: 4:29 - loss: 2.6654 - regression_loss: 2.0690 - classification_loss: 0.5964
 404/1000 [===========>..................] - ETA: 4:29 - loss: 2.6588 - regression_loss: 2.0639 - classification_loss: 0.5949
 405/1000 [===========>..................] - ETA: 4:29 - loss: 2.6522 - regression_loss: 2.0588 - classification_loss: 0.5934
 406/1000 [===========>..................] - ETA: 4:28 - loss: 2.6519 - regression_loss: 2.0590 - classification_loss: 0.5929
 407/1000 [===========>..................] - ETA: 4:28 - loss: 2.6517 - regression_loss: 2.0587 - classification_loss: 0.5930
 408/1000 [===========>..................] - ETA: 4:27 - loss: 2.6544 - regression_loss: 2.0610 - classification_loss: 0.5934
 409/1000 [===========>..................] - ETA: 4:27 - loss: 2.6541 - regression_loss: 2.0598 - classification_loss: 0.5942
 410/1000 [===========>..................] - ETA: 4:26 - loss: 2.6597 - regression_loss: 2.0642 - classification_loss: 0.5956
 411/1000 [===========>..................] - ETA: 4:26 - loss: 2.6533 - regression_loss: 2.0592 - classification_loss: 0.5941
 412/1000 [===========>..................] - ETA: 4:25 - loss: 2.6574 - regression_loss: 2.0608 - classification_loss: 0.5966
 413/1000 [===========>..................] - ETA: 4:25 - loss: 2.6600 - regression_loss: 2.0630 - classification_loss: 0.5970
 414/1000 [===========>..................] - ETA: 4:24 - loss: 2.6615 - regression_loss: 2.0639 - classification_loss: 0.5976
 415/1000 [===========>..................] - ETA: 4:24 - loss: 2.6680 - regression_loss: 2.0680 - classification_loss: 0.6000
 416/1000 [===========>..................] - ETA: 4:24 - loss: 2.6616 - regression_loss: 2.0631 - classification_loss: 0.5985
 417/1000 [===========>..................] - ETA: 4:23 - loss: 2.6706 - regression_loss: 2.0711 - classification_loss: 0.5995
 418/1000 [===========>..................] - ETA: 4:23 - loss: 2.6712 - regression_loss: 2.0716 - classification_loss: 0.5996
 419/1000 [===========>..................] - ETA: 4:22 - loss: 2.6743 - regression_loss: 2.0740 - classification_loss: 0.6003
 420/1000 [===========>..................] - ETA: 4:22 - loss: 2.6680 - regression_loss: 2.0690 - classification_loss: 0.5990
 421/1000 [===========>..................] - ETA: 4:21 - loss: 2.6681 - regression_loss: 2.0689 - classification_loss: 0.5992
 422/1000 [===========>..................] - ETA: 4:21 - loss: 2.6713 - regression_loss: 2.0724 - classification_loss: 0.5989
 423/1000 [===========>..................] - ETA: 4:20 - loss: 2.6715 - regression_loss: 2.0724 - classification_loss: 0.5991
 424/1000 [===========>..................] - ETA: 4:20 - loss: 2.6736 - regression_loss: 2.0723 - classification_loss: 0.6013
 425/1000 [===========>..................] - ETA: 4:20 - loss: 2.6783 - regression_loss: 2.0749 - classification_loss: 0.6033
 426/1000 [===========>..................] - ETA: 4:19 - loss: 2.6720 - regression_loss: 2.0701 - classification_loss: 0.6019
 427/1000 [===========>..................] - ETA: 4:19 - loss: 2.6657 - regression_loss: 2.0652 - classification_loss: 0.6005
 428/1000 [===========>..................] - ETA: 4:18 - loss: 2.6668 - regression_loss: 2.0660 - classification_loss: 0.6007
 429/1000 [===========>..................] - ETA: 4:18 - loss: 2.6683 - regression_loss: 2.0677 - classification_loss: 0.6005
 430/1000 [===========>..................] - ETA: 4:17 - loss: 2.6665 - regression_loss: 2.0666 - classification_loss: 0.5999
 431/1000 [===========>..................] - ETA: 4:17 - loss: 2.6650 - regression_loss: 2.0656 - classification_loss: 0.5994
 432/1000 [===========>..................] - ETA: 4:16 - loss: 2.6628 - regression_loss: 2.0608 - classification_loss: 0.6020
 433/1000 [===========>..................] - ETA: 4:16 - loss: 2.6672 - regression_loss: 2.0628 - classification_loss: 0.6044
 434/1000 [============>.................] - ETA: 4:15 - loss: 2.6700 - regression_loss: 2.0656 - classification_loss: 0.6044
 435/1000 [============>.................] - ETA: 4:15 - loss: 2.6638 - regression_loss: 2.0608 - classification_loss: 0.6030
 436/1000 [============>.................] - ETA: 4:15 - loss: 2.6655 - regression_loss: 2.0622 - classification_loss: 0.6032
 437/1000 [============>.................] - ETA: 4:14 - loss: 2.6596 - regression_loss: 2.0575 - classification_loss: 0.6020
 438/1000 [============>.................] - ETA: 4:14 - loss: 2.6658 - regression_loss: 2.0604 - classification_loss: 0.6054
 439/1000 [============>.................] - ETA: 4:13 - loss: 2.6683 - regression_loss: 2.0629 - classification_loss: 0.6054
 440/1000 [============>.................] - ETA: 4:13 - loss: 2.6699 - regression_loss: 2.0639 - classification_loss: 0.6060
 441/1000 [============>.................] - ETA: 4:12 - loss: 2.6688 - regression_loss: 2.0630 - classification_loss: 0.6058
 442/1000 [============>.................] - ETA: 4:12 - loss: 2.6731 - regression_loss: 2.0666 - classification_loss: 0.6065
 443/1000 [============>.................] - ETA: 4:11 - loss: 2.6720 - regression_loss: 2.0658 - classification_loss: 0.6062
 444/1000 [============>.................] - ETA: 4:11 - loss: 2.6731 - regression_loss: 2.0664 - classification_loss: 0.6067
 445/1000 [============>.................] - ETA: 4:10 - loss: 2.6671 - regression_loss: 2.0618 - classification_loss: 0.6053
 446/1000 [============>.................] - ETA: 4:10 - loss: 2.6641 - regression_loss: 2.0597 - classification_loss: 0.6044
 447/1000 [============>.................] - ETA: 4:10 - loss: 2.6674 - regression_loss: 2.0629 - classification_loss: 0.6045
 448/1000 [============>.................] - ETA: 4:09 - loss: 2.6654 - regression_loss: 2.0615 - classification_loss: 0.6040
 449/1000 [============>.................] - ETA: 4:09 - loss: 2.6706 - regression_loss: 2.0647 - classification_loss: 0.6059
 450/1000 [============>.................] - ETA: 4:08 - loss: 2.6744 - regression_loss: 2.0681 - classification_loss: 0.6064
 451/1000 [============>.................] - ETA: 4:08 - loss: 2.6804 - regression_loss: 2.0722 - classification_loss: 0.6082
 452/1000 [============>.................] - ETA: 4:07 - loss: 2.6811 - regression_loss: 2.0735 - classification_loss: 0.6076
 453/1000 [============>.................] - ETA: 4:07 - loss: 2.6828 - regression_loss: 2.0752 - classification_loss: 0.6076
 454/1000 [============>.................] - ETA: 4:06 - loss: 2.6769 - regression_loss: 2.0706 - classification_loss: 0.6062
 455/1000 [============>.................] - ETA: 4:06 - loss: 2.6777 - regression_loss: 2.0718 - classification_loss: 0.6059
 456/1000 [============>.................] - ETA: 4:06 - loss: 2.6770 - regression_loss: 2.0718 - classification_loss: 0.6052
 457/1000 [============>.................] - ETA: 4:05 - loss: 2.6776 - regression_loss: 2.0729 - classification_loss: 0.6048
 458/1000 [============>.................] - ETA: 4:05 - loss: 2.6816 - regression_loss: 2.0756 - classification_loss: 0.6060
 459/1000 [============>.................] - ETA: 4:04 - loss: 2.6758 - regression_loss: 2.0710 - classification_loss: 0.6047
 460/1000 [============>.................] - ETA: 4:04 - loss: 2.6772 - regression_loss: 2.0730 - classification_loss: 0.6042
 461/1000 [============>.................] - ETA: 4:03 - loss: 2.6806 - regression_loss: 2.0756 - classification_loss: 0.6050
 462/1000 [============>.................] - ETA: 4:03 - loss: 2.6812 - regression_loss: 2.0762 - classification_loss: 0.6050
 463/1000 [============>.................] - ETA: 4:02 - loss: 2.6831 - regression_loss: 2.0783 - classification_loss: 0.6048
 464/1000 [============>.................] - ETA: 4:02 - loss: 2.6773 - regression_loss: 2.0738 - classification_loss: 0.6035
 465/1000 [============>.................] - ETA: 4:01 - loss: 2.6791 - regression_loss: 2.0742 - classification_loss: 0.6049
 466/1000 [============>.................] - ETA: 4:01 - loss: 2.6791 - regression_loss: 2.0747 - classification_loss: 0.6044
 467/1000 [=============>................] - ETA: 4:01 - loss: 2.6815 - regression_loss: 2.0766 - classification_loss: 0.6048
 468/1000 [=============>................] - ETA: 4:00 - loss: 2.6814 - regression_loss: 2.0769 - classification_loss: 0.6045
 469/1000 [=============>................] - ETA: 4:00 - loss: 2.6841 - regression_loss: 2.0791 - classification_loss: 0.6049
 470/1000 [=============>................] - ETA: 3:59 - loss: 2.6837 - regression_loss: 2.0794 - classification_loss: 0.6043
 471/1000 [=============>................] - ETA: 3:59 - loss: 2.6841 - regression_loss: 2.0800 - classification_loss: 0.6041
 472/1000 [=============>................] - ETA: 3:58 - loss: 2.6836 - regression_loss: 2.0801 - classification_loss: 0.6035
 473/1000 [=============>................] - ETA: 3:58 - loss: 2.6849 - regression_loss: 2.0814 - classification_loss: 0.6035
 474/1000 [=============>................] - ETA: 3:57 - loss: 2.6793 - regression_loss: 2.0770 - classification_loss: 0.6023
 475/1000 [=============>................] - ETA: 3:57 - loss: 2.6823 - regression_loss: 2.0781 - classification_loss: 0.6042
 476/1000 [=============>................] - ETA: 3:56 - loss: 2.6863 - regression_loss: 2.0809 - classification_loss: 0.6054
 477/1000 [=============>................] - ETA: 3:56 - loss: 2.6854 - regression_loss: 2.0808 - classification_loss: 0.6046
 478/1000 [=============>................] - ETA: 3:56 - loss: 2.6845 - regression_loss: 2.0806 - classification_loss: 0.6038
 479/1000 [=============>................] - ETA: 3:55 - loss: 2.6789 - regression_loss: 2.0763 - classification_loss: 0.6026
 480/1000 [=============>................] - ETA: 3:55 - loss: 2.6734 - regression_loss: 2.0719 - classification_loss: 0.6015
 481/1000 [=============>................] - ETA: 3:54 - loss: 2.6766 - regression_loss: 2.0736 - classification_loss: 0.6031
 482/1000 [=============>................] - ETA: 3:54 - loss: 2.6743 - regression_loss: 2.0718 - classification_loss: 0.6025
 483/1000 [=============>................] - ETA: 3:53 - loss: 2.6784 - regression_loss: 2.0740 - classification_loss: 0.6045
 484/1000 [=============>................] - ETA: 3:53 - loss: 2.6729 - regression_loss: 2.0697 - classification_loss: 0.6032
 485/1000 [=============>................] - ETA: 3:52 - loss: 2.6763 - regression_loss: 2.0722 - classification_loss: 0.6042
 486/1000 [=============>................] - ETA: 3:52 - loss: 2.6708 - regression_loss: 2.0679 - classification_loss: 0.6029
 487/1000 [=============>................] - ETA: 3:51 - loss: 2.6653 - regression_loss: 2.0636 - classification_loss: 0.6017
 488/1000 [=============>................] - ETA: 3:51 - loss: 2.6643 - regression_loss: 2.0628 - classification_loss: 0.6014
 489/1000 [=============>................] - ETA: 3:51 - loss: 2.6588 - regression_loss: 2.0586 - classification_loss: 0.6002
 490/1000 [=============>................] - ETA: 3:50 - loss: 2.6581 - regression_loss: 2.0586 - classification_loss: 0.5996
 491/1000 [=============>................] - ETA: 3:50 - loss: 2.6587 - regression_loss: 2.0587 - classification_loss: 0.6000
 492/1000 [=============>................] - ETA: 3:49 - loss: 2.6619 - regression_loss: 2.0595 - classification_loss: 0.6024
 493/1000 [=============>................] - ETA: 3:49 - loss: 2.6668 - regression_loss: 2.0645 - classification_loss: 0.6024
 494/1000 [=============>................] - ETA: 3:48 - loss: 2.6614 - regression_loss: 2.0603 - classification_loss: 0.6011
 495/1000 [=============>................] - ETA: 3:48 - loss: 2.6663 - regression_loss: 2.0628 - classification_loss: 0.6035
 496/1000 [=============>................] - ETA: 3:47 - loss: 2.6668 - regression_loss: 2.0626 - classification_loss: 0.6042
 497/1000 [=============>................] - ETA: 3:47 - loss: 2.6697 - regression_loss: 2.0654 - classification_loss: 0.6042
 498/1000 [=============>................] - ETA: 3:46 - loss: 2.6695 - regression_loss: 2.0655 - classification_loss: 0.6040
 499/1000 [=============>................] - ETA: 3:46 - loss: 2.6700 - regression_loss: 2.0661 - classification_loss: 0.6039
 500/1000 [==============>...............] - ETA: 3:46 - loss: 2.6654 - regression_loss: 2.0620 - classification_loss: 0.6034
 501/1000 [==============>...............] - ETA: 3:45 - loss: 2.6652 - regression_loss: 2.0620 - classification_loss: 0.6032
 502/1000 [==============>...............] - ETA: 3:45 - loss: 2.6674 - regression_loss: 2.0631 - classification_loss: 0.6043
 503/1000 [==============>...............] - ETA: 3:44 - loss: 2.6669 - regression_loss: 2.0632 - classification_loss: 0.6037
 504/1000 [==============>...............] - ETA: 3:44 - loss: 2.6616 - regression_loss: 2.0591 - classification_loss: 0.6025
 505/1000 [==============>...............] - ETA: 3:43 - loss: 2.6609 - regression_loss: 2.0592 - classification_loss: 0.6016
 506/1000 [==============>...............] - ETA: 3:43 - loss: 2.6556 - regression_loss: 2.0552 - classification_loss: 0.6004
 507/1000 [==============>...............] - ETA: 3:42 - loss: 2.6547 - regression_loss: 2.0549 - classification_loss: 0.5998
 508/1000 [==============>...............] - ETA: 3:42 - loss: 2.6494 - regression_loss: 2.0508 - classification_loss: 0.5986
 509/1000 [==============>...............] - ETA: 3:41 - loss: 2.6518 - regression_loss: 2.0510 - classification_loss: 0.6008
 510/1000 [==============>...............] - ETA: 3:41 - loss: 2.6504 - regression_loss: 2.0501 - classification_loss: 0.6003
 511/1000 [==============>...............] - ETA: 3:41 - loss: 2.6535 - regression_loss: 2.0509 - classification_loss: 0.6026
 512/1000 [==============>...............] - ETA: 3:40 - loss: 2.6483 - regression_loss: 2.0469 - classification_loss: 0.6014
 513/1000 [==============>...............] - ETA: 3:40 - loss: 2.6508 - regression_loss: 2.0482 - classification_loss: 0.6026
 514/1000 [==============>...............] - ETA: 3:39 - loss: 2.6535 - regression_loss: 2.0486 - classification_loss: 0.6048
 515/1000 [==============>...............] - ETA: 3:39 - loss: 2.6483 - regression_loss: 2.0447 - classification_loss: 0.6037
 516/1000 [==============>...............] - ETA: 3:38 - loss: 2.6516 - regression_loss: 2.0465 - classification_loss: 0.6051
 517/1000 [==============>...............] - ETA: 3:38 - loss: 2.6554 - regression_loss: 2.0493 - classification_loss: 0.6060
 518/1000 [==============>...............] - ETA: 3:37 - loss: 2.6578 - regression_loss: 2.0519 - classification_loss: 0.6059
 519/1000 [==============>...............] - ETA: 3:37 - loss: 2.6527 - regression_loss: 2.0480 - classification_loss: 0.6047
 520/1000 [==============>...............] - ETA: 3:37 - loss: 2.6575 - regression_loss: 2.0497 - classification_loss: 0.6078
 521/1000 [==============>...............] - ETA: 3:36 - loss: 2.6584 - regression_loss: 2.0503 - classification_loss: 0.6081
 522/1000 [==============>...............] - ETA: 3:36 - loss: 2.6559 - regression_loss: 2.0483 - classification_loss: 0.6076
 523/1000 [==============>...............] - ETA: 3:35 - loss: 2.6582 - regression_loss: 2.0501 - classification_loss: 0.6081
 524/1000 [==============>...............] - ETA: 3:35 - loss: 2.6594 - regression_loss: 2.0515 - classification_loss: 0.6079
 525/1000 [==============>...............] - ETA: 3:34 - loss: 2.6615 - regression_loss: 2.0530 - classification_loss: 0.6085
 526/1000 [==============>...............] - ETA: 3:34 - loss: 2.6629 - regression_loss: 2.0549 - classification_loss: 0.6080
 527/1000 [==============>...............] - ETA: 3:33 - loss: 2.6659 - regression_loss: 2.0561 - classification_loss: 0.6098
 528/1000 [==============>...............] - ETA: 3:33 - loss: 2.6660 - regression_loss: 2.0569 - classification_loss: 0.6091
 529/1000 [==============>...............] - ETA: 3:32 - loss: 2.6610 - regression_loss: 2.0530 - classification_loss: 0.6080
 530/1000 [==============>...............] - ETA: 3:32 - loss: 2.6642 - regression_loss: 2.0551 - classification_loss: 0.6091
 531/1000 [==============>...............] - ETA: 3:32 - loss: 2.6679 - regression_loss: 2.0581 - classification_loss: 0.6098
 532/1000 [==============>...............] - ETA: 3:31 - loss: 2.6676 - regression_loss: 2.0576 - classification_loss: 0.6101
 533/1000 [==============>...............] - ETA: 3:31 - loss: 2.6657 - regression_loss: 2.0563 - classification_loss: 0.6095
 534/1000 [===============>..............] - ETA: 3:30 - loss: 2.6653 - regression_loss: 2.0561 - classification_loss: 0.6092
 535/1000 [===============>..............] - ETA: 3:30 - loss: 2.6621 - regression_loss: 2.0522 - classification_loss: 0.6098
 536/1000 [===============>..............] - ETA: 3:29 - loss: 2.6640 - regression_loss: 2.0541 - classification_loss: 0.6098
 537/1000 [===============>..............] - ETA: 3:29 - loss: 2.6590 - regression_loss: 2.0503 - classification_loss: 0.6087
 538/1000 [===============>..............] - ETA: 3:28 - loss: 2.6590 - regression_loss: 2.0504 - classification_loss: 0.6086
 539/1000 [===============>..............] - ETA: 3:28 - loss: 2.6596 - regression_loss: 2.0512 - classification_loss: 0.6084
 540/1000 [===============>..............] - ETA: 3:27 - loss: 2.6608 - regression_loss: 2.0527 - classification_loss: 0.6081
 541/1000 [===============>..............] - ETA: 3:27 - loss: 2.6623 - regression_loss: 2.0546 - classification_loss: 0.6077
 542/1000 [===============>..............] - ETA: 3:27 - loss: 2.6626 - regression_loss: 2.0556 - classification_loss: 0.6070
 543/1000 [===============>..............] - ETA: 3:26 - loss: 2.6577 - regression_loss: 2.0518 - classification_loss: 0.6059
 544/1000 [===============>..............] - ETA: 3:26 - loss: 2.6638 - regression_loss: 2.0555 - classification_loss: 0.6083
 545/1000 [===============>..............] - ETA: 3:25 - loss: 2.6669 - regression_loss: 2.0570 - classification_loss: 0.6099
 546/1000 [===============>..............] - ETA: 3:25 - loss: 2.6672 - regression_loss: 2.0574 - classification_loss: 0.6098
 547/1000 [===============>..............] - ETA: 3:24 - loss: 2.6664 - regression_loss: 2.0570 - classification_loss: 0.6093
 548/1000 [===============>..............] - ETA: 3:24 - loss: 2.6615 - regression_loss: 2.0533 - classification_loss: 0.6082
 549/1000 [===============>..............] - ETA: 3:23 - loss: 2.6628 - regression_loss: 2.0551 - classification_loss: 0.6077
 550/1000 [===============>..............] - ETA: 3:23 - loss: 2.6672 - regression_loss: 2.0593 - classification_loss: 0.6079
 551/1000 [===============>..............] - ETA: 3:22 - loss: 2.6697 - regression_loss: 2.0608 - classification_loss: 0.6090
 552/1000 [===============>..............] - ETA: 3:22 - loss: 2.6708 - regression_loss: 2.0623 - classification_loss: 0.6086
 553/1000 [===============>..............] - ETA: 3:22 - loss: 2.6660 - regression_loss: 2.0586 - classification_loss: 0.6075
 554/1000 [===============>..............] - ETA: 3:21 - loss: 2.6661 - regression_loss: 2.0589 - classification_loss: 0.6072
 555/1000 [===============>..............] - ETA: 3:21 - loss: 2.6663 - regression_loss: 2.0590 - classification_loss: 0.6072
 556/1000 [===============>..............] - ETA: 3:20 - loss: 2.6694 - regression_loss: 2.0619 - classification_loss: 0.6075
 557/1000 [===============>..............] - ETA: 3:20 - loss: 2.6646 - regression_loss: 2.0582 - classification_loss: 0.6064
 558/1000 [===============>..............] - ETA: 3:19 - loss: 2.6637 - regression_loss: 2.0576 - classification_loss: 0.6061
 559/1000 [===============>..............] - ETA: 3:19 - loss: 2.6632 - regression_loss: 2.0578 - classification_loss: 0.6054
 560/1000 [===============>..............] - ETA: 3:18 - loss: 2.6686 - regression_loss: 2.0633 - classification_loss: 0.6054
 561/1000 [===============>..............] - ETA: 3:18 - loss: 2.6639 - regression_loss: 2.0596 - classification_loss: 0.6043
 562/1000 [===============>..............] - ETA: 3:17 - loss: 2.6683 - regression_loss: 2.0628 - classification_loss: 0.6055
 563/1000 [===============>..............] - ETA: 3:17 - loss: 2.6699 - regression_loss: 2.0628 - classification_loss: 0.6071
 564/1000 [===============>..............] - ETA: 3:17 - loss: 2.6711 - regression_loss: 2.0643 - classification_loss: 0.6067
 565/1000 [===============>..............] - ETA: 3:16 - loss: 2.6713 - regression_loss: 2.0645 - classification_loss: 0.6067
 566/1000 [===============>..............] - ETA: 3:16 - loss: 2.6716 - regression_loss: 2.0653 - classification_loss: 0.6063
 567/1000 [================>.............] - ETA: 3:15 - loss: 2.6719 - regression_loss: 2.0659 - classification_loss: 0.6060
 568/1000 [================>.............] - ETA: 3:15 - loss: 2.6739 - regression_loss: 2.0670 - classification_loss: 0.6070
 569/1000 [================>.............] - ETA: 3:14 - loss: 2.6741 - regression_loss: 2.0674 - classification_loss: 0.6066
 570/1000 [================>.............] - ETA: 3:14 - loss: 2.6755 - regression_loss: 2.0685 - classification_loss: 0.6070
 571/1000 [================>.............] - ETA: 3:13 - loss: 2.6770 - regression_loss: 2.0703 - classification_loss: 0.6066
 572/1000 [================>.............] - ETA: 3:13 - loss: 2.6757 - regression_loss: 2.0696 - classification_loss: 0.6061
 573/1000 [================>.............] - ETA: 3:13 - loss: 2.6740 - regression_loss: 2.0687 - classification_loss: 0.6053
 574/1000 [================>.............] - ETA: 3:12 - loss: 2.6694 - regression_loss: 2.0651 - classification_loss: 0.6043
 575/1000 [================>.............] - ETA: 3:12 - loss: 2.6688 - regression_loss: 2.0648 - classification_loss: 0.6040
 576/1000 [================>.............] - ETA: 3:11 - loss: 2.6693 - regression_loss: 2.0642 - classification_loss: 0.6051
 577/1000 [================>.............] - ETA: 3:11 - loss: 2.6696 - regression_loss: 2.0642 - classification_loss: 0.6054
 578/1000 [================>.............] - ETA: 3:10 - loss: 2.6716 - regression_loss: 2.0658 - classification_loss: 0.6058
 579/1000 [================>.............] - ETA: 3:10 - loss: 2.6670 - regression_loss: 2.0622 - classification_loss: 0.6048
 580/1000 [================>.............] - ETA: 3:09 - loss: 2.6657 - regression_loss: 2.0616 - classification_loss: 0.6042
 581/1000 [================>.............] - ETA: 3:09 - loss: 2.6677 - regression_loss: 2.0639 - classification_loss: 0.6038
 582/1000 [================>.............] - ETA: 3:08 - loss: 2.6712 - regression_loss: 2.0659 - classification_loss: 0.6053
 583/1000 [================>.............] - ETA: 3:08 - loss: 2.6736 - regression_loss: 2.0676 - classification_loss: 0.6059
 584/1000 [================>.............] - ETA: 3:08 - loss: 2.6735 - regression_loss: 2.0675 - classification_loss: 0.6060
 585/1000 [================>.............] - ETA: 3:07 - loss: 2.6747 - regression_loss: 2.0689 - classification_loss: 0.6058
 586/1000 [================>.............] - ETA: 3:07 - loss: 2.6732 - regression_loss: 2.0681 - classification_loss: 0.6051
 587/1000 [================>.............] - ETA: 3:06 - loss: 2.6747 - regression_loss: 2.0689 - classification_loss: 0.6058
 588/1000 [================>.............] - ETA: 3:06 - loss: 2.6790 - regression_loss: 2.0733 - classification_loss: 0.6057
 589/1000 [================>.............] - ETA: 3:05 - loss: 2.6844 - regression_loss: 2.0772 - classification_loss: 0.6073
 590/1000 [================>.............] - ETA: 3:05 - loss: 2.6855 - regression_loss: 2.0784 - classification_loss: 0.6070
 591/1000 [================>.............] - ETA: 3:04 - loss: 2.6868 - regression_loss: 2.0793 - classification_loss: 0.6074
 592/1000 [================>.............] - ETA: 3:04 - loss: 2.6881 - regression_loss: 2.0805 - classification_loss: 0.6076
 593/1000 [================>.............] - ETA: 3:04 - loss: 2.6836 - regression_loss: 2.0770 - classification_loss: 0.6066
 594/1000 [================>.............] - ETA: 3:03 - loss: 2.6830 - regression_loss: 2.0763 - classification_loss: 0.6067
 595/1000 [================>.............] - ETA: 3:03 - loss: 2.6866 - regression_loss: 2.0784 - classification_loss: 0.6083
 596/1000 [================>.............] - ETA: 3:02 - loss: 2.6821 - regression_loss: 2.0749 - classification_loss: 0.6072
 597/1000 [================>.............] - ETA: 3:02 - loss: 2.6823 - regression_loss: 2.0755 - classification_loss: 0.6068
 598/1000 [================>.............] - ETA: 3:01 - loss: 2.6780 - regression_loss: 2.0720 - classification_loss: 0.6060
 599/1000 [================>.............] - ETA: 3:01 - loss: 2.6789 - regression_loss: 2.0731 - classification_loss: 0.6058
 600/1000 [=================>............] - ETA: 3:00 - loss: 2.6783 - regression_loss: 2.0731 - classification_loss: 0.6052
 601/1000 [=================>............] - ETA: 3:00 - loss: 2.6841 - regression_loss: 2.0785 - classification_loss: 0.6056
 602/1000 [=================>............] - ETA: 2:59 - loss: 2.6832 - regression_loss: 2.0781 - classification_loss: 0.6051
 603/1000 [=================>............] - ETA: 2:59 - loss: 2.6838 - regression_loss: 2.0787 - classification_loss: 0.6051
 604/1000 [=================>............] - ETA: 2:59 - loss: 2.6795 - regression_loss: 2.0753 - classification_loss: 0.6043
 605/1000 [=================>............] - ETA: 2:58 - loss: 2.6764 - regression_loss: 2.0718 - classification_loss: 0.6046
 606/1000 [=================>............] - ETA: 2:58 - loss: 2.6803 - regression_loss: 2.0743 - classification_loss: 0.6061
 607/1000 [=================>............] - ETA: 2:57 - loss: 2.6791 - regression_loss: 2.0709 - classification_loss: 0.6082
 608/1000 [=================>............] - ETA: 2:57 - loss: 2.6747 - regression_loss: 2.0675 - classification_loss: 0.6072
 609/1000 [=================>............] - ETA: 2:56 - loss: 2.6747 - regression_loss: 2.0673 - classification_loss: 0.6074
 610/1000 [=================>............] - ETA: 2:56 - loss: 2.6753 - regression_loss: 2.0681 - classification_loss: 0.6073
 611/1000 [=================>............] - ETA: 2:55 - loss: 2.6788 - regression_loss: 2.0712 - classification_loss: 0.6076
 612/1000 [=================>............] - ETA: 2:55 - loss: 2.6820 - regression_loss: 2.0740 - classification_loss: 0.6080
 613/1000 [=================>............] - ETA: 2:55 - loss: 2.6832 - regression_loss: 2.0747 - classification_loss: 0.6084
 614/1000 [=================>............] - ETA: 2:54 - loss: 2.6837 - regression_loss: 2.0746 - classification_loss: 0.6091
 615/1000 [=================>............] - ETA: 2:54 - loss: 2.6840 - regression_loss: 2.0749 - classification_loss: 0.6091
 616/1000 [=================>............] - ETA: 2:53 - loss: 2.6842 - regression_loss: 2.0746 - classification_loss: 0.6096
 617/1000 [=================>............] - ETA: 2:53 - loss: 2.6857 - regression_loss: 2.0755 - classification_loss: 0.6102
 618/1000 [=================>............] - ETA: 2:52 - loss: 2.6870 - regression_loss: 2.0764 - classification_loss: 0.6106
 619/1000 [=================>............] - ETA: 2:52 - loss: 2.6888 - regression_loss: 2.0780 - classification_loss: 0.6108
 620/1000 [=================>............] - ETA: 2:51 - loss: 2.6874 - regression_loss: 2.0773 - classification_loss: 0.6101
 621/1000 [=================>............] - ETA: 2:51 - loss: 2.6917 - regression_loss: 2.0801 - classification_loss: 0.6116
 622/1000 [=================>............] - ETA: 2:50 - loss: 2.6911 - regression_loss: 2.0795 - classification_loss: 0.6117
 623/1000 [=================>............] - ETA: 2:50 - loss: 2.6916 - regression_loss: 2.0797 - classification_loss: 0.6119
 624/1000 [=================>............] - ETA: 2:50 - loss: 2.6873 - regression_loss: 2.0763 - classification_loss: 0.6110
 625/1000 [=================>............] - ETA: 2:49 - loss: 2.6905 - regression_loss: 2.0785 - classification_loss: 0.6120
 626/1000 [=================>............] - ETA: 2:49 - loss: 2.6862 - regression_loss: 2.0752 - classification_loss: 0.6110
 627/1000 [=================>............] - ETA: 2:48 - loss: 2.6885 - regression_loss: 2.0772 - classification_loss: 0.6113
 628/1000 [=================>............] - ETA: 2:48 - loss: 2.6911 - regression_loss: 2.0783 - classification_loss: 0.6127
 629/1000 [=================>............] - ETA: 2:47 - loss: 2.6926 - regression_loss: 2.0794 - classification_loss: 0.6132
 630/1000 [=================>............] - ETA: 2:47 - loss: 2.6920 - regression_loss: 2.0786 - classification_loss: 0.6134
 631/1000 [=================>............] - ETA: 2:46 - loss: 2.6939 - regression_loss: 2.0796 - classification_loss: 0.6144
 632/1000 [=================>............] - ETA: 2:46 - loss: 2.6927 - regression_loss: 2.0788 - classification_loss: 0.6139
 633/1000 [=================>............] - ETA: 2:45 - loss: 2.6927 - regression_loss: 2.0789 - classification_loss: 0.6138
 634/1000 [==================>...........] - ETA: 2:45 - loss: 2.6957 - regression_loss: 2.0803 - classification_loss: 0.6154
 635/1000 [==================>...........] - ETA: 2:45 - loss: 2.6915 - regression_loss: 2.0770 - classification_loss: 0.6145
 636/1000 [==================>...........] - ETA: 2:44 - loss: 2.6921 - regression_loss: 2.0778 - classification_loss: 0.6143
 637/1000 [==================>...........] - ETA: 2:44 - loss: 2.6937 - regression_loss: 2.0797 - classification_loss: 0.6140
 638/1000 [==================>...........] - ETA: 2:43 - loss: 2.6952 - regression_loss: 2.0811 - classification_loss: 0.6142
 639/1000 [==================>...........] - ETA: 2:43 - loss: 2.6945 - regression_loss: 2.0806 - classification_loss: 0.6139
 640/1000 [==================>...........] - ETA: 2:42 - loss: 2.6957 - regression_loss: 2.0815 - classification_loss: 0.6142
 641/1000 [==================>...........] - ETA: 2:42 - loss: 2.6915 - regression_loss: 2.0783 - classification_loss: 0.6132
 642/1000 [==================>...........] - ETA: 2:41 - loss: 2.6873 - regression_loss: 2.0750 - classification_loss: 0.6123
 643/1000 [==================>...........] - ETA: 2:41 - loss: 2.6885 - regression_loss: 2.0754 - classification_loss: 0.6131
 644/1000 [==================>...........] - ETA: 2:40 - loss: 2.6882 - regression_loss: 2.0756 - classification_loss: 0.6127
 645/1000 [==================>...........] - ETA: 2:40 - loss: 2.6900 - regression_loss: 2.0761 - classification_loss: 0.6139
 646/1000 [==================>...........] - ETA: 2:40 - loss: 2.6909 - regression_loss: 2.0772 - classification_loss: 0.6137
 647/1000 [==================>...........] - ETA: 2:39 - loss: 2.6867 - regression_loss: 2.0740 - classification_loss: 0.6128
 648/1000 [==================>...........] - ETA: 2:39 - loss: 2.6826 - regression_loss: 2.0708 - classification_loss: 0.6118
 649/1000 [==================>...........] - ETA: 2:38 - loss: 2.6825 - regression_loss: 2.0710 - classification_loss: 0.6115
 650/1000 [==================>...........] - ETA: 2:38 - loss: 2.6840 - regression_loss: 2.0729 - classification_loss: 0.6111
 651/1000 [==================>...........] - ETA: 2:37 - loss: 2.6886 - regression_loss: 2.0761 - classification_loss: 0.6125
 652/1000 [==================>...........] - ETA: 2:37 - loss: 2.6895 - regression_loss: 2.0775 - classification_loss: 0.6120
 653/1000 [==================>...........] - ETA: 2:36 - loss: 2.6896 - regression_loss: 2.0770 - classification_loss: 0.6126
 654/1000 [==================>...........] - ETA: 2:36 - loss: 2.6891 - regression_loss: 2.0770 - classification_loss: 0.6121
 655/1000 [==================>...........] - ETA: 2:36 - loss: 2.6907 - regression_loss: 2.0787 - classification_loss: 0.6120
 656/1000 [==================>...........] - ETA: 2:35 - loss: 2.6930 - regression_loss: 2.0809 - classification_loss: 0.6121
 657/1000 [==================>...........] - ETA: 2:35 - loss: 2.6930 - regression_loss: 2.0812 - classification_loss: 0.6118
 658/1000 [==================>...........] - ETA: 2:34 - loss: 2.6919 - regression_loss: 2.0802 - classification_loss: 0.6117
 659/1000 [==================>...........] - ETA: 2:34 - loss: 2.6927 - regression_loss: 2.0805 - classification_loss: 0.6122
 660/1000 [==================>...........] - ETA: 2:33 - loss: 2.6886 - regression_loss: 2.0774 - classification_loss: 0.6112
 661/1000 [==================>...........] - ETA: 2:33 - loss: 2.6845 - regression_loss: 2.0742 - classification_loss: 0.6103
 662/1000 [==================>...........] - ETA: 2:32 - loss: 2.6894 - regression_loss: 2.0785 - classification_loss: 0.6108
 663/1000 [==================>...........] - ETA: 2:32 - loss: 2.6906 - regression_loss: 2.0784 - classification_loss: 0.6122
 664/1000 [==================>...........] - ETA: 2:31 - loss: 2.6945 - regression_loss: 2.0812 - classification_loss: 0.6133
 665/1000 [==================>...........] - ETA: 2:31 - loss: 2.6936 - regression_loss: 2.0807 - classification_loss: 0.6129
 666/1000 [==================>...........] - ETA: 2:31 - loss: 2.6950 - regression_loss: 2.0806 - classification_loss: 0.6143
 667/1000 [===================>..........] - ETA: 2:30 - loss: 2.6909 - regression_loss: 2.0775 - classification_loss: 0.6134
 668/1000 [===================>..........] - ETA: 2:30 - loss: 2.6869 - regression_loss: 2.0744 - classification_loss: 0.6125
 669/1000 [===================>..........] - ETA: 2:29 - loss: 2.6881 - regression_loss: 2.0755 - classification_loss: 0.6125
 670/1000 [===================>..........] - ETA: 2:29 - loss: 2.6892 - regression_loss: 2.0770 - classification_loss: 0.6123
 671/1000 [===================>..........] - ETA: 2:28 - loss: 2.6875 - regression_loss: 2.0755 - classification_loss: 0.6120
 672/1000 [===================>..........] - ETA: 2:28 - loss: 2.6876 - regression_loss: 2.0756 - classification_loss: 0.6120
 673/1000 [===================>..........] - ETA: 2:27 - loss: 2.6877 - regression_loss: 2.0756 - classification_loss: 0.6120
 674/1000 [===================>..........] - ETA: 2:27 - loss: 2.6872 - regression_loss: 2.0755 - classification_loss: 0.6117
 675/1000 [===================>..........] - ETA: 2:26 - loss: 2.6877 - regression_loss: 2.0759 - classification_loss: 0.6117
 676/1000 [===================>..........] - ETA: 2:26 - loss: 2.6876 - regression_loss: 2.0758 - classification_loss: 0.6118
 677/1000 [===================>..........] - ETA: 2:26 - loss: 2.6836 - regression_loss: 2.0727 - classification_loss: 0.6109
 678/1000 [===================>..........] - ETA: 2:25 - loss: 2.6864 - regression_loss: 2.0750 - classification_loss: 0.6114
 679/1000 [===================>..........] - ETA: 2:25 - loss: 2.6854 - regression_loss: 2.0720 - classification_loss: 0.6134
 680/1000 [===================>..........] - ETA: 2:24 - loss: 2.6814 - regression_loss: 2.0689 - classification_loss: 0.6125
 681/1000 [===================>..........] - ETA: 2:24 - loss: 2.6775 - regression_loss: 2.0659 - classification_loss: 0.6116
 682/1000 [===================>..........] - ETA: 2:23 - loss: 2.6736 - regression_loss: 2.0629 - classification_loss: 0.6107
 683/1000 [===================>..........] - ETA: 2:23 - loss: 2.6752 - regression_loss: 2.0639 - classification_loss: 0.6113
 684/1000 [===================>..........] - ETA: 2:22 - loss: 2.6765 - regression_loss: 2.0646 - classification_loss: 0.6119
 685/1000 [===================>..........] - ETA: 2:22 - loss: 2.6781 - regression_loss: 2.0644 - classification_loss: 0.6137
 686/1000 [===================>..........] - ETA: 2:22 - loss: 2.6831 - regression_loss: 2.0683 - classification_loss: 0.6149
 687/1000 [===================>..........] - ETA: 2:21 - loss: 2.6814 - regression_loss: 2.0670 - classification_loss: 0.6144
 688/1000 [===================>..........] - ETA: 2:21 - loss: 2.6775 - regression_loss: 2.0640 - classification_loss: 0.6135
 689/1000 [===================>..........] - ETA: 2:20 - loss: 2.6736 - regression_loss: 2.0610 - classification_loss: 0.6126
 690/1000 [===================>..........] - ETA: 2:20 - loss: 2.6758 - regression_loss: 2.0625 - classification_loss: 0.6133
 691/1000 [===================>..........] - ETA: 2:19 - loss: 2.6721 - regression_loss: 2.0595 - classification_loss: 0.6125
 692/1000 [===================>..........] - ETA: 2:19 - loss: 2.6736 - regression_loss: 2.0607 - classification_loss: 0.6129
 693/1000 [===================>..........] - ETA: 2:18 - loss: 2.6754 - regression_loss: 2.0624 - classification_loss: 0.6130
 694/1000 [===================>..........] - ETA: 2:18 - loss: 2.6763 - regression_loss: 2.0631 - classification_loss: 0.6132
 695/1000 [===================>..........] - ETA: 2:17 - loss: 2.6780 - regression_loss: 2.0639 - classification_loss: 0.6141
 696/1000 [===================>..........] - ETA: 2:17 - loss: 2.6803 - regression_loss: 2.0652 - classification_loss: 0.6151
 697/1000 [===================>..........] - ETA: 2:17 - loss: 2.6812 - regression_loss: 2.0650 - classification_loss: 0.6162
 698/1000 [===================>..........] - ETA: 2:16 - loss: 2.6805 - regression_loss: 2.0647 - classification_loss: 0.6158
 699/1000 [===================>..........] - ETA: 2:16 - loss: 2.6813 - regression_loss: 2.0656 - classification_loss: 0.6157
 700/1000 [====================>.........] - ETA: 2:15 - loss: 2.6775 - regression_loss: 2.0626 - classification_loss: 0.6149
 701/1000 [====================>.........] - ETA: 2:15 - loss: 2.6781 - regression_loss: 2.0628 - classification_loss: 0.6153
 702/1000 [====================>.........] - ETA: 2:14 - loss: 2.6743 - regression_loss: 2.0599 - classification_loss: 0.6144
 703/1000 [====================>.........] - ETA: 2:14 - loss: 2.6705 - regression_loss: 2.0570 - classification_loss: 0.6136
 704/1000 [====================>.........] - ETA: 2:13 - loss: 2.6673 - regression_loss: 2.0540 - classification_loss: 0.6133
 705/1000 [====================>.........] - ETA: 2:13 - loss: 2.6676 - regression_loss: 2.0544 - classification_loss: 0.6132
 706/1000 [====================>.........] - ETA: 2:12 - loss: 2.6698 - regression_loss: 2.0557 - classification_loss: 0.6141
 707/1000 [====================>.........] - ETA: 2:12 - loss: 2.6661 - regression_loss: 2.0528 - classification_loss: 0.6133
 708/1000 [====================>.........] - ETA: 2:12 - loss: 2.6654 - regression_loss: 2.0520 - classification_loss: 0.6133
 709/1000 [====================>.........] - ETA: 2:11 - loss: 2.6642 - regression_loss: 2.0510 - classification_loss: 0.6132
 710/1000 [====================>.........] - ETA: 2:11 - loss: 2.6646 - regression_loss: 2.0515 - classification_loss: 0.6132
 711/1000 [====================>.........] - ETA: 2:10 - loss: 2.6654 - regression_loss: 2.0520 - classification_loss: 0.6134
 712/1000 [====================>.........] - ETA: 2:10 - loss: 2.6649 - regression_loss: 2.0516 - classification_loss: 0.6133
 713/1000 [====================>.........] - ETA: 2:09 - loss: 2.6657 - regression_loss: 2.0520 - classification_loss: 0.6137
 714/1000 [====================>.........] - ETA: 2:09 - loss: 2.6663 - regression_loss: 2.0527 - classification_loss: 0.6136
 715/1000 [====================>.........] - ETA: 2:08 - loss: 2.6626 - regression_loss: 2.0498 - classification_loss: 0.6128
 716/1000 [====================>.........] - ETA: 2:08 - loss: 2.6591 - regression_loss: 2.0470 - classification_loss: 0.6121
 717/1000 [====================>.........] - ETA: 2:08 - loss: 2.6587 - regression_loss: 2.0471 - classification_loss: 0.6117
 718/1000 [====================>.........] - ETA: 2:07 - loss: 2.6550 - regression_loss: 2.0442 - classification_loss: 0.6108
 719/1000 [====================>.........] - ETA: 2:07 - loss: 2.6550 - regression_loss: 2.0446 - classification_loss: 0.6104
 720/1000 [====================>.........] - ETA: 2:06 - loss: 2.6561 - regression_loss: 2.0457 - classification_loss: 0.6104
 721/1000 [====================>.........] - ETA: 2:06 - loss: 2.6583 - regression_loss: 2.0474 - classification_loss: 0.6109
 722/1000 [====================>.........] - ETA: 2:05 - loss: 2.6574 - regression_loss: 2.0466 - classification_loss: 0.6108
 723/1000 [====================>.........] - ETA: 2:05 - loss: 2.6567 - regression_loss: 2.0462 - classification_loss: 0.6105
 724/1000 [====================>.........] - ETA: 2:04 - loss: 2.6569 - regression_loss: 2.0467 - classification_loss: 0.6102
 725/1000 [====================>.........] - ETA: 2:04 - loss: 2.6574 - regression_loss: 2.0466 - classification_loss: 0.6108
 726/1000 [====================>.........] - ETA: 2:03 - loss: 2.6586 - regression_loss: 2.0481 - classification_loss: 0.6105
 727/1000 [====================>.........] - ETA: 2:03 - loss: 2.6609 - regression_loss: 2.0500 - classification_loss: 0.6109
 728/1000 [====================>.........] - ETA: 2:03 - loss: 2.6608 - regression_loss: 2.0498 - classification_loss: 0.6109
 729/1000 [====================>.........] - ETA: 2:02 - loss: 2.6619 - regression_loss: 2.0505 - classification_loss: 0.6114
 730/1000 [====================>.........] - ETA: 2:02 - loss: 2.6618 - regression_loss: 2.0508 - classification_loss: 0.6110
 731/1000 [====================>.........] - ETA: 2:01 - loss: 2.6629 - regression_loss: 2.0523 - classification_loss: 0.6106
 732/1000 [====================>.........] - ETA: 2:01 - loss: 2.6628 - regression_loss: 2.0520 - classification_loss: 0.6108
 733/1000 [====================>.........] - ETA: 2:00 - loss: 2.6635 - regression_loss: 2.0526 - classification_loss: 0.6109
 734/1000 [=====================>........] - ETA: 2:00 - loss: 2.6656 - regression_loss: 2.0546 - classification_loss: 0.6110
 735/1000 [=====================>........] - ETA: 1:59 - loss: 2.6619 - regression_loss: 2.0518 - classification_loss: 0.6101
 736/1000 [=====================>........] - ETA: 1:59 - loss: 2.6583 - regression_loss: 2.0490 - classification_loss: 0.6093
 737/1000 [=====================>........] - ETA: 1:58 - loss: 2.6547 - regression_loss: 2.0462 - classification_loss: 0.6085
 738/1000 [=====================>........] - ETA: 1:58 - loss: 2.6597 - regression_loss: 2.0506 - classification_loss: 0.6091
 739/1000 [=====================>........] - ETA: 1:58 - loss: 2.6608 - regression_loss: 2.0518 - classification_loss: 0.6091
 740/1000 [=====================>........] - ETA: 1:57 - loss: 2.6572 - regression_loss: 2.0490 - classification_loss: 0.6082
 741/1000 [=====================>........] - ETA: 1:57 - loss: 2.6538 - regression_loss: 2.0462 - classification_loss: 0.6076
 742/1000 [=====================>........] - ETA: 1:56 - loss: 2.6558 - regression_loss: 2.0483 - classification_loss: 0.6075
 743/1000 [=====================>........] - ETA: 1:56 - loss: 2.6522 - regression_loss: 2.0455 - classification_loss: 0.6067
 744/1000 [=====================>........] - ETA: 1:55 - loss: 2.6546 - regression_loss: 2.0459 - classification_loss: 0.6087
 745/1000 [=====================>........] - ETA: 1:55 - loss: 2.6631 - regression_loss: 2.0432 - classification_loss: 0.6200
 746/1000 [=====================>........] - ETA: 1:54 - loss: 2.6636 - regression_loss: 2.0439 - classification_loss: 0.6197
 747/1000 [=====================>........] - ETA: 1:54 - loss: 2.6648 - regression_loss: 2.0443 - classification_loss: 0.6205
 748/1000 [=====================>........] - ETA: 1:53 - loss: 2.6693 - regression_loss: 2.0470 - classification_loss: 0.6223
 749/1000 [=====================>........] - ETA: 1:53 - loss: 2.6717 - regression_loss: 2.0480 - classification_loss: 0.6238
 750/1000 [=====================>........] - ETA: 1:53 - loss: 2.6711 - regression_loss: 2.0475 - classification_loss: 0.6236
 751/1000 [=====================>........] - ETA: 1:52 - loss: 2.6718 - regression_loss: 2.0486 - classification_loss: 0.6232
 752/1000 [=====================>........] - ETA: 1:52 - loss: 2.6725 - regression_loss: 2.0498 - classification_loss: 0.6228
 753/1000 [=====================>........] - ETA: 1:51 - loss: 2.6733 - regression_loss: 2.0506 - classification_loss: 0.6227
 754/1000 [=====================>........] - ETA: 1:51 - loss: 2.6737 - regression_loss: 2.0511 - classification_loss: 0.6226
 755/1000 [=====================>........] - ETA: 1:50 - loss: 2.6790 - regression_loss: 2.0549 - classification_loss: 0.6241
 756/1000 [=====================>........] - ETA: 1:50 - loss: 2.6755 - regression_loss: 2.0522 - classification_loss: 0.6233
 757/1000 [=====================>........] - ETA: 1:49 - loss: 2.6787 - regression_loss: 2.0538 - classification_loss: 0.6249
 758/1000 [=====================>........] - ETA: 1:49 - loss: 2.6752 - regression_loss: 2.0511 - classification_loss: 0.6241
 759/1000 [=====================>........] - ETA: 1:49 - loss: 2.6760 - regression_loss: 2.0521 - classification_loss: 0.6239
 760/1000 [=====================>........] - ETA: 1:48 - loss: 2.6780 - regression_loss: 2.0539 - classification_loss: 0.6241
 761/1000 [=====================>........] - ETA: 1:48 - loss: 2.6789 - regression_loss: 2.0552 - classification_loss: 0.6238
 762/1000 [=====================>........] - ETA: 1:47 - loss: 2.6787 - regression_loss: 2.0555 - classification_loss: 0.6233
 763/1000 [=====================>........] - ETA: 1:47 - loss: 2.6802 - regression_loss: 2.0566 - classification_loss: 0.6236
 764/1000 [=====================>........] - ETA: 1:46 - loss: 2.6770 - regression_loss: 2.0539 - classification_loss: 0.6231
 765/1000 [=====================>........] - ETA: 1:46 - loss: 2.6790 - regression_loss: 2.0549 - classification_loss: 0.6241
 766/1000 [=====================>........] - ETA: 1:45 - loss: 2.6775 - regression_loss: 2.0539 - classification_loss: 0.6235
 767/1000 [======================>.......] - ETA: 1:45 - loss: 2.6770 - regression_loss: 2.0539 - classification_loss: 0.6230
 768/1000 [======================>.......] - ETA: 1:44 - loss: 2.6799 - regression_loss: 2.0560 - classification_loss: 0.6240
 769/1000 [======================>.......] - ETA: 1:44 - loss: 2.6806 - regression_loss: 2.0558 - classification_loss: 0.6248
 770/1000 [======================>.......] - ETA: 1:44 - loss: 2.6818 - regression_loss: 2.0571 - classification_loss: 0.6247
 771/1000 [======================>.......] - ETA: 1:43 - loss: 2.6829 - regression_loss: 2.0583 - classification_loss: 0.6246
 772/1000 [======================>.......] - ETA: 1:43 - loss: 2.6836 - regression_loss: 2.0586 - classification_loss: 0.6251
 773/1000 [======================>.......] - ETA: 1:42 - loss: 2.6826 - regression_loss: 2.0578 - classification_loss: 0.6248
 774/1000 [======================>.......] - ETA: 1:42 - loss: 2.6824 - regression_loss: 2.0580 - classification_loss: 0.6244
 775/1000 [======================>.......] - ETA: 1:41 - loss: 2.6829 - regression_loss: 2.0589 - classification_loss: 0.6240
 776/1000 [======================>.......] - ETA: 1:41 - loss: 2.6855 - regression_loss: 2.0605 - classification_loss: 0.6250
 777/1000 [======================>.......] - ETA: 1:40 - loss: 2.6846 - regression_loss: 2.0600 - classification_loss: 0.6245
 778/1000 [======================>.......] - ETA: 1:40 - loss: 2.6855 - regression_loss: 2.0613 - classification_loss: 0.6242
 779/1000 [======================>.......] - ETA: 1:39 - loss: 2.6878 - regression_loss: 2.0631 - classification_loss: 0.6247
 780/1000 [======================>.......] - ETA: 1:39 - loss: 2.6873 - regression_loss: 2.0631 - classification_loss: 0.6242
 781/1000 [======================>.......] - ETA: 1:39 - loss: 2.6870 - regression_loss: 2.0632 - classification_loss: 0.6237
 782/1000 [======================>.......] - ETA: 1:38 - loss: 2.6879 - regression_loss: 2.0643 - classification_loss: 0.6236
 783/1000 [======================>.......] - ETA: 1:38 - loss: 2.6844 - regression_loss: 2.0616 - classification_loss: 0.6228
 784/1000 [======================>.......] - ETA: 1:37 - loss: 2.6847 - regression_loss: 2.0624 - classification_loss: 0.6224
 785/1000 [======================>.......] - ETA: 1:37 - loss: 2.6856 - regression_loss: 2.0634 - classification_loss: 0.6222
 786/1000 [======================>.......] - ETA: 1:36 - loss: 2.6822 - regression_loss: 2.0608 - classification_loss: 0.6214
 787/1000 [======================>.......] - ETA: 1:36 - loss: 2.6825 - regression_loss: 2.0613 - classification_loss: 0.6212
 788/1000 [======================>.......] - ETA: 1:35 - loss: 2.6821 - regression_loss: 2.0609 - classification_loss: 0.6211
 789/1000 [======================>.......] - ETA: 1:35 - loss: 2.6818 - regression_loss: 2.0610 - classification_loss: 0.6208
 790/1000 [======================>.......] - ETA: 1:34 - loss: 2.6823 - regression_loss: 2.0616 - classification_loss: 0.6207
 791/1000 [======================>.......] - ETA: 1:34 - loss: 2.6789 - regression_loss: 2.0590 - classification_loss: 0.6199
 792/1000 [======================>.......] - ETA: 1:34 - loss: 2.6778 - regression_loss: 2.0583 - classification_loss: 0.6195
 793/1000 [======================>.......] - ETA: 1:33 - loss: 2.6771 - regression_loss: 2.0557 - classification_loss: 0.6214
 794/1000 [======================>.......] - ETA: 1:33 - loss: 2.6794 - regression_loss: 2.0579 - classification_loss: 0.6215
 795/1000 [======================>.......] - ETA: 1:32 - loss: 2.6805 - regression_loss: 2.0587 - classification_loss: 0.6218
 796/1000 [======================>.......] - ETA: 1:32 - loss: 2.6818 - regression_loss: 2.0595 - classification_loss: 0.6223
 797/1000 [======================>.......] - ETA: 1:31 - loss: 2.6824 - regression_loss: 2.0603 - classification_loss: 0.6222
 798/1000 [======================>.......] - ETA: 1:31 - loss: 2.6829 - regression_loss: 2.0612 - classification_loss: 0.6217
 799/1000 [======================>.......] - ETA: 1:30 - loss: 2.6850 - regression_loss: 2.0633 - classification_loss: 0.6216
 800/1000 [=======================>......] - ETA: 1:30 - loss: 2.6858 - regression_loss: 2.0633 - classification_loss: 0.6225
 801/1000 [=======================>......] - ETA: 1:30 - loss: 2.6852 - regression_loss: 2.0629 - classification_loss: 0.6223
 802/1000 [=======================>......] - ETA: 1:29 - loss: 2.6890 - regression_loss: 2.0651 - classification_loss: 0.6239
 803/1000 [=======================>......] - ETA: 1:29 - loss: 2.6883 - regression_loss: 2.0648 - classification_loss: 0.6234
 804/1000 [=======================>......] - ETA: 1:28 - loss: 2.6899 - regression_loss: 2.0659 - classification_loss: 0.6240
 805/1000 [=======================>......] - ETA: 1:28 - loss: 2.6893 - regression_loss: 2.0656 - classification_loss: 0.6237
 806/1000 [=======================>......] - ETA: 1:27 - loss: 2.6911 - regression_loss: 2.0671 - classification_loss: 0.6240
 807/1000 [=======================>......] - ETA: 1:27 - loss: 2.6946 - regression_loss: 2.0695 - classification_loss: 0.6251
 808/1000 [=======================>......] - ETA: 1:26 - loss: 2.6971 - regression_loss: 2.0721 - classification_loss: 0.6250
 809/1000 [=======================>......] - ETA: 1:26 - loss: 2.6969 - regression_loss: 2.0722 - classification_loss: 0.6247
 810/1000 [=======================>......] - ETA: 1:25 - loss: 2.6988 - regression_loss: 2.0741 - classification_loss: 0.6247
 811/1000 [=======================>......] - ETA: 1:25 - loss: 2.6993 - regression_loss: 2.0743 - classification_loss: 0.6250
 812/1000 [=======================>......] - ETA: 1:25 - loss: 2.6960 - regression_loss: 2.0718 - classification_loss: 0.6243
 813/1000 [=======================>......] - ETA: 1:24 - loss: 2.6927 - regression_loss: 2.0692 - classification_loss: 0.6235
 814/1000 [=======================>......] - ETA: 1:24 - loss: 2.6936 - regression_loss: 2.0701 - classification_loss: 0.6235
 815/1000 [=======================>......] - ETA: 1:23 - loss: 2.6936 - regression_loss: 2.0701 - classification_loss: 0.6235
 816/1000 [=======================>......] - ETA: 1:23 - loss: 2.6903 - regression_loss: 2.0676 - classification_loss: 0.6227
 817/1000 [=======================>......] - ETA: 1:22 - loss: 2.6873 - regression_loss: 2.0651 - classification_loss: 0.6223
 818/1000 [=======================>......] - ETA: 1:22 - loss: 2.6913 - regression_loss: 2.0685 - classification_loss: 0.6228
 819/1000 [=======================>......] - ETA: 1:21 - loss: 2.6932 - regression_loss: 2.0691 - classification_loss: 0.6241
 820/1000 [=======================>......] - ETA: 1:21 - loss: 2.6931 - regression_loss: 2.0693 - classification_loss: 0.6238
 821/1000 [=======================>......] - ETA: 1:20 - loss: 2.6899 - regression_loss: 2.0668 - classification_loss: 0.6231
 822/1000 [=======================>......] - ETA: 1:20 - loss: 2.6918 - regression_loss: 2.0687 - classification_loss: 0.6232
 823/1000 [=======================>......] - ETA: 1:20 - loss: 2.6939 - regression_loss: 2.0705 - classification_loss: 0.6234
 824/1000 [=======================>......] - ETA: 1:19 - loss: 2.6906 - regression_loss: 2.0680 - classification_loss: 0.6226
 825/1000 [=======================>......] - ETA: 1:19 - loss: 2.6924 - regression_loss: 2.0685 - classification_loss: 0.6240
 826/1000 [=======================>......] - ETA: 1:18 - loss: 2.6892 - regression_loss: 2.0660 - classification_loss: 0.6232
 827/1000 [=======================>......] - ETA: 1:18 - loss: 2.6913 - regression_loss: 2.0671 - classification_loss: 0.6241
 828/1000 [=======================>......] - ETA: 1:17 - loss: 2.6880 - regression_loss: 2.0646 - classification_loss: 0.6234
 829/1000 [=======================>......] - ETA: 1:17 - loss: 2.6875 - regression_loss: 2.0644 - classification_loss: 0.6231
 830/1000 [=======================>......] - ETA: 1:16 - loss: 2.6843 - regression_loss: 2.0619 - classification_loss: 0.6224
 831/1000 [=======================>......] - ETA: 1:16 - loss: 2.6855 - regression_loss: 2.0631 - classification_loss: 0.6224
 832/1000 [=======================>......] - ETA: 1:15 - loss: 2.6860 - regression_loss: 2.0638 - classification_loss: 0.6223
 833/1000 [=======================>......] - ETA: 1:15 - loss: 2.6870 - regression_loss: 2.0647 - classification_loss: 0.6224
 834/1000 [========================>.....] - ETA: 1:15 - loss: 2.6890 - regression_loss: 2.0666 - classification_loss: 0.6225
 835/1000 [========================>.....] - ETA: 1:14 - loss: 2.6897 - regression_loss: 2.0669 - classification_loss: 0.6227
 836/1000 [========================>.....] - ETA: 1:14 - loss: 2.6920 - regression_loss: 2.0681 - classification_loss: 0.6239
 837/1000 [========================>.....] - ETA: 1:13 - loss: 2.6921 - regression_loss: 2.0681 - classification_loss: 0.6240
 838/1000 [========================>.....] - ETA: 1:13 - loss: 2.6929 - regression_loss: 2.0685 - classification_loss: 0.6244
 839/1000 [========================>.....] - ETA: 1:12 - loss: 2.6942 - regression_loss: 2.0689 - classification_loss: 0.6253
 840/1000 [========================>.....] - ETA: 1:12 - loss: 2.6937 - regression_loss: 2.0685 - classification_loss: 0.6252
 841/1000 [========================>.....] - ETA: 1:11 - loss: 2.6935 - regression_loss: 2.0687 - classification_loss: 0.6248
 842/1000 [========================>.....] - ETA: 1:11 - loss: 2.6955 - regression_loss: 2.0708 - classification_loss: 0.6248
 843/1000 [========================>.....] - ETA: 1:11 - loss: 2.6923 - regression_loss: 2.0683 - classification_loss: 0.6240
 844/1000 [========================>.....] - ETA: 1:10 - loss: 2.6929 - regression_loss: 2.0688 - classification_loss: 0.6241
 845/1000 [========================>.....] - ETA: 1:10 - loss: 2.6897 - regression_loss: 2.0663 - classification_loss: 0.6234
 846/1000 [========================>.....] - ETA: 1:09 - loss: 2.6902 - regression_loss: 2.0671 - classification_loss: 0.6231
 847/1000 [========================>.....] - ETA: 1:09 - loss: 2.6900 - regression_loss: 2.0672 - classification_loss: 0.6228
 848/1000 [========================>.....] - ETA: 1:08 - loss: 2.6907 - regression_loss: 2.0676 - classification_loss: 0.6231
 849/1000 [========================>.....] - ETA: 1:08 - loss: 2.6875 - regression_loss: 2.0652 - classification_loss: 0.6224
 850/1000 [========================>.....] - ETA: 1:07 - loss: 2.6887 - regression_loss: 2.0663 - classification_loss: 0.6224
 851/1000 [========================>.....] - ETA: 1:07 - loss: 2.6856 - regression_loss: 2.0639 - classification_loss: 0.6217
 852/1000 [========================>.....] - ETA: 1:06 - loss: 2.6859 - regression_loss: 2.0645 - classification_loss: 0.6214
 853/1000 [========================>.....] - ETA: 1:06 - loss: 2.6874 - regression_loss: 2.0653 - classification_loss: 0.6221
 854/1000 [========================>.....] - ETA: 1:06 - loss: 2.6842 - regression_loss: 2.0629 - classification_loss: 0.6214
 855/1000 [========================>.....] - ETA: 1:05 - loss: 2.6869 - regression_loss: 2.0642 - classification_loss: 0.6227
 856/1000 [========================>.....] - ETA: 1:05 - loss: 2.6882 - regression_loss: 2.0654 - classification_loss: 0.6228
 857/1000 [========================>.....] - ETA: 1:04 - loss: 2.6903 - regression_loss: 2.0671 - classification_loss: 0.6231
 858/1000 [========================>.....] - ETA: 1:04 - loss: 2.6913 - regression_loss: 2.0681 - classification_loss: 0.6232
 859/1000 [========================>.....] - ETA: 1:03 - loss: 2.6917 - regression_loss: 2.0686 - classification_loss: 0.6231
 860/1000 [========================>.....] - ETA: 1:03 - loss: 2.6917 - regression_loss: 2.0688 - classification_loss: 0.6229
 861/1000 [========================>.....] - ETA: 1:02 - loss: 2.6942 - regression_loss: 2.0708 - classification_loss: 0.6234
 862/1000 [========================>.....] - ETA: 1:02 - loss: 2.6947 - regression_loss: 2.0714 - classification_loss: 0.6232
 863/1000 [========================>.....] - ETA: 1:01 - loss: 2.6949 - regression_loss: 2.0716 - classification_loss: 0.6233
 864/1000 [========================>.....] - ETA: 1:01 - loss: 2.6945 - regression_loss: 2.0715 - classification_loss: 0.6230
 865/1000 [========================>.....] - ETA: 1:01 - loss: 2.6954 - regression_loss: 2.0725 - classification_loss: 0.6229
 866/1000 [========================>.....] - ETA: 1:00 - loss: 2.6942 - regression_loss: 2.0718 - classification_loss: 0.6224
 867/1000 [=========================>....] - ETA: 1:00 - loss: 2.6921 - regression_loss: 2.0694 - classification_loss: 0.6227
 868/1000 [=========================>....] - ETA: 59s - loss: 2.6928 - regression_loss: 2.0703 - classification_loss: 0.6225 
 869/1000 [=========================>....] - ETA: 59s - loss: 2.6956 - regression_loss: 2.0729 - classification_loss: 0.6226
 870/1000 [=========================>....] - ETA: 58s - loss: 2.6985 - regression_loss: 2.0752 - classification_loss: 0.6233
 871/1000 [=========================>....] - ETA: 58s - loss: 2.6956 - regression_loss: 2.0728 - classification_loss: 0.6228
 872/1000 [=========================>....] - ETA: 57s - loss: 2.6943 - regression_loss: 2.0718 - classification_loss: 0.6225
 873/1000 [=========================>....] - ETA: 57s - loss: 2.6947 - regression_loss: 2.0721 - classification_loss: 0.6226
 874/1000 [=========================>....] - ETA: 56s - loss: 2.6977 - regression_loss: 2.0738 - classification_loss: 0.6240
 875/1000 [=========================>....] - ETA: 56s - loss: 2.6972 - regression_loss: 2.0735 - classification_loss: 0.6237
 876/1000 [=========================>....] - ETA: 56s - loss: 2.6971 - regression_loss: 2.0731 - classification_loss: 0.6241
 877/1000 [=========================>....] - ETA: 55s - loss: 2.6984 - regression_loss: 2.0740 - classification_loss: 0.6244
 878/1000 [=========================>....] - ETA: 55s - loss: 2.6953 - regression_loss: 2.0716 - classification_loss: 0.6237
 879/1000 [=========================>....] - ETA: 54s - loss: 2.6963 - regression_loss: 2.0725 - classification_loss: 0.6237
 880/1000 [=========================>....] - ETA: 54s - loss: 2.6932 - regression_loss: 2.0702 - classification_loss: 0.6231
 881/1000 [=========================>....] - ETA: 53s - loss: 2.6960 - regression_loss: 2.0728 - classification_loss: 0.6232
 882/1000 [=========================>....] - ETA: 53s - loss: 2.6962 - regression_loss: 2.0731 - classification_loss: 0.6232
 883/1000 [=========================>....] - ETA: 52s - loss: 2.6932 - regression_loss: 2.0707 - classification_loss: 0.6225
 884/1000 [=========================>....] - ETA: 52s - loss: 2.6902 - regression_loss: 2.0684 - classification_loss: 0.6218
 885/1000 [=========================>....] - ETA: 52s - loss: 2.6921 - regression_loss: 2.0691 - classification_loss: 0.6229
 886/1000 [=========================>....] - ETA: 51s - loss: 2.6890 - regression_loss: 2.0668 - classification_loss: 0.6222
 887/1000 [=========================>....] - ETA: 51s - loss: 2.6883 - regression_loss: 2.0664 - classification_loss: 0.6219
 888/1000 [=========================>....] - ETA: 50s - loss: 2.6907 - regression_loss: 2.0682 - classification_loss: 0.6225
 889/1000 [=========================>....] - ETA: 50s - loss: 2.6924 - regression_loss: 2.0686 - classification_loss: 0.6238
 890/1000 [=========================>....] - ETA: 49s - loss: 2.6929 - regression_loss: 2.0691 - classification_loss: 0.6238
 891/1000 [=========================>....] - ETA: 49s - loss: 2.6945 - regression_loss: 2.0692 - classification_loss: 0.6253
 892/1000 [=========================>....] - ETA: 48s - loss: 2.6939 - regression_loss: 2.0689 - classification_loss: 0.6249
 893/1000 [=========================>....] - ETA: 48s - loss: 2.6942 - regression_loss: 2.0695 - classification_loss: 0.6247
 894/1000 [=========================>....] - ETA: 47s - loss: 2.6946 - regression_loss: 2.0702 - classification_loss: 0.6244
 895/1000 [=========================>....] - ETA: 47s - loss: 2.6963 - regression_loss: 2.0721 - classification_loss: 0.6242
 896/1000 [=========================>....] - ETA: 47s - loss: 2.6964 - regression_loss: 2.0724 - classification_loss: 0.6240
 897/1000 [=========================>....] - ETA: 46s - loss: 2.6934 - regression_loss: 2.0701 - classification_loss: 0.6233
 898/1000 [=========================>....] - ETA: 46s - loss: 2.6904 - regression_loss: 2.0678 - classification_loss: 0.6226
 899/1000 [=========================>....] - ETA: 45s - loss: 2.6902 - regression_loss: 2.0680 - classification_loss: 0.6222
 900/1000 [==========================>...] - ETA: 45s - loss: 2.6908 - regression_loss: 2.0686 - classification_loss: 0.6222
 901/1000 [==========================>...] - ETA: 44s - loss: 2.6942 - regression_loss: 2.0707 - classification_loss: 0.6235
 902/1000 [==========================>...] - ETA: 44s - loss: 2.6912 - regression_loss: 2.0684 - classification_loss: 0.6228
 903/1000 [==========================>...] - ETA: 43s - loss: 2.6927 - regression_loss: 2.0687 - classification_loss: 0.6239
 904/1000 [==========================>...] - ETA: 43s - loss: 2.6957 - regression_loss: 2.0706 - classification_loss: 0.6252
 905/1000 [==========================>...] - ETA: 42s - loss: 2.6954 - regression_loss: 2.0705 - classification_loss: 0.6249
 906/1000 [==========================>...] - ETA: 42s - loss: 2.6977 - regression_loss: 2.0718 - classification_loss: 0.6258
 907/1000 [==========================>...] - ETA: 42s - loss: 2.6947 - regression_loss: 2.0696 - classification_loss: 0.6251
 908/1000 [==========================>...] - ETA: 41s - loss: 2.6972 - regression_loss: 2.0711 - classification_loss: 0.6261
 909/1000 [==========================>...] - ETA: 41s - loss: 2.6942 - regression_loss: 2.0688 - classification_loss: 0.6254
 910/1000 [==========================>...] - ETA: 40s - loss: 2.6962 - regression_loss: 2.0706 - classification_loss: 0.6256
 911/1000 [==========================>...] - ETA: 40s - loss: 2.6982 - regression_loss: 2.0730 - classification_loss: 0.6252
 912/1000 [==========================>...] - ETA: 39s - loss: 2.6988 - regression_loss: 2.0735 - classification_loss: 0.6253
 913/1000 [==========================>...] - ETA: 39s - loss: 2.6988 - regression_loss: 2.0738 - classification_loss: 0.6250
 914/1000 [==========================>...] - ETA: 38s - loss: 2.7001 - regression_loss: 2.0750 - classification_loss: 0.6251
 915/1000 [==========================>...] - ETA: 38s - loss: 2.7001 - regression_loss: 2.0750 - classification_loss: 0.6251
 916/1000 [==========================>...] - ETA: 37s - loss: 2.7011 - regression_loss: 2.0758 - classification_loss: 0.6253
 917/1000 [==========================>...] - ETA: 37s - loss: 2.7010 - regression_loss: 2.0758 - classification_loss: 0.6253
 918/1000 [==========================>...] - ETA: 37s - loss: 2.6981 - regression_loss: 2.0735 - classification_loss: 0.6246
 919/1000 [==========================>...] - ETA: 36s - loss: 2.6969 - regression_loss: 2.0727 - classification_loss: 0.6242
 920/1000 [==========================>...] - ETA: 36s - loss: 2.6963 - regression_loss: 2.0722 - classification_loss: 0.6240
 921/1000 [==========================>...] - ETA: 35s - loss: 2.6965 - regression_loss: 2.0727 - classification_loss: 0.6238
 922/1000 [==========================>...] - ETA: 35s - loss: 2.6964 - regression_loss: 2.0728 - classification_loss: 0.6236
 923/1000 [==========================>...] - ETA: 34s - loss: 2.6986 - regression_loss: 2.0742 - classification_loss: 0.6244
 924/1000 [==========================>...] - ETA: 34s - loss: 2.6986 - regression_loss: 2.0743 - classification_loss: 0.6243
 925/1000 [==========================>...] - ETA: 33s - loss: 2.6989 - regression_loss: 2.0748 - classification_loss: 0.6241
 926/1000 [==========================>...] - ETA: 33s - loss: 2.6994 - regression_loss: 2.0754 - classification_loss: 0.6240
 927/1000 [==========================>...] - ETA: 33s - loss: 2.6984 - regression_loss: 2.0749 - classification_loss: 0.6236
 928/1000 [==========================>...] - ETA: 32s - loss: 2.6985 - regression_loss: 2.0751 - classification_loss: 0.6234
 929/1000 [==========================>...] - ETA: 32s - loss: 2.6956 - regression_loss: 2.0729 - classification_loss: 0.6228
 930/1000 [==========================>...] - ETA: 31s - loss: 2.6963 - regression_loss: 2.0736 - classification_loss: 0.6227
 931/1000 [==========================>...] - ETA: 31s - loss: 2.6987 - regression_loss: 2.0752 - classification_loss: 0.6235
 932/1000 [==========================>...] - ETA: 30s - loss: 2.6975 - regression_loss: 2.0744 - classification_loss: 0.6231
 933/1000 [==========================>...] - ETA: 30s - loss: 2.6965 - regression_loss: 2.0739 - classification_loss: 0.6226
 934/1000 [===========================>..] - ETA: 29s - loss: 2.6996 - regression_loss: 2.0755 - classification_loss: 0.6240
 935/1000 [===========================>..] - ETA: 29s - loss: 2.6967 - regression_loss: 2.0733 - classification_loss: 0.6234
 936/1000 [===========================>..] - ETA: 28s - loss: 2.6998 - regression_loss: 2.0753 - classification_loss: 0.6244
 937/1000 [===========================>..] - ETA: 28s - loss: 2.7024 - regression_loss: 2.0765 - classification_loss: 0.6259
 938/1000 [===========================>..] - ETA: 28s - loss: 2.7041 - regression_loss: 2.0786 - classification_loss: 0.6255
 939/1000 [===========================>..] - ETA: 27s - loss: 2.7038 - regression_loss: 2.0787 - classification_loss: 0.6251
 940/1000 [===========================>..] - ETA: 27s - loss: 2.7045 - regression_loss: 2.0795 - classification_loss: 0.6250
 941/1000 [===========================>..] - ETA: 26s - loss: 2.7059 - regression_loss: 2.0808 - classification_loss: 0.6251
 942/1000 [===========================>..] - ETA: 26s - loss: 2.7030 - regression_loss: 2.0786 - classification_loss: 0.6244
 943/1000 [===========================>..] - ETA: 25s - loss: 2.7037 - regression_loss: 2.0797 - classification_loss: 0.6240
 944/1000 [===========================>..] - ETA: 25s - loss: 2.7027 - regression_loss: 2.0790 - classification_loss: 0.6237
 945/1000 [===========================>..] - ETA: 24s - loss: 2.7031 - regression_loss: 2.0793 - classification_loss: 0.6239
 946/1000 [===========================>..] - ETA: 24s - loss: 2.7003 - regression_loss: 2.0771 - classification_loss: 0.6232
 947/1000 [===========================>..] - ETA: 23s - loss: 2.7007 - regression_loss: 2.0777 - classification_loss: 0.6230
 948/1000 [===========================>..] - ETA: 23s - loss: 2.7017 - regression_loss: 2.0787 - classification_loss: 0.6230
 949/1000 [===========================>..] - ETA: 23s - loss: 2.6989 - regression_loss: 2.0765 - classification_loss: 0.6224
 950/1000 [===========================>..] - ETA: 22s - loss: 2.7009 - regression_loss: 2.0780 - classification_loss: 0.6229
 951/1000 [===========================>..] - ETA: 22s - loss: 2.7008 - regression_loss: 2.0783 - classification_loss: 0.6225
 952/1000 [===========================>..] - ETA: 21s - loss: 2.6980 - regression_loss: 2.0761 - classification_loss: 0.6219
 953/1000 [===========================>..] - ETA: 21s - loss: 2.7006 - regression_loss: 2.0771 - classification_loss: 0.6235
 954/1000 [===========================>..] - ETA: 20s - loss: 2.7024 - regression_loss: 2.0781 - classification_loss: 0.6242
 955/1000 [===========================>..] - ETA: 20s - loss: 2.7024 - regression_loss: 2.0777 - classification_loss: 0.6246
 956/1000 [===========================>..] - ETA: 19s - loss: 2.6995 - regression_loss: 2.0756 - classification_loss: 0.6240
 957/1000 [===========================>..] - ETA: 19s - loss: 2.6991 - regression_loss: 2.0754 - classification_loss: 0.6237
 958/1000 [===========================>..] - ETA: 18s - loss: 2.7009 - regression_loss: 2.0772 - classification_loss: 0.6237
 959/1000 [===========================>..] - ETA: 18s - loss: 2.7037 - regression_loss: 2.0797 - classification_loss: 0.6240
 960/1000 [===========================>..] - ETA: 18s - loss: 2.7046 - regression_loss: 2.0808 - classification_loss: 0.6238
 961/1000 [===========================>..] - ETA: 17s - loss: 2.7051 - regression_loss: 2.0815 - classification_loss: 0.6236
 962/1000 [===========================>..] - ETA: 17s - loss: 2.7062 - regression_loss: 2.0826 - classification_loss: 0.6236
 963/1000 [===========================>..] - ETA: 16s - loss: 2.7071 - regression_loss: 2.0837 - classification_loss: 0.6234
 964/1000 [===========================>..] - ETA: 16s - loss: 2.7077 - regression_loss: 2.0844 - classification_loss: 0.6233
 965/1000 [===========================>..] - ETA: 15s - loss: 2.7098 - regression_loss: 2.0861 - classification_loss: 0.6238
 966/1000 [===========================>..] - ETA: 15s - loss: 2.7108 - regression_loss: 2.0860 - classification_loss: 0.6249
 967/1000 [============================>.] - ETA: 14s - loss: 2.7080 - regression_loss: 2.0838 - classification_loss: 0.6242
 968/1000 [============================>.] - ETA: 14s - loss: 2.7052 - regression_loss: 2.0817 - classification_loss: 0.6236
 969/1000 [============================>.] - ETA: 14s - loss: 2.7046 - regression_loss: 2.0812 - classification_loss: 0.6234
 970/1000 [============================>.] - ETA: 13s - loss: 2.7018 - regression_loss: 2.0791 - classification_loss: 0.6228
 971/1000 [============================>.] - ETA: 13s - loss: 2.7024 - regression_loss: 2.0795 - classification_loss: 0.6229
 972/1000 [============================>.] - ETA: 12s - loss: 2.7003 - regression_loss: 2.0773 - classification_loss: 0.6230
 973/1000 [============================>.] - ETA: 12s - loss: 2.7036 - regression_loss: 2.0794 - classification_loss: 0.6241
 974/1000 [============================>.] - ETA: 11s - loss: 2.7008 - regression_loss: 2.0773 - classification_loss: 0.6235
 975/1000 [============================>.] - ETA: 11s - loss: 2.7030 - regression_loss: 2.0782 - classification_loss: 0.6248
 976/1000 [============================>.] - ETA: 10s - loss: 2.7032 - regression_loss: 2.0787 - classification_loss: 0.6245
 977/1000 [============================>.] - ETA: 10s - loss: 2.7049 - regression_loss: 2.0800 - classification_loss: 0.6249
 978/1000 [============================>.] - ETA: 9s - loss: 2.7059 - regression_loss: 2.0811 - classification_loss: 0.6247 
 979/1000 [============================>.] - ETA: 9s - loss: 2.7033 - regression_loss: 2.0790 - classification_loss: 0.6243
 980/1000 [============================>.] - ETA: 9s - loss: 2.7040 - regression_loss: 2.0798 - classification_loss: 0.6242
 981/1000 [============================>.] - ETA: 8s - loss: 2.7061 - regression_loss: 2.0810 - classification_loss: 0.6250
 982/1000 [============================>.] - ETA: 8s - loss: 2.7058 - regression_loss: 2.0812 - classification_loss: 0.6246
 983/1000 [============================>.] - ETA: 7s - loss: 2.7054 - regression_loss: 2.0811 - classification_loss: 0.6243
 984/1000 [============================>.] - ETA: 7s - loss: 2.7027 - regression_loss: 2.0790 - classification_loss: 0.6237
 985/1000 [============================>.] - ETA: 6s - loss: 2.7019 - regression_loss: 2.0786 - classification_loss: 0.6232
 986/1000 [============================>.] - ETA: 6s - loss: 2.6991 - regression_loss: 2.0765 - classification_loss: 0.6226
 987/1000 [============================>.] - ETA: 5s - loss: 2.6989 - regression_loss: 2.0762 - classification_loss: 0.6226
 988/1000 [============================>.] - ETA: 5s - loss: 2.6977 - regression_loss: 2.0755 - classification_loss: 0.6222
 989/1000 [============================>.] - ETA: 4s - loss: 2.6985 - regression_loss: 2.0765 - classification_loss: 0.6219
 990/1000 [============================>.] - ETA: 4s - loss: 2.6997 - regression_loss: 2.0773 - classification_loss: 0.6224
 991/1000 [============================>.] - ETA: 4s - loss: 2.6995 - regression_loss: 2.0775 - classification_loss: 0.6220
 992/1000 [============================>.] - ETA: 3s - loss: 2.6994 - regression_loss: 2.0777 - classification_loss: 0.6217
 993/1000 [============================>.] - ETA: 3s - loss: 2.7021 - regression_loss: 2.0797 - classification_loss: 0.6224
 994/1000 [============================>.] - ETA: 2s - loss: 2.7020 - regression_loss: 2.0795 - classification_loss: 0.6225
 995/1000 [============================>.] - ETA: 2s - loss: 2.7022 - regression_loss: 2.0798 - classification_loss: 0.6225
 996/1000 [============================>.] - ETA: 1s - loss: 2.7024 - regression_loss: 2.0800 - classification_loss: 0.6224
 997/1000 [============================>.] - ETA: 1s - loss: 2.6997 - regression_loss: 2.0779 - classification_loss: 0.6218
 998/1000 [============================>.] - ETA: 0s - loss: 2.7020 - regression_loss: 2.0793 - classification_loss: 0.6228
 999/1000 [============================>.] - ETA: 0s - loss: 2.7020 - regression_loss: 2.0794 - classification_loss: 0.6226
1000/1000 [==============================] - 452s 452ms/step - loss: 2.7015 - regression_loss: 2.0794 - classification_loss: 0.6221

Epoch 00020: saving model to ./snapshots/resnet50_csv_20.h5
0/2001/2002/2003/2004/2005/2006/2007/2008/2009/20010/20011/20012/20013/20014/20015/20016/20017/20018/20019/20020/20021/20022/20023/20024/20025/20026/20027/20028/20029/20030/20031/20032/20033/20034/20035/20036/20037/20038/20039/20040/20041/20042/20043/20044/20045/20046/20047/20048/20049/20050/20051/20052/20053/20054/20055/20056/20057/20058/20059/20060/20061/20062/20063/20064/20065/20066/20067/20068/20069/20070/20071/20072/20073/20074/20075/20076/20077/20078/20079/20080/20081/20082/20083/20084/20085/20086/20087/20088/20089/20090/20091/20092/20093/20094/20095/20096/20097/20098/20099/200100/200101/200102/200103/200104/200105/200106/200107/200108/200109/200110/200111/200112/200113/200114/200115/200116/200117/200118/200119/200120/200121/200122/200123/200124/200125/200126/200127/200128/200129/200130/200131/200132/200133/200134/200135/200136/200137/200138/200139/200140/200141/200142/200143/200144/200145/200146/200147/200148/200149/200150/200151/200152/200153/200154/200155/200156/200157/200158/200159/200160/200161/200162/200163/200164/200165/200166/200167/200168/200169/200170/200171/200172/200173/200174/200175/200176/200177/200178/200179/200180/200181/200182/200183/200184/200185/200186/200187/200188/200189/200190/200191/200192/200193/200194/200195/200196/200197/200198/200199/2000/2001/2002/2003/2004/2005/2006/2007/2008/2009/20010/20011/20012/20013/20014/20015/20016/20017/20018/20019/20020/20021/20022/20023/20024/20025/20026/20027/20028/20029/20030/20031/20032/20033/20034/20035/20036/20037/20038/20039/20040/20041/20042/20043/20044/20045/20046/20047/20048/20049/20050/20051/20052/20053/20054/20055/20056/20057/20058/20059/20060/20061/20062/20063/20064/20065/20066/20067/20068/20069/20070/20071/20072/20073/20074/20075/20076/20077/20078/20079/20080/20081/20082/20083/20084/20085/20086/20087/20088/20089/20090/20091/20092/20093/20094/20095/20096/20097/20098/20099/200100/200101/200102/200103/200104/200105/200106/200107/200108/200109/200110/200111/200112/200113/200114/200115/200116/200117/200118/200119/200120/200121/200122/200123/200124/200125/200126/200127/200128/200129/200130/200131/200132/200133/200134/200135/200136/200137/200138/200139/200140/200141/200142/200143/200144/200145/200146/200147/200148/200149/200150/200151/200152/200153/200154/200155/200156/200157/200158/200159/200160/200161/200162/200163/200164/200165/200166/200167/200168/200169/200170/200171/200172/200173/200174/200175/200176/200177/200178/200179/200180/200181/200182/200183/200184/200185/200186/200187/200188/200189/200190/200191/200192/200193/200194/200195/200196/200197/200198/200199/200

M 0.2312
N 0.0242
mAP: 0.1277
Epoch 21/30

   1/1000 [..............................] - ETA: 7:18 - loss: 4.1744 - regression_loss: 2.8483 - classification_loss: 1.3262
   2/1000 [..............................] - ETA: 7:24 - loss: 3.1761 - regression_loss: 2.3865 - classification_loss: 0.7896
   3/1000 [..............................] - ETA: 7:24 - loss: 2.1174 - regression_loss: 1.5910 - classification_loss: 0.5264
   4/1000 [..............................] - ETA: 7:27 - loss: 2.3220 - regression_loss: 1.7545 - classification_loss: 0.5675
   5/1000 [..............................] - ETA: 7:26 - loss: 2.3684 - regression_loss: 1.8547 - classification_loss: 0.5137
   6/1000 [..............................] - ETA: 7:20 - loss: 2.7778 - regression_loss: 2.0799 - classification_loss: 0.6978
   7/1000 [..............................] - ETA: 7:19 - loss: 3.1570 - regression_loss: 2.3576 - classification_loss: 0.7994
   8/1000 [..............................] - ETA: 7:20 - loss: 3.3792 - regression_loss: 2.5621 - classification_loss: 0.8171
   9/1000 [..............................] - ETA: 7:20 - loss: 3.2881 - regression_loss: 2.5098 - classification_loss: 0.7783
  10/1000 [..............................] - ETA: 7:21 - loss: 2.9593 - regression_loss: 2.2588 - classification_loss: 0.7005
  11/1000 [..............................] - ETA: 7:21 - loss: 2.9947 - regression_loss: 2.3062 - classification_loss: 0.6886
  12/1000 [..............................] - ETA: 7:21 - loss: 2.9512 - regression_loss: 2.2810 - classification_loss: 0.6702
  13/1000 [..............................] - ETA: 7:21 - loss: 3.0362 - regression_loss: 2.3691 - classification_loss: 0.6672
  14/1000 [..............................] - ETA: 7:21 - loss: 3.1385 - regression_loss: 2.4381 - classification_loss: 0.7004
  15/1000 [..............................] - ETA: 7:19 - loss: 3.1618 - regression_loss: 2.4638 - classification_loss: 0.6980
  16/1000 [..............................] - ETA: 7:18 - loss: 3.1676 - regression_loss: 2.4704 - classification_loss: 0.6972
  17/1000 [..............................] - ETA: 7:18 - loss: 3.1288 - regression_loss: 2.4418 - classification_loss: 0.6871
  18/1000 [..............................] - ETA: 7:18 - loss: 3.0837 - regression_loss: 2.4012 - classification_loss: 0.6825
  19/1000 [..............................] - ETA: 7:18 - loss: 3.1042 - regression_loss: 2.4194 - classification_loss: 0.6848
  20/1000 [..............................] - ETA: 7:17 - loss: 3.1282 - regression_loss: 2.4602 - classification_loss: 0.6680
  21/1000 [..............................] - ETA: 7:17 - loss: 3.0659 - regression_loss: 2.4222 - classification_loss: 0.6437
  22/1000 [..............................] - ETA: 7:17 - loss: 3.1172 - regression_loss: 2.4663 - classification_loss: 0.6510
  23/1000 [..............................] - ETA: 7:16 - loss: 2.9818 - regression_loss: 2.3590 - classification_loss: 0.6227
  24/1000 [..............................] - ETA: 7:16 - loss: 2.9951 - regression_loss: 2.3767 - classification_loss: 0.6183
  25/1000 [..............................] - ETA: 7:16 - loss: 2.9434 - regression_loss: 2.2817 - classification_loss: 0.6617
  26/1000 [..............................] - ETA: 7:16 - loss: 2.8303 - regression_loss: 2.1939 - classification_loss: 0.6364
  27/1000 [..............................] - ETA: 7:15 - loss: 2.8724 - regression_loss: 2.2298 - classification_loss: 0.6426
  28/1000 [..............................] - ETA: 7:15 - loss: 2.8755 - regression_loss: 2.2395 - classification_loss: 0.6360
  29/1000 [..............................] - ETA: 7:15 - loss: 2.8652 - regression_loss: 2.2397 - classification_loss: 0.6254
  30/1000 [..............................] - ETA: 7:14 - loss: 2.9157 - regression_loss: 2.2921 - classification_loss: 0.6236
  31/1000 [..............................] - ETA: 7:14 - loss: 2.9443 - regression_loss: 2.3218 - classification_loss: 0.6224
  32/1000 [..............................] - ETA: 7:14 - loss: 2.9301 - regression_loss: 2.3199 - classification_loss: 0.6103
  33/1000 [..............................] - ETA: 7:12 - loss: 2.9506 - regression_loss: 2.3347 - classification_loss: 0.6159
  34/1000 [>.............................] - ETA: 7:11 - loss: 2.8643 - regression_loss: 2.2661 - classification_loss: 0.5983
  35/1000 [>.............................] - ETA: 7:10 - loss: 2.8540 - regression_loss: 2.2472 - classification_loss: 0.6069
  36/1000 [>.............................] - ETA: 7:10 - loss: 2.8396 - regression_loss: 2.2386 - classification_loss: 0.6010
  37/1000 [>.............................] - ETA: 7:10 - loss: 2.8198 - regression_loss: 2.2247 - classification_loss: 0.5952
  38/1000 [>.............................] - ETA: 7:09 - loss: 2.8095 - regression_loss: 2.2139 - classification_loss: 0.5956
  39/1000 [>.............................] - ETA: 7:09 - loss: 2.7840 - regression_loss: 2.1951 - classification_loss: 0.5889
  40/1000 [>.............................] - ETA: 7:09 - loss: 2.7186 - regression_loss: 2.1402 - classification_loss: 0.5783
  41/1000 [>.............................] - ETA: 7:09 - loss: 2.7556 - regression_loss: 2.1564 - classification_loss: 0.5992
  42/1000 [>.............................] - ETA: 7:08 - loss: 2.7480 - regression_loss: 2.1554 - classification_loss: 0.5926
  43/1000 [>.............................] - ETA: 7:08 - loss: 2.7586 - regression_loss: 2.1618 - classification_loss: 0.5967
  44/1000 [>.............................] - ETA: 7:07 - loss: 2.6972 - regression_loss: 2.1127 - classification_loss: 0.5845
  45/1000 [>.............................] - ETA: 7:07 - loss: 2.6949 - regression_loss: 2.1168 - classification_loss: 0.5782
  46/1000 [>.............................] - ETA: 7:07 - loss: 2.7025 - regression_loss: 2.1245 - classification_loss: 0.5779
  47/1000 [>.............................] - ETA: 7:06 - loss: 2.6450 - regression_loss: 2.0793 - classification_loss: 0.5656
  48/1000 [>.............................] - ETA: 7:06 - loss: 2.6615 - regression_loss: 2.0877 - classification_loss: 0.5738
  49/1000 [>.............................] - ETA: 7:06 - loss: 2.6752 - regression_loss: 2.0951 - classification_loss: 0.5801
  50/1000 [>.............................] - ETA: 7:05 - loss: 2.6217 - regression_loss: 2.0532 - classification_loss: 0.5685
  51/1000 [>.............................] - ETA: 7:05 - loss: 2.6296 - regression_loss: 2.0620 - classification_loss: 0.5676
  52/1000 [>.............................] - ETA: 7:05 - loss: 2.6304 - regression_loss: 2.0632 - classification_loss: 0.5673
  53/1000 [>.............................] - ETA: 7:04 - loss: 2.6367 - regression_loss: 2.0704 - classification_loss: 0.5663
  54/1000 [>.............................] - ETA: 7:04 - loss: 2.5916 - regression_loss: 2.0320 - classification_loss: 0.5596
  55/1000 [>.............................] - ETA: 7:03 - loss: 2.5980 - regression_loss: 2.0402 - classification_loss: 0.5578
  56/1000 [>.............................] - ETA: 7:03 - loss: 2.6086 - regression_loss: 2.0532 - classification_loss: 0.5553
  57/1000 [>.............................] - ETA: 7:03 - loss: 2.6413 - regression_loss: 2.0905 - classification_loss: 0.5509
  58/1000 [>.............................] - ETA: 7:02 - loss: 2.6824 - regression_loss: 2.1182 - classification_loss: 0.5642
  59/1000 [>.............................] - ETA: 7:02 - loss: 2.7221 - regression_loss: 2.1408 - classification_loss: 0.5813
  60/1000 [>.............................] - ETA: 7:01 - loss: 2.7417 - regression_loss: 2.1589 - classification_loss: 0.5828
  61/1000 [>.............................] - ETA: 7:00 - loss: 2.7910 - regression_loss: 2.1868 - classification_loss: 0.6042
  62/1000 [>.............................] - ETA: 7:00 - loss: 2.7788 - regression_loss: 2.1804 - classification_loss: 0.5984
  63/1000 [>.............................] - ETA: 6:59 - loss: 2.8484 - regression_loss: 2.2243 - classification_loss: 0.6242
  64/1000 [>.............................] - ETA: 6:59 - loss: 2.8621 - regression_loss: 2.2398 - classification_loss: 0.6223
  65/1000 [>.............................] - ETA: 6:59 - loss: 2.8738 - regression_loss: 2.2533 - classification_loss: 0.6205
  66/1000 [>.............................] - ETA: 6:58 - loss: 2.8860 - regression_loss: 2.2563 - classification_loss: 0.6297
  67/1000 [=>............................] - ETA: 6:58 - loss: 2.8759 - regression_loss: 2.2503 - classification_loss: 0.6257
  68/1000 [=>............................] - ETA: 6:58 - loss: 2.8337 - regression_loss: 2.2172 - classification_loss: 0.6165
  69/1000 [=>............................] - ETA: 6:57 - loss: 2.8418 - regression_loss: 2.2280 - classification_loss: 0.6138
  70/1000 [=>............................] - ETA: 6:57 - loss: 2.8318 - regression_loss: 2.2241 - classification_loss: 0.6076
  71/1000 [=>............................] - ETA: 6:56 - loss: 2.7919 - regression_loss: 2.1928 - classification_loss: 0.5991
  72/1000 [=>............................] - ETA: 6:56 - loss: 2.7882 - regression_loss: 2.1900 - classification_loss: 0.5981
  73/1000 [=>............................] - ETA: 6:56 - loss: 2.7949 - regression_loss: 2.1995 - classification_loss: 0.5954
  74/1000 [=>............................] - ETA: 6:55 - loss: 2.7968 - regression_loss: 2.2006 - classification_loss: 0.5962
  75/1000 [=>............................] - ETA: 6:55 - loss: 2.8040 - regression_loss: 2.2078 - classification_loss: 0.5962
  76/1000 [=>............................] - ETA: 6:55 - loss: 2.7672 - regression_loss: 2.1787 - classification_loss: 0.5885
  77/1000 [=>............................] - ETA: 6:54 - loss: 2.7313 - regression_loss: 2.1505 - classification_loss: 0.5809
  78/1000 [=>............................] - ETA: 6:54 - loss: 2.7015 - regression_loss: 2.1229 - classification_loss: 0.5786
  79/1000 [=>............................] - ETA: 6:54 - loss: 2.6826 - regression_loss: 2.0960 - classification_loss: 0.5866
  80/1000 [=>............................] - ETA: 6:53 - loss: 2.6954 - regression_loss: 2.1085 - classification_loss: 0.5869
  81/1000 [=>............................] - ETA: 6:53 - loss: 2.7353 - regression_loss: 2.1330 - classification_loss: 0.6024
  82/1000 [=>............................] - ETA: 6:52 - loss: 2.7198 - regression_loss: 2.1210 - classification_loss: 0.5988
  83/1000 [=>............................] - ETA: 6:52 - loss: 2.7248 - regression_loss: 2.1243 - classification_loss: 0.6004
  84/1000 [=>............................] - ETA: 6:51 - loss: 2.7281 - regression_loss: 2.1295 - classification_loss: 0.5985
  85/1000 [=>............................] - ETA: 6:51 - loss: 2.7352 - regression_loss: 2.1362 - classification_loss: 0.5989
  86/1000 [=>............................] - ETA: 6:51 - loss: 2.7034 - regression_loss: 2.1114 - classification_loss: 0.5920